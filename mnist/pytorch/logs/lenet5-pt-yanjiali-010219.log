----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 6, 28, 28]             156
         AvgPool2d-2            [-1, 6, 14, 14]               0
           Sigmoid-3            [-1, 6, 14, 14]               0
            Conv2d-4           [-1, 16, 10, 10]           2,416
         AvgPool2d-5             [-1, 16, 5, 5]               0
           Sigmoid-6             [-1, 16, 5, 5]               0
            Linear-7                  [-1, 120]          48,120
              Tanh-8                  [-1, 120]               0
            Linear-9                   [-1, 84]          10,164
             Tanh-10                   [-1, 84]               0
           Linear-11                   [-1, 10]             850
================================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.08
Params size (MB): 0.24
Estimated Total Size (MB): 0.31
----------------------------------------------------------------
Epoch: 0, Validation Top 1 acc: 6.926751613616943
Epoch: 0, Validation Top 5 acc: 50.28861618041992
Epoch: 0, Validation Set Loss: 2.3059606552124023
Start training epoch 1
Time, 2019-01-01T21:58:45, Epoch: 1, Batch: 10, Training Loss: 2.324077582359314, LR: 0.1
Time, 2019-01-01T21:58:46, Epoch: 1, Batch: 20, Training Loss: 2.3044925689697267, LR: 0.1
Time, 2019-01-01T21:58:47, Epoch: 1, Batch: 30, Training Loss: 2.298307681083679, LR: 0.1
Time, 2019-01-01T21:58:47, Epoch: 1, Batch: 40, Training Loss: 2.253557014465332, LR: 0.1
Time, 2019-01-01T21:58:48, Epoch: 1, Batch: 50, Training Loss: 1.9834200263023376, LR: 0.1
Time, 2019-01-01T21:58:49, Epoch: 1, Batch: 60, Training Loss: 1.5843263626098634, LR: 0.1
Time, 2019-01-01T21:58:50, Epoch: 1, Batch: 70, Training Loss: 1.0641149044036866, LR: 0.1
Time, 2019-01-01T21:58:50, Epoch: 1, Batch: 80, Training Loss: 0.9223842561244965, LR: 0.1
Time, 2019-01-01T21:58:51, Epoch: 1, Batch: 90, Training Loss: 0.8770721733570099, LR: 0.1
Time, 2019-01-01T21:58:52, Epoch: 1, Batch: 100, Training Loss: 0.7424644768238068, LR: 0.1
Time, 2019-01-01T21:58:53, Epoch: 1, Batch: 110, Training Loss: 0.6946640491485596, LR: 0.1
Time, 2019-01-01T21:58:54, Epoch: 1, Batch: 120, Training Loss: 0.5348972290754318, LR: 0.1
Time, 2019-01-01T21:58:54, Epoch: 1, Batch: 130, Training Loss: 0.5783073902130127, LR: 0.1
Time, 2019-01-01T21:58:55, Epoch: 1, Batch: 140, Training Loss: 0.515510618686676, LR: 0.1
Time, 2019-01-01T21:58:56, Epoch: 1, Batch: 150, Training Loss: 0.5890560656785965, LR: 0.1
Time, 2019-01-01T21:58:57, Epoch: 1, Batch: 160, Training Loss: 0.5159587919712066, LR: 0.1
Time, 2019-01-01T21:58:57, Epoch: 1, Batch: 170, Training Loss: 0.45718511044979093, LR: 0.1
Time, 2019-01-01T21:58:58, Epoch: 1, Batch: 180, Training Loss: 0.4871635645627975, LR: 0.1
Time, 2019-01-01T21:58:59, Epoch: 1, Batch: 190, Training Loss: 0.4945403218269348, LR: 0.1
Time, 2019-01-01T21:59:00, Epoch: 1, Batch: 200, Training Loss: 0.4776930004358292, LR: 0.1
Time, 2019-01-01T21:59:00, Epoch: 1, Batch: 210, Training Loss: 0.454089093208313, LR: 0.1
Time, 2019-01-01T21:59:01, Epoch: 1, Batch: 220, Training Loss: 0.38262736797332764, LR: 0.1
Time, 2019-01-01T21:59:02, Epoch: 1, Batch: 230, Training Loss: 0.38790874779224394, LR: 0.1
Time, 2019-01-01T21:59:03, Epoch: 1, Batch: 240, Training Loss: 0.31378981471061707, LR: 0.1
Time, 2019-01-01T21:59:03, Epoch: 1, Batch: 250, Training Loss: 0.3730856001377106, LR: 0.1
Time, 2019-01-01T21:59:04, Epoch: 1, Batch: 260, Training Loss: 0.33388713747262955, LR: 0.1
Time, 2019-01-01T21:59:05, Epoch: 1, Batch: 270, Training Loss: 0.3313697025179863, LR: 0.1
Time, 2019-01-01T21:59:06, Epoch: 1, Batch: 280, Training Loss: 0.29756744056940077, LR: 0.1
Time, 2019-01-01T21:59:06, Epoch: 1, Batch: 290, Training Loss: 0.3664329268038273, LR: 0.1
Time, 2019-01-01T21:59:07, Epoch: 1, Batch: 300, Training Loss: 0.31373020708560945, LR: 0.1
Time, 2019-01-01T21:59:08, Epoch: 1, Batch: 310, Training Loss: 0.32363663166761397, LR: 0.1
Time, 2019-01-01T21:59:08, Epoch: 1, Batch: 320, Training Loss: 0.29645670354366305, LR: 0.1
Time, 2019-01-01T21:59:09, Epoch: 1, Batch: 330, Training Loss: 0.29077292084693906, LR: 0.1
Time, 2019-01-01T21:59:10, Epoch: 1, Batch: 340, Training Loss: 0.24551713168621064, LR: 0.1
Time, 2019-01-01T21:59:11, Epoch: 1, Batch: 350, Training Loss: 0.21981947869062424, LR: 0.1
Time, 2019-01-01T21:59:12, Epoch: 1, Batch: 360, Training Loss: 0.24492970034480094, LR: 0.1
Time, 2019-01-01T21:59:12, Epoch: 1, Batch: 370, Training Loss: 0.2921483889222145, LR: 0.1
Time, 2019-01-01T21:59:13, Epoch: 1, Batch: 380, Training Loss: 0.2986536487936974, LR: 0.1
Time, 2019-01-01T21:59:14, Epoch: 1, Batch: 390, Training Loss: 0.2682990714907646, LR: 0.1
Time, 2019-01-01T21:59:15, Epoch: 1, Batch: 400, Training Loss: 0.28128347992897035, LR: 0.1
Time, 2019-01-01T21:59:16, Epoch: 1, Batch: 410, Training Loss: 0.3059164881706238, LR: 0.1
Time, 2019-01-01T21:59:16, Epoch: 1, Batch: 420, Training Loss: 0.2897427469491959, LR: 0.1
Time, 2019-01-01T21:59:17, Epoch: 1, Batch: 430, Training Loss: 0.24862949624657632, LR: 0.1
Time, 2019-01-01T21:59:18, Epoch: 1, Batch: 440, Training Loss: 0.30784410089254377, LR: 0.1
Time, 2019-01-01T21:59:19, Epoch: 1, Batch: 450, Training Loss: 0.23443858623504638, LR: 0.1
Time, 2019-01-01T21:59:20, Epoch: 1, Batch: 460, Training Loss: 0.24676198735833169, LR: 0.1
Time, 2019-01-01T21:59:20, Epoch: 1, Batch: 470, Training Loss: 0.31614103764295576, LR: 0.1
Time, 2019-01-01T21:59:21, Epoch: 1, Batch: 480, Training Loss: 0.32686776369810105, LR: 0.1
Time, 2019-01-01T21:59:22, Epoch: 1, Batch: 490, Training Loss: 0.3295023262500763, LR: 0.1
Time, 2019-01-01T21:59:23, Epoch: 1, Batch: 500, Training Loss: 0.30727053955197337, LR: 0.1
Time, 2019-01-01T21:59:23, Epoch: 1, Batch: 510, Training Loss: 0.25369604378938676, LR: 0.1
Time, 2019-01-01T21:59:24, Epoch: 1, Batch: 520, Training Loss: 0.2539853759109974, LR: 0.1
Time, 2019-01-01T21:59:25, Epoch: 1, Batch: 530, Training Loss: 0.23732186555862428, LR: 0.1
Time, 2019-01-01T21:59:26, Epoch: 1, Batch: 540, Training Loss: 0.2629176199436188, LR: 0.1
Time, 2019-01-01T21:59:26, Epoch: 1, Batch: 550, Training Loss: 0.215299591422081, LR: 0.1
Time, 2019-01-01T21:59:27, Epoch: 1, Batch: 560, Training Loss: 0.20564327985048295, LR: 0.1
Time, 2019-01-01T21:59:28, Epoch: 1, Batch: 570, Training Loss: 0.24753569960594177, LR: 0.1
Time, 2019-01-01T21:59:29, Epoch: 1, Batch: 580, Training Loss: 0.2948125772178173, LR: 0.1
Time, 2019-01-01T21:59:30, Epoch: 1, Batch: 590, Training Loss: 0.2638204723596573, LR: 0.1
Time, 2019-01-01T21:59:31, Epoch: 1, Batch: 600, Training Loss: 0.18700962290167808, LR: 0.1
Time, 2019-01-01T21:59:32, Epoch: 1, Batch: 610, Training Loss: 0.24276960417628288, LR: 0.1
Time, 2019-01-01T21:59:32, Epoch: 1, Batch: 620, Training Loss: 0.28603409640491007, LR: 0.1
Time, 2019-01-01T21:59:33, Epoch: 1, Batch: 630, Training Loss: 0.27891873121261596, LR: 0.1
Time, 2019-01-01T21:59:34, Epoch: 1, Batch: 640, Training Loss: 0.32711683958768845, LR: 0.1
Time, 2019-01-01T21:59:34, Epoch: 1, Batch: 650, Training Loss: 0.29923486411571504, LR: 0.1
Time, 2019-01-01T21:59:35, Epoch: 1, Batch: 660, Training Loss: 0.29455197900533675, LR: 0.1
Time, 2019-01-01T21:59:35, Epoch: 1, Batch: 670, Training Loss: 0.21942718774080278, LR: 0.1
Time, 2019-01-01T21:59:36, Epoch: 1, Batch: 680, Training Loss: 0.2805111348628998, LR: 0.1
Time, 2019-01-01T21:59:36, Epoch: 1, Batch: 690, Training Loss: 0.25364178866147996, LR: 0.1
Time, 2019-01-01T21:59:37, Epoch: 1, Batch: 700, Training Loss: 0.21904541105031966, LR: 0.1
Time, 2019-01-01T21:59:37, Epoch: 1, Batch: 710, Training Loss: 0.2591405019164085, LR: 0.1
Time, 2019-01-01T21:59:38, Epoch: 1, Batch: 720, Training Loss: 0.1800515502691269, LR: 0.1
Time, 2019-01-01T21:59:39, Epoch: 1, Batch: 730, Training Loss: 0.24550998359918594, LR: 0.1
Time, 2019-01-01T21:59:39, Epoch: 1, Batch: 740, Training Loss: 0.24704635962843896, LR: 0.1
Time, 2019-01-01T21:59:40, Epoch: 1, Batch: 750, Training Loss: 0.18665383905172347, LR: 0.1
Time, 2019-01-01T21:59:41, Epoch: 1, Batch: 760, Training Loss: 0.31882616952061654, LR: 0.1
Time, 2019-01-01T21:59:41, Epoch: 1, Batch: 770, Training Loss: 0.25784505605697633, LR: 0.1
Time, 2019-01-01T21:59:42, Epoch: 1, Batch: 780, Training Loss: 0.34169519543647764, LR: 0.1
Time, 2019-01-01T21:59:42, Epoch: 1, Batch: 790, Training Loss: 0.3166138917207718, LR: 0.1
Time, 2019-01-01T21:59:43, Epoch: 1, Batch: 800, Training Loss: 0.3184425741434097, LR: 0.1
Time, 2019-01-01T21:59:44, Epoch: 1, Batch: 810, Training Loss: 0.304849598556757, LR: 0.1
Time, 2019-01-01T21:59:44, Epoch: 1, Batch: 820, Training Loss: 0.2917677827179432, LR: 0.1
Time, 2019-01-01T21:59:45, Epoch: 1, Batch: 830, Training Loss: 0.26235816776752474, LR: 0.1
Time, 2019-01-01T21:59:45, Epoch: 1, Batch: 840, Training Loss: 0.31391352564096453, LR: 0.1
Time, 2019-01-01T21:59:46, Epoch: 1, Batch: 850, Training Loss: 0.24190227761864663, LR: 0.1
Time, 2019-01-01T21:59:47, Epoch: 1, Batch: 860, Training Loss: 0.30323380678892137, LR: 0.1
Time, 2019-01-01T21:59:47, Epoch: 1, Batch: 870, Training Loss: 0.34055009931325914, LR: 0.1
Time, 2019-01-01T21:59:48, Epoch: 1, Batch: 880, Training Loss: 0.2683772578835487, LR: 0.1
Time, 2019-01-01T21:59:49, Epoch: 1, Batch: 890, Training Loss: 0.2853387862443924, LR: 0.1
Time, 2019-01-01T21:59:50, Epoch: 1, Batch: 900, Training Loss: 0.212654722481966, LR: 0.1
Time, 2019-01-01T21:59:50, Epoch: 1, Batch: 910, Training Loss: 0.15466054379940034, LR: 0.1
Time, 2019-01-01T21:59:51, Epoch: 1, Batch: 920, Training Loss: 0.23061335757374762, LR: 0.1
Time, 2019-01-01T21:59:52, Epoch: 1, Batch: 930, Training Loss: 0.23043027445673941, LR: 0.1
Epoch: 1, Validation Top 1 acc: 94.56608581542969
Epoch: 1, Validation Top 5 acc: 99.8905258178711
Epoch: 1, Validation Set Loss: 0.16770602762699127
Start training epoch 2
Time, 2019-01-01T21:59:57, Epoch: 2, Batch: 10, Training Loss: 0.17642319425940514, LR: 0.1
Time, 2019-01-01T21:59:58, Epoch: 2, Batch: 20, Training Loss: 0.20721958726644515, LR: 0.1
Time, 2019-01-01T21:59:59, Epoch: 2, Batch: 30, Training Loss: 0.2557941645383835, LR: 0.1
Time, 2019-01-01T21:59:59, Epoch: 2, Batch: 40, Training Loss: 0.22526803091168404, LR: 0.1
Time, 2019-01-01T22:00:00, Epoch: 2, Batch: 50, Training Loss: 0.18846608251333236, LR: 0.1
Time, 2019-01-01T22:00:01, Epoch: 2, Batch: 60, Training Loss: 0.19848618358373643, LR: 0.1
Time, 2019-01-01T22:00:01, Epoch: 2, Batch: 70, Training Loss: 0.15003578700125217, LR: 0.1
Time, 2019-01-01T22:00:02, Epoch: 2, Batch: 80, Training Loss: 0.18009512796998023, LR: 0.1
Time, 2019-01-01T22:00:03, Epoch: 2, Batch: 90, Training Loss: 0.2401939384639263, LR: 0.1
Time, 2019-01-01T22:00:04, Epoch: 2, Batch: 100, Training Loss: 0.14102549850940704, LR: 0.1
Time, 2019-01-01T22:00:04, Epoch: 2, Batch: 110, Training Loss: 0.17584616243839263, LR: 0.1
Time, 2019-01-01T22:00:05, Epoch: 2, Batch: 120, Training Loss: 0.2278255619108677, LR: 0.1
Time, 2019-01-01T22:00:06, Epoch: 2, Batch: 130, Training Loss: 0.18071722835302353, LR: 0.1
Time, 2019-01-01T22:00:07, Epoch: 2, Batch: 140, Training Loss: 0.17204542681574822, LR: 0.1
Time, 2019-01-01T22:00:07, Epoch: 2, Batch: 150, Training Loss: 0.18486712053418158, LR: 0.1
Time, 2019-01-01T22:00:08, Epoch: 2, Batch: 160, Training Loss: 0.13367655053734778, LR: 0.1
Time, 2019-01-01T22:00:09, Epoch: 2, Batch: 170, Training Loss: 0.1898192971944809, LR: 0.1
Time, 2019-01-01T22:00:09, Epoch: 2, Batch: 180, Training Loss: 0.18771983981132506, LR: 0.1
Time, 2019-01-01T22:00:10, Epoch: 2, Batch: 190, Training Loss: 0.1737990729510784, LR: 0.1
Time, 2019-01-01T22:00:11, Epoch: 2, Batch: 200, Training Loss: 0.17381170466542245, LR: 0.1
Time, 2019-01-01T22:00:12, Epoch: 2, Batch: 210, Training Loss: 0.1495511181652546, LR: 0.1
Time, 2019-01-01T22:00:12, Epoch: 2, Batch: 220, Training Loss: 0.16318263337016106, LR: 0.1
Time, 2019-01-01T22:00:13, Epoch: 2, Batch: 230, Training Loss: 0.23297141194343568, LR: 0.1
Time, 2019-01-01T22:00:14, Epoch: 2, Batch: 240, Training Loss: 0.15023077838122845, LR: 0.1
Time, 2019-01-01T22:00:15, Epoch: 2, Batch: 250, Training Loss: 0.16587975472211838, LR: 0.1
Time, 2019-01-01T22:00:16, Epoch: 2, Batch: 260, Training Loss: 0.19679228216409683, LR: 0.1
Time, 2019-01-01T22:00:16, Epoch: 2, Batch: 270, Training Loss: 0.1643973968923092, LR: 0.1
Time, 2019-01-01T22:00:17, Epoch: 2, Batch: 280, Training Loss: 0.17322640120983124, LR: 0.1
Time, 2019-01-01T22:00:18, Epoch: 2, Batch: 290, Training Loss: 0.206374604254961, LR: 0.1
Time, 2019-01-01T22:00:19, Epoch: 2, Batch: 300, Training Loss: 0.21811311244964598, LR: 0.1
Time, 2019-01-01T22:00:19, Epoch: 2, Batch: 310, Training Loss: 0.18732462897896768, LR: 0.1
Time, 2019-01-01T22:00:20, Epoch: 2, Batch: 320, Training Loss: 0.23879125192761422, LR: 0.1
Time, 2019-01-01T22:00:21, Epoch: 2, Batch: 330, Training Loss: 0.21322409212589263, LR: 0.1
Time, 2019-01-01T22:00:22, Epoch: 2, Batch: 340, Training Loss: 0.20049236714839935, LR: 0.1
Time, 2019-01-01T22:00:22, Epoch: 2, Batch: 350, Training Loss: 0.2490556500852108, LR: 0.1
Time, 2019-01-01T22:00:23, Epoch: 2, Batch: 360, Training Loss: 0.2703594364225864, LR: 0.1
Time, 2019-01-01T22:00:24, Epoch: 2, Batch: 370, Training Loss: 0.22185570672154425, LR: 0.1
Time, 2019-01-01T22:00:24, Epoch: 2, Batch: 380, Training Loss: 0.22849727123975755, LR: 0.1
Time, 2019-01-01T22:00:25, Epoch: 2, Batch: 390, Training Loss: 0.21299003735184668, LR: 0.1
Time, 2019-01-01T22:00:26, Epoch: 2, Batch: 400, Training Loss: 0.2090868055820465, LR: 0.1
Time, 2019-01-01T22:00:27, Epoch: 2, Batch: 410, Training Loss: 0.21302189752459527, LR: 0.1
Time, 2019-01-01T22:00:27, Epoch: 2, Batch: 420, Training Loss: 0.1805737093091011, LR: 0.1
Time, 2019-01-01T22:00:28, Epoch: 2, Batch: 430, Training Loss: 0.1909789875149727, LR: 0.1
Time, 2019-01-01T22:00:29, Epoch: 2, Batch: 440, Training Loss: 0.2310950428247452, LR: 0.1
Time, 2019-01-01T22:00:29, Epoch: 2, Batch: 450, Training Loss: 0.2231570228934288, LR: 0.1
Time, 2019-01-01T22:00:30, Epoch: 2, Batch: 460, Training Loss: 0.210718335211277, LR: 0.1
Time, 2019-01-01T22:00:31, Epoch: 2, Batch: 470, Training Loss: 0.16989900544285774, LR: 0.1
Time, 2019-01-01T22:00:31, Epoch: 2, Batch: 480, Training Loss: 0.2570171445608139, LR: 0.1
Time, 2019-01-01T22:00:32, Epoch: 2, Batch: 490, Training Loss: 0.22348102182149887, LR: 0.1
Time, 2019-01-01T22:00:33, Epoch: 2, Batch: 500, Training Loss: 0.208133065700531, LR: 0.1
Time, 2019-01-01T22:00:33, Epoch: 2, Batch: 510, Training Loss: 0.1234903208911419, LR: 0.1
Time, 2019-01-01T22:00:34, Epoch: 2, Batch: 520, Training Loss: 0.19844959303736687, LR: 0.1
Time, 2019-01-01T22:00:35, Epoch: 2, Batch: 530, Training Loss: 0.18733439445495606, LR: 0.1
Time, 2019-01-01T22:00:35, Epoch: 2, Batch: 540, Training Loss: 0.20226023048162461, LR: 0.1
Time, 2019-01-01T22:00:36, Epoch: 2, Batch: 550, Training Loss: 0.18894241005182266, LR: 0.1
Time, 2019-01-01T22:00:37, Epoch: 2, Batch: 560, Training Loss: 0.17628739550709724, LR: 0.1
Time, 2019-01-01T22:00:37, Epoch: 2, Batch: 570, Training Loss: 0.17916702032089232, LR: 0.1
Time, 2019-01-01T22:00:38, Epoch: 2, Batch: 580, Training Loss: 0.199722333997488, LR: 0.1
Time, 2019-01-01T22:00:39, Epoch: 2, Batch: 590, Training Loss: 0.1564658150076866, LR: 0.1
Time, 2019-01-01T22:00:39, Epoch: 2, Batch: 600, Training Loss: 0.19546756222844125, LR: 0.1
Time, 2019-01-01T22:00:40, Epoch: 2, Batch: 610, Training Loss: 0.2244991049170494, LR: 0.1
Time, 2019-01-01T22:00:41, Epoch: 2, Batch: 620, Training Loss: 0.2445369094610214, LR: 0.1
Time, 2019-01-01T22:00:41, Epoch: 2, Batch: 630, Training Loss: 0.13309801667928695, LR: 0.1
Time, 2019-01-01T22:00:42, Epoch: 2, Batch: 640, Training Loss: 0.20743028521537782, LR: 0.1
Time, 2019-01-01T22:00:43, Epoch: 2, Batch: 650, Training Loss: 0.16451500356197357, LR: 0.1
Time, 2019-01-01T22:00:43, Epoch: 2, Batch: 660, Training Loss: 0.2602944828569889, LR: 0.1
Time, 2019-01-01T22:00:44, Epoch: 2, Batch: 670, Training Loss: 0.2496782772243023, LR: 0.1
Time, 2019-01-01T22:00:44, Epoch: 2, Batch: 680, Training Loss: 0.21592001616954803, LR: 0.1
Time, 2019-01-01T22:00:45, Epoch: 2, Batch: 690, Training Loss: 0.20530989691615104, LR: 0.1
Time, 2019-01-01T22:00:46, Epoch: 2, Batch: 700, Training Loss: 0.19689808115363122, LR: 0.1
Time, 2019-01-01T22:00:46, Epoch: 2, Batch: 710, Training Loss: 0.22234799340367317, LR: 0.1
Time, 2019-01-01T22:00:47, Epoch: 2, Batch: 720, Training Loss: 0.22103694900870324, LR: 0.1
Time, 2019-01-01T22:00:48, Epoch: 2, Batch: 730, Training Loss: 0.15984933413565158, LR: 0.1
Time, 2019-01-01T22:00:48, Epoch: 2, Batch: 740, Training Loss: 0.16814401000738144, LR: 0.1
Time, 2019-01-01T22:00:49, Epoch: 2, Batch: 750, Training Loss: 0.2017465613782406, LR: 0.1
Time, 2019-01-01T22:00:50, Epoch: 2, Batch: 760, Training Loss: 0.1800020232796669, LR: 0.1
Time, 2019-01-01T22:00:50, Epoch: 2, Batch: 770, Training Loss: 0.1536768488585949, LR: 0.1
Time, 2019-01-01T22:00:51, Epoch: 2, Batch: 780, Training Loss: 0.17234923020005227, LR: 0.1
Time, 2019-01-01T22:00:52, Epoch: 2, Batch: 790, Training Loss: 0.18337368741631507, LR: 0.1
Time, 2019-01-01T22:00:52, Epoch: 2, Batch: 800, Training Loss: 0.21469815000891684, LR: 0.1
Time, 2019-01-01T22:00:53, Epoch: 2, Batch: 810, Training Loss: 0.26077345162630083, LR: 0.1
Time, 2019-01-01T22:00:54, Epoch: 2, Batch: 820, Training Loss: 0.22880590781569482, LR: 0.1
Time, 2019-01-01T22:00:54, Epoch: 2, Batch: 830, Training Loss: 0.24121859595179557, LR: 0.1
Time, 2019-01-01T22:00:55, Epoch: 2, Batch: 840, Training Loss: 0.19644835367798805, LR: 0.1
Time, 2019-01-01T22:00:56, Epoch: 2, Batch: 850, Training Loss: 0.16787911877036094, LR: 0.1
Time, 2019-01-01T22:00:56, Epoch: 2, Batch: 860, Training Loss: 0.19886003583669662, LR: 0.1
Time, 2019-01-01T22:00:57, Epoch: 2, Batch: 870, Training Loss: 0.1998286098241806, LR: 0.1
Time, 2019-01-01T22:00:58, Epoch: 2, Batch: 880, Training Loss: 0.24164209961891175, LR: 0.1
Time, 2019-01-01T22:00:58, Epoch: 2, Batch: 890, Training Loss: 0.25299516022205354, LR: 0.1
Time, 2019-01-01T22:00:59, Epoch: 2, Batch: 900, Training Loss: 0.2191816247999668, LR: 0.1
Time, 2019-01-01T22:01:00, Epoch: 2, Batch: 910, Training Loss: 0.31218637377023695, LR: 0.1
Time, 2019-01-01T22:01:00, Epoch: 2, Batch: 920, Training Loss: 0.20229070782661437, LR: 0.1
Time, 2019-01-01T22:01:01, Epoch: 2, Batch: 930, Training Loss: 0.20955198332667352, LR: 0.1
Epoch: 2, Validation Top 1 acc: 92.78463745117188
Epoch: 2, Validation Top 5 acc: 99.86067199707031
Epoch: 2, Validation Set Loss: 0.21907393634319305
Start training epoch 3
Time, 2019-01-01T22:01:07, Epoch: 3, Batch: 10, Training Loss: 0.15703434497117996, LR: 0.1
Time, 2019-01-01T22:01:07, Epoch: 3, Batch: 20, Training Loss: 0.25207128599286077, LR: 0.1
Time, 2019-01-01T22:01:08, Epoch: 3, Batch: 30, Training Loss: 0.24016135632991792, LR: 0.1
Time, 2019-01-01T22:01:09, Epoch: 3, Batch: 40, Training Loss: 0.2109738379716873, LR: 0.1
Time, 2019-01-01T22:01:09, Epoch: 3, Batch: 50, Training Loss: 0.2102523148059845, LR: 0.1
Time, 2019-01-01T22:01:10, Epoch: 3, Batch: 60, Training Loss: 0.1902471236884594, LR: 0.1
Time, 2019-01-01T22:01:11, Epoch: 3, Batch: 70, Training Loss: 0.19197502061724664, LR: 0.1
Time, 2019-01-01T22:01:11, Epoch: 3, Batch: 80, Training Loss: 0.24493211209774018, LR: 0.1
Time, 2019-01-01T22:01:12, Epoch: 3, Batch: 90, Training Loss: 0.16093452088534832, LR: 0.1
Time, 2019-01-01T22:01:13, Epoch: 3, Batch: 100, Training Loss: 0.1721729725599289, LR: 0.1
Time, 2019-01-01T22:01:14, Epoch: 3, Batch: 110, Training Loss: 0.15528652034699916, LR: 0.1
Time, 2019-01-01T22:01:14, Epoch: 3, Batch: 120, Training Loss: 0.16270090863108636, LR: 0.1
Time, 2019-01-01T22:01:15, Epoch: 3, Batch: 130, Training Loss: 0.19975570291280748, LR: 0.1
Time, 2019-01-01T22:01:16, Epoch: 3, Batch: 140, Training Loss: 0.2252451255917549, LR: 0.1
Time, 2019-01-01T22:01:16, Epoch: 3, Batch: 150, Training Loss: 0.18056938275694848, LR: 0.1
Time, 2019-01-01T22:01:17, Epoch: 3, Batch: 160, Training Loss: 0.1571621388196945, LR: 0.1
Time, 2019-01-01T22:01:18, Epoch: 3, Batch: 170, Training Loss: 0.1786437951028347, LR: 0.1
Time, 2019-01-01T22:01:19, Epoch: 3, Batch: 180, Training Loss: 0.19312333539128304, LR: 0.1
Time, 2019-01-01T22:01:19, Epoch: 3, Batch: 190, Training Loss: 0.14536824747920035, LR: 0.1
Time, 2019-01-01T22:01:20, Epoch: 3, Batch: 200, Training Loss: 0.2077119380235672, LR: 0.1
Time, 2019-01-01T22:01:21, Epoch: 3, Batch: 210, Training Loss: 0.14024023860692977, LR: 0.1
Time, 2019-01-01T22:01:22, Epoch: 3, Batch: 220, Training Loss: 0.2173130191862583, LR: 0.1
Time, 2019-01-01T22:01:22, Epoch: 3, Batch: 230, Training Loss: 0.1349896304309368, LR: 0.1
Time, 2019-01-01T22:01:23, Epoch: 3, Batch: 240, Training Loss: 0.2349092736840248, LR: 0.1
Time, 2019-01-01T22:01:24, Epoch: 3, Batch: 250, Training Loss: 0.17038228437304498, LR: 0.1
Time, 2019-01-01T22:01:25, Epoch: 3, Batch: 260, Training Loss: 0.1437399722635746, LR: 0.1
Time, 2019-01-01T22:01:26, Epoch: 3, Batch: 270, Training Loss: 0.1898874744772911, LR: 0.1
Time, 2019-01-01T22:01:27, Epoch: 3, Batch: 280, Training Loss: 0.19160927571356295, LR: 0.1
Time, 2019-01-01T22:01:28, Epoch: 3, Batch: 290, Training Loss: 0.15496519207954407, LR: 0.1
Time, 2019-01-01T22:01:29, Epoch: 3, Batch: 300, Training Loss: 0.1571657419204712, LR: 0.1
Time, 2019-01-01T22:01:30, Epoch: 3, Batch: 310, Training Loss: 0.12665989995002747, LR: 0.1
Time, 2019-01-01T22:01:32, Epoch: 3, Batch: 320, Training Loss: 0.1749471440911293, LR: 0.1
Time, 2019-01-01T22:01:34, Epoch: 3, Batch: 330, Training Loss: 0.19177149161696433, LR: 0.1
Time, 2019-01-01T22:01:35, Epoch: 3, Batch: 340, Training Loss: 0.25368115603923796, LR: 0.1
Time, 2019-01-01T22:01:36, Epoch: 3, Batch: 350, Training Loss: 0.17122727930545806, LR: 0.1
Time, 2019-01-01T22:01:36, Epoch: 3, Batch: 360, Training Loss: 0.16922232136130333, LR: 0.1
Time, 2019-01-01T22:01:37, Epoch: 3, Batch: 370, Training Loss: 0.16071999333798886, LR: 0.1
Time, 2019-01-01T22:01:38, Epoch: 3, Batch: 380, Training Loss: 0.15392080023884774, LR: 0.1
Time, 2019-01-01T22:01:39, Epoch: 3, Batch: 390, Training Loss: 0.13867554366588591, LR: 0.1
Time, 2019-01-01T22:01:39, Epoch: 3, Batch: 400, Training Loss: 0.15446023270487785, LR: 0.1
Time, 2019-01-01T22:01:40, Epoch: 3, Batch: 410, Training Loss: 0.14562880024313926, LR: 0.1
Time, 2019-01-01T22:01:41, Epoch: 3, Batch: 420, Training Loss: 0.12332015708088875, LR: 0.1
Time, 2019-01-01T22:01:41, Epoch: 3, Batch: 430, Training Loss: 0.11610076949000359, LR: 0.1
Time, 2019-01-01T22:01:42, Epoch: 3, Batch: 440, Training Loss: 0.17753297612071037, LR: 0.1
Time, 2019-01-01T22:01:43, Epoch: 3, Batch: 450, Training Loss: 0.15927969589829444, LR: 0.1
Time, 2019-01-01T22:01:44, Epoch: 3, Batch: 460, Training Loss: 0.14761982411146163, LR: 0.1
Time, 2019-01-01T22:01:45, Epoch: 3, Batch: 470, Training Loss: 0.15635287910699844, LR: 0.1
Time, 2019-01-01T22:01:45, Epoch: 3, Batch: 480, Training Loss: 0.1708094011992216, LR: 0.1
Time, 2019-01-01T22:01:46, Epoch: 3, Batch: 490, Training Loss: 0.17661135643720627, LR: 0.1
Time, 2019-01-01T22:01:47, Epoch: 3, Batch: 500, Training Loss: 0.1394786551594734, LR: 0.1
Time, 2019-01-01T22:01:47, Epoch: 3, Batch: 510, Training Loss: 0.11832169406116008, LR: 0.1
Time, 2019-01-01T22:01:48, Epoch: 3, Batch: 520, Training Loss: 0.1265408843755722, LR: 0.1
Time, 2019-01-01T22:01:49, Epoch: 3, Batch: 530, Training Loss: 0.15134001672267913, LR: 0.1
Time, 2019-01-01T22:01:50, Epoch: 3, Batch: 540, Training Loss: 0.14342395663261415, LR: 0.1
Time, 2019-01-01T22:01:50, Epoch: 3, Batch: 550, Training Loss: 0.1455175057053566, LR: 0.1
Time, 2019-01-01T22:01:51, Epoch: 3, Batch: 560, Training Loss: 0.11659789457917213, LR: 0.1
Time, 2019-01-01T22:01:52, Epoch: 3, Batch: 570, Training Loss: 0.15167442485690116, LR: 0.1
Time, 2019-01-01T22:01:53, Epoch: 3, Batch: 580, Training Loss: 0.18520752266049384, LR: 0.1
Time, 2019-01-01T22:01:53, Epoch: 3, Batch: 590, Training Loss: 0.15811198875308036, LR: 0.1
Time, 2019-01-01T22:01:54, Epoch: 3, Batch: 600, Training Loss: 0.17639294192194938, LR: 0.1
Time, 2019-01-01T22:01:55, Epoch: 3, Batch: 610, Training Loss: 0.1452885538339615, LR: 0.1
Time, 2019-01-01T22:01:55, Epoch: 3, Batch: 620, Training Loss: 0.1700233332812786, LR: 0.1
Time, 2019-01-01T22:01:56, Epoch: 3, Batch: 630, Training Loss: 0.15703471079468728, LR: 0.1
Time, 2019-01-01T22:01:57, Epoch: 3, Batch: 640, Training Loss: 0.15747568011283875, LR: 0.1
Time, 2019-01-01T22:01:58, Epoch: 3, Batch: 650, Training Loss: 0.18078978434205056, LR: 0.1
Time, 2019-01-01T22:01:58, Epoch: 3, Batch: 660, Training Loss: 0.13932080715894699, LR: 0.1
Time, 2019-01-01T22:01:59, Epoch: 3, Batch: 670, Training Loss: 0.12834463641047478, LR: 0.1
Time, 2019-01-01T22:02:00, Epoch: 3, Batch: 680, Training Loss: 0.1100445955991745, LR: 0.1
Time, 2019-01-01T22:02:01, Epoch: 3, Batch: 690, Training Loss: 0.10267236195504666, LR: 0.1
Time, 2019-01-01T22:02:01, Epoch: 3, Batch: 700, Training Loss: 0.09893770888447762, LR: 0.1
Time, 2019-01-01T22:02:02, Epoch: 3, Batch: 710, Training Loss: 0.1552184909582138, LR: 0.1
Time, 2019-01-01T22:02:03, Epoch: 3, Batch: 720, Training Loss: 0.11395073123276234, LR: 0.1
Time, 2019-01-01T22:02:03, Epoch: 3, Batch: 730, Training Loss: 0.13520324900746344, LR: 0.1
Time, 2019-01-01T22:02:04, Epoch: 3, Batch: 740, Training Loss: 0.18058377578854562, LR: 0.1
Time, 2019-01-01T22:02:05, Epoch: 3, Batch: 750, Training Loss: 0.1485050980001688, LR: 0.1
Time, 2019-01-01T22:02:05, Epoch: 3, Batch: 760, Training Loss: 0.13196029737591744, LR: 0.1
Time, 2019-01-01T22:02:06, Epoch: 3, Batch: 770, Training Loss: 0.13849134370684624, LR: 0.1
Time, 2019-01-01T22:02:07, Epoch: 3, Batch: 780, Training Loss: 0.11879138946533203, LR: 0.1
Time, 2019-01-01T22:02:07, Epoch: 3, Batch: 790, Training Loss: 0.14077229499816896, LR: 0.1
Time, 2019-01-01T22:02:08, Epoch: 3, Batch: 800, Training Loss: 0.11535174623131753, LR: 0.1
Time, 2019-01-01T22:02:09, Epoch: 3, Batch: 810, Training Loss: 0.13489009216427802, LR: 0.1
Time, 2019-01-01T22:02:10, Epoch: 3, Batch: 820, Training Loss: 0.12495327740907669, LR: 0.1
Time, 2019-01-01T22:02:10, Epoch: 3, Batch: 830, Training Loss: 0.12340439409017563, LR: 0.1
Time, 2019-01-01T22:02:11, Epoch: 3, Batch: 840, Training Loss: 0.12200196459889412, LR: 0.1
Time, 2019-01-01T22:02:12, Epoch: 3, Batch: 850, Training Loss: 0.1788364291191101, LR: 0.1
Time, 2019-01-01T22:02:13, Epoch: 3, Batch: 860, Training Loss: 0.20124024152755737, LR: 0.1
Time, 2019-01-01T22:02:14, Epoch: 3, Batch: 870, Training Loss: 0.13151277229189873, LR: 0.1
Time, 2019-01-01T22:02:14, Epoch: 3, Batch: 880, Training Loss: 0.13885457217693328, LR: 0.1
Time, 2019-01-01T22:02:15, Epoch: 3, Batch: 890, Training Loss: 0.13026286885142327, LR: 0.1
Time, 2019-01-01T22:02:16, Epoch: 3, Batch: 900, Training Loss: 0.1653424870222807, LR: 0.1
Time, 2019-01-01T22:02:17, Epoch: 3, Batch: 910, Training Loss: 0.17964212819933892, LR: 0.1
Time, 2019-01-01T22:02:18, Epoch: 3, Batch: 920, Training Loss: 0.18590089529752732, LR: 0.1
Time, 2019-01-01T22:02:19, Epoch: 3, Batch: 930, Training Loss: 0.23546847999095916, LR: 0.1
Epoch: 3, Validation Top 1 acc: 94.64569854736328
Epoch: 3, Validation Top 5 acc: 99.88056945800781
Epoch: 3, Validation Set Loss: 0.16165584325790405
Start training epoch 4
Time, 2019-01-01T22:02:25, Epoch: 4, Batch: 10, Training Loss: 0.1668920174241066, LR: 0.1
Time, 2019-01-01T22:02:26, Epoch: 4, Batch: 20, Training Loss: 0.19486955627799035, LR: 0.1
Time, 2019-01-01T22:02:26, Epoch: 4, Batch: 30, Training Loss: 0.1775366760790348, LR: 0.1
Time, 2019-01-01T22:02:27, Epoch: 4, Batch: 40, Training Loss: 0.1753854751586914, LR: 0.1
Time, 2019-01-01T22:02:28, Epoch: 4, Batch: 50, Training Loss: 0.17162329852581024, LR: 0.1
Time, 2019-01-01T22:02:29, Epoch: 4, Batch: 60, Training Loss: 0.17779825255274773, LR: 0.1
Time, 2019-01-01T22:02:29, Epoch: 4, Batch: 70, Training Loss: 0.16116656437516214, LR: 0.1
Time, 2019-01-01T22:02:30, Epoch: 4, Batch: 80, Training Loss: 0.18342928886413573, LR: 0.1
Time, 2019-01-01T22:02:31, Epoch: 4, Batch: 90, Training Loss: 0.18757417649030686, LR: 0.1
Time, 2019-01-01T22:02:31, Epoch: 4, Batch: 100, Training Loss: 0.1736213319003582, LR: 0.1
Time, 2019-01-01T22:02:32, Epoch: 4, Batch: 110, Training Loss: 0.17888957634568214, LR: 0.1
Time, 2019-01-01T22:02:33, Epoch: 4, Batch: 120, Training Loss: 0.15660653486847878, LR: 0.1
Time, 2019-01-01T22:02:34, Epoch: 4, Batch: 130, Training Loss: 0.1123936913907528, LR: 0.1
Time, 2019-01-01T22:02:34, Epoch: 4, Batch: 140, Training Loss: 0.15713142976164818, LR: 0.1
Time, 2019-01-01T22:02:35, Epoch: 4, Batch: 150, Training Loss: 0.1408815450966358, LR: 0.1
Time, 2019-01-01T22:02:36, Epoch: 4, Batch: 160, Training Loss: 0.19176733493804932, LR: 0.1
Time, 2019-01-01T22:02:37, Epoch: 4, Batch: 170, Training Loss: 0.16933837942779065, LR: 0.1
Time, 2019-01-01T22:02:37, Epoch: 4, Batch: 180, Training Loss: 0.16321781426668167, LR: 0.1
Time, 2019-01-01T22:02:38, Epoch: 4, Batch: 190, Training Loss: 0.10673653483390808, LR: 0.1
Time, 2019-01-01T22:02:39, Epoch: 4, Batch: 200, Training Loss: 0.15391708090901374, LR: 0.1
Time, 2019-01-01T22:02:40, Epoch: 4, Batch: 210, Training Loss: 0.14490998834371566, LR: 0.1
Time, 2019-01-01T22:02:41, Epoch: 4, Batch: 220, Training Loss: 0.10542074516415596, LR: 0.1
Time, 2019-01-01T22:02:41, Epoch: 4, Batch: 230, Training Loss: 0.10517719984054566, LR: 0.1
Time, 2019-01-01T22:02:42, Epoch: 4, Batch: 240, Training Loss: 0.09961115792393685, LR: 0.1
Time, 2019-01-01T22:02:43, Epoch: 4, Batch: 250, Training Loss: 0.12414912059903145, LR: 0.1
Time, 2019-01-01T22:02:43, Epoch: 4, Batch: 260, Training Loss: 0.18585733845829963, LR: 0.1
Time, 2019-01-01T22:02:44, Epoch: 4, Batch: 270, Training Loss: 0.14783607572317123, LR: 0.1
Time, 2019-01-01T22:02:45, Epoch: 4, Batch: 280, Training Loss: 0.14682098254561424, LR: 0.1
Time, 2019-01-01T22:02:46, Epoch: 4, Batch: 290, Training Loss: 0.12702647522091864, LR: 0.1
Time, 2019-01-01T22:02:46, Epoch: 4, Batch: 300, Training Loss: 0.10779420733451843, LR: 0.1
Time, 2019-01-01T22:02:47, Epoch: 4, Batch: 310, Training Loss: 0.14129381775856018, LR: 0.1
Time, 2019-01-01T22:02:48, Epoch: 4, Batch: 320, Training Loss: 0.12205001935362816, LR: 0.1
Time, 2019-01-01T22:02:48, Epoch: 4, Batch: 330, Training Loss: 0.15232774317264558, LR: 0.1
Time, 2019-01-01T22:02:49, Epoch: 4, Batch: 340, Training Loss: 0.12768995091319085, LR: 0.1
Time, 2019-01-01T22:02:50, Epoch: 4, Batch: 350, Training Loss: 0.18415067866444587, LR: 0.1
Time, 2019-01-01T22:02:51, Epoch: 4, Batch: 360, Training Loss: 0.10990072786808014, LR: 0.1
Time, 2019-01-01T22:02:51, Epoch: 4, Batch: 370, Training Loss: 0.12750850096344948, LR: 0.1
Time, 2019-01-01T22:02:52, Epoch: 4, Batch: 380, Training Loss: 0.1424584984779358, LR: 0.1
Time, 2019-01-01T22:02:53, Epoch: 4, Batch: 390, Training Loss: 0.1578999161720276, LR: 0.1
Time, 2019-01-01T22:02:54, Epoch: 4, Batch: 400, Training Loss: 0.19092523530125619, LR: 0.1
Time, 2019-01-01T22:02:55, Epoch: 4, Batch: 410, Training Loss: 0.18209717497229577, LR: 0.1
Time, 2019-01-01T22:02:55, Epoch: 4, Batch: 420, Training Loss: 0.136668860912323, LR: 0.1
Time, 2019-01-01T22:02:56, Epoch: 4, Batch: 430, Training Loss: 0.12419942542910575, LR: 0.1
Time, 2019-01-01T22:02:57, Epoch: 4, Batch: 440, Training Loss: 0.1735242486000061, LR: 0.1
Time, 2019-01-01T22:02:58, Epoch: 4, Batch: 450, Training Loss: 0.1555776208639145, LR: 0.1
Time, 2019-01-01T22:02:58, Epoch: 4, Batch: 460, Training Loss: 0.10797118954360485, LR: 0.1
Time, 2019-01-01T22:02:59, Epoch: 4, Batch: 470, Training Loss: 0.10676575377583504, LR: 0.1
Time, 2019-01-01T22:03:00, Epoch: 4, Batch: 480, Training Loss: 0.14983266815543175, LR: 0.1
Time, 2019-01-01T22:03:01, Epoch: 4, Batch: 490, Training Loss: 0.16016421392560004, LR: 0.1
Time, 2019-01-01T22:03:01, Epoch: 4, Batch: 500, Training Loss: 0.08749905228614807, LR: 0.1
Time, 2019-01-01T22:03:02, Epoch: 4, Batch: 510, Training Loss: 0.09539104774594306, LR: 0.1
Time, 2019-01-01T22:03:03, Epoch: 4, Batch: 520, Training Loss: 0.12653582990169526, LR: 0.1
Time, 2019-01-01T22:03:04, Epoch: 4, Batch: 530, Training Loss: 0.11139581128954887, LR: 0.1
Time, 2019-01-01T22:03:04, Epoch: 4, Batch: 540, Training Loss: 0.14319618791341782, LR: 0.1
Time, 2019-01-01T22:03:05, Epoch: 4, Batch: 550, Training Loss: 0.15639025755226613, LR: 0.1
Time, 2019-01-01T22:03:06, Epoch: 4, Batch: 560, Training Loss: 0.11724455058574676, LR: 0.1
Time, 2019-01-01T22:03:07, Epoch: 4, Batch: 570, Training Loss: 0.15017789155244826, LR: 0.1
Time, 2019-01-01T22:03:07, Epoch: 4, Batch: 580, Training Loss: 0.15709893107414247, LR: 0.1
Time, 2019-01-01T22:03:08, Epoch: 4, Batch: 590, Training Loss: 0.14320737048983573, LR: 0.1
Time, 2019-01-01T22:03:09, Epoch: 4, Batch: 600, Training Loss: 0.17110470160841942, LR: 0.1
Time, 2019-01-01T22:03:10, Epoch: 4, Batch: 610, Training Loss: 0.0974713634699583, LR: 0.1
Time, 2019-01-01T22:03:10, Epoch: 4, Batch: 620, Training Loss: 0.18118770122528077, LR: 0.1
Time, 2019-01-01T22:03:11, Epoch: 4, Batch: 630, Training Loss: 0.1417294144630432, LR: 0.1
Time, 2019-01-01T22:03:12, Epoch: 4, Batch: 640, Training Loss: 0.11247061900794506, LR: 0.1
Time, 2019-01-01T22:03:12, Epoch: 4, Batch: 650, Training Loss: 0.18959058970212936, LR: 0.1
Time, 2019-01-01T22:03:13, Epoch: 4, Batch: 660, Training Loss: 0.134552688151598, LR: 0.1
Time, 2019-01-01T22:03:14, Epoch: 4, Batch: 670, Training Loss: 0.13732124194502832, LR: 0.1
Time, 2019-01-01T22:03:14, Epoch: 4, Batch: 680, Training Loss: 0.11712673008441925, LR: 0.1
Time, 2019-01-01T22:03:15, Epoch: 4, Batch: 690, Training Loss: 0.18214686661958696, LR: 0.1
Time, 2019-01-01T22:03:16, Epoch: 4, Batch: 700, Training Loss: 0.19578304663300514, LR: 0.1
Time, 2019-01-01T22:03:17, Epoch: 4, Batch: 710, Training Loss: 0.0913180097937584, LR: 0.1
Time, 2019-01-01T22:03:17, Epoch: 4, Batch: 720, Training Loss: 0.1243471696972847, LR: 0.1
Time, 2019-01-01T22:03:18, Epoch: 4, Batch: 730, Training Loss: 0.1162259340286255, LR: 0.1
Time, 2019-01-01T22:03:19, Epoch: 4, Batch: 740, Training Loss: 0.11157365366816521, LR: 0.1
Time, 2019-01-01T22:03:20, Epoch: 4, Batch: 750, Training Loss: 0.08873872607946395, LR: 0.1
Time, 2019-01-01T22:03:20, Epoch: 4, Batch: 760, Training Loss: 0.09684615433216096, LR: 0.1
Time, 2019-01-01T22:03:21, Epoch: 4, Batch: 770, Training Loss: 0.13078845478594303, LR: 0.1
Time, 2019-01-01T22:03:22, Epoch: 4, Batch: 780, Training Loss: 0.10836784355342388, LR: 0.1
Time, 2019-01-01T22:03:23, Epoch: 4, Batch: 790, Training Loss: 0.15956016331911088, LR: 0.1
Time, 2019-01-01T22:03:23, Epoch: 4, Batch: 800, Training Loss: 0.1315420798957348, LR: 0.1
Time, 2019-01-01T22:03:24, Epoch: 4, Batch: 810, Training Loss: 0.13211144134402275, LR: 0.1
Time, 2019-01-01T22:03:25, Epoch: 4, Batch: 820, Training Loss: 0.20336877033114434, LR: 0.1
Time, 2019-01-01T22:03:25, Epoch: 4, Batch: 830, Training Loss: 0.14041181206703185, LR: 0.1
Time, 2019-01-01T22:03:26, Epoch: 4, Batch: 840, Training Loss: 0.1459543075412512, LR: 0.1
Time, 2019-01-01T22:03:27, Epoch: 4, Batch: 850, Training Loss: 0.10247162356972694, LR: 0.1
Time, 2019-01-01T22:03:27, Epoch: 4, Batch: 860, Training Loss: 0.13929130285978317, LR: 0.1
Time, 2019-01-01T22:03:28, Epoch: 4, Batch: 870, Training Loss: 0.16218006908893584, LR: 0.1
Time, 2019-01-01T22:03:29, Epoch: 4, Batch: 880, Training Loss: 0.15554153136909007, LR: 0.1
Time, 2019-01-01T22:03:30, Epoch: 4, Batch: 890, Training Loss: 0.11849796772003174, LR: 0.1
Time, 2019-01-01T22:03:30, Epoch: 4, Batch: 900, Training Loss: 0.18813965171575547, LR: 0.1
Time, 2019-01-01T22:03:31, Epoch: 4, Batch: 910, Training Loss: 0.21202818006277085, LR: 0.1
Time, 2019-01-01T22:03:32, Epoch: 4, Batch: 920, Training Loss: 0.22690027430653573, LR: 0.1
Time, 2019-01-01T22:03:33, Epoch: 4, Batch: 930, Training Loss: 0.2554305508732796, LR: 0.1
Epoch: 4, Validation Top 1 acc: 91.53065490722656
Epoch: 4, Validation Top 5 acc: 99.62181854248047
Epoch: 4, Validation Set Loss: 0.2765672206878662
Start training epoch 5
Time, 2019-01-01T22:03:38, Epoch: 5, Batch: 10, Training Loss: 0.19977882131934166, LR: 0.1
Time, 2019-01-01T22:03:39, Epoch: 5, Batch: 20, Training Loss: 0.21169688403606415, LR: 0.1
Time, 2019-01-01T22:03:40, Epoch: 5, Batch: 30, Training Loss: 0.22435723543167113, LR: 0.1
Time, 2019-01-01T22:03:40, Epoch: 5, Batch: 40, Training Loss: 0.21146144345402718, LR: 0.1
Time, 2019-01-01T22:03:41, Epoch: 5, Batch: 50, Training Loss: 0.1896036945283413, LR: 0.1
Time, 2019-01-01T22:03:42, Epoch: 5, Batch: 60, Training Loss: 0.15788672417402266, LR: 0.1
Time, 2019-01-01T22:03:42, Epoch: 5, Batch: 70, Training Loss: 0.12285548374056816, LR: 0.1
Time, 2019-01-01T22:03:43, Epoch: 5, Batch: 80, Training Loss: 0.17208835035562514, LR: 0.1
Time, 2019-01-01T22:03:44, Epoch: 5, Batch: 90, Training Loss: 0.18584690801799297, LR: 0.1
Time, 2019-01-01T22:03:45, Epoch: 5, Batch: 100, Training Loss: 0.1894256927073002, LR: 0.1
Time, 2019-01-01T22:03:45, Epoch: 5, Batch: 110, Training Loss: 0.1487155556678772, LR: 0.1
Time, 2019-01-01T22:03:46, Epoch: 5, Batch: 120, Training Loss: 0.12677758783102036, LR: 0.1
Time, 2019-01-01T22:03:47, Epoch: 5, Batch: 130, Training Loss: 0.164268296957016, LR: 0.1
Time, 2019-01-01T22:03:48, Epoch: 5, Batch: 140, Training Loss: 0.14689869433641434, LR: 0.1
Time, 2019-01-01T22:03:49, Epoch: 5, Batch: 150, Training Loss: 0.119709824770689, LR: 0.1
Time, 2019-01-01T22:03:49, Epoch: 5, Batch: 160, Training Loss: 0.12891156747937202, LR: 0.1
Time, 2019-01-01T22:03:50, Epoch: 5, Batch: 170, Training Loss: 0.15226305462419987, LR: 0.1
Time, 2019-01-01T22:03:51, Epoch: 5, Batch: 180, Training Loss: 0.1345077246427536, LR: 0.1
Time, 2019-01-01T22:03:52, Epoch: 5, Batch: 190, Training Loss: 0.1507447585463524, LR: 0.1
Time, 2019-01-01T22:03:52, Epoch: 5, Batch: 200, Training Loss: 0.16417036950588226, LR: 0.1
Time, 2019-01-01T22:03:53, Epoch: 5, Batch: 210, Training Loss: 0.1409404307603836, LR: 0.1
Time, 2019-01-01T22:03:54, Epoch: 5, Batch: 220, Training Loss: 0.15488330870866776, LR: 0.1
Time, 2019-01-01T22:03:55, Epoch: 5, Batch: 230, Training Loss: 0.11962520107626914, LR: 0.1
Time, 2019-01-01T22:03:55, Epoch: 5, Batch: 240, Training Loss: 0.11043976843357087, LR: 0.1
Time, 2019-01-01T22:03:56, Epoch: 5, Batch: 250, Training Loss: 0.09750276282429696, LR: 0.1
Time, 2019-01-01T22:03:57, Epoch: 5, Batch: 260, Training Loss: 0.09082616232335568, LR: 0.1
Time, 2019-01-01T22:03:58, Epoch: 5, Batch: 270, Training Loss: 0.10804010033607483, LR: 0.1
Time, 2019-01-01T22:03:58, Epoch: 5, Batch: 280, Training Loss: 0.1061832807958126, LR: 0.1
Time, 2019-01-01T22:03:59, Epoch: 5, Batch: 290, Training Loss: 0.13394968062639237, LR: 0.1
Time, 2019-01-01T22:04:00, Epoch: 5, Batch: 300, Training Loss: 0.11641564853489399, LR: 0.1
Time, 2019-01-01T22:04:01, Epoch: 5, Batch: 310, Training Loss: 0.11902555376291275, LR: 0.1
Time, 2019-01-01T22:04:02, Epoch: 5, Batch: 320, Training Loss: 0.15134381577372552, LR: 0.1
Time, 2019-01-01T22:04:03, Epoch: 5, Batch: 330, Training Loss: 0.11203746944665909, LR: 0.1
Time, 2019-01-01T22:04:03, Epoch: 5, Batch: 340, Training Loss: 0.11776965111494064, LR: 0.1
Time, 2019-01-01T22:04:04, Epoch: 5, Batch: 350, Training Loss: 0.13186290338635445, LR: 0.1
Time, 2019-01-01T22:04:05, Epoch: 5, Batch: 360, Training Loss: 0.11882481649518013, LR: 0.1
Time, 2019-01-01T22:04:06, Epoch: 5, Batch: 370, Training Loss: 0.1390165716409683, LR: 0.1
Time, 2019-01-01T22:04:07, Epoch: 5, Batch: 380, Training Loss: 0.10578738674521446, LR: 0.1
Time, 2019-01-01T22:04:07, Epoch: 5, Batch: 390, Training Loss: 0.15258548706769942, LR: 0.1
Time, 2019-01-01T22:04:08, Epoch: 5, Batch: 400, Training Loss: 0.1422005131840706, LR: 0.1
Time, 2019-01-01T22:04:09, Epoch: 5, Batch: 410, Training Loss: 0.13259045779705048, LR: 0.1
Time, 2019-01-01T22:04:10, Epoch: 5, Batch: 420, Training Loss: 0.13640338629484178, LR: 0.1
Time, 2019-01-01T22:04:11, Epoch: 5, Batch: 430, Training Loss: 0.1100376334041357, LR: 0.1
Time, 2019-01-01T22:04:11, Epoch: 5, Batch: 440, Training Loss: 0.10764251723885536, LR: 0.1
Time, 2019-01-01T22:04:12, Epoch: 5, Batch: 450, Training Loss: 0.1281294345855713, LR: 0.1
Time, 2019-01-01T22:04:13, Epoch: 5, Batch: 460, Training Loss: 0.17519863918423653, LR: 0.1
Time, 2019-01-01T22:04:14, Epoch: 5, Batch: 470, Training Loss: 0.11257769167423248, LR: 0.1
Time, 2019-01-01T22:04:15, Epoch: 5, Batch: 480, Training Loss: 0.17000563032925128, LR: 0.1
Time, 2019-01-01T22:04:16, Epoch: 5, Batch: 490, Training Loss: 0.1221149642020464, LR: 0.1
Time, 2019-01-01T22:04:16, Epoch: 5, Batch: 500, Training Loss: 0.11025539860129356, LR: 0.1
Time, 2019-01-01T22:04:17, Epoch: 5, Batch: 510, Training Loss: 0.1211946576833725, LR: 0.1
Time, 2019-01-01T22:04:18, Epoch: 5, Batch: 520, Training Loss: 0.13630522042512894, LR: 0.1
Time, 2019-01-01T22:04:19, Epoch: 5, Batch: 530, Training Loss: 0.20739600360393523, LR: 0.1
Time, 2019-01-01T22:04:20, Epoch: 5, Batch: 540, Training Loss: 0.16502788811922073, LR: 0.1
Time, 2019-01-01T22:04:21, Epoch: 5, Batch: 550, Training Loss: 0.15982548147439957, LR: 0.1
Time, 2019-01-01T22:04:21, Epoch: 5, Batch: 560, Training Loss: 0.1646315760910511, LR: 0.1
Time, 2019-01-01T22:04:22, Epoch: 5, Batch: 570, Training Loss: 0.10184202790260315, LR: 0.1
Time, 2019-01-01T22:04:23, Epoch: 5, Batch: 580, Training Loss: 0.1484552189707756, LR: 0.1
Time, 2019-01-01T22:04:24, Epoch: 5, Batch: 590, Training Loss: 0.1054504543542862, LR: 0.1
Time, 2019-01-01T22:04:25, Epoch: 5, Batch: 600, Training Loss: 0.12847533077001572, LR: 0.1
Time, 2019-01-01T22:04:26, Epoch: 5, Batch: 610, Training Loss: 0.08331821486353874, LR: 0.1
Time, 2019-01-01T22:04:26, Epoch: 5, Batch: 620, Training Loss: 0.13003415241837502, LR: 0.1
Time, 2019-01-01T22:04:27, Epoch: 5, Batch: 630, Training Loss: 0.15800566896796225, LR: 0.1
Time, 2019-01-01T22:04:28, Epoch: 5, Batch: 640, Training Loss: 0.18018074855208396, LR: 0.1
Time, 2019-01-01T22:04:29, Epoch: 5, Batch: 650, Training Loss: 0.12154598534107208, LR: 0.1
Time, 2019-01-01T22:04:30, Epoch: 5, Batch: 660, Training Loss: 0.1746266596019268, LR: 0.1
Time, 2019-01-01T22:04:30, Epoch: 5, Batch: 670, Training Loss: 0.10651045739650726, LR: 0.1
Time, 2019-01-01T22:04:31, Epoch: 5, Batch: 680, Training Loss: 0.11316766887903214, LR: 0.1
Time, 2019-01-01T22:04:32, Epoch: 5, Batch: 690, Training Loss: 0.1540394552052021, LR: 0.1
Time, 2019-01-01T22:04:33, Epoch: 5, Batch: 700, Training Loss: 0.09847651869058609, LR: 0.1
Time, 2019-01-01T22:04:34, Epoch: 5, Batch: 710, Training Loss: 0.11494840383529663, LR: 0.1
Time, 2019-01-01T22:04:35, Epoch: 5, Batch: 720, Training Loss: 0.13503788113594056, LR: 0.1
Time, 2019-01-01T22:04:36, Epoch: 5, Batch: 730, Training Loss: 0.09599584117531776, LR: 0.1
Time, 2019-01-01T22:04:37, Epoch: 5, Batch: 740, Training Loss: 0.14034622758626938, LR: 0.1
Time, 2019-01-01T22:04:37, Epoch: 5, Batch: 750, Training Loss: 0.12207217514514923, LR: 0.1
Time, 2019-01-01T22:04:39, Epoch: 5, Batch: 760, Training Loss: 0.11695838868618011, LR: 0.1
Time, 2019-01-01T22:04:39, Epoch: 5, Batch: 770, Training Loss: 0.0968841552734375, LR: 0.1
Time, 2019-01-01T22:04:40, Epoch: 5, Batch: 780, Training Loss: 0.10829076617956161, LR: 0.1
Time, 2019-01-01T22:04:41, Epoch: 5, Batch: 790, Training Loss: 0.12857868522405624, LR: 0.1
Time, 2019-01-01T22:04:42, Epoch: 5, Batch: 800, Training Loss: 0.12299099639058113, LR: 0.1
Time, 2019-01-01T22:04:43, Epoch: 5, Batch: 810, Training Loss: 0.13865204602479936, LR: 0.1
Time, 2019-01-01T22:04:44, Epoch: 5, Batch: 820, Training Loss: 0.13862183541059495, LR: 0.1
Time, 2019-01-01T22:04:45, Epoch: 5, Batch: 830, Training Loss: 0.15528091453015805, LR: 0.1
Time, 2019-01-01T22:04:45, Epoch: 5, Batch: 840, Training Loss: 0.1658156294375658, LR: 0.1
Time, 2019-01-01T22:04:46, Epoch: 5, Batch: 850, Training Loss: 0.14803088456392288, LR: 0.1
Time, 2019-01-01T22:04:47, Epoch: 5, Batch: 860, Training Loss: 0.17155578434467317, LR: 0.1
Time, 2019-01-01T22:04:48, Epoch: 5, Batch: 870, Training Loss: 0.15257246382534503, LR: 0.1
Time, 2019-01-01T22:04:49, Epoch: 5, Batch: 880, Training Loss: 0.12143181338906288, LR: 0.1
Time, 2019-01-01T22:04:50, Epoch: 5, Batch: 890, Training Loss: 0.13171265386044978, LR: 0.1
Time, 2019-01-01T22:04:50, Epoch: 5, Batch: 900, Training Loss: 0.13119330033659934, LR: 0.1
Time, 2019-01-01T22:04:51, Epoch: 5, Batch: 910, Training Loss: 0.1277698501944542, LR: 0.1
Time, 2019-01-01T22:04:52, Epoch: 5, Batch: 920, Training Loss: 0.12981600314378738, LR: 0.1
Time, 2019-01-01T22:04:53, Epoch: 5, Batch: 930, Training Loss: 0.10045206025242806, LR: 0.1
Epoch: 5, Validation Top 1 acc: 96.875
Epoch: 5, Validation Top 5 acc: 99.95024108886719
Epoch: 5, Validation Set Loss: 0.09410110861063004
Start training epoch 6
Time, 2019-01-01T22:04:59, Epoch: 6, Batch: 10, Training Loss: 0.1144341766834259, LR: 0.1
Time, 2019-01-01T22:05:00, Epoch: 6, Batch: 20, Training Loss: 0.17556047700345517, LR: 0.1
Time, 2019-01-01T22:05:01, Epoch: 6, Batch: 30, Training Loss: 0.21416261419653893, LR: 0.1
Time, 2019-01-01T22:05:01, Epoch: 6, Batch: 40, Training Loss: 0.1845751978456974, LR: 0.1
Time, 2019-01-01T22:05:02, Epoch: 6, Batch: 50, Training Loss: 0.15671074837446214, LR: 0.1
Time, 2019-01-01T22:05:03, Epoch: 6, Batch: 60, Training Loss: 0.17523117065429689, LR: 0.1
Time, 2019-01-01T22:05:04, Epoch: 6, Batch: 70, Training Loss: 0.13032577596604825, LR: 0.1
Time, 2019-01-01T22:05:04, Epoch: 6, Batch: 80, Training Loss: 0.19295597821474075, LR: 0.1
Time, 2019-01-01T22:05:05, Epoch: 6, Batch: 90, Training Loss: 0.15641770735383034, LR: 0.1
Time, 2019-01-01T22:05:06, Epoch: 6, Batch: 100, Training Loss: 0.15411227494478225, LR: 0.1
Time, 2019-01-01T22:05:06, Epoch: 6, Batch: 110, Training Loss: 0.1334953263401985, LR: 0.1
Time, 2019-01-01T22:05:07, Epoch: 6, Batch: 120, Training Loss: 0.1183677151799202, LR: 0.1
Time, 2019-01-01T22:05:08, Epoch: 6, Batch: 130, Training Loss: 0.1321689199656248, LR: 0.1
Time, 2019-01-01T22:05:09, Epoch: 6, Batch: 140, Training Loss: 0.10185767598450184, LR: 0.1
Time, 2019-01-01T22:05:10, Epoch: 6, Batch: 150, Training Loss: 0.10254667140543461, LR: 0.1
Time, 2019-01-01T22:05:11, Epoch: 6, Batch: 160, Training Loss: 0.1609111286699772, LR: 0.1
Time, 2019-01-01T22:05:11, Epoch: 6, Batch: 170, Training Loss: 0.17555683851242065, LR: 0.1
Time, 2019-01-01T22:05:12, Epoch: 6, Batch: 180, Training Loss: 0.19353930056095123, LR: 0.1
Time, 2019-01-01T22:05:13, Epoch: 6, Batch: 190, Training Loss: 0.1695176914334297, LR: 0.1
Time, 2019-01-01T22:05:14, Epoch: 6, Batch: 200, Training Loss: 0.12472382262349128, LR: 0.1
Time, 2019-01-01T22:05:15, Epoch: 6, Batch: 210, Training Loss: 0.13235485069453717, LR: 0.1
Time, 2019-01-01T22:05:15, Epoch: 6, Batch: 220, Training Loss: 0.1205214612185955, LR: 0.1
Time, 2019-01-01T22:05:16, Epoch: 6, Batch: 230, Training Loss: 0.11052373945713043, LR: 0.1
Time, 2019-01-01T22:05:17, Epoch: 6, Batch: 240, Training Loss: 0.11853654831647872, LR: 0.1
Time, 2019-01-01T22:05:18, Epoch: 6, Batch: 250, Training Loss: 0.09446082785725593, LR: 0.1
Time, 2019-01-01T22:05:18, Epoch: 6, Batch: 260, Training Loss: 0.0968192160129547, LR: 0.1
Time, 2019-01-01T22:05:19, Epoch: 6, Batch: 270, Training Loss: 0.12129831165075303, LR: 0.1
Time, 2019-01-01T22:05:20, Epoch: 6, Batch: 280, Training Loss: 0.1294427014887333, LR: 0.1
Time, 2019-01-01T22:05:21, Epoch: 6, Batch: 290, Training Loss: 0.11617292016744614, LR: 0.1
Time, 2019-01-01T22:05:21, Epoch: 6, Batch: 300, Training Loss: 0.14156736060976982, LR: 0.1
Time, 2019-01-01T22:05:22, Epoch: 6, Batch: 310, Training Loss: 0.10513366274535656, LR: 0.1
Time, 2019-01-01T22:05:23, Epoch: 6, Batch: 320, Training Loss: 0.11036237850785255, LR: 0.1
Time, 2019-01-01T22:05:24, Epoch: 6, Batch: 330, Training Loss: 0.08613684922456741, LR: 0.1
Time, 2019-01-01T22:05:24, Epoch: 6, Batch: 340, Training Loss: 0.08823724314570427, LR: 0.1
Time, 2019-01-01T22:05:25, Epoch: 6, Batch: 350, Training Loss: 0.122586939483881, LR: 0.1
Time, 2019-01-01T22:05:26, Epoch: 6, Batch: 360, Training Loss: 0.12289371713995934, LR: 0.1
Time, 2019-01-01T22:05:27, Epoch: 6, Batch: 370, Training Loss: 0.11287311017513275, LR: 0.1
Time, 2019-01-01T22:05:28, Epoch: 6, Batch: 380, Training Loss: 0.07338921055197715, LR: 0.1
Time, 2019-01-01T22:05:29, Epoch: 6, Batch: 390, Training Loss: 0.07544557005167007, LR: 0.1
Time, 2019-01-01T22:05:30, Epoch: 6, Batch: 400, Training Loss: 0.12734732106328012, LR: 0.1
Time, 2019-01-01T22:05:31, Epoch: 6, Batch: 410, Training Loss: 0.12620267421007156, LR: 0.1
Time, 2019-01-01T22:05:31, Epoch: 6, Batch: 420, Training Loss: 0.1539076864719391, LR: 0.1
Time, 2019-01-01T22:05:32, Epoch: 6, Batch: 430, Training Loss: 0.19444778859615325, LR: 0.1
Time, 2019-01-01T22:05:33, Epoch: 6, Batch: 440, Training Loss: 0.1542830564081669, LR: 0.1
Time, 2019-01-01T22:05:34, Epoch: 6, Batch: 450, Training Loss: 0.13599200993776323, LR: 0.1
Time, 2019-01-01T22:05:35, Epoch: 6, Batch: 460, Training Loss: 0.18844729512929917, LR: 0.1
Time, 2019-01-01T22:05:36, Epoch: 6, Batch: 470, Training Loss: 0.127879823371768, LR: 0.1
Time, 2019-01-01T22:05:36, Epoch: 6, Batch: 480, Training Loss: 0.1159082069993019, LR: 0.1
Time, 2019-01-01T22:05:37, Epoch: 6, Batch: 490, Training Loss: 0.13039573282003403, LR: 0.1
Time, 2019-01-01T22:05:38, Epoch: 6, Batch: 500, Training Loss: 0.1508812230080366, LR: 0.1
Time, 2019-01-01T22:05:39, Epoch: 6, Batch: 510, Training Loss: 0.15823365636169912, LR: 0.1
Time, 2019-01-01T22:05:40, Epoch: 6, Batch: 520, Training Loss: 0.11575038619339466, LR: 0.1
Time, 2019-01-01T22:05:40, Epoch: 6, Batch: 530, Training Loss: 0.11483666524291039, LR: 0.1
Time, 2019-01-01T22:05:41, Epoch: 6, Batch: 540, Training Loss: 0.13159595802426338, LR: 0.1
Time, 2019-01-01T22:05:42, Epoch: 6, Batch: 550, Training Loss: 0.15294260978698732, LR: 0.1
Time, 2019-01-01T22:05:43, Epoch: 6, Batch: 560, Training Loss: 0.1223461389541626, LR: 0.1
Time, 2019-01-01T22:05:44, Epoch: 6, Batch: 570, Training Loss: 0.11228947043418884, LR: 0.1
Time, 2019-01-01T22:05:44, Epoch: 6, Batch: 580, Training Loss: 0.12784608639776707, LR: 0.1
Time, 2019-01-01T22:05:45, Epoch: 6, Batch: 590, Training Loss: 0.10907753482460976, LR: 0.1
Time, 2019-01-01T22:05:46, Epoch: 6, Batch: 600, Training Loss: 0.08842289745807648, LR: 0.1
Time, 2019-01-01T22:05:47, Epoch: 6, Batch: 610, Training Loss: 0.11522839851677417, LR: 0.1
Time, 2019-01-01T22:05:48, Epoch: 6, Batch: 620, Training Loss: 0.10795516818761826, LR: 0.1
Time, 2019-01-01T22:05:49, Epoch: 6, Batch: 630, Training Loss: 0.14599340707063674, LR: 0.1
Time, 2019-01-01T22:05:49, Epoch: 6, Batch: 640, Training Loss: 0.1067236166447401, LR: 0.1
Time, 2019-01-01T22:05:50, Epoch: 6, Batch: 650, Training Loss: 0.1056730356067419, LR: 0.1
Time, 2019-01-01T22:05:51, Epoch: 6, Batch: 660, Training Loss: 0.1069312907755375, LR: 0.1
Time, 2019-01-01T22:05:52, Epoch: 6, Batch: 670, Training Loss: 0.15924714133143425, LR: 0.1
Time, 2019-01-01T22:05:53, Epoch: 6, Batch: 680, Training Loss: 0.11423116326332092, LR: 0.1
Time, 2019-01-01T22:05:54, Epoch: 6, Batch: 690, Training Loss: 0.14605351835489272, LR: 0.1
Time, 2019-01-01T22:05:55, Epoch: 6, Batch: 700, Training Loss: 0.12189091145992278, LR: 0.1
Time, 2019-01-01T22:05:55, Epoch: 6, Batch: 710, Training Loss: 0.0978161484003067, LR: 0.1
Time, 2019-01-01T22:05:56, Epoch: 6, Batch: 720, Training Loss: 0.08445594534277916, LR: 0.1
Time, 2019-01-01T22:05:57, Epoch: 6, Batch: 730, Training Loss: 0.156077940762043, LR: 0.1
Time, 2019-01-01T22:05:58, Epoch: 6, Batch: 740, Training Loss: 0.10001606158912182, LR: 0.1
Time, 2019-01-01T22:05:59, Epoch: 6, Batch: 750, Training Loss: 0.06738951317965984, LR: 0.1
Time, 2019-01-01T22:06:00, Epoch: 6, Batch: 760, Training Loss: 0.10674438290297986, LR: 0.1
Time, 2019-01-01T22:06:01, Epoch: 6, Batch: 770, Training Loss: 0.08894577249884605, LR: 0.1
Time, 2019-01-01T22:06:02, Epoch: 6, Batch: 780, Training Loss: 0.18274918720126151, LR: 0.1
Time, 2019-01-01T22:06:02, Epoch: 6, Batch: 790, Training Loss: 0.1328199476003647, LR: 0.1
Time, 2019-01-01T22:06:03, Epoch: 6, Batch: 800, Training Loss: 0.15298152938485146, LR: 0.1
Time, 2019-01-01T22:06:04, Epoch: 6, Batch: 810, Training Loss: 0.12368491776287556, LR: 0.1
Time, 2019-01-01T22:06:05, Epoch: 6, Batch: 820, Training Loss: 0.11434176042675973, LR: 0.1
Time, 2019-01-01T22:06:06, Epoch: 6, Batch: 830, Training Loss: 0.101231799274683, LR: 0.1
Time, 2019-01-01T22:06:07, Epoch: 6, Batch: 840, Training Loss: 0.12136441469192505, LR: 0.1
Time, 2019-01-01T22:06:08, Epoch: 6, Batch: 850, Training Loss: 0.11352041587233544, LR: 0.1
Time, 2019-01-01T22:06:09, Epoch: 6, Batch: 860, Training Loss: 0.11851641200482846, LR: 0.1
Time, 2019-01-01T22:06:10, Epoch: 6, Batch: 870, Training Loss: 0.12404431104660034, LR: 0.1
Time, 2019-01-01T22:06:11, Epoch: 6, Batch: 880, Training Loss: 0.12166763916611671, LR: 0.1
Time, 2019-01-01T22:06:12, Epoch: 6, Batch: 890, Training Loss: 0.09471449106931687, LR: 0.1
Time, 2019-01-01T22:06:13, Epoch: 6, Batch: 900, Training Loss: 0.0932054989039898, LR: 0.1
Time, 2019-01-01T22:06:14, Epoch: 6, Batch: 910, Training Loss: 0.13187361434102057, LR: 0.1
Time, 2019-01-01T22:06:15, Epoch: 6, Batch: 920, Training Loss: 0.08422193266451358, LR: 0.1
Time, 2019-01-01T22:06:16, Epoch: 6, Batch: 930, Training Loss: 0.09899262860417365, LR: 0.1
Epoch: 6, Validation Top 1 acc: 96.97452545166016
Epoch: 6, Validation Top 5 acc: 99.97014617919922
Epoch: 6, Validation Set Loss: 0.09027345478534698
Start training epoch 7
Time, 2019-01-01T22:06:23, Epoch: 7, Batch: 10, Training Loss: 0.10026193074882031, LR: 0.1
Time, 2019-01-01T22:06:24, Epoch: 7, Batch: 20, Training Loss: 0.1162102185189724, LR: 0.1
Time, 2019-01-01T22:06:25, Epoch: 7, Batch: 30, Training Loss: 0.11677409335970879, LR: 0.1
Time, 2019-01-01T22:06:26, Epoch: 7, Batch: 40, Training Loss: 0.10471783950924873, LR: 0.1
Time, 2019-01-01T22:06:27, Epoch: 7, Batch: 50, Training Loss: 0.10802088379859924, LR: 0.1
Time, 2019-01-01T22:06:28, Epoch: 7, Batch: 60, Training Loss: 0.11732495203614235, LR: 0.1
Time, 2019-01-01T22:06:29, Epoch: 7, Batch: 70, Training Loss: 0.14848995693027972, LR: 0.1
Time, 2019-01-01T22:06:30, Epoch: 7, Batch: 80, Training Loss: 0.09921544194221496, LR: 0.1
Time, 2019-01-01T22:06:30, Epoch: 7, Batch: 90, Training Loss: 0.11018394455313682, LR: 0.1
Time, 2019-01-01T22:06:31, Epoch: 7, Batch: 100, Training Loss: 0.13444432541728019, LR: 0.1
Time, 2019-01-01T22:06:32, Epoch: 7, Batch: 110, Training Loss: 0.14247236251831055, LR: 0.1
Time, 2019-01-01T22:06:33, Epoch: 7, Batch: 120, Training Loss: 0.12341775000095367, LR: 0.1
Time, 2019-01-01T22:06:34, Epoch: 7, Batch: 130, Training Loss: 0.12270708158612251, LR: 0.1
Time, 2019-01-01T22:06:35, Epoch: 7, Batch: 140, Training Loss: 0.11950780302286149, LR: 0.1
Time, 2019-01-01T22:06:36, Epoch: 7, Batch: 150, Training Loss: 0.07012652605772018, LR: 0.1
Time, 2019-01-01T22:06:36, Epoch: 7, Batch: 160, Training Loss: 0.0899466447532177, LR: 0.1
Time, 2019-01-01T22:06:37, Epoch: 7, Batch: 170, Training Loss: 0.12224957048892975, LR: 0.1
Time, 2019-01-01T22:06:38, Epoch: 7, Batch: 180, Training Loss: 0.11077816262841225, LR: 0.1
Time, 2019-01-01T22:06:39, Epoch: 7, Batch: 190, Training Loss: 0.1187812015414238, LR: 0.1
Time, 2019-01-01T22:06:40, Epoch: 7, Batch: 200, Training Loss: 0.08565467596054077, LR: 0.1
Time, 2019-01-01T22:06:40, Epoch: 7, Batch: 210, Training Loss: 0.08598982207477093, LR: 0.1
Time, 2019-01-01T22:06:41, Epoch: 7, Batch: 220, Training Loss: 0.10382521450519562, LR: 0.1
Time, 2019-01-01T22:06:42, Epoch: 7, Batch: 230, Training Loss: 0.0968907430768013, LR: 0.1
Time, 2019-01-01T22:06:43, Epoch: 7, Batch: 240, Training Loss: 0.08220068961381913, LR: 0.1
Time, 2019-01-01T22:06:44, Epoch: 7, Batch: 250, Training Loss: 0.15655990093946456, LR: 0.1
Time, 2019-01-01T22:06:45, Epoch: 7, Batch: 260, Training Loss: 0.16402931585907937, LR: 0.1
Time, 2019-01-01T22:06:45, Epoch: 7, Batch: 270, Training Loss: 0.13931382149457933, LR: 0.1
Time, 2019-01-01T22:06:46, Epoch: 7, Batch: 280, Training Loss: 0.13993491157889365, LR: 0.1
Time, 2019-01-01T22:06:47, Epoch: 7, Batch: 290, Training Loss: 0.1289950765669346, LR: 0.1
Time, 2019-01-01T22:06:48, Epoch: 7, Batch: 300, Training Loss: 0.14130461141467093, LR: 0.1
Time, 2019-01-01T22:06:49, Epoch: 7, Batch: 310, Training Loss: 0.16960998624563217, LR: 0.1
Time, 2019-01-01T22:06:50, Epoch: 7, Batch: 320, Training Loss: 0.18062350153923035, LR: 0.1
Time, 2019-01-01T22:06:51, Epoch: 7, Batch: 330, Training Loss: 0.16409284770488738, LR: 0.1
Time, 2019-01-01T22:06:51, Epoch: 7, Batch: 340, Training Loss: 0.12580180019140244, LR: 0.1
Time, 2019-01-01T22:06:52, Epoch: 7, Batch: 350, Training Loss: 0.14081895276904105, LR: 0.1
Time, 2019-01-01T22:06:53, Epoch: 7, Batch: 360, Training Loss: 0.16705756187438964, LR: 0.1
Time, 2019-01-01T22:06:54, Epoch: 7, Batch: 370, Training Loss: 0.18064482063055037, LR: 0.1
Time, 2019-01-01T22:06:55, Epoch: 7, Batch: 380, Training Loss: 0.22191150560975076, LR: 0.1
Time, 2019-01-01T22:06:55, Epoch: 7, Batch: 390, Training Loss: 0.15167178362607955, LR: 0.1
Time, 2019-01-01T22:06:56, Epoch: 7, Batch: 400, Training Loss: 0.17802464514970778, LR: 0.1
Time, 2019-01-01T22:06:57, Epoch: 7, Batch: 410, Training Loss: 0.179238773137331, LR: 0.1
Time, 2019-01-01T22:06:58, Epoch: 7, Batch: 420, Training Loss: 0.16825732216238976, LR: 0.1
Time, 2019-01-01T22:06:59, Epoch: 7, Batch: 430, Training Loss: 0.14847718551754951, LR: 0.1
Time, 2019-01-01T22:06:59, Epoch: 7, Batch: 440, Training Loss: 0.1706020191311836, LR: 0.1
Time, 2019-01-01T22:07:00, Epoch: 7, Batch: 450, Training Loss: 0.2075555980205536, LR: 0.1
Time, 2019-01-01T22:07:01, Epoch: 7, Batch: 460, Training Loss: 0.19551023617386817, LR: 0.1
Time, 2019-01-01T22:07:02, Epoch: 7, Batch: 470, Training Loss: 0.15882142186164855, LR: 0.1
Time, 2019-01-01T22:07:03, Epoch: 7, Batch: 480, Training Loss: 0.16165004372596742, LR: 0.1
Time, 2019-01-01T22:07:04, Epoch: 7, Batch: 490, Training Loss: 0.15082119777798653, LR: 0.1
Time, 2019-01-01T22:07:05, Epoch: 7, Batch: 500, Training Loss: 0.15677790269255637, LR: 0.1
Time, 2019-01-01T22:07:05, Epoch: 7, Batch: 510, Training Loss: 0.2085527405142784, LR: 0.1
Time, 2019-01-01T22:07:06, Epoch: 7, Batch: 520, Training Loss: 0.20069581419229507, LR: 0.1
Time, 2019-01-01T22:07:07, Epoch: 7, Batch: 530, Training Loss: 0.1635762184858322, LR: 0.1
Time, 2019-01-01T22:07:08, Epoch: 7, Batch: 540, Training Loss: 0.1677958533167839, LR: 0.1
Time, 2019-01-01T22:07:09, Epoch: 7, Batch: 550, Training Loss: 0.2134036563336849, LR: 0.1
Time, 2019-01-01T22:07:09, Epoch: 7, Batch: 560, Training Loss: 0.20642771944403648, LR: 0.1
Time, 2019-01-01T22:07:10, Epoch: 7, Batch: 570, Training Loss: 0.2164765052497387, LR: 0.1
Time, 2019-01-01T22:07:11, Epoch: 7, Batch: 580, Training Loss: 0.1877976782619953, LR: 0.1
Time, 2019-01-01T22:07:12, Epoch: 7, Batch: 590, Training Loss: 0.17641788646578788, LR: 0.1
Time, 2019-01-01T22:07:12, Epoch: 7, Batch: 600, Training Loss: 0.14049234464764596, LR: 0.1
Time, 2019-01-01T22:07:13, Epoch: 7, Batch: 610, Training Loss: 0.16358490027487277, LR: 0.1
Time, 2019-01-01T22:07:14, Epoch: 7, Batch: 620, Training Loss: 0.23617174699902535, LR: 0.1
Time, 2019-01-01T22:07:15, Epoch: 7, Batch: 630, Training Loss: 0.23714511096477509, LR: 0.1
Time, 2019-01-01T22:07:16, Epoch: 7, Batch: 640, Training Loss: 0.16051719337701797, LR: 0.1
Time, 2019-01-01T22:07:17, Epoch: 7, Batch: 650, Training Loss: 0.13675112649798393, LR: 0.1
Time, 2019-01-01T22:07:17, Epoch: 7, Batch: 660, Training Loss: 0.10211033374071121, LR: 0.1
Time, 2019-01-01T22:07:18, Epoch: 7, Batch: 670, Training Loss: 0.1667451500892639, LR: 0.1
Time, 2019-01-01T22:07:19, Epoch: 7, Batch: 680, Training Loss: 0.15048491768538952, LR: 0.1
Time, 2019-01-01T22:07:20, Epoch: 7, Batch: 690, Training Loss: 0.1806732453405857, LR: 0.1
Time, 2019-01-01T22:07:21, Epoch: 7, Batch: 700, Training Loss: 0.16273583322763444, LR: 0.1
Time, 2019-01-01T22:07:21, Epoch: 7, Batch: 710, Training Loss: 0.19019385948777198, LR: 0.1
Time, 2019-01-01T22:07:22, Epoch: 7, Batch: 720, Training Loss: 0.12760329470038415, LR: 0.1
Time, 2019-01-01T22:07:23, Epoch: 7, Batch: 730, Training Loss: 0.11764943934977054, LR: 0.1
Time, 2019-01-01T22:07:24, Epoch: 7, Batch: 740, Training Loss: 0.13762749508023261, LR: 0.1
Time, 2019-01-01T22:07:25, Epoch: 7, Batch: 750, Training Loss: 0.13351705335080624, LR: 0.1
Time, 2019-01-01T22:07:25, Epoch: 7, Batch: 760, Training Loss: 0.12450410798192024, LR: 0.1
Time, 2019-01-01T22:07:26, Epoch: 7, Batch: 770, Training Loss: 0.11191324815154076, LR: 0.1
Time, 2019-01-01T22:07:27, Epoch: 7, Batch: 780, Training Loss: 0.11732242926955223, LR: 0.1
Time, 2019-01-01T22:07:28, Epoch: 7, Batch: 790, Training Loss: 0.12267715260386466, LR: 0.1
Time, 2019-01-01T22:07:29, Epoch: 7, Batch: 800, Training Loss: 0.14748686254024507, LR: 0.1
Time, 2019-01-01T22:07:30, Epoch: 7, Batch: 810, Training Loss: 0.10527115911245347, LR: 0.1
Time, 2019-01-01T22:07:31, Epoch: 7, Batch: 820, Training Loss: 0.13101131170988084, LR: 0.1
Time, 2019-01-01T22:07:31, Epoch: 7, Batch: 830, Training Loss: 0.11484897658228874, LR: 0.1
Time, 2019-01-01T22:07:32, Epoch: 7, Batch: 840, Training Loss: 0.0821707345545292, LR: 0.1
Time, 2019-01-01T22:07:33, Epoch: 7, Batch: 850, Training Loss: 0.06978821605443955, LR: 0.1
Time, 2019-01-01T22:07:34, Epoch: 7, Batch: 860, Training Loss: 0.13087174780666827, LR: 0.1
Time, 2019-01-01T22:07:35, Epoch: 7, Batch: 870, Training Loss: 0.0859836470335722, LR: 0.1
Time, 2019-01-01T22:07:36, Epoch: 7, Batch: 880, Training Loss: 0.12007305733859538, LR: 0.1
Time, 2019-01-01T22:07:37, Epoch: 7, Batch: 890, Training Loss: 0.1372908756136894, LR: 0.1
Time, 2019-01-01T22:07:37, Epoch: 7, Batch: 900, Training Loss: 0.11232146658003331, LR: 0.1
Time, 2019-01-01T22:07:38, Epoch: 7, Batch: 910, Training Loss: 0.15406906232237816, LR: 0.1
Time, 2019-01-01T22:07:39, Epoch: 7, Batch: 920, Training Loss: 0.10738979130983353, LR: 0.1
Time, 2019-01-01T22:07:40, Epoch: 7, Batch: 930, Training Loss: 0.11790274903178215, LR: 0.1
Epoch: 7, Validation Top 1 acc: 96.6460952758789
Epoch: 7, Validation Top 5 acc: 99.97014617919922
Epoch: 7, Validation Set Loss: 0.10269433259963989
Start training epoch 8
Time, 2019-01-01T22:07:47, Epoch: 8, Batch: 10, Training Loss: 0.14369821175932884, LR: 0.1
Time, 2019-01-01T22:07:48, Epoch: 8, Batch: 20, Training Loss: 0.10248277857899665, LR: 0.1
Time, 2019-01-01T22:07:49, Epoch: 8, Batch: 30, Training Loss: 0.10408642888069153, LR: 0.1
Time, 2019-01-01T22:07:50, Epoch: 8, Batch: 40, Training Loss: 0.1202562317252159, LR: 0.1
Time, 2019-01-01T22:07:51, Epoch: 8, Batch: 50, Training Loss: 0.1308475501835346, LR: 0.1
Time, 2019-01-01T22:07:52, Epoch: 8, Batch: 60, Training Loss: 0.12560123652219773, LR: 0.1
Time, 2019-01-01T22:07:52, Epoch: 8, Batch: 70, Training Loss: 0.1304052487015724, LR: 0.1
Time, 2019-01-01T22:07:53, Epoch: 8, Batch: 80, Training Loss: 0.12689802274107934, LR: 0.1
Time, 2019-01-01T22:07:54, Epoch: 8, Batch: 90, Training Loss: 0.15131357684731483, LR: 0.1
Time, 2019-01-01T22:07:55, Epoch: 8, Batch: 100, Training Loss: 0.14615558981895446, LR: 0.1
Time, 2019-01-01T22:07:56, Epoch: 8, Batch: 110, Training Loss: 0.13985131978988646, LR: 0.1
Time, 2019-01-01T22:07:57, Epoch: 8, Batch: 120, Training Loss: 0.13805918470025064, LR: 0.1
Time, 2019-01-01T22:07:58, Epoch: 8, Batch: 130, Training Loss: 0.2190261758863926, LR: 0.1
Time, 2019-01-01T22:07:59, Epoch: 8, Batch: 140, Training Loss: 0.11935184821486473, LR: 0.1
Time, 2019-01-01T22:08:00, Epoch: 8, Batch: 150, Training Loss: 0.13680642247200012, LR: 0.1
Time, 2019-01-01T22:08:01, Epoch: 8, Batch: 160, Training Loss: 0.10613700449466705, LR: 0.1
Time, 2019-01-01T22:08:02, Epoch: 8, Batch: 170, Training Loss: 0.09822708107531071, LR: 0.1
Time, 2019-01-01T22:08:03, Epoch: 8, Batch: 180, Training Loss: 0.10908227749168872, LR: 0.1
Time, 2019-01-01T22:08:04, Epoch: 8, Batch: 190, Training Loss: 0.09733166359364986, LR: 0.1
Time, 2019-01-01T22:08:05, Epoch: 8, Batch: 200, Training Loss: 0.10570841804146766, LR: 0.1
Time, 2019-01-01T22:08:06, Epoch: 8, Batch: 210, Training Loss: 0.06659113466739655, LR: 0.1
Time, 2019-01-01T22:08:07, Epoch: 8, Batch: 220, Training Loss: 0.07912202328443527, LR: 0.1
Time, 2019-01-01T22:08:07, Epoch: 8, Batch: 230, Training Loss: 0.146138334274292, LR: 0.1
Time, 2019-01-01T22:08:08, Epoch: 8, Batch: 240, Training Loss: 0.11830356121063232, LR: 0.1
Time, 2019-01-01T22:08:09, Epoch: 8, Batch: 250, Training Loss: 0.11725115291774273, LR: 0.1
Time, 2019-01-01T22:08:10, Epoch: 8, Batch: 260, Training Loss: 0.11742803081870079, LR: 0.1
Time, 2019-01-01T22:08:11, Epoch: 8, Batch: 270, Training Loss: 0.16245380267500878, LR: 0.1
Time, 2019-01-01T22:08:12, Epoch: 8, Batch: 280, Training Loss: 0.1512858308851719, LR: 0.1
Time, 2019-01-01T22:08:13, Epoch: 8, Batch: 290, Training Loss: 0.15849315337836742, LR: 0.1
Time, 2019-01-01T22:08:14, Epoch: 8, Batch: 300, Training Loss: 0.20158493667840957, LR: 0.1
Time, 2019-01-01T22:08:15, Epoch: 8, Batch: 310, Training Loss: 0.2135890692472458, LR: 0.1
Time, 2019-01-01T22:08:16, Epoch: 8, Batch: 320, Training Loss: 0.23431176394224168, LR: 0.1
Time, 2019-01-01T22:08:17, Epoch: 8, Batch: 330, Training Loss: 0.21361860185861586, LR: 0.1
Time, 2019-01-01T22:08:18, Epoch: 8, Batch: 340, Training Loss: 0.18985528126358986, LR: 0.1
Time, 2019-01-01T22:08:19, Epoch: 8, Batch: 350, Training Loss: 0.2715065360069275, LR: 0.1
Time, 2019-01-01T22:08:20, Epoch: 8, Batch: 360, Training Loss: 0.2557909362018108, LR: 0.1
Time, 2019-01-01T22:08:21, Epoch: 8, Batch: 370, Training Loss: 0.16260480657219886, LR: 0.1
Time, 2019-01-01T22:08:22, Epoch: 8, Batch: 380, Training Loss: 0.11039153560996055, LR: 0.1
Time, 2019-01-01T22:08:23, Epoch: 8, Batch: 390, Training Loss: 0.13161615133285523, LR: 0.1
Time, 2019-01-01T22:08:24, Epoch: 8, Batch: 400, Training Loss: 0.12667657509446145, LR: 0.1
Time, 2019-01-01T22:08:25, Epoch: 8, Batch: 410, Training Loss: 0.1750199906527996, LR: 0.1
Time, 2019-01-01T22:08:26, Epoch: 8, Batch: 420, Training Loss: 0.13958500027656556, LR: 0.1
Time, 2019-01-01T22:08:27, Epoch: 8, Batch: 430, Training Loss: 0.18614238649606704, LR: 0.1
Time, 2019-01-01T22:08:28, Epoch: 8, Batch: 440, Training Loss: 0.17756056785583496, LR: 0.1
Time, 2019-01-01T22:08:29, Epoch: 8, Batch: 450, Training Loss: 0.1464150182902813, LR: 0.1
Time, 2019-01-01T22:08:30, Epoch: 8, Batch: 460, Training Loss: 0.16846480295062066, LR: 0.1
Time, 2019-01-01T22:08:31, Epoch: 8, Batch: 470, Training Loss: 0.10790050104260444, LR: 0.1
Time, 2019-01-01T22:08:32, Epoch: 8, Batch: 480, Training Loss: 0.16067602820694446, LR: 0.1
Time, 2019-01-01T22:08:33, Epoch: 8, Batch: 490, Training Loss: 0.12874815091490746, LR: 0.1
Time, 2019-01-01T22:08:34, Epoch: 8, Batch: 500, Training Loss: 0.17862518616020678, LR: 0.1
Time, 2019-01-01T22:08:35, Epoch: 8, Batch: 510, Training Loss: 0.10961848683655262, LR: 0.1
Time, 2019-01-01T22:08:36, Epoch: 8, Batch: 520, Training Loss: 0.12493159621953964, LR: 0.1
Time, 2019-01-01T22:08:37, Epoch: 8, Batch: 530, Training Loss: 0.11902869790792465, LR: 0.1
Time, 2019-01-01T22:08:38, Epoch: 8, Batch: 540, Training Loss: 0.13619558960199357, LR: 0.1
Time, 2019-01-01T22:08:39, Epoch: 8, Batch: 550, Training Loss: 0.12166293486952781, LR: 0.1
Time, 2019-01-01T22:08:40, Epoch: 8, Batch: 560, Training Loss: 0.1144729919731617, LR: 0.1
Time, 2019-01-01T22:08:40, Epoch: 8, Batch: 570, Training Loss: 0.12562118768692015, LR: 0.1
Time, 2019-01-01T22:08:41, Epoch: 8, Batch: 580, Training Loss: 0.11087177246809006, LR: 0.1
Time, 2019-01-01T22:08:42, Epoch: 8, Batch: 590, Training Loss: 0.13970616608858108, LR: 0.1
Time, 2019-01-01T22:08:43, Epoch: 8, Batch: 600, Training Loss: 0.10334374383091927, LR: 0.1
Time, 2019-01-01T22:08:44, Epoch: 8, Batch: 610, Training Loss: 0.11805622056126594, LR: 0.1
Time, 2019-01-01T22:08:45, Epoch: 8, Batch: 620, Training Loss: 0.11595387160778045, LR: 0.1
Time, 2019-01-01T22:08:46, Epoch: 8, Batch: 630, Training Loss: 0.1326231487095356, LR: 0.1
Time, 2019-01-01T22:08:47, Epoch: 8, Batch: 640, Training Loss: 0.13739111348986627, LR: 0.1
Time, 2019-01-01T22:08:48, Epoch: 8, Batch: 650, Training Loss: 0.12391175404191017, LR: 0.1
Time, 2019-01-01T22:08:49, Epoch: 8, Batch: 660, Training Loss: 0.12368416264653206, LR: 0.1
Time, 2019-01-01T22:08:49, Epoch: 8, Batch: 670, Training Loss: 0.09717275947332382, LR: 0.1
Time, 2019-01-01T22:08:50, Epoch: 8, Batch: 680, Training Loss: 0.1294203769415617, LR: 0.1
Time, 2019-01-01T22:08:51, Epoch: 8, Batch: 690, Training Loss: 0.0895646408200264, LR: 0.1
Time, 2019-01-01T22:08:52, Epoch: 8, Batch: 700, Training Loss: 0.10870954692363739, LR: 0.1
Time, 2019-01-01T22:08:53, Epoch: 8, Batch: 710, Training Loss: 0.12438480630517006, LR: 0.1
Time, 2019-01-01T22:08:54, Epoch: 8, Batch: 720, Training Loss: 0.10382577404379845, LR: 0.1
Time, 2019-01-01T22:08:55, Epoch: 8, Batch: 730, Training Loss: 0.16104986742138863, LR: 0.1
Time, 2019-01-01T22:08:56, Epoch: 8, Batch: 740, Training Loss: 0.1299179933965206, LR: 0.1
Time, 2019-01-01T22:08:57, Epoch: 8, Batch: 750, Training Loss: 0.11567886918783188, LR: 0.1
Time, 2019-01-01T22:08:58, Epoch: 8, Batch: 760, Training Loss: 0.1386319302022457, LR: 0.1
Time, 2019-01-01T22:08:59, Epoch: 8, Batch: 770, Training Loss: 0.1095113217830658, LR: 0.1
Time, 2019-01-01T22:09:00, Epoch: 8, Batch: 780, Training Loss: 0.10395495146512986, LR: 0.1
Time, 2019-01-01T22:09:01, Epoch: 8, Batch: 790, Training Loss: 0.11008487343788147, LR: 0.1
Time, 2019-01-01T22:09:02, Epoch: 8, Batch: 800, Training Loss: 0.11844401136040687, LR: 0.1
Time, 2019-01-01T22:09:03, Epoch: 8, Batch: 810, Training Loss: 0.12552832439541817, LR: 0.1
Time, 2019-01-01T22:09:04, Epoch: 8, Batch: 820, Training Loss: 0.13337073102593422, LR: 0.1
Time, 2019-01-01T22:09:05, Epoch: 8, Batch: 830, Training Loss: 0.0920336052775383, LR: 0.1
Time, 2019-01-01T22:09:06, Epoch: 8, Batch: 840, Training Loss: 0.08357551693916321, LR: 0.1
Time, 2019-01-01T22:09:06, Epoch: 8, Batch: 850, Training Loss: 0.11918145716190338, LR: 0.1
Time, 2019-01-01T22:09:07, Epoch: 8, Batch: 860, Training Loss: 0.12931400537490845, LR: 0.1
Time, 2019-01-01T22:09:08, Epoch: 8, Batch: 870, Training Loss: 0.11728321202099323, LR: 0.1
Time, 2019-01-01T22:09:09, Epoch: 8, Batch: 880, Training Loss: 0.12280408069491386, LR: 0.1
Time, 2019-01-01T22:09:10, Epoch: 8, Batch: 890, Training Loss: 0.1426567830145359, LR: 0.1
Time, 2019-01-01T22:09:11, Epoch: 8, Batch: 900, Training Loss: 0.10904324799776077, LR: 0.1
Time, 2019-01-01T22:09:12, Epoch: 8, Batch: 910, Training Loss: 0.1395600490272045, LR: 0.1
Time, 2019-01-01T22:09:13, Epoch: 8, Batch: 920, Training Loss: 0.14852548986673356, LR: 0.1
Time, 2019-01-01T22:09:14, Epoch: 8, Batch: 930, Training Loss: 0.1155907467007637, LR: 0.1
Epoch: 8, Validation Top 1 acc: 96.10868072509766
Epoch: 8, Validation Top 5 acc: 99.93033599853516
Epoch: 8, Validation Set Loss: 0.1151764988899231
Start training epoch 9
Time, 2019-01-01T22:09:21, Epoch: 9, Batch: 10, Training Loss: 0.08631928637623787, LR: 0.1
Time, 2019-01-01T22:09:22, Epoch: 9, Batch: 20, Training Loss: 0.09484190344810486, LR: 0.1
Time, 2019-01-01T22:09:23, Epoch: 9, Batch: 30, Training Loss: 0.10069235563278198, LR: 0.1
Time, 2019-01-01T22:09:24, Epoch: 9, Batch: 40, Training Loss: 0.09331507124006748, LR: 0.1
Time, 2019-01-01T22:09:25, Epoch: 9, Batch: 50, Training Loss: 0.13657184466719627, LR: 0.1
Time, 2019-01-01T22:09:26, Epoch: 9, Batch: 60, Training Loss: 0.12762439548969268, LR: 0.1
Time, 2019-01-01T22:09:27, Epoch: 9, Batch: 70, Training Loss: 0.1256034679710865, LR: 0.1
Time, 2019-01-01T22:09:28, Epoch: 9, Batch: 80, Training Loss: 0.19258342310786247, LR: 0.1
Time, 2019-01-01T22:09:29, Epoch: 9, Batch: 90, Training Loss: 0.1306184634566307, LR: 0.1
Time, 2019-01-01T22:09:30, Epoch: 9, Batch: 100, Training Loss: 0.15289163663983346, LR: 0.1
Time, 2019-01-01T22:09:31, Epoch: 9, Batch: 110, Training Loss: 0.12918586879968644, LR: 0.1
Time, 2019-01-01T22:09:32, Epoch: 9, Batch: 120, Training Loss: 0.16851655915379524, LR: 0.1
Time, 2019-01-01T22:09:33, Epoch: 9, Batch: 130, Training Loss: 0.1346747126430273, LR: 0.1
Time, 2019-01-01T22:09:34, Epoch: 9, Batch: 140, Training Loss: 0.15295911058783532, LR: 0.1
Time, 2019-01-01T22:09:35, Epoch: 9, Batch: 150, Training Loss: 0.14101462624967098, LR: 0.1
Time, 2019-01-01T22:09:36, Epoch: 9, Batch: 160, Training Loss: 0.11555487066507339, LR: 0.1
Time, 2019-01-01T22:09:38, Epoch: 9, Batch: 170, Training Loss: 0.09846356436610222, LR: 0.1
Time, 2019-01-01T22:09:39, Epoch: 9, Batch: 180, Training Loss: 0.08327737525105476, LR: 0.1
Time, 2019-01-01T22:09:40, Epoch: 9, Batch: 190, Training Loss: 0.12741767093539239, LR: 0.1
Time, 2019-01-01T22:09:41, Epoch: 9, Batch: 200, Training Loss: 0.10771741271018982, LR: 0.1
Time, 2019-01-01T22:09:42, Epoch: 9, Batch: 210, Training Loss: 0.09323554635047912, LR: 0.1
Time, 2019-01-01T22:09:43, Epoch: 9, Batch: 220, Training Loss: 0.07462222054600716, LR: 0.1
Time, 2019-01-01T22:09:43, Epoch: 9, Batch: 230, Training Loss: 0.12126854546368122, LR: 0.1
Time, 2019-01-01T22:09:44, Epoch: 9, Batch: 240, Training Loss: 0.11601706445217133, LR: 0.1
Time, 2019-01-01T22:09:45, Epoch: 9, Batch: 250, Training Loss: 0.09671363830566407, LR: 0.1
Time, 2019-01-01T22:09:46, Epoch: 9, Batch: 260, Training Loss: 0.140209174901247, LR: 0.1
Time, 2019-01-01T22:09:47, Epoch: 9, Batch: 270, Training Loss: 0.11861688680946827, LR: 0.1
Time, 2019-01-01T22:09:48, Epoch: 9, Batch: 280, Training Loss: 0.12084967494010926, LR: 0.1
Time, 2019-01-01T22:09:49, Epoch: 9, Batch: 290, Training Loss: 0.14836394563317298, LR: 0.1
Time, 2019-01-01T22:09:50, Epoch: 9, Batch: 300, Training Loss: 0.15093930065631866, LR: 0.1
Time, 2019-01-01T22:09:51, Epoch: 9, Batch: 310, Training Loss: 0.2133880838751793, LR: 0.1
Time, 2019-01-01T22:09:52, Epoch: 9, Batch: 320, Training Loss: 0.2660724461078644, LR: 0.1
Time, 2019-01-01T22:09:52, Epoch: 9, Batch: 330, Training Loss: 0.1902014821767807, LR: 0.1
Time, 2019-01-01T22:09:53, Epoch: 9, Batch: 340, Training Loss: 0.12012147977948189, LR: 0.1
Time, 2019-01-01T22:09:54, Epoch: 9, Batch: 350, Training Loss: 0.11486718580126762, LR: 0.1
Time, 2019-01-01T22:09:55, Epoch: 9, Batch: 360, Training Loss: 0.14251912981271744, LR: 0.1
Time, 2019-01-01T22:09:56, Epoch: 9, Batch: 370, Training Loss: 0.14322225451469422, LR: 0.1
Time, 2019-01-01T22:09:57, Epoch: 9, Batch: 380, Training Loss: 0.0959839515388012, LR: 0.1
Time, 2019-01-01T22:09:58, Epoch: 9, Batch: 390, Training Loss: 0.11717545650899411, LR: 0.1
Time, 2019-01-01T22:09:59, Epoch: 9, Batch: 400, Training Loss: 0.1556312270462513, LR: 0.1
Time, 2019-01-01T22:10:00, Epoch: 9, Batch: 410, Training Loss: 0.1408505469560623, LR: 0.1
Time, 2019-01-01T22:10:01, Epoch: 9, Batch: 420, Training Loss: 0.08850434683263302, LR: 0.1
Time, 2019-01-01T22:10:02, Epoch: 9, Batch: 430, Training Loss: 0.10036275759339333, LR: 0.1
Time, 2019-01-01T22:10:02, Epoch: 9, Batch: 440, Training Loss: 0.13432334624230863, LR: 0.1
Time, 2019-01-01T22:10:03, Epoch: 9, Batch: 450, Training Loss: 0.11481448449194431, LR: 0.1
Time, 2019-01-01T22:10:04, Epoch: 9, Batch: 460, Training Loss: 0.10835541784763336, LR: 0.1
Time, 2019-01-01T22:10:05, Epoch: 9, Batch: 470, Training Loss: 0.08617413714528084, LR: 0.1
Time, 2019-01-01T22:10:06, Epoch: 9, Batch: 480, Training Loss: 0.12680083364248276, LR: 0.1
Time, 2019-01-01T22:10:07, Epoch: 9, Batch: 490, Training Loss: 0.11714059710502625, LR: 0.1
Time, 2019-01-01T22:10:08, Epoch: 9, Batch: 500, Training Loss: 0.1733051620423794, LR: 0.1
Time, 2019-01-01T22:10:09, Epoch: 9, Batch: 510, Training Loss: 0.14635137245059013, LR: 0.1
Time, 2019-01-01T22:10:10, Epoch: 9, Batch: 520, Training Loss: 0.14004454687237738, LR: 0.1
Time, 2019-01-01T22:10:11, Epoch: 9, Batch: 530, Training Loss: 0.12398601919412613, LR: 0.1
Time, 2019-01-01T22:10:12, Epoch: 9, Batch: 540, Training Loss: 0.10891580656170845, LR: 0.1
Time, 2019-01-01T22:10:12, Epoch: 9, Batch: 550, Training Loss: 0.091278775036335, LR: 0.1
Time, 2019-01-01T22:10:13, Epoch: 9, Batch: 560, Training Loss: 0.15485776215791702, LR: 0.1
Time, 2019-01-01T22:10:14, Epoch: 9, Batch: 570, Training Loss: 0.13004739880561828, LR: 0.1
Time, 2019-01-01T22:10:15, Epoch: 9, Batch: 580, Training Loss: 0.1206875503063202, LR: 0.1
Time, 2019-01-01T22:10:16, Epoch: 9, Batch: 590, Training Loss: 0.16491419598460197, LR: 0.1
Time, 2019-01-01T22:10:17, Epoch: 9, Batch: 600, Training Loss: 0.15604621842503547, LR: 0.1
Time, 2019-01-01T22:10:18, Epoch: 9, Batch: 610, Training Loss: 0.15491943508386613, LR: 0.1
Time, 2019-01-01T22:10:19, Epoch: 9, Batch: 620, Training Loss: 0.16475822925567626, LR: 0.1
Time, 2019-01-01T22:10:20, Epoch: 9, Batch: 630, Training Loss: 0.1383317679166794, LR: 0.1
Time, 2019-01-01T22:10:21, Epoch: 9, Batch: 640, Training Loss: 0.2003614127635956, LR: 0.1
Time, 2019-01-01T22:10:21, Epoch: 9, Batch: 650, Training Loss: 0.1209348950535059, LR: 0.1
Time, 2019-01-01T22:10:22, Epoch: 9, Batch: 660, Training Loss: 0.11124992184340954, LR: 0.1
Time, 2019-01-01T22:10:23, Epoch: 9, Batch: 670, Training Loss: 0.10128934793174267, LR: 0.1
Time, 2019-01-01T22:10:24, Epoch: 9, Batch: 680, Training Loss: 0.1352559395134449, LR: 0.1
Time, 2019-01-01T22:10:25, Epoch: 9, Batch: 690, Training Loss: 0.14852767176926135, LR: 0.1
Time, 2019-01-01T22:10:26, Epoch: 9, Batch: 700, Training Loss: 0.141901183873415, LR: 0.1
Time, 2019-01-01T22:10:27, Epoch: 9, Batch: 710, Training Loss: 0.09780636765062808, LR: 0.1
Time, 2019-01-01T22:10:28, Epoch: 9, Batch: 720, Training Loss: 0.09840933904051781, LR: 0.1
Time, 2019-01-01T22:10:29, Epoch: 9, Batch: 730, Training Loss: 0.12294853180646896, LR: 0.1
Time, 2019-01-01T22:10:30, Epoch: 9, Batch: 740, Training Loss: 0.13695647120475768, LR: 0.1
Time, 2019-01-01T22:10:31, Epoch: 9, Batch: 750, Training Loss: 0.10238080024719239, LR: 0.1
Time, 2019-01-01T22:10:31, Epoch: 9, Batch: 760, Training Loss: 0.13752372488379477, LR: 0.1
Time, 2019-01-01T22:10:32, Epoch: 9, Batch: 770, Training Loss: 0.13229563534259797, LR: 0.1
Time, 2019-01-01T22:10:33, Epoch: 9, Batch: 780, Training Loss: 0.07955875694751739, LR: 0.1
Time, 2019-01-01T22:10:34, Epoch: 9, Batch: 790, Training Loss: 0.11972663775086403, LR: 0.1
Time, 2019-01-01T22:10:35, Epoch: 9, Batch: 800, Training Loss: 0.06965760588645935, LR: 0.1
Time, 2019-01-01T22:10:36, Epoch: 9, Batch: 810, Training Loss: 0.10045380927622319, LR: 0.1
Time, 2019-01-01T22:10:37, Epoch: 9, Batch: 820, Training Loss: 0.0909106470644474, LR: 0.1
Time, 2019-01-01T22:10:38, Epoch: 9, Batch: 830, Training Loss: 0.09715266898274422, LR: 0.1
Time, 2019-01-01T22:10:39, Epoch: 9, Batch: 840, Training Loss: 0.1323367841541767, LR: 0.1
Time, 2019-01-01T22:10:40, Epoch: 9, Batch: 850, Training Loss: 0.11595695242285728, LR: 0.1
Time, 2019-01-01T22:10:41, Epoch: 9, Batch: 860, Training Loss: 0.08014719448983669, LR: 0.1
Time, 2019-01-01T22:10:42, Epoch: 9, Batch: 870, Training Loss: 0.07397782430052757, LR: 0.1
Time, 2019-01-01T22:10:43, Epoch: 9, Batch: 880, Training Loss: 0.11840436458587647, LR: 0.1
Time, 2019-01-01T22:10:44, Epoch: 9, Batch: 890, Training Loss: 0.079322399944067, LR: 0.1
Time, 2019-01-01T22:10:45, Epoch: 9, Batch: 900, Training Loss: 0.12800227031111716, LR: 0.1
Time, 2019-01-01T22:10:47, Epoch: 9, Batch: 910, Training Loss: 0.12350692600011826, LR: 0.1
Time, 2019-01-01T22:10:48, Epoch: 9, Batch: 920, Training Loss: 0.10711873024702072, LR: 0.1
Time, 2019-01-01T22:10:49, Epoch: 9, Batch: 930, Training Loss: 0.11116653010249138, LR: 0.1
Epoch: 9, Validation Top 1 acc: 96.75556945800781
Epoch: 9, Validation Top 5 acc: 99.96018981933594
Epoch: 9, Validation Set Loss: 0.10079596191644669
Start training epoch 10
Time, 2019-01-01T22:10:57, Epoch: 10, Batch: 10, Training Loss: 0.11133279278874397, LR: 0.1
Time, 2019-01-01T22:10:58, Epoch: 10, Batch: 20, Training Loss: 0.10766562670469285, LR: 0.1
Time, 2019-01-01T22:10:58, Epoch: 10, Batch: 30, Training Loss: 0.12098071053624153, LR: 0.1
Time, 2019-01-01T22:10:59, Epoch: 10, Batch: 40, Training Loss: 0.1300615381449461, LR: 0.1
Time, 2019-01-01T22:11:00, Epoch: 10, Batch: 50, Training Loss: 0.07962991744279861, LR: 0.1
Time, 2019-01-01T22:11:01, Epoch: 10, Batch: 60, Training Loss: 0.08403775542974472, LR: 0.1
Time, 2019-01-01T22:11:03, Epoch: 10, Batch: 70, Training Loss: 0.11795580089092254, LR: 0.1
Time, 2019-01-01T22:11:04, Epoch: 10, Batch: 80, Training Loss: 0.14766552075743675, LR: 0.1
Time, 2019-01-01T22:11:05, Epoch: 10, Batch: 90, Training Loss: 0.11075162440538407, LR: 0.1
Time, 2019-01-01T22:11:06, Epoch: 10, Batch: 100, Training Loss: 0.0931267961859703, LR: 0.1
Time, 2019-01-01T22:11:07, Epoch: 10, Batch: 110, Training Loss: 0.12159411981701851, LR: 0.1
Time, 2019-01-01T22:11:07, Epoch: 10, Batch: 120, Training Loss: 0.12614632695913314, LR: 0.1
Time, 2019-01-01T22:11:08, Epoch: 10, Batch: 130, Training Loss: 0.10802989900112152, LR: 0.1
Time, 2019-01-01T22:11:09, Epoch: 10, Batch: 140, Training Loss: 0.15010862983763218, LR: 0.1
Time, 2019-01-01T22:11:10, Epoch: 10, Batch: 150, Training Loss: 0.11045480705797672, LR: 0.1
Time, 2019-01-01T22:11:11, Epoch: 10, Batch: 160, Training Loss: 0.13864158615469932, LR: 0.1
Time, 2019-01-01T22:11:12, Epoch: 10, Batch: 170, Training Loss: 0.1603171043097973, LR: 0.1
Time, 2019-01-01T22:11:13, Epoch: 10, Batch: 180, Training Loss: 0.10477591082453727, LR: 0.1
Time, 2019-01-01T22:11:14, Epoch: 10, Batch: 190, Training Loss: 0.21576824262738228, LR: 0.1
Time, 2019-01-01T22:11:15, Epoch: 10, Batch: 200, Training Loss: 0.19324590265750885, LR: 0.1
Time, 2019-01-01T22:11:16, Epoch: 10, Batch: 210, Training Loss: 0.1353297360241413, LR: 0.1
Time, 2019-01-01T22:11:17, Epoch: 10, Batch: 220, Training Loss: 0.12874554246664047, LR: 0.1
Time, 2019-01-01T22:11:18, Epoch: 10, Batch: 230, Training Loss: 0.19142212867736816, LR: 0.1
Time, 2019-01-01T22:11:19, Epoch: 10, Batch: 240, Training Loss: 0.17293774262070655, LR: 0.1
Time, 2019-01-01T22:11:20, Epoch: 10, Batch: 250, Training Loss: 0.13789690881967545, LR: 0.1
Time, 2019-01-01T22:11:21, Epoch: 10, Batch: 260, Training Loss: 0.12213038429617881, LR: 0.1
Time, 2019-01-01T22:11:22, Epoch: 10, Batch: 270, Training Loss: 0.19093453288078308, LR: 0.1
Time, 2019-01-01T22:11:23, Epoch: 10, Batch: 280, Training Loss: 0.1848932147026062, LR: 0.1
Time, 2019-01-01T22:11:23, Epoch: 10, Batch: 290, Training Loss: 0.2365190491080284, LR: 0.1
Time, 2019-01-01T22:11:24, Epoch: 10, Batch: 300, Training Loss: 0.16348133310675622, LR: 0.1
Time, 2019-01-01T22:11:25, Epoch: 10, Batch: 310, Training Loss: 0.18898014426231385, LR: 0.1
Time, 2019-01-01T22:11:26, Epoch: 10, Batch: 320, Training Loss: 0.18491149842739105, LR: 0.1
Time, 2019-01-01T22:11:27, Epoch: 10, Batch: 330, Training Loss: 0.23574379459023476, LR: 0.1
Time, 2019-01-01T22:11:28, Epoch: 10, Batch: 340, Training Loss: 0.2157551109790802, LR: 0.1
Time, 2019-01-01T22:11:29, Epoch: 10, Batch: 350, Training Loss: 0.21063399985432624, LR: 0.1
Time, 2019-01-01T22:11:30, Epoch: 10, Batch: 360, Training Loss: 0.23916460573673248, LR: 0.1
Time, 2019-01-01T22:11:31, Epoch: 10, Batch: 370, Training Loss: 0.22111921310424804, LR: 0.1
Time, 2019-01-01T22:11:32, Epoch: 10, Batch: 380, Training Loss: 0.2616253197193146, LR: 0.1
Time, 2019-01-01T22:11:33, Epoch: 10, Batch: 390, Training Loss: 0.3141014963388443, LR: 0.1
Time, 2019-01-01T22:11:34, Epoch: 10, Batch: 400, Training Loss: 0.329165555536747, LR: 0.1
Time, 2019-01-01T22:11:35, Epoch: 10, Batch: 410, Training Loss: 0.23958011344075203, LR: 0.1
Time, 2019-01-01T22:11:36, Epoch: 10, Batch: 420, Training Loss: 0.20433664992451667, LR: 0.1
Time, 2019-01-01T22:11:37, Epoch: 10, Batch: 430, Training Loss: 0.18709587901830674, LR: 0.1
Time, 2019-01-01T22:11:38, Epoch: 10, Batch: 440, Training Loss: 0.2231904923915863, LR: 0.1
Time, 2019-01-01T22:11:38, Epoch: 10, Batch: 450, Training Loss: 0.17694998010993004, LR: 0.1
Time, 2019-01-01T22:11:39, Epoch: 10, Batch: 460, Training Loss: 0.1656629517674446, LR: 0.1
Time, 2019-01-01T22:11:40, Epoch: 10, Batch: 470, Training Loss: 0.13652047887444496, LR: 0.1
Time, 2019-01-01T22:11:41, Epoch: 10, Batch: 480, Training Loss: 0.12418177574872971, LR: 0.1
Time, 2019-01-01T22:11:42, Epoch: 10, Batch: 490, Training Loss: 0.1707557335495949, LR: 0.1
Time, 2019-01-01T22:11:43, Epoch: 10, Batch: 500, Training Loss: 0.139071961119771, LR: 0.1
Time, 2019-01-01T22:11:44, Epoch: 10, Batch: 510, Training Loss: 0.14556510522961616, LR: 0.1
Time, 2019-01-01T22:11:45, Epoch: 10, Batch: 520, Training Loss: 0.12741154432296753, LR: 0.1
Time, 2019-01-01T22:11:46, Epoch: 10, Batch: 530, Training Loss: 0.16998853646218776, LR: 0.1
Time, 2019-01-01T22:11:47, Epoch: 10, Batch: 540, Training Loss: 0.14509770572185515, LR: 0.1
Time, 2019-01-01T22:11:48, Epoch: 10, Batch: 550, Training Loss: 0.16405439078807832, LR: 0.1
Time, 2019-01-01T22:11:48, Epoch: 10, Batch: 560, Training Loss: 0.1854322612285614, LR: 0.1
Time, 2019-01-01T22:11:49, Epoch: 10, Batch: 570, Training Loss: 0.15712044537067413, LR: 0.1
Time, 2019-01-01T22:11:50, Epoch: 10, Batch: 580, Training Loss: 0.19937521368265151, LR: 0.1
Time, 2019-01-01T22:11:51, Epoch: 10, Batch: 590, Training Loss: 0.15299499183893203, LR: 0.1
Time, 2019-01-01T22:11:52, Epoch: 10, Batch: 600, Training Loss: 0.14194987192749978, LR: 0.1
Time, 2019-01-01T22:11:53, Epoch: 10, Batch: 610, Training Loss: 0.20283888205885886, LR: 0.1
Time, 2019-01-01T22:11:54, Epoch: 10, Batch: 620, Training Loss: 0.1562591440975666, LR: 0.1
Time, 2019-01-01T22:11:55, Epoch: 10, Batch: 630, Training Loss: 0.19163685962557792, LR: 0.1
Time, 2019-01-01T22:11:56, Epoch: 10, Batch: 640, Training Loss: 0.14178816080093384, LR: 0.1
Time, 2019-01-01T22:11:57, Epoch: 10, Batch: 650, Training Loss: 0.17482053488492966, LR: 0.1
Time, 2019-01-01T22:11:58, Epoch: 10, Batch: 660, Training Loss: 0.16914840862154962, LR: 0.1
Time, 2019-01-01T22:11:59, Epoch: 10, Batch: 670, Training Loss: 0.17112713307142258, LR: 0.1
Time, 2019-01-01T22:12:00, Epoch: 10, Batch: 680, Training Loss: 0.1235091395676136, LR: 0.1
Time, 2019-01-01T22:12:01, Epoch: 10, Batch: 690, Training Loss: 0.12411857321858406, LR: 0.1
Time, 2019-01-01T22:12:02, Epoch: 10, Batch: 700, Training Loss: 0.15557164400815965, LR: 0.1
Time, 2019-01-01T22:12:03, Epoch: 10, Batch: 710, Training Loss: 0.1529015153646469, LR: 0.1
Time, 2019-01-01T22:12:04, Epoch: 10, Batch: 720, Training Loss: 0.11753624305129051, LR: 0.1
Time, 2019-01-01T22:12:05, Epoch: 10, Batch: 730, Training Loss: 0.15343326330184937, LR: 0.1
Time, 2019-01-01T22:12:07, Epoch: 10, Batch: 740, Training Loss: 0.13148288205265998, LR: 0.1
Time, 2019-01-01T22:12:07, Epoch: 10, Batch: 750, Training Loss: 0.13355504721403122, LR: 0.1
Time, 2019-01-01T22:12:08, Epoch: 10, Batch: 760, Training Loss: 0.14048928022384644, LR: 0.1
Time, 2019-01-01T22:12:09, Epoch: 10, Batch: 770, Training Loss: 0.12938847802579403, LR: 0.1
Time, 2019-01-01T22:12:10, Epoch: 10, Batch: 780, Training Loss: 0.16354798078536986, LR: 0.1
Time, 2019-01-01T22:12:11, Epoch: 10, Batch: 790, Training Loss: 0.147700747102499, LR: 0.1
Time, 2019-01-01T22:12:12, Epoch: 10, Batch: 800, Training Loss: 0.1806630864739418, LR: 0.1
Time, 2019-01-01T22:12:13, Epoch: 10, Batch: 810, Training Loss: 0.1812827005982399, LR: 0.1
Time, 2019-01-01T22:12:14, Epoch: 10, Batch: 820, Training Loss: 0.1649813398718834, LR: 0.1
Time, 2019-01-01T22:12:15, Epoch: 10, Batch: 830, Training Loss: 0.1356283962726593, LR: 0.1
Time, 2019-01-01T22:12:15, Epoch: 10, Batch: 840, Training Loss: 0.11663456968963146, LR: 0.1
Time, 2019-01-01T22:12:16, Epoch: 10, Batch: 850, Training Loss: 0.11494271494448186, LR: 0.1
Time, 2019-01-01T22:12:17, Epoch: 10, Batch: 860, Training Loss: 0.1405550241470337, LR: 0.1
Time, 2019-01-01T22:12:18, Epoch: 10, Batch: 870, Training Loss: 0.16357409954071045, LR: 0.1
Time, 2019-01-01T22:12:19, Epoch: 10, Batch: 880, Training Loss: 0.18556752800941467, LR: 0.1
Time, 2019-01-01T22:12:20, Epoch: 10, Batch: 890, Training Loss: 0.16445152536034585, LR: 0.1
Time, 2019-01-01T22:12:21, Epoch: 10, Batch: 900, Training Loss: 0.17790903076529502, LR: 0.1
Time, 2019-01-01T22:12:22, Epoch: 10, Batch: 910, Training Loss: 0.19365706890821457, LR: 0.1
Time, 2019-01-01T22:12:22, Epoch: 10, Batch: 920, Training Loss: 0.17747034579515458, LR: 0.1
Time, 2019-01-01T22:12:23, Epoch: 10, Batch: 930, Training Loss: 0.15246481522917749, LR: 0.1
Epoch: 10, Validation Top 1 acc: 95.01393127441406
Epoch: 10, Validation Top 5 acc: 99.96018981933594
Epoch: 10, Validation Set Loss: 0.15289881825447083
Start training epoch 11
Time, 2019-01-01T22:12:31, Epoch: 11, Batch: 10, Training Loss: 0.13811846561729907, LR: 0.1
Time, 2019-01-01T22:12:31, Epoch: 11, Batch: 20, Training Loss: 0.13630521818995475, LR: 0.1
Time, 2019-01-01T22:12:32, Epoch: 11, Batch: 30, Training Loss: 0.1805142991244793, LR: 0.1
Time, 2019-01-01T22:12:33, Epoch: 11, Batch: 40, Training Loss: 0.23597210496664048, LR: 0.1
Time, 2019-01-01T22:12:34, Epoch: 11, Batch: 50, Training Loss: 0.21397728174924852, LR: 0.1
Time, 2019-01-01T22:12:35, Epoch: 11, Batch: 60, Training Loss: 0.18037279620766639, LR: 0.1
Time, 2019-01-01T22:12:36, Epoch: 11, Batch: 70, Training Loss: 0.17493028491735457, LR: 0.1
Time, 2019-01-01T22:12:37, Epoch: 11, Batch: 80, Training Loss: 0.21536048203706742, LR: 0.1
Time, 2019-01-01T22:12:38, Epoch: 11, Batch: 90, Training Loss: 0.23397089391946793, LR: 0.1
Time, 2019-01-01T22:12:39, Epoch: 11, Batch: 100, Training Loss: 0.28407076746225357, LR: 0.1
Time, 2019-01-01T22:12:40, Epoch: 11, Batch: 110, Training Loss: 0.1775755189359188, LR: 0.1
Time, 2019-01-01T22:12:41, Epoch: 11, Batch: 120, Training Loss: 0.16734374240040778, LR: 0.1
Time, 2019-01-01T22:12:42, Epoch: 11, Batch: 130, Training Loss: 0.15244440883398055, LR: 0.1
Time, 2019-01-01T22:12:43, Epoch: 11, Batch: 140, Training Loss: 0.11357195004820823, LR: 0.1
Time, 2019-01-01T22:12:43, Epoch: 11, Batch: 150, Training Loss: 0.10888580307364464, LR: 0.1
Time, 2019-01-01T22:12:44, Epoch: 11, Batch: 160, Training Loss: 0.1357384253293276, LR: 0.1
Time, 2019-01-01T22:12:45, Epoch: 11, Batch: 170, Training Loss: 0.13447740077972412, LR: 0.1
Time, 2019-01-01T22:12:46, Epoch: 11, Batch: 180, Training Loss: 0.2062006488442421, LR: 0.1
Time, 2019-01-01T22:12:47, Epoch: 11, Batch: 190, Training Loss: 0.1288475811481476, LR: 0.1
Time, 2019-01-01T22:12:48, Epoch: 11, Batch: 200, Training Loss: 0.2225305140018463, LR: 0.1
Time, 2019-01-01T22:12:49, Epoch: 11, Batch: 210, Training Loss: 0.17475876361131668, LR: 0.1
Time, 2019-01-01T22:12:50, Epoch: 11, Batch: 220, Training Loss: 0.15010115653276443, LR: 0.1
Time, 2019-01-01T22:12:50, Epoch: 11, Batch: 230, Training Loss: 0.14748814180493355, LR: 0.1
Time, 2019-01-01T22:12:51, Epoch: 11, Batch: 240, Training Loss: 0.18598079979419707, LR: 0.1
Time, 2019-01-01T22:12:52, Epoch: 11, Batch: 250, Training Loss: 0.13045572489500046, LR: 0.1
Time, 2019-01-01T22:12:53, Epoch: 11, Batch: 260, Training Loss: 0.15703118443489075, LR: 0.1
Time, 2019-01-01T22:12:54, Epoch: 11, Batch: 270, Training Loss: 0.16879844292998314, LR: 0.1
Time, 2019-01-01T22:12:54, Epoch: 11, Batch: 280, Training Loss: 0.17436244636774062, LR: 0.1
Time, 2019-01-01T22:12:55, Epoch: 11, Batch: 290, Training Loss: 0.19673846513032914, LR: 0.1
Time, 2019-01-01T22:12:56, Epoch: 11, Batch: 300, Training Loss: 0.155781127512455, LR: 0.1
Time, 2019-01-01T22:12:57, Epoch: 11, Batch: 310, Training Loss: 0.17399262338876725, LR: 0.1
Time, 2019-01-01T22:12:58, Epoch: 11, Batch: 320, Training Loss: 0.14575800448656082, LR: 0.1
Time, 2019-01-01T22:12:59, Epoch: 11, Batch: 330, Training Loss: 0.15069810971617698, LR: 0.1
Time, 2019-01-01T22:13:00, Epoch: 11, Batch: 340, Training Loss: 0.12836036160588266, LR: 0.1
Time, 2019-01-01T22:13:01, Epoch: 11, Batch: 350, Training Loss: 0.16767886243760585, LR: 0.1
Time, 2019-01-01T22:13:02, Epoch: 11, Batch: 360, Training Loss: 0.1416358008980751, LR: 0.1
Time, 2019-01-01T22:13:03, Epoch: 11, Batch: 370, Training Loss: 0.17655781134963036, LR: 0.1
Time, 2019-01-01T22:13:03, Epoch: 11, Batch: 380, Training Loss: 0.17866640239953996, LR: 0.1
Time, 2019-01-01T22:13:04, Epoch: 11, Batch: 390, Training Loss: 0.13252412602305413, LR: 0.1
Time, 2019-01-01T22:13:05, Epoch: 11, Batch: 400, Training Loss: 0.15608191676437855, LR: 0.1
Time, 2019-01-01T22:13:06, Epoch: 11, Batch: 410, Training Loss: 0.144496039301157, LR: 0.1
Time, 2019-01-01T22:13:07, Epoch: 11, Batch: 420, Training Loss: 0.1384803842753172, LR: 0.1
Time, 2019-01-01T22:13:08, Epoch: 11, Batch: 430, Training Loss: 0.11237557753920555, LR: 0.1
Time, 2019-01-01T22:13:09, Epoch: 11, Batch: 440, Training Loss: 0.12439494170248508, LR: 0.1
Time, 2019-01-01T22:13:10, Epoch: 11, Batch: 450, Training Loss: 0.148075919598341, LR: 0.1
Time, 2019-01-01T22:13:10, Epoch: 11, Batch: 460, Training Loss: 0.1111202783882618, LR: 0.1
Time, 2019-01-01T22:13:11, Epoch: 11, Batch: 470, Training Loss: 0.14258121475577354, LR: 0.1
Time, 2019-01-01T22:13:12, Epoch: 11, Batch: 480, Training Loss: 0.10314206928014755, LR: 0.1
Time, 2019-01-01T22:13:13, Epoch: 11, Batch: 490, Training Loss: 0.13101413771510123, LR: 0.1
Time, 2019-01-01T22:13:14, Epoch: 11, Batch: 500, Training Loss: 0.13517995551228523, LR: 0.1
Time, 2019-01-01T22:13:15, Epoch: 11, Batch: 510, Training Loss: 0.1757406886667013, LR: 0.1
Time, 2019-01-01T22:13:16, Epoch: 11, Batch: 520, Training Loss: 0.1276126317679882, LR: 0.1
Time, 2019-01-01T22:13:17, Epoch: 11, Batch: 530, Training Loss: 0.18115729689598084, LR: 0.1
Time, 2019-01-01T22:13:18, Epoch: 11, Batch: 540, Training Loss: 0.20667971521615983, LR: 0.1
Time, 2019-01-01T22:13:19, Epoch: 11, Batch: 550, Training Loss: 0.151576479524374, LR: 0.1
Time, 2019-01-01T22:13:20, Epoch: 11, Batch: 560, Training Loss: 0.17946544513106347, LR: 0.1
Time, 2019-01-01T22:13:21, Epoch: 11, Batch: 570, Training Loss: 0.19998692721128464, LR: 0.1
Time, 2019-01-01T22:13:22, Epoch: 11, Batch: 580, Training Loss: 0.2634929738938808, LR: 0.1
Time, 2019-01-01T22:13:23, Epoch: 11, Batch: 590, Training Loss: 0.24806200265884398, LR: 0.1
Time, 2019-01-01T22:13:24, Epoch: 11, Batch: 600, Training Loss: 0.20670462995767594, LR: 0.1
Time, 2019-01-01T22:13:24, Epoch: 11, Batch: 610, Training Loss: 0.15269367024302483, LR: 0.1
Time, 2019-01-01T22:13:25, Epoch: 11, Batch: 620, Training Loss: 0.16624689921736718, LR: 0.1
Time, 2019-01-01T22:13:26, Epoch: 11, Batch: 630, Training Loss: 0.21497727334499359, LR: 0.1
Time, 2019-01-01T22:13:27, Epoch: 11, Batch: 640, Training Loss: 0.21607196033000947, LR: 0.1
Time, 2019-01-01T22:13:28, Epoch: 11, Batch: 650, Training Loss: 0.18272506445646286, LR: 0.1
Time, 2019-01-01T22:13:29, Epoch: 11, Batch: 660, Training Loss: 0.18339648768305777, LR: 0.1
Time, 2019-01-01T22:13:30, Epoch: 11, Batch: 670, Training Loss: 0.21885715946555137, LR: 0.1
Time, 2019-01-01T22:13:31, Epoch: 11, Batch: 680, Training Loss: 0.15247193314135074, LR: 0.1
Time, 2019-01-01T22:13:32, Epoch: 11, Batch: 690, Training Loss: 0.13551743924617768, LR: 0.1
Time, 2019-01-01T22:13:33, Epoch: 11, Batch: 700, Training Loss: 0.1680603303015232, LR: 0.1
Time, 2019-01-01T22:13:34, Epoch: 11, Batch: 710, Training Loss: 0.14381145015358926, LR: 0.1
Time, 2019-01-01T22:13:35, Epoch: 11, Batch: 720, Training Loss: 0.18660837635397912, LR: 0.1
Time, 2019-01-01T22:13:36, Epoch: 11, Batch: 730, Training Loss: 0.18774555772542953, LR: 0.1
Time, 2019-01-01T22:13:37, Epoch: 11, Batch: 740, Training Loss: 0.14327642135322094, LR: 0.1
Time, 2019-01-01T22:13:38, Epoch: 11, Batch: 750, Training Loss: 0.1556515023112297, LR: 0.1
Time, 2019-01-01T22:13:39, Epoch: 11, Batch: 760, Training Loss: 0.1416730247437954, LR: 0.1
Time, 2019-01-01T22:13:39, Epoch: 11, Batch: 770, Training Loss: 0.14165789410471916, LR: 0.1
Time, 2019-01-01T22:13:40, Epoch: 11, Batch: 780, Training Loss: 0.09738666862249375, LR: 0.1
Time, 2019-01-01T22:13:41, Epoch: 11, Batch: 790, Training Loss: 0.12845872193574906, LR: 0.1
Time, 2019-01-01T22:13:42, Epoch: 11, Batch: 800, Training Loss: 0.13450162410736083, LR: 0.1
Time, 2019-01-01T22:13:43, Epoch: 11, Batch: 810, Training Loss: 0.1418560229241848, LR: 0.1
Time, 2019-01-01T22:13:44, Epoch: 11, Batch: 820, Training Loss: 0.13355154320597648, LR: 0.1
Time, 2019-01-01T22:13:45, Epoch: 11, Batch: 830, Training Loss: 0.1361476458609104, LR: 0.1
Time, 2019-01-01T22:13:45, Epoch: 11, Batch: 840, Training Loss: 0.10108706951141358, LR: 0.1
Time, 2019-01-01T22:13:46, Epoch: 11, Batch: 850, Training Loss: 0.11504904739558697, LR: 0.1
Time, 2019-01-01T22:13:47, Epoch: 11, Batch: 860, Training Loss: 0.15810262337327002, LR: 0.1
Time, 2019-01-01T22:13:48, Epoch: 11, Batch: 870, Training Loss: 0.12726213373243808, LR: 0.1
Time, 2019-01-01T22:13:49, Epoch: 11, Batch: 880, Training Loss: 0.12156164422631263, LR: 0.1
Time, 2019-01-01T22:13:50, Epoch: 11, Batch: 890, Training Loss: 0.1253940150141716, LR: 0.1
Time, 2019-01-01T22:13:51, Epoch: 11, Batch: 900, Training Loss: 0.12648995965719223, LR: 0.1
Time, 2019-01-01T22:13:52, Epoch: 11, Batch: 910, Training Loss: 0.11325090825557708, LR: 0.1
Time, 2019-01-01T22:13:53, Epoch: 11, Batch: 920, Training Loss: 0.08632857985794544, LR: 0.1
Time, 2019-01-01T22:13:54, Epoch: 11, Batch: 930, Training Loss: 0.15816687420010567, LR: 0.1
Epoch: 11, Validation Top 1 acc: 95.50159454345703
Epoch: 11, Validation Top 5 acc: 99.92037963867188
Epoch: 11, Validation Set Loss: 0.14442922174930573
Start training epoch 12
Time, 2019-01-01T22:14:01, Epoch: 12, Batch: 10, Training Loss: 0.14825976490974427, LR: 0.1
Time, 2019-01-01T22:14:02, Epoch: 12, Batch: 20, Training Loss: 0.09526183418929576, LR: 0.1
Time, 2019-01-01T22:14:03, Epoch: 12, Batch: 30, Training Loss: 0.07445494756102562, LR: 0.1
Time, 2019-01-01T22:14:03, Epoch: 12, Batch: 40, Training Loss: 0.1185952927917242, LR: 0.1
Time, 2019-01-01T22:14:04, Epoch: 12, Batch: 50, Training Loss: 0.11466925106942653, LR: 0.1
Time, 2019-01-01T22:14:05, Epoch: 12, Batch: 60, Training Loss: 0.1090763382613659, LR: 0.1
Time, 2019-01-01T22:14:06, Epoch: 12, Batch: 70, Training Loss: 0.10116109848022461, LR: 0.1
Time, 2019-01-01T22:14:07, Epoch: 12, Batch: 80, Training Loss: 0.09575112983584404, LR: 0.1
Time, 2019-01-01T22:14:08, Epoch: 12, Batch: 90, Training Loss: 0.10383446887135506, LR: 0.1
Time, 2019-01-01T22:14:09, Epoch: 12, Batch: 100, Training Loss: 0.11298970282077789, LR: 0.1
Time, 2019-01-01T22:14:10, Epoch: 12, Batch: 110, Training Loss: 0.11816874668002128, LR: 0.1
Time, 2019-01-01T22:14:11, Epoch: 12, Batch: 120, Training Loss: 0.1104794442653656, LR: 0.1
Time, 2019-01-01T22:14:12, Epoch: 12, Batch: 130, Training Loss: 0.1488727003335953, LR: 0.1
Time, 2019-01-01T22:14:13, Epoch: 12, Batch: 140, Training Loss: 0.1651095397770405, LR: 0.1
Time, 2019-01-01T22:14:14, Epoch: 12, Batch: 150, Training Loss: 0.1828876703977585, LR: 0.1
Time, 2019-01-01T22:14:15, Epoch: 12, Batch: 160, Training Loss: 0.13046297878026963, LR: 0.1
Time, 2019-01-01T22:14:15, Epoch: 12, Batch: 170, Training Loss: 0.1064850702881813, LR: 0.1
Time, 2019-01-01T22:14:16, Epoch: 12, Batch: 180, Training Loss: 0.16482916325330735, LR: 0.1
Time, 2019-01-01T22:14:17, Epoch: 12, Batch: 190, Training Loss: 0.10725052505731583, LR: 0.1
Time, 2019-01-01T22:14:18, Epoch: 12, Batch: 200, Training Loss: 0.14270104020833968, LR: 0.1
Time, 2019-01-01T22:14:19, Epoch: 12, Batch: 210, Training Loss: 0.09530735202133656, LR: 0.1
Time, 2019-01-01T22:14:20, Epoch: 12, Batch: 220, Training Loss: 0.11410157531499862, LR: 0.1
Time, 2019-01-01T22:14:21, Epoch: 12, Batch: 230, Training Loss: 0.13542135953903198, LR: 0.1
Time, 2019-01-01T22:14:22, Epoch: 12, Batch: 240, Training Loss: 0.10818324238061905, LR: 0.1
Time, 2019-01-01T22:14:23, Epoch: 12, Batch: 250, Training Loss: 0.09201161190867424, LR: 0.1
Time, 2019-01-01T22:14:24, Epoch: 12, Batch: 260, Training Loss: 0.09961139149963856, LR: 0.1
Time, 2019-01-01T22:14:25, Epoch: 12, Batch: 270, Training Loss: 0.07573354579508304, LR: 0.1
Time, 2019-01-01T22:14:26, Epoch: 12, Batch: 280, Training Loss: 0.1504730500280857, LR: 0.1
Time, 2019-01-01T22:14:27, Epoch: 12, Batch: 290, Training Loss: 0.12171628549695016, LR: 0.1
Time, 2019-01-01T22:14:27, Epoch: 12, Batch: 300, Training Loss: 0.12203527912497521, LR: 0.1
Time, 2019-01-01T22:14:28, Epoch: 12, Batch: 310, Training Loss: 0.14101598560810089, LR: 0.1
Time, 2019-01-01T22:14:29, Epoch: 12, Batch: 320, Training Loss: 0.10082466080784798, LR: 0.1
Time, 2019-01-01T22:14:30, Epoch: 12, Batch: 330, Training Loss: 0.09816164225339889, LR: 0.1
Time, 2019-01-01T22:14:31, Epoch: 12, Batch: 340, Training Loss: 0.09746292382478713, LR: 0.1
Time, 2019-01-01T22:14:32, Epoch: 12, Batch: 350, Training Loss: 0.09251534193754196, LR: 0.1
Time, 2019-01-01T22:14:33, Epoch: 12, Batch: 360, Training Loss: 0.08314820677042008, LR: 0.1
Time, 2019-01-01T22:14:34, Epoch: 12, Batch: 370, Training Loss: 0.10333004184067249, LR: 0.1
Time, 2019-01-01T22:14:35, Epoch: 12, Batch: 380, Training Loss: 0.0954858899116516, LR: 0.1
Time, 2019-01-01T22:14:36, Epoch: 12, Batch: 390, Training Loss: 0.10617471113801003, LR: 0.1
Time, 2019-01-01T22:14:37, Epoch: 12, Batch: 400, Training Loss: 0.11837630495429038, LR: 0.1
Time, 2019-01-01T22:14:37, Epoch: 12, Batch: 410, Training Loss: 0.1007753036916256, LR: 0.1
Time, 2019-01-01T22:14:38, Epoch: 12, Batch: 420, Training Loss: 0.14283351376652717, LR: 0.1
Time, 2019-01-01T22:14:39, Epoch: 12, Batch: 430, Training Loss: 0.09718516618013381, LR: 0.1
Time, 2019-01-01T22:14:40, Epoch: 12, Batch: 440, Training Loss: 0.08903635516762734, LR: 0.1
Time, 2019-01-01T22:14:41, Epoch: 12, Batch: 450, Training Loss: 0.12327526286244392, LR: 0.1
Time, 2019-01-01T22:14:42, Epoch: 12, Batch: 460, Training Loss: 0.07901387736201286, LR: 0.1
Time, 2019-01-01T22:14:43, Epoch: 12, Batch: 470, Training Loss: 0.07194114997982978, LR: 0.1
Time, 2019-01-01T22:14:44, Epoch: 12, Batch: 480, Training Loss: 0.09494477733969689, LR: 0.1
Time, 2019-01-01T22:14:45, Epoch: 12, Batch: 490, Training Loss: 0.10974105149507522, LR: 0.1
Time, 2019-01-01T22:14:46, Epoch: 12, Batch: 500, Training Loss: 0.08452708050608634, LR: 0.1
Time, 2019-01-01T22:14:47, Epoch: 12, Batch: 510, Training Loss: 0.10501431822776794, LR: 0.1
Time, 2019-01-01T22:14:48, Epoch: 12, Batch: 520, Training Loss: 0.08097875714302064, LR: 0.1
Time, 2019-01-01T22:14:48, Epoch: 12, Batch: 530, Training Loss: 0.15981655865907668, LR: 0.1
Time, 2019-01-01T22:14:50, Epoch: 12, Batch: 540, Training Loss: 0.10265878885984421, LR: 0.1
Time, 2019-01-01T22:14:51, Epoch: 12, Batch: 550, Training Loss: 0.12848952710628508, LR: 0.1
Time, 2019-01-01T22:14:52, Epoch: 12, Batch: 560, Training Loss: 0.11306810118258, LR: 0.1
Time, 2019-01-01T22:14:52, Epoch: 12, Batch: 570, Training Loss: 0.14788769334554672, LR: 0.1
Time, 2019-01-01T22:14:53, Epoch: 12, Batch: 580, Training Loss: 0.12714066877961158, LR: 0.1
Time, 2019-01-01T22:14:54, Epoch: 12, Batch: 590, Training Loss: 0.10681382454931736, LR: 0.1
Time, 2019-01-01T22:14:55, Epoch: 12, Batch: 600, Training Loss: 0.084456305205822, LR: 0.1
Time, 2019-01-01T22:14:56, Epoch: 12, Batch: 610, Training Loss: 0.09407255575060844, LR: 0.1
Time, 2019-01-01T22:14:57, Epoch: 12, Batch: 620, Training Loss: 0.09163338989019394, LR: 0.1
Time, 2019-01-01T22:14:58, Epoch: 12, Batch: 630, Training Loss: 0.11400432512164116, LR: 0.1
Time, 2019-01-01T22:14:59, Epoch: 12, Batch: 640, Training Loss: 0.11445841118693352, LR: 0.1
Time, 2019-01-01T22:15:00, Epoch: 12, Batch: 650, Training Loss: 0.1288199957460165, LR: 0.1
Time, 2019-01-01T22:15:01, Epoch: 12, Batch: 660, Training Loss: 0.08940742686390876, LR: 0.1
Time, 2019-01-01T22:15:01, Epoch: 12, Batch: 670, Training Loss: 0.08752587363123894, LR: 0.1
Time, 2019-01-01T22:15:02, Epoch: 12, Batch: 680, Training Loss: 0.11076409816741943, LR: 0.1
Time, 2019-01-01T22:15:03, Epoch: 12, Batch: 690, Training Loss: 0.06595046296715737, LR: 0.1
Time, 2019-01-01T22:15:04, Epoch: 12, Batch: 700, Training Loss: 0.10085185766220092, LR: 0.1
Time, 2019-01-01T22:15:05, Epoch: 12, Batch: 710, Training Loss: 0.09588848575949668, LR: 0.1
Time, 2019-01-01T22:15:06, Epoch: 12, Batch: 720, Training Loss: 0.09762126505374909, LR: 0.1
Time, 2019-01-01T22:15:07, Epoch: 12, Batch: 730, Training Loss: 0.10237867683172226, LR: 0.1
Time, 2019-01-01T22:15:08, Epoch: 12, Batch: 740, Training Loss: 0.10500093325972557, LR: 0.1
Time, 2019-01-01T22:15:09, Epoch: 12, Batch: 750, Training Loss: 0.1201037347316742, LR: 0.1
Time, 2019-01-01T22:15:10, Epoch: 12, Batch: 760, Training Loss: 0.2875629276037216, LR: 0.1
Time, 2019-01-01T22:15:11, Epoch: 12, Batch: 770, Training Loss: 0.1721506081521511, LR: 0.1
Time, 2019-01-01T22:15:11, Epoch: 12, Batch: 780, Training Loss: 0.16088059544563293, LR: 0.1
Time, 2019-01-01T22:15:12, Epoch: 12, Batch: 790, Training Loss: 0.19438769295811653, LR: 0.1
Time, 2019-01-01T22:15:13, Epoch: 12, Batch: 800, Training Loss: 0.14082982316613196, LR: 0.1
Time, 2019-01-01T22:15:14, Epoch: 12, Batch: 810, Training Loss: 0.12374719381332397, LR: 0.1
Time, 2019-01-01T22:15:15, Epoch: 12, Batch: 820, Training Loss: 0.2255706623196602, LR: 0.1
Time, 2019-01-01T22:15:16, Epoch: 12, Batch: 830, Training Loss: 0.1387336827814579, LR: 0.1
Time, 2019-01-01T22:15:17, Epoch: 12, Batch: 840, Training Loss: 0.17414313107728957, LR: 0.1
Time, 2019-01-01T22:15:18, Epoch: 12, Batch: 850, Training Loss: 0.19165412411093713, LR: 0.1
Time, 2019-01-01T22:15:19, Epoch: 12, Batch: 860, Training Loss: 0.16217906922101974, LR: 0.1
Time, 2019-01-01T22:15:20, Epoch: 12, Batch: 870, Training Loss: 0.13672173172235488, LR: 0.1
Time, 2019-01-01T22:15:20, Epoch: 12, Batch: 880, Training Loss: 0.16833006963133812, LR: 0.1
Time, 2019-01-01T22:15:21, Epoch: 12, Batch: 890, Training Loss: 0.1549481213092804, LR: 0.1
Time, 2019-01-01T22:15:22, Epoch: 12, Batch: 900, Training Loss: 0.15903076939284802, LR: 0.1
Time, 2019-01-01T22:15:23, Epoch: 12, Batch: 910, Training Loss: 0.13626112267374993, LR: 0.1
Time, 2019-01-01T22:15:24, Epoch: 12, Batch: 920, Training Loss: 0.12280532494187354, LR: 0.1
Time, 2019-01-01T22:15:25, Epoch: 12, Batch: 930, Training Loss: 0.1674041859805584, LR: 0.1
Epoch: 12, Validation Top 1 acc: 96.84514617919922
Epoch: 12, Validation Top 5 acc: 99.97014617919922
Epoch: 12, Validation Set Loss: 0.09887437522411346
Start training epoch 13
Time, 2019-01-01T22:15:32, Epoch: 13, Batch: 10, Training Loss: 0.11235045045614242, LR: 0.1
Time, 2019-01-01T22:15:33, Epoch: 13, Batch: 20, Training Loss: 0.09724534302949905, LR: 0.1
Time, 2019-01-01T22:15:34, Epoch: 13, Batch: 30, Training Loss: 0.12291346862912178, LR: 0.1
Time, 2019-01-01T22:15:35, Epoch: 13, Batch: 40, Training Loss: 0.09944676458835602, LR: 0.1
Time, 2019-01-01T22:15:36, Epoch: 13, Batch: 50, Training Loss: 0.062425815314054486, LR: 0.1
Time, 2019-01-01T22:15:37, Epoch: 13, Batch: 60, Training Loss: 0.06912568919360637, LR: 0.1
Time, 2019-01-01T22:15:37, Epoch: 13, Batch: 70, Training Loss: 0.1208558700978756, LR: 0.1
Time, 2019-01-01T22:15:38, Epoch: 13, Batch: 80, Training Loss: 0.1439217146486044, LR: 0.1
Time, 2019-01-01T22:15:39, Epoch: 13, Batch: 90, Training Loss: 0.11384002640843391, LR: 0.1
Time, 2019-01-01T22:15:40, Epoch: 13, Batch: 100, Training Loss: 0.1159126952290535, LR: 0.1
Time, 2019-01-01T22:15:41, Epoch: 13, Batch: 110, Training Loss: 0.11910818964242935, LR: 0.1
Time, 2019-01-01T22:15:42, Epoch: 13, Batch: 120, Training Loss: 0.12697716504335405, LR: 0.1
Time, 2019-01-01T22:15:43, Epoch: 13, Batch: 130, Training Loss: 0.10806854888796806, LR: 0.1
Time, 2019-01-01T22:15:43, Epoch: 13, Batch: 140, Training Loss: 0.1936155930161476, LR: 0.1
Time, 2019-01-01T22:15:44, Epoch: 13, Batch: 150, Training Loss: 0.11176769211888313, LR: 0.1
Time, 2019-01-01T22:15:45, Epoch: 13, Batch: 160, Training Loss: 0.13121788948774338, LR: 0.1
Time, 2019-01-01T22:15:46, Epoch: 13, Batch: 170, Training Loss: 0.10254966393113137, LR: 0.1
Time, 2019-01-01T22:15:47, Epoch: 13, Batch: 180, Training Loss: 0.11255288645625114, LR: 0.1
Time, 2019-01-01T22:15:48, Epoch: 13, Batch: 190, Training Loss: 0.1256964761763811, LR: 0.1
Time, 2019-01-01T22:15:49, Epoch: 13, Batch: 200, Training Loss: 0.1003447338938713, LR: 0.1
Time, 2019-01-01T22:15:50, Epoch: 13, Batch: 210, Training Loss: 0.1156468853354454, LR: 0.1
Time, 2019-01-01T22:15:51, Epoch: 13, Batch: 220, Training Loss: 0.13258468434214593, LR: 0.1
Time, 2019-01-01T22:15:52, Epoch: 13, Batch: 230, Training Loss: 0.13218866065144538, LR: 0.1
Time, 2019-01-01T22:15:53, Epoch: 13, Batch: 240, Training Loss: 0.1412617929279804, LR: 0.1
Time, 2019-01-01T22:15:53, Epoch: 13, Batch: 250, Training Loss: 0.12650363594293595, LR: 0.1
Time, 2019-01-01T22:15:54, Epoch: 13, Batch: 260, Training Loss: 0.11805646121501923, LR: 0.1
Time, 2019-01-01T22:15:55, Epoch: 13, Batch: 270, Training Loss: 0.13203337788581848, LR: 0.1
Time, 2019-01-01T22:15:56, Epoch: 13, Batch: 280, Training Loss: 0.13265820667147638, LR: 0.1
Time, 2019-01-01T22:15:57, Epoch: 13, Batch: 290, Training Loss: 0.16936631873250008, LR: 0.1
Time, 2019-01-01T22:15:58, Epoch: 13, Batch: 300, Training Loss: 0.11657075807452202, LR: 0.1
Time, 2019-01-01T22:15:59, Epoch: 13, Batch: 310, Training Loss: 0.1833893172442913, LR: 0.1
Time, 2019-01-01T22:15:59, Epoch: 13, Batch: 320, Training Loss: 0.18221799954771994, LR: 0.1
Time, 2019-01-01T22:16:00, Epoch: 13, Batch: 330, Training Loss: 0.19049555957317352, LR: 0.1
Time, 2019-01-01T22:16:01, Epoch: 13, Batch: 340, Training Loss: 0.13948271423578262, LR: 0.1
Time, 2019-01-01T22:16:02, Epoch: 13, Batch: 350, Training Loss: 0.13630361258983612, LR: 0.1
Time, 2019-01-01T22:16:03, Epoch: 13, Batch: 360, Training Loss: 0.11822756230831147, LR: 0.1
Time, 2019-01-01T22:16:04, Epoch: 13, Batch: 370, Training Loss: 0.10518975593149663, LR: 0.1
Time, 2019-01-01T22:16:04, Epoch: 13, Batch: 380, Training Loss: 0.11705292388796806, LR: 0.1
Time, 2019-01-01T22:16:05, Epoch: 13, Batch: 390, Training Loss: 0.10828786417841911, LR: 0.1
Time, 2019-01-01T22:16:06, Epoch: 13, Batch: 400, Training Loss: 0.13743869140744208, LR: 0.1
Time, 2019-01-01T22:16:07, Epoch: 13, Batch: 410, Training Loss: 0.1521727919578552, LR: 0.1
Time, 2019-01-01T22:16:08, Epoch: 13, Batch: 420, Training Loss: 0.16383042186498642, LR: 0.1
Time, 2019-01-01T22:16:09, Epoch: 13, Batch: 430, Training Loss: 0.14768288880586625, LR: 0.1
Time, 2019-01-01T22:16:09, Epoch: 13, Batch: 440, Training Loss: 0.12857564277946948, LR: 0.1
Time, 2019-01-01T22:16:10, Epoch: 13, Batch: 450, Training Loss: 0.09507018215954303, LR: 0.1
Time, 2019-01-01T22:16:11, Epoch: 13, Batch: 460, Training Loss: 0.16818220913410187, LR: 0.1
Time, 2019-01-01T22:16:12, Epoch: 13, Batch: 470, Training Loss: 0.12145770862698554, LR: 0.1
Time, 2019-01-01T22:16:13, Epoch: 13, Batch: 480, Training Loss: 0.11920369565486907, LR: 0.1
Time, 2019-01-01T22:16:13, Epoch: 13, Batch: 490, Training Loss: 0.13606909960508345, LR: 0.1
Time, 2019-01-01T22:16:14, Epoch: 13, Batch: 500, Training Loss: 0.11928784996271133, LR: 0.1
Time, 2019-01-01T22:16:15, Epoch: 13, Batch: 510, Training Loss: 0.0950001336634159, LR: 0.1
Time, 2019-01-01T22:16:16, Epoch: 13, Batch: 520, Training Loss: 0.11394840627908706, LR: 0.1
Time, 2019-01-01T22:16:17, Epoch: 13, Batch: 530, Training Loss: 0.10656523406505584, LR: 0.1
Time, 2019-01-01T22:16:17, Epoch: 13, Batch: 540, Training Loss: 0.08331611715257167, LR: 0.1
Time, 2019-01-01T22:16:18, Epoch: 13, Batch: 550, Training Loss: 0.12331336811184883, LR: 0.1
Time, 2019-01-01T22:16:19, Epoch: 13, Batch: 560, Training Loss: 0.120891073346138, LR: 0.1
Time, 2019-01-01T22:16:20, Epoch: 13, Batch: 570, Training Loss: 0.12869286425411702, LR: 0.1
Time, 2019-01-01T22:16:21, Epoch: 13, Batch: 580, Training Loss: 0.12283252775669098, LR: 0.1
Time, 2019-01-01T22:16:21, Epoch: 13, Batch: 590, Training Loss: 0.12892852276563643, LR: 0.1
Time, 2019-01-01T22:16:22, Epoch: 13, Batch: 600, Training Loss: 0.15784648917615413, LR: 0.1
Time, 2019-01-01T22:16:23, Epoch: 13, Batch: 610, Training Loss: 0.10911945253610611, LR: 0.1
Time, 2019-01-01T22:16:24, Epoch: 13, Batch: 620, Training Loss: 0.1712542325258255, LR: 0.1
Time, 2019-01-01T22:16:25, Epoch: 13, Batch: 630, Training Loss: 0.11457167491316796, LR: 0.1
Time, 2019-01-01T22:16:25, Epoch: 13, Batch: 640, Training Loss: 0.10127318166196346, LR: 0.1
Time, 2019-01-01T22:16:26, Epoch: 13, Batch: 650, Training Loss: 0.11459742411971093, LR: 0.1
Time, 2019-01-01T22:16:27, Epoch: 13, Batch: 660, Training Loss: 0.12667016237974166, LR: 0.1
Time, 2019-01-01T22:16:28, Epoch: 13, Batch: 670, Training Loss: 0.12818001657724382, LR: 0.1
Time, 2019-01-01T22:16:28, Epoch: 13, Batch: 680, Training Loss: 0.13858379796147346, LR: 0.1
Time, 2019-01-01T22:16:29, Epoch: 13, Batch: 690, Training Loss: 0.16347204521298409, LR: 0.1
Time, 2019-01-01T22:16:30, Epoch: 13, Batch: 700, Training Loss: 0.14759106636047364, LR: 0.1
Time, 2019-01-01T22:16:31, Epoch: 13, Batch: 710, Training Loss: 0.12195718362927437, LR: 0.1
Time, 2019-01-01T22:16:32, Epoch: 13, Batch: 720, Training Loss: 0.091663508862257, LR: 0.1
Time, 2019-01-01T22:16:32, Epoch: 13, Batch: 730, Training Loss: 0.07699416913092136, LR: 0.1
Time, 2019-01-01T22:16:33, Epoch: 13, Batch: 740, Training Loss: 0.0985127717256546, LR: 0.1
Time, 2019-01-01T22:16:34, Epoch: 13, Batch: 750, Training Loss: 0.09701257795095444, LR: 0.1
Time, 2019-01-01T22:16:35, Epoch: 13, Batch: 760, Training Loss: 0.13128105103969573, LR: 0.1
Time, 2019-01-01T22:16:36, Epoch: 13, Batch: 770, Training Loss: 0.14511398784816265, LR: 0.1
Time, 2019-01-01T22:16:36, Epoch: 13, Batch: 780, Training Loss: 0.09306915290653706, LR: 0.1
Time, 2019-01-01T22:16:37, Epoch: 13, Batch: 790, Training Loss: 0.11496343910694122, LR: 0.1
Time, 2019-01-01T22:16:38, Epoch: 13, Batch: 800, Training Loss: 0.11342651247978211, LR: 0.1
Time, 2019-01-01T22:16:39, Epoch: 13, Batch: 810, Training Loss: 0.08377635776996613, LR: 0.1
Time, 2019-01-01T22:16:40, Epoch: 13, Batch: 820, Training Loss: 0.10767821483314037, LR: 0.1
Time, 2019-01-01T22:16:40, Epoch: 13, Batch: 830, Training Loss: 0.09719164259731769, LR: 0.1
Time, 2019-01-01T22:16:41, Epoch: 13, Batch: 840, Training Loss: 0.12237179130315781, LR: 0.1
Time, 2019-01-01T22:16:42, Epoch: 13, Batch: 850, Training Loss: 0.10331458784639835, LR: 0.1
Time, 2019-01-01T22:16:43, Epoch: 13, Batch: 860, Training Loss: 0.10950482115149499, LR: 0.1
Time, 2019-01-01T22:16:44, Epoch: 13, Batch: 870, Training Loss: 0.11030643880367279, LR: 0.1
Time, 2019-01-01T22:16:44, Epoch: 13, Batch: 880, Training Loss: 0.10178221203386784, LR: 0.1
Time, 2019-01-01T22:16:45, Epoch: 13, Batch: 890, Training Loss: 0.12477579414844513, LR: 0.1
Time, 2019-01-01T22:16:46, Epoch: 13, Batch: 900, Training Loss: 0.11245208531618119, LR: 0.1
Time, 2019-01-01T22:16:47, Epoch: 13, Batch: 910, Training Loss: 0.12569811269640924, LR: 0.1
Time, 2019-01-01T22:16:48, Epoch: 13, Batch: 920, Training Loss: 0.12297255173325539, LR: 0.1
Time, 2019-01-01T22:16:49, Epoch: 13, Batch: 930, Training Loss: 0.1481222975999117, LR: 0.1
Epoch: 13, Validation Top 1 acc: 95.64092254638672
Epoch: 13, Validation Top 5 acc: 99.90047454833984
Epoch: 13, Validation Set Loss: 0.13602033257484436
Start training epoch 14
Time, 2019-01-01T22:16:56, Epoch: 14, Batch: 10, Training Loss: 0.16261278390884398, LR: 0.1
Time, 2019-01-01T22:16:56, Epoch: 14, Batch: 20, Training Loss: 0.11425242349505424, LR: 0.1
Time, 2019-01-01T22:16:57, Epoch: 14, Batch: 30, Training Loss: 0.10501818805932998, LR: 0.1
Time, 2019-01-01T22:16:58, Epoch: 14, Batch: 40, Training Loss: 0.11377729438245296, LR: 0.1
Time, 2019-01-01T22:16:59, Epoch: 14, Batch: 50, Training Loss: 0.118130362033844, LR: 0.1
Time, 2019-01-01T22:17:00, Epoch: 14, Batch: 60, Training Loss: 0.12056713104248047, LR: 0.1
Time, 2019-01-01T22:17:01, Epoch: 14, Batch: 70, Training Loss: 0.13850075304508208, LR: 0.1
Time, 2019-01-01T22:17:02, Epoch: 14, Batch: 80, Training Loss: 0.10045449733734131, LR: 0.1
Time, 2019-01-01T22:17:03, Epoch: 14, Batch: 90, Training Loss: 0.10153785198926926, LR: 0.1
Time, 2019-01-01T22:17:03, Epoch: 14, Batch: 100, Training Loss: 0.07694565951824188, LR: 0.1
Time, 2019-01-01T22:17:04, Epoch: 14, Batch: 110, Training Loss: 0.06634405776858329, LR: 0.1
Time, 2019-01-01T22:17:05, Epoch: 14, Batch: 120, Training Loss: 0.11649410128593445, LR: 0.1
Time, 2019-01-01T22:17:06, Epoch: 14, Batch: 130, Training Loss: 0.09927426278591156, LR: 0.1
Time, 2019-01-01T22:17:07, Epoch: 14, Batch: 140, Training Loss: 0.13540442585945128, LR: 0.1
Time, 2019-01-01T22:17:08, Epoch: 14, Batch: 150, Training Loss: 0.10835532322525979, LR: 0.1
Time, 2019-01-01T22:17:08, Epoch: 14, Batch: 160, Training Loss: 0.11698451451957226, LR: 0.1
Time, 2019-01-01T22:17:09, Epoch: 14, Batch: 170, Training Loss: 0.1262744300067425, LR: 0.1
Time, 2019-01-01T22:17:10, Epoch: 14, Batch: 180, Training Loss: 0.11429404020309449, LR: 0.1
Time, 2019-01-01T22:17:11, Epoch: 14, Batch: 190, Training Loss: 0.11735634207725525, LR: 0.1
Time, 2019-01-01T22:17:12, Epoch: 14, Batch: 200, Training Loss: 0.10372223183512688, LR: 0.1
Time, 2019-01-01T22:17:12, Epoch: 14, Batch: 210, Training Loss: 0.08017927408218384, LR: 0.1
Time, 2019-01-01T22:17:13, Epoch: 14, Batch: 220, Training Loss: 0.10064507573843003, LR: 0.1
Time, 2019-01-01T22:17:14, Epoch: 14, Batch: 230, Training Loss: 0.10618386343121529, LR: 0.1
Time, 2019-01-01T22:17:15, Epoch: 14, Batch: 240, Training Loss: 0.08942304104566574, LR: 0.1
Time, 2019-01-01T22:17:16, Epoch: 14, Batch: 250, Training Loss: 0.10029670372605323, LR: 0.1
Time, 2019-01-01T22:17:16, Epoch: 14, Batch: 260, Training Loss: 0.10103053525090218, LR: 0.1
Time, 2019-01-01T22:17:17, Epoch: 14, Batch: 270, Training Loss: 0.11806190609931946, LR: 0.1
Time, 2019-01-01T22:17:18, Epoch: 14, Batch: 280, Training Loss: 0.10343117266893387, LR: 0.1
Time, 2019-01-01T22:17:19, Epoch: 14, Batch: 290, Training Loss: 0.12639204151928424, LR: 0.1
Time, 2019-01-01T22:17:20, Epoch: 14, Batch: 300, Training Loss: 0.10765579454600811, LR: 0.1
Time, 2019-01-01T22:17:21, Epoch: 14, Batch: 310, Training Loss: 0.14651990458369255, LR: 0.1
Time, 2019-01-01T22:17:21, Epoch: 14, Batch: 320, Training Loss: 0.16100287288427353, LR: 0.1
Time, 2019-01-01T22:17:22, Epoch: 14, Batch: 330, Training Loss: 0.12828681729733943, LR: 0.1
Time, 2019-01-01T22:17:23, Epoch: 14, Batch: 340, Training Loss: 0.095414038002491, LR: 0.1
Time, 2019-01-01T22:17:24, Epoch: 14, Batch: 350, Training Loss: 0.18088000938296317, LR: 0.1
Time, 2019-01-01T22:17:25, Epoch: 14, Batch: 360, Training Loss: 0.12861560881137848, LR: 0.1
Time, 2019-01-01T22:17:26, Epoch: 14, Batch: 370, Training Loss: 0.13768240213394164, LR: 0.1
Time, 2019-01-01T22:17:27, Epoch: 14, Batch: 380, Training Loss: 0.1591637808829546, LR: 0.1
Time, 2019-01-01T22:17:27, Epoch: 14, Batch: 390, Training Loss: 0.15691846013069152, LR: 0.1
Time, 2019-01-01T22:17:28, Epoch: 14, Batch: 400, Training Loss: 0.15576957911252975, LR: 0.1
Time, 2019-01-01T22:17:29, Epoch: 14, Batch: 410, Training Loss: 0.15700760632753372, LR: 0.1
Time, 2019-01-01T22:17:30, Epoch: 14, Batch: 420, Training Loss: 0.08859014995396138, LR: 0.1
Time, 2019-01-01T22:17:31, Epoch: 14, Batch: 430, Training Loss: 0.13114787340164186, LR: 0.1
Time, 2019-01-01T22:17:32, Epoch: 14, Batch: 440, Training Loss: 0.10733724758028984, LR: 0.1
Time, 2019-01-01T22:17:32, Epoch: 14, Batch: 450, Training Loss: 0.15439180359244348, LR: 0.1
Time, 2019-01-01T22:17:33, Epoch: 14, Batch: 460, Training Loss: 0.1451679691672325, LR: 0.1
Time, 2019-01-01T22:17:34, Epoch: 14, Batch: 470, Training Loss: 0.13049934953451156, LR: 0.1
Time, 2019-01-01T22:17:35, Epoch: 14, Batch: 480, Training Loss: 0.09689441993832588, LR: 0.1
Time, 2019-01-01T22:17:36, Epoch: 14, Batch: 490, Training Loss: 0.12394155710935592, LR: 0.1
Time, 2019-01-01T22:17:37, Epoch: 14, Batch: 500, Training Loss: 0.06175016537308693, LR: 0.1
Time, 2019-01-01T22:17:38, Epoch: 14, Batch: 510, Training Loss: 0.08611336015164853, LR: 0.1
Time, 2019-01-01T22:17:39, Epoch: 14, Batch: 520, Training Loss: 0.13066583089530467, LR: 0.1
Time, 2019-01-01T22:17:40, Epoch: 14, Batch: 530, Training Loss: 0.1267560265958309, LR: 0.1
Time, 2019-01-01T22:17:41, Epoch: 14, Batch: 540, Training Loss: 0.12613001838326454, LR: 0.1
Time, 2019-01-01T22:17:42, Epoch: 14, Batch: 550, Training Loss: 0.12393557205796242, LR: 0.1
Time, 2019-01-01T22:17:43, Epoch: 14, Batch: 560, Training Loss: 0.12366200089454651, LR: 0.1
Time, 2019-01-01T22:17:43, Epoch: 14, Batch: 570, Training Loss: 0.15628970712423323, LR: 0.1
Time, 2019-01-01T22:17:44, Epoch: 14, Batch: 580, Training Loss: 0.12259365841746331, LR: 0.1
Time, 2019-01-01T22:17:45, Epoch: 14, Batch: 590, Training Loss: 0.12878056690096856, LR: 0.1
Time, 2019-01-01T22:17:46, Epoch: 14, Batch: 600, Training Loss: 0.1313380841165781, LR: 0.1
Time, 2019-01-01T22:17:47, Epoch: 14, Batch: 610, Training Loss: 0.16247302293777466, LR: 0.1
Time, 2019-01-01T22:17:48, Epoch: 14, Batch: 620, Training Loss: 0.18181057870388032, LR: 0.1
Time, 2019-01-01T22:17:48, Epoch: 14, Batch: 630, Training Loss: 0.20165802612900735, LR: 0.1
Time, 2019-01-01T22:17:49, Epoch: 14, Batch: 640, Training Loss: 0.16095684617757797, LR: 0.1
Time, 2019-01-01T22:17:50, Epoch: 14, Batch: 650, Training Loss: 0.12836613021790982, LR: 0.1
Time, 2019-01-01T22:17:51, Epoch: 14, Batch: 660, Training Loss: 0.11461717858910561, LR: 0.1
Time, 2019-01-01T22:17:52, Epoch: 14, Batch: 670, Training Loss: 0.09839045107364655, LR: 0.1
Time, 2019-01-01T22:17:53, Epoch: 14, Batch: 680, Training Loss: 0.1248464610427618, LR: 0.1
Time, 2019-01-01T22:17:54, Epoch: 14, Batch: 690, Training Loss: 0.12967112362384797, LR: 0.1
Time, 2019-01-01T22:17:55, Epoch: 14, Batch: 700, Training Loss: 0.11637604534626007, LR: 0.1
Time, 2019-01-01T22:17:55, Epoch: 14, Batch: 710, Training Loss: 0.11778053790330886, LR: 0.1
Time, 2019-01-01T22:17:56, Epoch: 14, Batch: 720, Training Loss: 0.0999987430870533, LR: 0.1
Time, 2019-01-01T22:17:57, Epoch: 14, Batch: 730, Training Loss: 0.13858746290206908, LR: 0.1
Time, 2019-01-01T22:17:58, Epoch: 14, Batch: 740, Training Loss: 0.12388321682810784, LR: 0.1
Time, 2019-01-01T22:17:59, Epoch: 14, Batch: 750, Training Loss: 0.18029523342847825, LR: 0.1
Time, 2019-01-01T22:18:00, Epoch: 14, Batch: 760, Training Loss: 0.11347411572933197, LR: 0.1
Time, 2019-01-01T22:18:01, Epoch: 14, Batch: 770, Training Loss: 0.20268049016594886, LR: 0.1
Time, 2019-01-01T22:18:02, Epoch: 14, Batch: 780, Training Loss: 0.20522584095597268, LR: 0.1
Time, 2019-01-01T22:18:03, Epoch: 14, Batch: 790, Training Loss: 0.13126672320067884, LR: 0.1
Time, 2019-01-01T22:18:04, Epoch: 14, Batch: 800, Training Loss: 0.11386203318834305, LR: 0.1
Time, 2019-01-01T22:18:05, Epoch: 14, Batch: 810, Training Loss: 0.141028892993927, LR: 0.1
Time, 2019-01-01T22:18:06, Epoch: 14, Batch: 820, Training Loss: 0.15490213148295878, LR: 0.1
Time, 2019-01-01T22:18:07, Epoch: 14, Batch: 830, Training Loss: 0.16965087056159972, LR: 0.1
Time, 2019-01-01T22:18:07, Epoch: 14, Batch: 840, Training Loss: 0.13534784242510794, LR: 0.1
Time, 2019-01-01T22:18:08, Epoch: 14, Batch: 850, Training Loss: 0.16500372476875783, LR: 0.1
Time, 2019-01-01T22:18:09, Epoch: 14, Batch: 860, Training Loss: 0.1803438350558281, LR: 0.1
Time, 2019-01-01T22:18:10, Epoch: 14, Batch: 870, Training Loss: 0.18245977833867072, LR: 0.1
Time, 2019-01-01T22:18:11, Epoch: 14, Batch: 880, Training Loss: 0.15938444063067436, LR: 0.1
Time, 2019-01-01T22:18:12, Epoch: 14, Batch: 890, Training Loss: 0.11063944175839424, LR: 0.1
Time, 2019-01-01T22:18:12, Epoch: 14, Batch: 900, Training Loss: 0.13796853125095368, LR: 0.1
Time, 2019-01-01T22:18:13, Epoch: 14, Batch: 910, Training Loss: 0.13856221958994866, LR: 0.1
Time, 2019-01-01T22:18:14, Epoch: 14, Batch: 920, Training Loss: 0.13743504881858826, LR: 0.1
Time, 2019-01-01T22:18:15, Epoch: 14, Batch: 930, Training Loss: 0.1063340499997139, LR: 0.1
Epoch: 14, Validation Top 1 acc: 96.7655258178711
Epoch: 14, Validation Top 5 acc: 99.96018981933594
Epoch: 14, Validation Set Loss: 0.10665163397789001
Start training epoch 15
Time, 2019-01-01T22:18:21, Epoch: 15, Batch: 10, Training Loss: 0.10856475085020065, LR: 0.1
Time, 2019-01-01T22:18:22, Epoch: 15, Batch: 20, Training Loss: 0.07461423128843307, LR: 0.1
Time, 2019-01-01T22:18:23, Epoch: 15, Batch: 30, Training Loss: 0.11114435121417046, LR: 0.1
Time, 2019-01-01T22:18:24, Epoch: 15, Batch: 40, Training Loss: 0.10979559905827045, LR: 0.1
Time, 2019-01-01T22:18:24, Epoch: 15, Batch: 50, Training Loss: 0.11574437841773033, LR: 0.1
Time, 2019-01-01T22:18:25, Epoch: 15, Batch: 60, Training Loss: 0.12365617007017135, LR: 0.1
Time, 2019-01-01T22:18:26, Epoch: 15, Batch: 70, Training Loss: 0.10563671551644802, LR: 0.1
Time, 2019-01-01T22:18:27, Epoch: 15, Batch: 80, Training Loss: 0.08779204860329629, LR: 0.1
Time, 2019-01-01T22:18:28, Epoch: 15, Batch: 90, Training Loss: 0.17088188156485556, LR: 0.1
Time, 2019-01-01T22:18:29, Epoch: 15, Batch: 100, Training Loss: 0.15414694845676422, LR: 0.1
Time, 2019-01-01T22:18:29, Epoch: 15, Batch: 110, Training Loss: 0.12069673389196396, LR: 0.1
Time, 2019-01-01T22:18:30, Epoch: 15, Batch: 120, Training Loss: 0.13786330297589303, LR: 0.1
Time, 2019-01-01T22:18:31, Epoch: 15, Batch: 130, Training Loss: 0.10939012616872787, LR: 0.1
Time, 2019-01-01T22:18:32, Epoch: 15, Batch: 140, Training Loss: 0.08057448007166386, LR: 0.1
Time, 2019-01-01T22:18:33, Epoch: 15, Batch: 150, Training Loss: 0.11294712126255035, LR: 0.1
Time, 2019-01-01T22:18:34, Epoch: 15, Batch: 160, Training Loss: 0.18765153661370276, LR: 0.1
Time, 2019-01-01T22:18:35, Epoch: 15, Batch: 170, Training Loss: 0.17320228479802607, LR: 0.1
Time, 2019-01-01T22:18:36, Epoch: 15, Batch: 180, Training Loss: 0.11565260887145996, LR: 0.1
Time, 2019-01-01T22:18:37, Epoch: 15, Batch: 190, Training Loss: 0.11957811415195466, LR: 0.1
Time, 2019-01-01T22:18:38, Epoch: 15, Batch: 200, Training Loss: 0.13850791081786157, LR: 0.1
Time, 2019-01-01T22:18:39, Epoch: 15, Batch: 210, Training Loss: 0.10580526068806648, LR: 0.1
Time, 2019-01-01T22:18:40, Epoch: 15, Batch: 220, Training Loss: 0.10594489797949791, LR: 0.1
Time, 2019-01-01T22:18:41, Epoch: 15, Batch: 230, Training Loss: 0.11745032593607903, LR: 0.1
Time, 2019-01-01T22:18:41, Epoch: 15, Batch: 240, Training Loss: 0.13171760663390158, LR: 0.1
Time, 2019-01-01T22:18:42, Epoch: 15, Batch: 250, Training Loss: 0.1199998252093792, LR: 0.1
Time, 2019-01-01T22:18:43, Epoch: 15, Batch: 260, Training Loss: 0.10822653919458389, LR: 0.1
Time, 2019-01-01T22:18:44, Epoch: 15, Batch: 270, Training Loss: 0.11935809776186942, LR: 0.1
Time, 2019-01-01T22:18:45, Epoch: 15, Batch: 280, Training Loss: 0.11513586342334747, LR: 0.1
Time, 2019-01-01T22:18:46, Epoch: 15, Batch: 290, Training Loss: 0.09382140040397643, LR: 0.1
Time, 2019-01-01T22:18:46, Epoch: 15, Batch: 300, Training Loss: 0.1253211721777916, LR: 0.1
Time, 2019-01-01T22:18:47, Epoch: 15, Batch: 310, Training Loss: 0.12539970874786377, LR: 0.1
Time, 2019-01-01T22:18:48, Epoch: 15, Batch: 320, Training Loss: 0.11525136157870293, LR: 0.1
Time, 2019-01-01T22:18:49, Epoch: 15, Batch: 330, Training Loss: 0.12562436759471893, LR: 0.1
Time, 2019-01-01T22:18:50, Epoch: 15, Batch: 340, Training Loss: 0.0817231584340334, LR: 0.1
Time, 2019-01-01T22:18:51, Epoch: 15, Batch: 350, Training Loss: 0.08958152309060097, LR: 0.1
Time, 2019-01-01T22:18:52, Epoch: 15, Batch: 360, Training Loss: 0.09321438670158386, LR: 0.1
Time, 2019-01-01T22:18:53, Epoch: 15, Batch: 370, Training Loss: 0.11577830165624618, LR: 0.1
Time, 2019-01-01T22:18:54, Epoch: 15, Batch: 380, Training Loss: 0.10231726169586182, LR: 0.1
Time, 2019-01-01T22:18:55, Epoch: 15, Batch: 390, Training Loss: 0.1230248149484396, LR: 0.1
Time, 2019-01-01T22:18:55, Epoch: 15, Batch: 400, Training Loss: 0.1432400844991207, LR: 0.1
Time, 2019-01-01T22:18:56, Epoch: 15, Batch: 410, Training Loss: 0.09832893311977386, LR: 0.1
Time, 2019-01-01T22:18:57, Epoch: 15, Batch: 420, Training Loss: 0.12769625820219516, LR: 0.1
Time, 2019-01-01T22:18:58, Epoch: 15, Batch: 430, Training Loss: 0.10777410194277763, LR: 0.1
Time, 2019-01-01T22:18:59, Epoch: 15, Batch: 440, Training Loss: 0.1142425335943699, LR: 0.1
Time, 2019-01-01T22:19:00, Epoch: 15, Batch: 450, Training Loss: 0.13341852724552156, LR: 0.1
Time, 2019-01-01T22:19:01, Epoch: 15, Batch: 460, Training Loss: 0.1202025681734085, LR: 0.1
Time, 2019-01-01T22:19:02, Epoch: 15, Batch: 470, Training Loss: 0.12622977718710898, LR: 0.1
Time, 2019-01-01T22:19:02, Epoch: 15, Batch: 480, Training Loss: 0.17870670631527902, LR: 0.1
Time, 2019-01-01T22:19:03, Epoch: 15, Batch: 490, Training Loss: 0.09474553391337395, LR: 0.1
Time, 2019-01-01T22:19:04, Epoch: 15, Batch: 500, Training Loss: 0.1167139619588852, LR: 0.1
Time, 2019-01-01T22:19:05, Epoch: 15, Batch: 510, Training Loss: 0.126578526571393, LR: 0.1
Time, 2019-01-01T22:19:06, Epoch: 15, Batch: 520, Training Loss: 0.14704880565404893, LR: 0.1
Time, 2019-01-01T22:19:07, Epoch: 15, Batch: 530, Training Loss: 0.1258837655186653, LR: 0.1
Time, 2019-01-01T22:19:07, Epoch: 15, Batch: 540, Training Loss: 0.14161226004362107, LR: 0.1
Time, 2019-01-01T22:19:08, Epoch: 15, Batch: 550, Training Loss: 0.13518716916441917, LR: 0.1
Time, 2019-01-01T22:19:09, Epoch: 15, Batch: 560, Training Loss: 0.1554899625480175, LR: 0.1
Time, 2019-01-01T22:19:10, Epoch: 15, Batch: 570, Training Loss: 0.13196486830711365, LR: 0.1
Time, 2019-01-01T22:19:11, Epoch: 15, Batch: 580, Training Loss: 0.1287808209657669, LR: 0.1
Time, 2019-01-01T22:19:12, Epoch: 15, Batch: 590, Training Loss: 0.1617643591016531, LR: 0.1
Time, 2019-01-01T22:19:13, Epoch: 15, Batch: 600, Training Loss: 0.16140831038355827, LR: 0.1
Time, 2019-01-01T22:19:13, Epoch: 15, Batch: 610, Training Loss: 0.105797428637743, LR: 0.1
Time, 2019-01-01T22:19:14, Epoch: 15, Batch: 620, Training Loss: 0.1186853751540184, LR: 0.1
Time, 2019-01-01T22:19:15, Epoch: 15, Batch: 630, Training Loss: 0.1049128893762827, LR: 0.1
Time, 2019-01-01T22:19:16, Epoch: 15, Batch: 640, Training Loss: 0.12222860231995583, LR: 0.1
Time, 2019-01-01T22:19:17, Epoch: 15, Batch: 650, Training Loss: 0.12575076073408126, LR: 0.1
Time, 2019-01-01T22:19:18, Epoch: 15, Batch: 660, Training Loss: 0.1420304946601391, LR: 0.1
Time, 2019-01-01T22:19:18, Epoch: 15, Batch: 670, Training Loss: 0.12301769852638245, LR: 0.1
Time, 2019-01-01T22:19:19, Epoch: 15, Batch: 680, Training Loss: 0.11631653010845185, LR: 0.1
Time, 2019-01-01T22:19:20, Epoch: 15, Batch: 690, Training Loss: 0.16707255840301513, LR: 0.1
Time, 2019-01-01T22:19:21, Epoch: 15, Batch: 700, Training Loss: 0.19957431331276893, LR: 0.1
Time, 2019-01-01T22:19:22, Epoch: 15, Batch: 710, Training Loss: 0.1255859039723873, LR: 0.1
Time, 2019-01-01T22:19:23, Epoch: 15, Batch: 720, Training Loss: 0.11961579620838166, LR: 0.1
Time, 2019-01-01T22:19:23, Epoch: 15, Batch: 730, Training Loss: 0.13406205773353577, LR: 0.1
Time, 2019-01-01T22:19:24, Epoch: 15, Batch: 740, Training Loss: 0.11588979214429855, LR: 0.1
Time, 2019-01-01T22:19:25, Epoch: 15, Batch: 750, Training Loss: 0.13857489712536336, LR: 0.1
Time, 2019-01-01T22:19:26, Epoch: 15, Batch: 760, Training Loss: 0.08726700097322464, LR: 0.1
Time, 2019-01-01T22:19:27, Epoch: 15, Batch: 770, Training Loss: 0.15638766065239906, LR: 0.1
Time, 2019-01-01T22:19:28, Epoch: 15, Batch: 780, Training Loss: 0.11044472530484199, LR: 0.1
Time, 2019-01-01T22:19:28, Epoch: 15, Batch: 790, Training Loss: 0.13344932943582535, LR: 0.1
Time, 2019-01-01T22:19:29, Epoch: 15, Batch: 800, Training Loss: 0.13350329995155336, LR: 0.1
Time, 2019-01-01T22:19:30, Epoch: 15, Batch: 810, Training Loss: 0.12073067352175712, LR: 0.1
Time, 2019-01-01T22:19:31, Epoch: 15, Batch: 820, Training Loss: 0.11909987479448318, LR: 0.1
Time, 2019-01-01T22:19:32, Epoch: 15, Batch: 830, Training Loss: 0.11529087871313096, LR: 0.1
Time, 2019-01-01T22:19:33, Epoch: 15, Batch: 840, Training Loss: 0.147794646024704, LR: 0.1
Time, 2019-01-01T22:19:34, Epoch: 15, Batch: 850, Training Loss: 0.14447465240955354, LR: 0.1
Time, 2019-01-01T22:19:35, Epoch: 15, Batch: 860, Training Loss: 0.10935664288699627, LR: 0.1
Time, 2019-01-01T22:19:36, Epoch: 15, Batch: 870, Training Loss: 0.14556638076901435, LR: 0.1
Time, 2019-01-01T22:19:37, Epoch: 15, Batch: 880, Training Loss: 0.10576540231704712, LR: 0.1
Time, 2019-01-01T22:19:37, Epoch: 15, Batch: 890, Training Loss: 0.10582739710807801, LR: 0.1
Time, 2019-01-01T22:19:38, Epoch: 15, Batch: 900, Training Loss: 0.12670461907982827, LR: 0.1
Time, 2019-01-01T22:19:39, Epoch: 15, Batch: 910, Training Loss: 0.1456384990364313, LR: 0.1
Time, 2019-01-01T22:19:40, Epoch: 15, Batch: 920, Training Loss: 0.11005320027470589, LR: 0.1
Time, 2019-01-01T22:19:41, Epoch: 15, Batch: 930, Training Loss: 0.12460574470460414, LR: 0.1
Epoch: 15, Validation Top 1 acc: 96.0688705444336
Epoch: 15, Validation Top 5 acc: 99.92037963867188
Epoch: 15, Validation Set Loss: 0.12665385007858276
Start training epoch 16
Time, 2019-01-01T22:19:48, Epoch: 16, Batch: 10, Training Loss: 0.1063214436173439, LR: 0.1
Time, 2019-01-01T22:19:48, Epoch: 16, Batch: 20, Training Loss: 0.11042443215847016, LR: 0.1
Time, 2019-01-01T22:19:49, Epoch: 16, Batch: 30, Training Loss: 0.10061116814613343, LR: 0.1
Time, 2019-01-01T22:19:50, Epoch: 16, Batch: 40, Training Loss: 0.10490708574652671, LR: 0.1
Time, 2019-01-01T22:19:51, Epoch: 16, Batch: 50, Training Loss: 0.14989744424819945, LR: 0.1
Time, 2019-01-01T22:19:52, Epoch: 16, Batch: 60, Training Loss: 0.16524435058236123, LR: 0.1
Time, 2019-01-01T22:19:53, Epoch: 16, Batch: 70, Training Loss: 0.13921624124050141, LR: 0.1
Time, 2019-01-01T22:19:53, Epoch: 16, Batch: 80, Training Loss: 0.10985959470272064, LR: 0.1
Time, 2019-01-01T22:19:54, Epoch: 16, Batch: 90, Training Loss: 0.09773776456713676, LR: 0.1
Time, 2019-01-01T22:19:55, Epoch: 16, Batch: 100, Training Loss: 0.12141360118985176, LR: 0.1
Time, 2019-01-01T22:19:56, Epoch: 16, Batch: 110, Training Loss: 0.13882256001234056, LR: 0.1
Time, 2019-01-01T22:19:57, Epoch: 16, Batch: 120, Training Loss: 0.10255246758460998, LR: 0.1
Time, 2019-01-01T22:19:58, Epoch: 16, Batch: 130, Training Loss: 0.15075018927454947, LR: 0.1
Time, 2019-01-01T22:19:58, Epoch: 16, Batch: 140, Training Loss: 0.11303461939096451, LR: 0.1
Time, 2019-01-01T22:19:59, Epoch: 16, Batch: 150, Training Loss: 0.15388950817286967, LR: 0.1
Time, 2019-01-01T22:20:00, Epoch: 16, Batch: 160, Training Loss: 0.14365345127880574, LR: 0.1
Time, 2019-01-01T22:20:01, Epoch: 16, Batch: 170, Training Loss: 0.15983294323086739, LR: 0.1
Time, 2019-01-01T22:20:01, Epoch: 16, Batch: 180, Training Loss: 0.13642885386943818, LR: 0.1
Time, 2019-01-01T22:20:02, Epoch: 16, Batch: 190, Training Loss: 0.08872661180794239, LR: 0.1
Time, 2019-01-01T22:20:03, Epoch: 16, Batch: 200, Training Loss: 0.08976179100573063, LR: 0.1
Time, 2019-01-01T22:20:04, Epoch: 16, Batch: 210, Training Loss: 0.07208802625536918, LR: 0.1
Time, 2019-01-01T22:20:05, Epoch: 16, Batch: 220, Training Loss: 0.13539402149617671, LR: 0.1
Time, 2019-01-01T22:20:05, Epoch: 16, Batch: 230, Training Loss: 0.08323731571435929, LR: 0.1
Time, 2019-01-01T22:20:06, Epoch: 16, Batch: 240, Training Loss: 0.09439235478639603, LR: 0.1
Time, 2019-01-01T22:20:07, Epoch: 16, Batch: 250, Training Loss: 0.10216658636927604, LR: 0.1
Time, 2019-01-01T22:20:08, Epoch: 16, Batch: 260, Training Loss: 0.09620878621935844, LR: 0.1
Time, 2019-01-01T22:20:09, Epoch: 16, Batch: 270, Training Loss: 0.130443774163723, LR: 0.1
Time, 2019-01-01T22:20:09, Epoch: 16, Batch: 280, Training Loss: 0.11178718134760857, LR: 0.1
Time, 2019-01-01T22:20:10, Epoch: 16, Batch: 290, Training Loss: 0.14461796805262567, LR: 0.1
Time, 2019-01-01T22:20:11, Epoch: 16, Batch: 300, Training Loss: 0.1426871672272682, LR: 0.1
Time, 2019-01-01T22:20:12, Epoch: 16, Batch: 310, Training Loss: 0.14887876510620118, LR: 0.1
Time, 2019-01-01T22:20:12, Epoch: 16, Batch: 320, Training Loss: 0.1647145614027977, LR: 0.1
Time, 2019-01-01T22:20:13, Epoch: 16, Batch: 330, Training Loss: 0.13031063713133334, LR: 0.1
Time, 2019-01-01T22:20:14, Epoch: 16, Batch: 340, Training Loss: 0.13496792688965797, LR: 0.1
Time, 2019-01-01T22:20:15, Epoch: 16, Batch: 350, Training Loss: 0.11395954713225365, LR: 0.1
Time, 2019-01-01T22:20:16, Epoch: 16, Batch: 360, Training Loss: 0.09951333552598954, LR: 0.1
Time, 2019-01-01T22:20:16, Epoch: 16, Batch: 370, Training Loss: 0.07304414883255958, LR: 0.1
Time, 2019-01-01T22:20:17, Epoch: 16, Batch: 380, Training Loss: 0.133570072054863, LR: 0.1
Time, 2019-01-01T22:20:18, Epoch: 16, Batch: 390, Training Loss: 0.08217203468084336, LR: 0.1
Time, 2019-01-01T22:20:19, Epoch: 16, Batch: 400, Training Loss: 0.09691564515233039, LR: 0.1
Time, 2019-01-01T22:20:20, Epoch: 16, Batch: 410, Training Loss: 0.10554833635687828, LR: 0.1
Time, 2019-01-01T22:20:20, Epoch: 16, Batch: 420, Training Loss: 0.10845252498984337, LR: 0.1
Time, 2019-01-01T22:20:21, Epoch: 16, Batch: 430, Training Loss: 0.14659781232476235, LR: 0.1
Time, 2019-01-01T22:20:22, Epoch: 16, Batch: 440, Training Loss: 0.12273609042167663, LR: 0.1
Time, 2019-01-01T22:20:23, Epoch: 16, Batch: 450, Training Loss: 0.11805917918682099, LR: 0.1
Time, 2019-01-01T22:20:24, Epoch: 16, Batch: 460, Training Loss: 0.15087064802646638, LR: 0.1
Time, 2019-01-01T22:20:25, Epoch: 16, Batch: 470, Training Loss: 0.14309628009796144, LR: 0.1
Time, 2019-01-01T22:20:25, Epoch: 16, Batch: 480, Training Loss: 0.2179217107594013, LR: 0.1
Time, 2019-01-01T22:20:26, Epoch: 16, Batch: 490, Training Loss: 0.18703759461641312, LR: 0.1
Time, 2019-01-01T22:20:27, Epoch: 16, Batch: 500, Training Loss: 0.1543461002409458, LR: 0.1
Time, 2019-01-01T22:20:28, Epoch: 16, Batch: 510, Training Loss: 0.13341655358672141, LR: 0.1
Time, 2019-01-01T22:20:28, Epoch: 16, Batch: 520, Training Loss: 0.11579681411385537, LR: 0.1
Time, 2019-01-01T22:20:29, Epoch: 16, Batch: 530, Training Loss: 0.11589128747582436, LR: 0.1
Time, 2019-01-01T22:20:30, Epoch: 16, Batch: 540, Training Loss: 0.15729728564620019, LR: 0.1
Time, 2019-01-01T22:20:31, Epoch: 16, Batch: 550, Training Loss: 0.12065879590809345, LR: 0.1
Time, 2019-01-01T22:20:32, Epoch: 16, Batch: 560, Training Loss: 0.1471971683204174, LR: 0.1
Time, 2019-01-01T22:20:32, Epoch: 16, Batch: 570, Training Loss: 0.11912482231855392, LR: 0.1
Time, 2019-01-01T22:20:33, Epoch: 16, Batch: 580, Training Loss: 0.12922256328165532, LR: 0.1
Time, 2019-01-01T22:20:34, Epoch: 16, Batch: 590, Training Loss: 0.12284817099571228, LR: 0.1
Time, 2019-01-01T22:20:35, Epoch: 16, Batch: 600, Training Loss: 0.15545556992292403, LR: 0.1
Time, 2019-01-01T22:20:36, Epoch: 16, Batch: 610, Training Loss: 0.20653503239154816, LR: 0.1
Time, 2019-01-01T22:20:36, Epoch: 16, Batch: 620, Training Loss: 0.13307054191827775, LR: 0.1
Time, 2019-01-01T22:20:37, Epoch: 16, Batch: 630, Training Loss: 0.0881808117032051, LR: 0.1
Time, 2019-01-01T22:20:38, Epoch: 16, Batch: 640, Training Loss: 0.1462860643863678, LR: 0.1
Time, 2019-01-01T22:20:39, Epoch: 16, Batch: 650, Training Loss: 0.12080094404518604, LR: 0.1
Time, 2019-01-01T22:20:39, Epoch: 16, Batch: 660, Training Loss: 0.11770388185977936, LR: 0.1
Time, 2019-01-01T22:20:40, Epoch: 16, Batch: 670, Training Loss: 0.14270807281136513, LR: 0.1
Time, 2019-01-01T22:20:41, Epoch: 16, Batch: 680, Training Loss: 0.16735280752182008, LR: 0.1
Time, 2019-01-01T22:20:42, Epoch: 16, Batch: 690, Training Loss: 0.16735913679003717, LR: 0.1
Time, 2019-01-01T22:20:43, Epoch: 16, Batch: 700, Training Loss: 0.10038415491580963, LR: 0.1
Time, 2019-01-01T22:20:43, Epoch: 16, Batch: 710, Training Loss: 0.11560709737241268, LR: 0.1
Time, 2019-01-01T22:20:44, Epoch: 16, Batch: 720, Training Loss: 0.1164484217762947, LR: 0.1
Time, 2019-01-01T22:20:45, Epoch: 16, Batch: 730, Training Loss: 0.08379448801279069, LR: 0.1
Time, 2019-01-01T22:20:46, Epoch: 16, Batch: 740, Training Loss: 0.09908021464943886, LR: 0.1
Time, 2019-01-01T22:20:47, Epoch: 16, Batch: 750, Training Loss: 0.1063680738210678, LR: 0.1
Time, 2019-01-01T22:20:47, Epoch: 16, Batch: 760, Training Loss: 0.10287760235369206, LR: 0.1
Time, 2019-01-01T22:20:48, Epoch: 16, Batch: 770, Training Loss: 0.10933847874403, LR: 0.1
Time, 2019-01-01T22:20:49, Epoch: 16, Batch: 780, Training Loss: 0.1394979752600193, LR: 0.1
Time, 2019-01-01T22:20:50, Epoch: 16, Batch: 790, Training Loss: 0.1217544510960579, LR: 0.1
Time, 2019-01-01T22:20:51, Epoch: 16, Batch: 800, Training Loss: 0.09826712608337403, LR: 0.1
Time, 2019-01-01T22:20:51, Epoch: 16, Batch: 810, Training Loss: 0.0797653190791607, LR: 0.1
Time, 2019-01-01T22:20:52, Epoch: 16, Batch: 820, Training Loss: 0.1387185588479042, LR: 0.1
Time, 2019-01-01T22:20:53, Epoch: 16, Batch: 830, Training Loss: 0.11217240877449512, LR: 0.1
Time, 2019-01-01T22:20:54, Epoch: 16, Batch: 840, Training Loss: 0.10557824447751045, LR: 0.1
Time, 2019-01-01T22:20:55, Epoch: 16, Batch: 850, Training Loss: 0.05158794037997723, LR: 0.1
Time, 2019-01-01T22:20:55, Epoch: 16, Batch: 860, Training Loss: 0.06936998888850213, LR: 0.1
Time, 2019-01-01T22:20:56, Epoch: 16, Batch: 870, Training Loss: 0.10762018561363221, LR: 0.1
Time, 2019-01-01T22:20:57, Epoch: 16, Batch: 880, Training Loss: 0.12775499373674393, LR: 0.1
Time, 2019-01-01T22:20:58, Epoch: 16, Batch: 890, Training Loss: 0.07031152583658695, LR: 0.1
Time, 2019-01-01T22:20:59, Epoch: 16, Batch: 900, Training Loss: 0.09646117389202118, LR: 0.1
Time, 2019-01-01T22:21:00, Epoch: 16, Batch: 910, Training Loss: 0.07766719833016396, LR: 0.1
Time, 2019-01-01T22:21:00, Epoch: 16, Batch: 920, Training Loss: 0.12548088058829307, LR: 0.1
Time, 2019-01-01T22:21:01, Epoch: 16, Batch: 930, Training Loss: 0.16128704249858855, LR: 0.1
Epoch: 16, Validation Top 1 acc: 96.04896545410156
Epoch: 16, Validation Top 5 acc: 99.95024108886719
Epoch: 16, Validation Set Loss: 0.12020839750766754
Start training epoch 17
Time, 2019-01-01T22:21:08, Epoch: 17, Batch: 10, Training Loss: 0.1290043793618679, LR: 0.1
Time, 2019-01-01T22:21:08, Epoch: 17, Batch: 20, Training Loss: 0.1401877224445343, LR: 0.1
Time, 2019-01-01T22:21:09, Epoch: 17, Batch: 30, Training Loss: 0.16268823444843292, LR: 0.1
Time, 2019-01-01T22:21:10, Epoch: 17, Batch: 40, Training Loss: 0.18074558153748513, LR: 0.1
Time, 2019-01-01T22:21:11, Epoch: 17, Batch: 50, Training Loss: 0.17496346384286882, LR: 0.1
Time, 2019-01-01T22:21:12, Epoch: 17, Batch: 60, Training Loss: 0.15809517949819565, LR: 0.1
Time, 2019-01-01T22:21:12, Epoch: 17, Batch: 70, Training Loss: 0.1530687317252159, LR: 0.1
Time, 2019-01-01T22:21:13, Epoch: 17, Batch: 80, Training Loss: 0.09720554873347283, LR: 0.1
Time, 2019-01-01T22:21:14, Epoch: 17, Batch: 90, Training Loss: 0.08735554032027722, LR: 0.1
Time, 2019-01-01T22:21:15, Epoch: 17, Batch: 100, Training Loss: 0.12754795625805854, LR: 0.1
Time, 2019-01-01T22:21:16, Epoch: 17, Batch: 110, Training Loss: 0.1359357140958309, LR: 0.1
Time, 2019-01-01T22:21:17, Epoch: 17, Batch: 120, Training Loss: 0.14368281736969948, LR: 0.1
Time, 2019-01-01T22:21:17, Epoch: 17, Batch: 130, Training Loss: 0.12086895070970058, LR: 0.1
Time, 2019-01-01T22:21:18, Epoch: 17, Batch: 140, Training Loss: 0.1472071476280689, LR: 0.1
Time, 2019-01-01T22:21:19, Epoch: 17, Batch: 150, Training Loss: 0.18252436071634293, LR: 0.1
Time, 2019-01-01T22:21:20, Epoch: 17, Batch: 160, Training Loss: 0.1787760965526104, LR: 0.1
Time, 2019-01-01T22:21:20, Epoch: 17, Batch: 170, Training Loss: 0.12537686191499234, LR: 0.1
Time, 2019-01-01T22:21:21, Epoch: 17, Batch: 180, Training Loss: 0.17185628935694694, LR: 0.1
Time, 2019-01-01T22:21:22, Epoch: 17, Batch: 190, Training Loss: 0.15143208280205728, LR: 0.1
Time, 2019-01-01T22:21:23, Epoch: 17, Batch: 200, Training Loss: 0.11201912388205529, LR: 0.1
Time, 2019-01-01T22:21:24, Epoch: 17, Batch: 210, Training Loss: 0.12238855510950089, LR: 0.1
Time, 2019-01-01T22:21:24, Epoch: 17, Batch: 220, Training Loss: 0.08754625022411347, LR: 0.1
Time, 2019-01-01T22:21:25, Epoch: 17, Batch: 230, Training Loss: 0.11126266121864319, LR: 0.1
Time, 2019-01-01T22:21:26, Epoch: 17, Batch: 240, Training Loss: 0.12281633727252483, LR: 0.1
Time, 2019-01-01T22:21:27, Epoch: 17, Batch: 250, Training Loss: 0.11597853675484657, LR: 0.1
Time, 2019-01-01T22:21:28, Epoch: 17, Batch: 260, Training Loss: 0.09804382957518101, LR: 0.1
Time, 2019-01-01T22:21:28, Epoch: 17, Batch: 270, Training Loss: 0.10970471911132336, LR: 0.1
Time, 2019-01-01T22:21:29, Epoch: 17, Batch: 280, Training Loss: 0.08871467933058738, LR: 0.1
Time, 2019-01-01T22:21:30, Epoch: 17, Batch: 290, Training Loss: 0.11997381038963795, LR: 0.1
Time, 2019-01-01T22:21:31, Epoch: 17, Batch: 300, Training Loss: 0.06609740070998668, LR: 0.1
Time, 2019-01-01T22:21:32, Epoch: 17, Batch: 310, Training Loss: 0.1414131447672844, LR: 0.1
Time, 2019-01-01T22:21:32, Epoch: 17, Batch: 320, Training Loss: 0.1073332741856575, LR: 0.1
Time, 2019-01-01T22:21:33, Epoch: 17, Batch: 330, Training Loss: 0.12328763529658318, LR: 0.1
Time, 2019-01-01T22:21:34, Epoch: 17, Batch: 340, Training Loss: 0.15275546982884408, LR: 0.1
Time, 2019-01-01T22:21:35, Epoch: 17, Batch: 350, Training Loss: 0.11417285650968552, LR: 0.1
Time, 2019-01-01T22:21:35, Epoch: 17, Batch: 360, Training Loss: 0.10826012790203095, LR: 0.1
Time, 2019-01-01T22:21:36, Epoch: 17, Batch: 370, Training Loss: 0.11715302690863609, LR: 0.1
Time, 2019-01-01T22:21:37, Epoch: 17, Batch: 380, Training Loss: 0.11456938832998276, LR: 0.1
Time, 2019-01-01T22:21:38, Epoch: 17, Batch: 390, Training Loss: 0.11493636667728424, LR: 0.1
Time, 2019-01-01T22:21:39, Epoch: 17, Batch: 400, Training Loss: 0.11326832249760628, LR: 0.1
Time, 2019-01-01T22:21:39, Epoch: 17, Batch: 410, Training Loss: 0.14574959725141526, LR: 0.1
Time, 2019-01-01T22:21:40, Epoch: 17, Batch: 420, Training Loss: 0.09957376793026924, LR: 0.1
Time, 2019-01-01T22:21:41, Epoch: 17, Batch: 430, Training Loss: 0.09805819317698479, LR: 0.1
Time, 2019-01-01T22:21:42, Epoch: 17, Batch: 440, Training Loss: 0.15865744799375534, LR: 0.1
Time, 2019-01-01T22:21:43, Epoch: 17, Batch: 450, Training Loss: 0.13277601264417171, LR: 0.1
Time, 2019-01-01T22:21:43, Epoch: 17, Batch: 460, Training Loss: 0.1729774072766304, LR: 0.1
Time, 2019-01-01T22:21:44, Epoch: 17, Batch: 470, Training Loss: 0.09301339015364647, LR: 0.1
Time, 2019-01-01T22:21:45, Epoch: 17, Batch: 480, Training Loss: 0.1284107506275177, LR: 0.1
Time, 2019-01-01T22:21:46, Epoch: 17, Batch: 490, Training Loss: 0.1651502624154091, LR: 0.1
Time, 2019-01-01T22:21:46, Epoch: 17, Batch: 500, Training Loss: 0.13548412099480628, LR: 0.1
Time, 2019-01-01T22:21:47, Epoch: 17, Batch: 510, Training Loss: 0.09953668490052223, LR: 0.1
Time, 2019-01-01T22:21:48, Epoch: 17, Batch: 520, Training Loss: 0.08705242685973644, LR: 0.1
Time, 2019-01-01T22:21:49, Epoch: 17, Batch: 530, Training Loss: 0.13684079870581628, LR: 0.1
Time, 2019-01-01T22:21:49, Epoch: 17, Batch: 540, Training Loss: 0.16120706796646117, LR: 0.1
Time, 2019-01-01T22:21:50, Epoch: 17, Batch: 550, Training Loss: 0.13628919012844562, LR: 0.1
Time, 2019-01-01T22:21:51, Epoch: 17, Batch: 560, Training Loss: 0.08144713416695595, LR: 0.1
Time, 2019-01-01T22:21:52, Epoch: 17, Batch: 570, Training Loss: 0.18341826163232328, LR: 0.1
Time, 2019-01-01T22:21:53, Epoch: 17, Batch: 580, Training Loss: 0.17022451385855675, LR: 0.1
Time, 2019-01-01T22:21:54, Epoch: 17, Batch: 590, Training Loss: 0.103332132473588, LR: 0.1
Time, 2019-01-01T22:21:54, Epoch: 17, Batch: 600, Training Loss: 0.13011173978447915, LR: 0.1
Time, 2019-01-01T22:21:55, Epoch: 17, Batch: 610, Training Loss: 0.10939735993742943, LR: 0.1
Time, 2019-01-01T22:21:56, Epoch: 17, Batch: 620, Training Loss: 0.09831660985946655, LR: 0.1
Time, 2019-01-01T22:21:57, Epoch: 17, Batch: 630, Training Loss: 0.1705476738512516, LR: 0.1
Time, 2019-01-01T22:21:58, Epoch: 17, Batch: 640, Training Loss: 0.16564933434128762, LR: 0.1
Time, 2019-01-01T22:21:58, Epoch: 17, Batch: 650, Training Loss: 0.14198257997632027, LR: 0.1
Time, 2019-01-01T22:21:59, Epoch: 17, Batch: 660, Training Loss: 0.11870009899139404, LR: 0.1
Time, 2019-01-01T22:22:00, Epoch: 17, Batch: 670, Training Loss: 0.0963347788900137, LR: 0.1
Time, 2019-01-01T22:22:01, Epoch: 17, Batch: 680, Training Loss: 0.12811837568879128, LR: 0.1
Time, 2019-01-01T22:22:02, Epoch: 17, Batch: 690, Training Loss: 0.1356307104229927, LR: 0.1
Time, 2019-01-01T22:22:02, Epoch: 17, Batch: 700, Training Loss: 0.14944036602973937, LR: 0.1
Time, 2019-01-01T22:22:03, Epoch: 17, Batch: 710, Training Loss: 0.08004640117287636, LR: 0.1
Time, 2019-01-01T22:22:04, Epoch: 17, Batch: 720, Training Loss: 0.1077375665307045, LR: 0.1
Time, 2019-01-01T22:22:05, Epoch: 17, Batch: 730, Training Loss: 0.1047693993896246, LR: 0.1
Time, 2019-01-01T22:22:06, Epoch: 17, Batch: 740, Training Loss: 0.07946536764502525, LR: 0.1
Time, 2019-01-01T22:22:06, Epoch: 17, Batch: 750, Training Loss: 0.10642825365066529, LR: 0.1
Time, 2019-01-01T22:22:07, Epoch: 17, Batch: 760, Training Loss: 0.1417081281542778, LR: 0.1
Time, 2019-01-01T22:22:08, Epoch: 17, Batch: 770, Training Loss: 0.1052680030465126, LR: 0.1
Time, 2019-01-01T22:22:09, Epoch: 17, Batch: 780, Training Loss: 0.11137133613228797, LR: 0.1
Time, 2019-01-01T22:22:10, Epoch: 17, Batch: 790, Training Loss: 0.15769580900669097, LR: 0.1
Time, 2019-01-01T22:22:10, Epoch: 17, Batch: 800, Training Loss: 0.1572214186191559, LR: 0.1
Time, 2019-01-01T22:22:11, Epoch: 17, Batch: 810, Training Loss: 0.1470956400036812, LR: 0.1
Time, 2019-01-01T22:22:12, Epoch: 17, Batch: 820, Training Loss: 0.157775866240263, LR: 0.1
Time, 2019-01-01T22:22:13, Epoch: 17, Batch: 830, Training Loss: 0.10464054346084595, LR: 0.1
Time, 2019-01-01T22:22:14, Epoch: 17, Batch: 840, Training Loss: 0.13025481700897218, LR: 0.1
Time, 2019-01-01T22:22:14, Epoch: 17, Batch: 850, Training Loss: 0.10809328705072403, LR: 0.1
Time, 2019-01-01T22:22:15, Epoch: 17, Batch: 860, Training Loss: 0.1309981681406498, LR: 0.1
Time, 2019-01-01T22:22:16, Epoch: 17, Batch: 870, Training Loss: 0.10216224044561387, LR: 0.1
Time, 2019-01-01T22:22:17, Epoch: 17, Batch: 880, Training Loss: 0.12223912924528121, LR: 0.1
Time, 2019-01-01T22:22:17, Epoch: 17, Batch: 890, Training Loss: 0.06870742812752724, LR: 0.1
Time, 2019-01-01T22:22:18, Epoch: 17, Batch: 900, Training Loss: 0.12561749368906022, LR: 0.1
Time, 2019-01-01T22:22:19, Epoch: 17, Batch: 910, Training Loss: 0.10555938184261322, LR: 0.1
Time, 2019-01-01T22:22:20, Epoch: 17, Batch: 920, Training Loss: 0.10442381650209427, LR: 0.1
Time, 2019-01-01T22:22:20, Epoch: 17, Batch: 930, Training Loss: 0.11356499120593071, LR: 0.1
Epoch: 17, Validation Top 1 acc: 96.90485382080078
Epoch: 17, Validation Top 5 acc: 99.98009490966797
Epoch: 17, Validation Set Loss: 0.09655029326677322
Start training epoch 18
Time, 2019-01-01T22:22:27, Epoch: 18, Batch: 10, Training Loss: 0.08881757259368897, LR: 0.010000000000000002
Time, 2019-01-01T22:22:27, Epoch: 18, Batch: 20, Training Loss: 0.0965858656913042, LR: 0.010000000000000002
Time, 2019-01-01T22:22:28, Epoch: 18, Batch: 30, Training Loss: 0.06166902929544449, LR: 0.010000000000000002
Time, 2019-01-01T22:22:29, Epoch: 18, Batch: 40, Training Loss: 0.06575086005032063, LR: 0.010000000000000002
Time, 2019-01-01T22:22:30, Epoch: 18, Batch: 50, Training Loss: 0.08813565149903298, LR: 0.010000000000000002
Time, 2019-01-01T22:22:30, Epoch: 18, Batch: 60, Training Loss: 0.04942693784832954, LR: 0.010000000000000002
Time, 2019-01-01T22:22:31, Epoch: 18, Batch: 70, Training Loss: 0.067055207118392, LR: 0.010000000000000002
Time, 2019-01-01T22:22:32, Epoch: 18, Batch: 80, Training Loss: 0.07388457693159581, LR: 0.010000000000000002
Time, 2019-01-01T22:22:33, Epoch: 18, Batch: 90, Training Loss: 0.04130099639296532, LR: 0.010000000000000002
Time, 2019-01-01T22:22:33, Epoch: 18, Batch: 100, Training Loss: 0.055583618208765985, LR: 0.010000000000000002
Time, 2019-01-01T22:22:34, Epoch: 18, Batch: 110, Training Loss: 0.07003477774560452, LR: 0.010000000000000002
Time, 2019-01-01T22:22:35, Epoch: 18, Batch: 120, Training Loss: 0.10141461044549942, LR: 0.010000000000000002
Time, 2019-01-01T22:22:36, Epoch: 18, Batch: 130, Training Loss: 0.07568604797124863, LR: 0.010000000000000002
Time, 2019-01-01T22:22:37, Epoch: 18, Batch: 140, Training Loss: 0.04879731610417366, LR: 0.010000000000000002
Time, 2019-01-01T22:22:37, Epoch: 18, Batch: 150, Training Loss: 0.0481721717864275, LR: 0.010000000000000002
Time, 2019-01-01T22:22:38, Epoch: 18, Batch: 160, Training Loss: 0.0651805505156517, LR: 0.010000000000000002
Time, 2019-01-01T22:22:39, Epoch: 18, Batch: 170, Training Loss: 0.05868619792163372, LR: 0.010000000000000002
Time, 2019-01-01T22:22:40, Epoch: 18, Batch: 180, Training Loss: 0.04938125386834145, LR: 0.010000000000000002
Time, 2019-01-01T22:22:40, Epoch: 18, Batch: 190, Training Loss: 0.08590158745646477, LR: 0.010000000000000002
Time, 2019-01-01T22:22:41, Epoch: 18, Batch: 200, Training Loss: 0.03592970184981823, LR: 0.010000000000000002
Time, 2019-01-01T22:22:42, Epoch: 18, Batch: 210, Training Loss: 0.090076270326972, LR: 0.010000000000000002
Time, 2019-01-01T22:22:43, Epoch: 18, Batch: 220, Training Loss: 0.0652546837925911, LR: 0.010000000000000002
Time, 2019-01-01T22:22:44, Epoch: 18, Batch: 230, Training Loss: 0.06328249201178551, LR: 0.010000000000000002
Time, 2019-01-01T22:22:44, Epoch: 18, Batch: 240, Training Loss: 0.06579752825200558, LR: 0.010000000000000002
Time, 2019-01-01T22:22:45, Epoch: 18, Batch: 250, Training Loss: 0.0780019637197256, LR: 0.010000000000000002
Time, 2019-01-01T22:22:46, Epoch: 18, Batch: 260, Training Loss: 0.0659429233521223, LR: 0.010000000000000002
Time, 2019-01-01T22:22:47, Epoch: 18, Batch: 270, Training Loss: 0.04617416635155678, LR: 0.010000000000000002
Time, 2019-01-01T22:22:48, Epoch: 18, Batch: 280, Training Loss: 0.06056567206978798, LR: 0.010000000000000002
Time, 2019-01-01T22:22:48, Epoch: 18, Batch: 290, Training Loss: 0.05652135834097862, LR: 0.010000000000000002
Time, 2019-01-01T22:22:49, Epoch: 18, Batch: 300, Training Loss: 0.04260882996022701, LR: 0.010000000000000002
Time, 2019-01-01T22:22:50, Epoch: 18, Batch: 310, Training Loss: 0.06487856321036815, LR: 0.010000000000000002
Time, 2019-01-01T22:22:51, Epoch: 18, Batch: 320, Training Loss: 0.04111489839851856, LR: 0.010000000000000002
Time, 2019-01-01T22:22:51, Epoch: 18, Batch: 330, Training Loss: 0.0684693370014429, LR: 0.010000000000000002
Time, 2019-01-01T22:22:52, Epoch: 18, Batch: 340, Training Loss: 0.08019836954772472, LR: 0.010000000000000002
Time, 2019-01-01T22:22:53, Epoch: 18, Batch: 350, Training Loss: 0.06704340167343617, LR: 0.010000000000000002
Time, 2019-01-01T22:22:54, Epoch: 18, Batch: 360, Training Loss: 0.07642420530319213, LR: 0.010000000000000002
Time, 2019-01-01T22:22:55, Epoch: 18, Batch: 370, Training Loss: 0.08916060253977776, LR: 0.010000000000000002
Time, 2019-01-01T22:22:55, Epoch: 18, Batch: 380, Training Loss: 0.07751770615577698, LR: 0.010000000000000002
Time, 2019-01-01T22:22:56, Epoch: 18, Batch: 390, Training Loss: 0.05429370552301407, LR: 0.010000000000000002
Time, 2019-01-01T22:22:57, Epoch: 18, Batch: 400, Training Loss: 0.0527207363396883, LR: 0.010000000000000002
Time, 2019-01-01T22:22:58, Epoch: 18, Batch: 410, Training Loss: 0.07040412500500678, LR: 0.010000000000000002
Time, 2019-01-01T22:22:59, Epoch: 18, Batch: 420, Training Loss: 0.034685497358441356, LR: 0.010000000000000002
Time, 2019-01-01T22:22:59, Epoch: 18, Batch: 430, Training Loss: 0.0667040079832077, LR: 0.010000000000000002
Time, 2019-01-01T22:23:00, Epoch: 18, Batch: 440, Training Loss: 0.07358869425952434, LR: 0.010000000000000002
Time, 2019-01-01T22:23:01, Epoch: 18, Batch: 450, Training Loss: 0.07939101457595825, LR: 0.010000000000000002
Time, 2019-01-01T22:23:02, Epoch: 18, Batch: 460, Training Loss: 0.052638963982462886, LR: 0.010000000000000002
Time, 2019-01-01T22:23:03, Epoch: 18, Batch: 470, Training Loss: 0.03123302310705185, LR: 0.010000000000000002
Time, 2019-01-01T22:23:03, Epoch: 18, Batch: 480, Training Loss: 0.04265662357211113, LR: 0.010000000000000002
Time, 2019-01-01T22:23:04, Epoch: 18, Batch: 490, Training Loss: 0.07801201269030571, LR: 0.010000000000000002
Time, 2019-01-01T22:23:05, Epoch: 18, Batch: 500, Training Loss: 0.07000540867447853, LR: 0.010000000000000002
Time, 2019-01-01T22:23:06, Epoch: 18, Batch: 510, Training Loss: 0.05722184032201767, LR: 0.010000000000000002
Time, 2019-01-01T22:23:06, Epoch: 18, Batch: 520, Training Loss: 0.05507220551371574, LR: 0.010000000000000002
Time, 2019-01-01T22:23:07, Epoch: 18, Batch: 530, Training Loss: 0.04574819467961788, LR: 0.010000000000000002
Time, 2019-01-01T22:23:08, Epoch: 18, Batch: 540, Training Loss: 0.051291939616203305, LR: 0.010000000000000002
Time, 2019-01-01T22:23:09, Epoch: 18, Batch: 550, Training Loss: 0.04714948683977127, LR: 0.010000000000000002
Time, 2019-01-01T22:23:10, Epoch: 18, Batch: 560, Training Loss: 0.05656220689415932, LR: 0.010000000000000002
Time, 2019-01-01T22:23:10, Epoch: 18, Batch: 570, Training Loss: 0.05499693788588047, LR: 0.010000000000000002
Time, 2019-01-01T22:23:11, Epoch: 18, Batch: 580, Training Loss: 0.053614648059010506, LR: 0.010000000000000002
Time, 2019-01-01T22:23:12, Epoch: 18, Batch: 590, Training Loss: 0.04343080669641495, LR: 0.010000000000000002
Time, 2019-01-01T22:23:13, Epoch: 18, Batch: 600, Training Loss: 0.06236005164682865, LR: 0.010000000000000002
Time, 2019-01-01T22:23:13, Epoch: 18, Batch: 610, Training Loss: 0.06209657453000546, LR: 0.010000000000000002
Time, 2019-01-01T22:23:14, Epoch: 18, Batch: 620, Training Loss: 0.0676827821880579, LR: 0.010000000000000002
Time, 2019-01-01T22:23:15, Epoch: 18, Batch: 630, Training Loss: 0.046311942487955095, LR: 0.010000000000000002
Time, 2019-01-01T22:23:16, Epoch: 18, Batch: 640, Training Loss: 0.07821686714887618, LR: 0.010000000000000002
Time, 2019-01-01T22:23:17, Epoch: 18, Batch: 650, Training Loss: 0.06806789301335811, LR: 0.010000000000000002
Time, 2019-01-01T22:23:17, Epoch: 18, Batch: 660, Training Loss: 0.06014391593635082, LR: 0.010000000000000002
Time, 2019-01-01T22:23:18, Epoch: 18, Batch: 670, Training Loss: 0.06969365626573562, LR: 0.010000000000000002
Time, 2019-01-01T22:23:19, Epoch: 18, Batch: 680, Training Loss: 0.05202995166182518, LR: 0.010000000000000002
Time, 2019-01-01T22:23:20, Epoch: 18, Batch: 690, Training Loss: 0.05355441831052303, LR: 0.010000000000000002
Time, 2019-01-01T22:23:21, Epoch: 18, Batch: 700, Training Loss: 0.06790763102471828, LR: 0.010000000000000002
Time, 2019-01-01T22:23:21, Epoch: 18, Batch: 710, Training Loss: 0.05015450119972229, LR: 0.010000000000000002
Time, 2019-01-01T22:23:22, Epoch: 18, Batch: 720, Training Loss: 0.04531948566436768, LR: 0.010000000000000002
Time, 2019-01-01T22:23:23, Epoch: 18, Batch: 730, Training Loss: 0.05148239955306053, LR: 0.010000000000000002
Time, 2019-01-01T22:23:24, Epoch: 18, Batch: 740, Training Loss: 0.060214847698807715, LR: 0.010000000000000002
Time, 2019-01-01T22:23:24, Epoch: 18, Batch: 750, Training Loss: 0.06224857084453106, LR: 0.010000000000000002
Time, 2019-01-01T22:23:25, Epoch: 18, Batch: 760, Training Loss: 0.09201486557722091, LR: 0.010000000000000002
Time, 2019-01-01T22:23:26, Epoch: 18, Batch: 770, Training Loss: 0.06334134377539158, LR: 0.010000000000000002
Time, 2019-01-01T22:23:27, Epoch: 18, Batch: 780, Training Loss: 0.04723877087235451, LR: 0.010000000000000002
Time, 2019-01-01T22:23:28, Epoch: 18, Batch: 790, Training Loss: 0.07540295496582985, LR: 0.010000000000000002
Time, 2019-01-01T22:23:28, Epoch: 18, Batch: 800, Training Loss: 0.06905249133706093, LR: 0.010000000000000002
Time, 2019-01-01T22:23:29, Epoch: 18, Batch: 810, Training Loss: 0.06195033267140389, LR: 0.010000000000000002
Time, 2019-01-01T22:23:30, Epoch: 18, Batch: 820, Training Loss: 0.07215983793139458, LR: 0.010000000000000002
Time, 2019-01-01T22:23:31, Epoch: 18, Batch: 830, Training Loss: 0.0817542053759098, LR: 0.010000000000000002
Time, 2019-01-01T22:23:31, Epoch: 18, Batch: 840, Training Loss: 0.05793586522340775, LR: 0.010000000000000002
Time, 2019-01-01T22:23:32, Epoch: 18, Batch: 850, Training Loss: 0.048661071062088015, LR: 0.010000000000000002
Time, 2019-01-01T22:23:33, Epoch: 18, Batch: 860, Training Loss: 0.0493134468793869, LR: 0.010000000000000002
Time, 2019-01-01T22:23:34, Epoch: 18, Batch: 870, Training Loss: 0.06741501316428185, LR: 0.010000000000000002
Time, 2019-01-01T22:23:35, Epoch: 18, Batch: 880, Training Loss: 0.038527961820364, LR: 0.010000000000000002
Time, 2019-01-01T22:23:35, Epoch: 18, Batch: 890, Training Loss: 0.0545176163315773, LR: 0.010000000000000002
Time, 2019-01-01T22:23:36, Epoch: 18, Batch: 900, Training Loss: 0.06433049738407134, LR: 0.010000000000000002
Time, 2019-01-01T22:23:37, Epoch: 18, Batch: 910, Training Loss: 0.06446621976792813, LR: 0.010000000000000002
Time, 2019-01-01T22:23:38, Epoch: 18, Batch: 920, Training Loss: 0.07573793344199657, LR: 0.010000000000000002
Time, 2019-01-01T22:23:39, Epoch: 18, Batch: 930, Training Loss: 0.07198627889156342, LR: 0.010000000000000002
Epoch: 18, Validation Top 1 acc: 98.04936218261719
Epoch: 18, Validation Top 5 acc: 100.0
Epoch: 18, Validation Set Loss: 0.06147995591163635
Start training epoch 19
Time, 2019-01-01T22:23:46, Epoch: 19, Batch: 10, Training Loss: 0.04129111170768738, LR: 0.010000000000000002
Time, 2019-01-01T22:23:46, Epoch: 19, Batch: 20, Training Loss: 0.05721703469753266, LR: 0.010000000000000002
Time, 2019-01-01T22:23:47, Epoch: 19, Batch: 30, Training Loss: 0.07753760218620301, LR: 0.010000000000000002
Time, 2019-01-01T22:23:48, Epoch: 19, Batch: 40, Training Loss: 0.04739866219460964, LR: 0.010000000000000002
Time, 2019-01-01T22:23:49, Epoch: 19, Batch: 50, Training Loss: 0.07490398921072483, LR: 0.010000000000000002
Time, 2019-01-01T22:23:50, Epoch: 19, Batch: 60, Training Loss: 0.06385673061013222, LR: 0.010000000000000002
Time, 2019-01-01T22:23:50, Epoch: 19, Batch: 70, Training Loss: 0.07598352991044521, LR: 0.010000000000000002
Time, 2019-01-01T22:23:51, Epoch: 19, Batch: 80, Training Loss: 0.06317331716418266, LR: 0.010000000000000002
Time, 2019-01-01T22:23:52, Epoch: 19, Batch: 90, Training Loss: 0.05069146528840065, LR: 0.010000000000000002
Time, 2019-01-01T22:23:53, Epoch: 19, Batch: 100, Training Loss: 0.03337404206395149, LR: 0.010000000000000002
Time, 2019-01-01T22:23:54, Epoch: 19, Batch: 110, Training Loss: 0.08761300966143608, LR: 0.010000000000000002
Time, 2019-01-01T22:23:54, Epoch: 19, Batch: 120, Training Loss: 0.0801420196890831, LR: 0.010000000000000002
Time, 2019-01-01T22:23:55, Epoch: 19, Batch: 130, Training Loss: 0.05222293734550476, LR: 0.010000000000000002
Time, 2019-01-01T22:23:56, Epoch: 19, Batch: 140, Training Loss: 0.05902812220156193, LR: 0.010000000000000002
Time, 2019-01-01T22:23:57, Epoch: 19, Batch: 150, Training Loss: 0.05366792045533657, LR: 0.010000000000000002
Time, 2019-01-01T22:23:58, Epoch: 19, Batch: 160, Training Loss: 0.07395639307796956, LR: 0.010000000000000002
Time, 2019-01-01T22:23:58, Epoch: 19, Batch: 170, Training Loss: 0.053222959861159325, LR: 0.010000000000000002
Time, 2019-01-01T22:23:59, Epoch: 19, Batch: 180, Training Loss: 0.03024125173687935, LR: 0.010000000000000002
Time, 2019-01-01T22:24:00, Epoch: 19, Batch: 190, Training Loss: 0.057322120293974876, LR: 0.010000000000000002
Time, 2019-01-01T22:24:01, Epoch: 19, Batch: 200, Training Loss: 0.07961724326014519, LR: 0.010000000000000002
Time, 2019-01-01T22:24:02, Epoch: 19, Batch: 210, Training Loss: 0.07456585206091404, LR: 0.010000000000000002
Time, 2019-01-01T22:24:02, Epoch: 19, Batch: 220, Training Loss: 0.038486375287175176, LR: 0.010000000000000002
Time, 2019-01-01T22:24:03, Epoch: 19, Batch: 230, Training Loss: 0.03895033113658428, LR: 0.010000000000000002
Time, 2019-01-01T22:24:04, Epoch: 19, Batch: 240, Training Loss: 0.053405844047665595, LR: 0.010000000000000002
Time, 2019-01-01T22:24:05, Epoch: 19, Batch: 250, Training Loss: 0.06332866325974465, LR: 0.010000000000000002
Time, 2019-01-01T22:24:06, Epoch: 19, Batch: 260, Training Loss: 0.06538176201283932, LR: 0.010000000000000002
Time, 2019-01-01T22:24:06, Epoch: 19, Batch: 270, Training Loss: 0.06597746275365353, LR: 0.010000000000000002
Time, 2019-01-01T22:24:07, Epoch: 19, Batch: 280, Training Loss: 0.06156239919364452, LR: 0.010000000000000002
Time, 2019-01-01T22:24:08, Epoch: 19, Batch: 290, Training Loss: 0.06137847565114498, LR: 0.010000000000000002
Time, 2019-01-01T22:24:09, Epoch: 19, Batch: 300, Training Loss: 0.049755419790744784, LR: 0.010000000000000002
Time, 2019-01-01T22:24:10, Epoch: 19, Batch: 310, Training Loss: 0.07337915413081646, LR: 0.010000000000000002
Time, 2019-01-01T22:24:10, Epoch: 19, Batch: 320, Training Loss: 0.0805973805487156, LR: 0.010000000000000002
Time, 2019-01-01T22:24:11, Epoch: 19, Batch: 330, Training Loss: 0.061003773286938665, LR: 0.010000000000000002
Time, 2019-01-01T22:24:12, Epoch: 19, Batch: 340, Training Loss: 0.056224673986434937, LR: 0.010000000000000002
Time, 2019-01-01T22:24:13, Epoch: 19, Batch: 350, Training Loss: 0.05881055705249309, LR: 0.010000000000000002
Time, 2019-01-01T22:24:13, Epoch: 19, Batch: 360, Training Loss: 0.06883333884179592, LR: 0.010000000000000002
Time, 2019-01-01T22:24:14, Epoch: 19, Batch: 370, Training Loss: 0.06927402652800083, LR: 0.010000000000000002
Time, 2019-01-01T22:24:15, Epoch: 19, Batch: 380, Training Loss: 0.039644476398825644, LR: 0.010000000000000002
Time, 2019-01-01T22:24:16, Epoch: 19, Batch: 390, Training Loss: 0.06543188542127609, LR: 0.010000000000000002
Time, 2019-01-01T22:24:16, Epoch: 19, Batch: 400, Training Loss: 0.04262150265276432, LR: 0.010000000000000002
Time, 2019-01-01T22:24:17, Epoch: 19, Batch: 410, Training Loss: 0.06657961569726467, LR: 0.010000000000000002
Time, 2019-01-01T22:24:18, Epoch: 19, Batch: 420, Training Loss: 0.07195560485124589, LR: 0.010000000000000002
Time, 2019-01-01T22:24:19, Epoch: 19, Batch: 430, Training Loss: 0.06840298101305961, LR: 0.010000000000000002
Time, 2019-01-01T22:24:20, Epoch: 19, Batch: 440, Training Loss: 0.047197912633419034, LR: 0.010000000000000002
Time, 2019-01-01T22:24:20, Epoch: 19, Batch: 450, Training Loss: 0.04960289150476456, LR: 0.010000000000000002
Time, 2019-01-01T22:24:21, Epoch: 19, Batch: 460, Training Loss: 0.04997725114226341, LR: 0.010000000000000002
Time, 2019-01-01T22:24:22, Epoch: 19, Batch: 470, Training Loss: 0.05172713771462441, LR: 0.010000000000000002
Time, 2019-01-01T22:24:23, Epoch: 19, Batch: 480, Training Loss: 0.04920761100947857, LR: 0.010000000000000002
Time, 2019-01-01T22:24:23, Epoch: 19, Batch: 490, Training Loss: 0.05234788917005062, LR: 0.010000000000000002
Time, 2019-01-01T22:24:24, Epoch: 19, Batch: 500, Training Loss: 0.07416991218924522, LR: 0.010000000000000002
Time, 2019-01-01T22:24:25, Epoch: 19, Batch: 510, Training Loss: 0.038344459235668184, LR: 0.010000000000000002
Time, 2019-01-01T22:24:26, Epoch: 19, Batch: 520, Training Loss: 0.040418777614831924, LR: 0.010000000000000002
Time, 2019-01-01T22:24:26, Epoch: 19, Batch: 530, Training Loss: 0.06332792639732361, LR: 0.010000000000000002
Time, 2019-01-01T22:24:27, Epoch: 19, Batch: 540, Training Loss: 0.041908327117562295, LR: 0.010000000000000002
Time, 2019-01-01T22:24:28, Epoch: 19, Batch: 550, Training Loss: 0.07536450736224651, LR: 0.010000000000000002
Time, 2019-01-01T22:24:29, Epoch: 19, Batch: 560, Training Loss: 0.04738781191408634, LR: 0.010000000000000002
Time, 2019-01-01T22:24:30, Epoch: 19, Batch: 570, Training Loss: 0.056919683888554576, LR: 0.010000000000000002
Time, 2019-01-01T22:24:31, Epoch: 19, Batch: 580, Training Loss: 0.04370577968657017, LR: 0.010000000000000002
Time, 2019-01-01T22:24:32, Epoch: 19, Batch: 590, Training Loss: 0.037862453982234, LR: 0.010000000000000002
Time, 2019-01-01T22:24:32, Epoch: 19, Batch: 600, Training Loss: 0.057292719930410387, LR: 0.010000000000000002
Time, 2019-01-01T22:24:33, Epoch: 19, Batch: 610, Training Loss: 0.07243803925812245, LR: 0.010000000000000002
Time, 2019-01-01T22:24:34, Epoch: 19, Batch: 620, Training Loss: 0.055813790485262874, LR: 0.010000000000000002
Time, 2019-01-01T22:24:35, Epoch: 19, Batch: 630, Training Loss: 0.06643397957086564, LR: 0.010000000000000002
Time, 2019-01-01T22:24:35, Epoch: 19, Batch: 640, Training Loss: 0.07360401414334775, LR: 0.010000000000000002
Time, 2019-01-01T22:24:36, Epoch: 19, Batch: 650, Training Loss: 0.04757383652031422, LR: 0.010000000000000002
Time, 2019-01-01T22:24:37, Epoch: 19, Batch: 660, Training Loss: 0.09532084167003632, LR: 0.010000000000000002
Time, 2019-01-01T22:24:38, Epoch: 19, Batch: 670, Training Loss: 0.08403003588318825, LR: 0.010000000000000002
Time, 2019-01-01T22:24:39, Epoch: 19, Batch: 680, Training Loss: 0.07307484373450279, LR: 0.010000000000000002
Time, 2019-01-01T22:24:40, Epoch: 19, Batch: 690, Training Loss: 0.04342277571558952, LR: 0.010000000000000002
Time, 2019-01-01T22:24:40, Epoch: 19, Batch: 700, Training Loss: 0.06205864660441875, LR: 0.010000000000000002
Time, 2019-01-01T22:24:41, Epoch: 19, Batch: 710, Training Loss: 0.05878425315022469, LR: 0.010000000000000002
Time, 2019-01-01T22:24:42, Epoch: 19, Batch: 720, Training Loss: 0.048792817443609235, LR: 0.010000000000000002
Time, 2019-01-01T22:24:43, Epoch: 19, Batch: 730, Training Loss: 0.05465160831809044, LR: 0.010000000000000002
Time, 2019-01-01T22:24:44, Epoch: 19, Batch: 740, Training Loss: 0.04294045493006706, LR: 0.010000000000000002
Time, 2019-01-01T22:24:44, Epoch: 19, Batch: 750, Training Loss: 0.040777682512998584, LR: 0.010000000000000002
Time, 2019-01-01T22:24:45, Epoch: 19, Batch: 760, Training Loss: 0.05121376998722553, LR: 0.010000000000000002
Time, 2019-01-01T22:24:46, Epoch: 19, Batch: 770, Training Loss: 0.06717293299734592, LR: 0.010000000000000002
Time, 2019-01-01T22:24:47, Epoch: 19, Batch: 780, Training Loss: 0.05386571362614632, LR: 0.010000000000000002
Time, 2019-01-01T22:24:48, Epoch: 19, Batch: 790, Training Loss: 0.04259098134934902, LR: 0.010000000000000002
Time, 2019-01-01T22:24:48, Epoch: 19, Batch: 800, Training Loss: 0.06409709267318249, LR: 0.010000000000000002
Time, 2019-01-01T22:24:49, Epoch: 19, Batch: 810, Training Loss: 0.06596491560339927, LR: 0.010000000000000002
Time, 2019-01-01T22:24:50, Epoch: 19, Batch: 820, Training Loss: 0.07219144441187382, LR: 0.010000000000000002
Time, 2019-01-01T22:24:51, Epoch: 19, Batch: 830, Training Loss: 0.04095547348260879, LR: 0.010000000000000002
Time, 2019-01-01T22:24:52, Epoch: 19, Batch: 840, Training Loss: 0.04530964642763138, LR: 0.010000000000000002
Time, 2019-01-01T22:24:52, Epoch: 19, Batch: 850, Training Loss: 0.05842285193502903, LR: 0.010000000000000002
Time, 2019-01-01T22:24:53, Epoch: 19, Batch: 860, Training Loss: 0.05715875811874867, LR: 0.010000000000000002
Time, 2019-01-01T22:24:54, Epoch: 19, Batch: 870, Training Loss: 0.08019713126122952, LR: 0.010000000000000002
Time, 2019-01-01T22:24:55, Epoch: 19, Batch: 880, Training Loss: 0.0656036876142025, LR: 0.010000000000000002
Time, 2019-01-01T22:24:56, Epoch: 19, Batch: 890, Training Loss: 0.05867962911725044, LR: 0.010000000000000002
Time, 2019-01-01T22:24:56, Epoch: 19, Batch: 900, Training Loss: 0.07256905473768711, LR: 0.010000000000000002
Time, 2019-01-01T22:24:57, Epoch: 19, Batch: 910, Training Loss: 0.06277763843536377, LR: 0.010000000000000002
Time, 2019-01-01T22:24:58, Epoch: 19, Batch: 920, Training Loss: 0.050904125720262525, LR: 0.010000000000000002
Time, 2019-01-01T22:24:59, Epoch: 19, Batch: 930, Training Loss: 0.045962393283843994, LR: 0.010000000000000002
Epoch: 19, Validation Top 1 acc: 98.03941345214844
Epoch: 19, Validation Top 5 acc: 100.0
Epoch: 19, Validation Set Loss: 0.06235186383128166
Start training epoch 20
Time, 2019-01-01T22:25:05, Epoch: 20, Batch: 10, Training Loss: 0.047697878628969195, LR: 0.010000000000000002
Time, 2019-01-01T22:25:06, Epoch: 20, Batch: 20, Training Loss: 0.05507008843123913, LR: 0.010000000000000002
Time, 2019-01-01T22:25:07, Epoch: 20, Batch: 30, Training Loss: 0.06763184703886509, LR: 0.010000000000000002
Time, 2019-01-01T22:25:08, Epoch: 20, Batch: 40, Training Loss: 0.041948572173714636, LR: 0.010000000000000002
Time, 2019-01-01T22:25:09, Epoch: 20, Batch: 50, Training Loss: 0.04498183280229569, LR: 0.010000000000000002
Time, 2019-01-01T22:25:09, Epoch: 20, Batch: 60, Training Loss: 0.058502509444952014, LR: 0.010000000000000002
Time, 2019-01-01T22:25:10, Epoch: 20, Batch: 70, Training Loss: 0.05514356940984726, LR: 0.010000000000000002
Time, 2019-01-01T22:25:11, Epoch: 20, Batch: 80, Training Loss: 0.06810178384184837, LR: 0.010000000000000002
Time, 2019-01-01T22:25:12, Epoch: 20, Batch: 90, Training Loss: 0.07359187044203282, LR: 0.010000000000000002
Time, 2019-01-01T22:25:12, Epoch: 20, Batch: 100, Training Loss: 0.05948988646268845, LR: 0.010000000000000002
Time, 2019-01-01T22:25:13, Epoch: 20, Batch: 110, Training Loss: 0.0674895204603672, LR: 0.010000000000000002
Time, 2019-01-01T22:25:14, Epoch: 20, Batch: 120, Training Loss: 0.050826049596071246, LR: 0.010000000000000002
Time, 2019-01-01T22:25:15, Epoch: 20, Batch: 130, Training Loss: 0.056358502432703975, LR: 0.010000000000000002
Time, 2019-01-01T22:25:16, Epoch: 20, Batch: 140, Training Loss: 0.06402009539306164, LR: 0.010000000000000002
Time, 2019-01-01T22:25:16, Epoch: 20, Batch: 150, Training Loss: 0.04020163863897323, LR: 0.010000000000000002
Time, 2019-01-01T22:25:17, Epoch: 20, Batch: 160, Training Loss: 0.055352358520030974, LR: 0.010000000000000002
Time, 2019-01-01T22:25:18, Epoch: 20, Batch: 170, Training Loss: 0.04117983914911747, LR: 0.010000000000000002
Time, 2019-01-01T22:25:19, Epoch: 20, Batch: 180, Training Loss: 0.057785028219223024, LR: 0.010000000000000002
Time, 2019-01-01T22:25:20, Epoch: 20, Batch: 190, Training Loss: 0.04563936330378056, LR: 0.010000000000000002
Time, 2019-01-01T22:25:20, Epoch: 20, Batch: 200, Training Loss: 0.06186734810471535, LR: 0.010000000000000002
Time, 2019-01-01T22:25:21, Epoch: 20, Batch: 210, Training Loss: 0.058672431483864786, LR: 0.010000000000000002
Time, 2019-01-01T22:25:22, Epoch: 20, Batch: 220, Training Loss: 0.054568533599376676, LR: 0.010000000000000002
Time, 2019-01-01T22:25:23, Epoch: 20, Batch: 230, Training Loss: 0.045096739754080774, LR: 0.010000000000000002
Time, 2019-01-01T22:25:24, Epoch: 20, Batch: 240, Training Loss: 0.05877343714237213, LR: 0.010000000000000002
Time, 2019-01-01T22:25:25, Epoch: 20, Batch: 250, Training Loss: 0.05180060751736164, LR: 0.010000000000000002
Time, 2019-01-01T22:25:25, Epoch: 20, Batch: 260, Training Loss: 0.04584974981844425, LR: 0.010000000000000002
Time, 2019-01-01T22:25:26, Epoch: 20, Batch: 270, Training Loss: 0.07921204902231693, LR: 0.010000000000000002
Time, 2019-01-01T22:25:27, Epoch: 20, Batch: 280, Training Loss: 0.07201683446764946, LR: 0.010000000000000002
Time, 2019-01-01T22:25:28, Epoch: 20, Batch: 290, Training Loss: 0.04985508918762207, LR: 0.010000000000000002
Time, 2019-01-01T22:25:29, Epoch: 20, Batch: 300, Training Loss: 0.06685547754168511, LR: 0.010000000000000002
Time, 2019-01-01T22:25:29, Epoch: 20, Batch: 310, Training Loss: 0.0626195251941681, LR: 0.010000000000000002
Time, 2019-01-01T22:25:30, Epoch: 20, Batch: 320, Training Loss: 0.07352544665336609, LR: 0.010000000000000002
Time, 2019-01-01T22:25:31, Epoch: 20, Batch: 330, Training Loss: 0.05242966040968895, LR: 0.010000000000000002
Time, 2019-01-01T22:25:32, Epoch: 20, Batch: 340, Training Loss: 0.0523902602493763, LR: 0.010000000000000002
Time, 2019-01-01T22:25:33, Epoch: 20, Batch: 350, Training Loss: 0.0712379690259695, LR: 0.010000000000000002
Time, 2019-01-01T22:25:34, Epoch: 20, Batch: 360, Training Loss: 0.06734071373939514, LR: 0.010000000000000002
Time, 2019-01-01T22:25:34, Epoch: 20, Batch: 370, Training Loss: 0.03409034386277199, LR: 0.010000000000000002
Time, 2019-01-01T22:25:35, Epoch: 20, Batch: 380, Training Loss: 0.06849909089505672, LR: 0.010000000000000002
Time, 2019-01-01T22:25:36, Epoch: 20, Batch: 390, Training Loss: 0.05495401360094547, LR: 0.010000000000000002
Time, 2019-01-01T22:25:37, Epoch: 20, Batch: 400, Training Loss: 0.05952091254293919, LR: 0.010000000000000002
Time, 2019-01-01T22:25:38, Epoch: 20, Batch: 410, Training Loss: 0.031766250357031824, LR: 0.010000000000000002
Time, 2019-01-01T22:25:38, Epoch: 20, Batch: 420, Training Loss: 0.0539291750639677, LR: 0.010000000000000002
Time, 2019-01-01T22:25:39, Epoch: 20, Batch: 430, Training Loss: 0.03837545700371266, LR: 0.010000000000000002
Time, 2019-01-01T22:25:40, Epoch: 20, Batch: 440, Training Loss: 0.04843740724027157, LR: 0.010000000000000002
Time, 2019-01-01T22:25:41, Epoch: 20, Batch: 450, Training Loss: 0.06923381425440311, LR: 0.010000000000000002
Time, 2019-01-01T22:25:42, Epoch: 20, Batch: 460, Training Loss: 0.09689149409532546, LR: 0.010000000000000002
Time, 2019-01-01T22:25:42, Epoch: 20, Batch: 470, Training Loss: 0.06664204075932503, LR: 0.010000000000000002
Time, 2019-01-01T22:25:43, Epoch: 20, Batch: 480, Training Loss: 0.05413458421826363, LR: 0.010000000000000002
Time, 2019-01-01T22:25:44, Epoch: 20, Batch: 490, Training Loss: 0.05076387226581573, LR: 0.010000000000000002
Time, 2019-01-01T22:25:45, Epoch: 20, Batch: 500, Training Loss: 0.05820782370865345, LR: 0.010000000000000002
Time, 2019-01-01T22:25:46, Epoch: 20, Batch: 510, Training Loss: 0.06554394327104092, LR: 0.010000000000000002
Time, 2019-01-01T22:25:46, Epoch: 20, Batch: 520, Training Loss: 0.056938962638378145, LR: 0.010000000000000002
Time, 2019-01-01T22:25:47, Epoch: 20, Batch: 530, Training Loss: 0.051217560470104215, LR: 0.010000000000000002
Time, 2019-01-01T22:25:48, Epoch: 20, Batch: 540, Training Loss: 0.0441900622099638, LR: 0.010000000000000002
Time, 2019-01-01T22:25:49, Epoch: 20, Batch: 550, Training Loss: 0.04690531194210053, LR: 0.010000000000000002
Time, 2019-01-01T22:25:50, Epoch: 20, Batch: 560, Training Loss: 0.06265631504356861, LR: 0.010000000000000002
Time, 2019-01-01T22:25:50, Epoch: 20, Batch: 570, Training Loss: 0.06074683330953121, LR: 0.010000000000000002
Time, 2019-01-01T22:25:51, Epoch: 20, Batch: 580, Training Loss: 0.053392956778407094, LR: 0.010000000000000002
Time, 2019-01-01T22:25:52, Epoch: 20, Batch: 590, Training Loss: 0.0672235656529665, LR: 0.010000000000000002
Time, 2019-01-01T22:25:53, Epoch: 20, Batch: 600, Training Loss: 0.04811603836715221, LR: 0.010000000000000002
Time, 2019-01-01T22:25:54, Epoch: 20, Batch: 610, Training Loss: 0.03763563483953476, LR: 0.010000000000000002
Time, 2019-01-01T22:25:55, Epoch: 20, Batch: 620, Training Loss: 0.056113970652222636, LR: 0.010000000000000002
Time, 2019-01-01T22:25:56, Epoch: 20, Batch: 630, Training Loss: 0.07099101208150387, LR: 0.010000000000000002
Time, 2019-01-01T22:25:56, Epoch: 20, Batch: 640, Training Loss: 0.060689673945307734, LR: 0.010000000000000002
Time, 2019-01-01T22:25:57, Epoch: 20, Batch: 650, Training Loss: 0.07835159189999104, LR: 0.010000000000000002
Time, 2019-01-01T22:25:58, Epoch: 20, Batch: 660, Training Loss: 0.051613102108240126, LR: 0.010000000000000002
Time, 2019-01-01T22:25:59, Epoch: 20, Batch: 670, Training Loss: 0.06172972023487091, LR: 0.010000000000000002
Time, 2019-01-01T22:26:00, Epoch: 20, Batch: 680, Training Loss: 0.07370270639657975, LR: 0.010000000000000002
Time, 2019-01-01T22:26:00, Epoch: 20, Batch: 690, Training Loss: 0.05270123556256294, LR: 0.010000000000000002
Time, 2019-01-01T22:26:01, Epoch: 20, Batch: 700, Training Loss: 0.08130268156528472, LR: 0.010000000000000002
Time, 2019-01-01T22:26:02, Epoch: 20, Batch: 710, Training Loss: 0.06267974190413952, LR: 0.010000000000000002
Time, 2019-01-01T22:26:03, Epoch: 20, Batch: 720, Training Loss: 0.04810578525066376, LR: 0.010000000000000002
Time, 2019-01-01T22:26:03, Epoch: 20, Batch: 730, Training Loss: 0.07042640075087547, LR: 0.010000000000000002
Time, 2019-01-01T22:26:04, Epoch: 20, Batch: 740, Training Loss: 0.03387436680495739, LR: 0.010000000000000002
Time, 2019-01-01T22:26:05, Epoch: 20, Batch: 750, Training Loss: 0.07214579172432423, LR: 0.010000000000000002
Time, 2019-01-01T22:26:06, Epoch: 20, Batch: 760, Training Loss: 0.05886069089174271, LR: 0.010000000000000002
Time, 2019-01-01T22:26:06, Epoch: 20, Batch: 770, Training Loss: 0.045211668312549594, LR: 0.010000000000000002
Time, 2019-01-01T22:26:07, Epoch: 20, Batch: 780, Training Loss: 0.05315239392220974, LR: 0.010000000000000002
Time, 2019-01-01T22:26:08, Epoch: 20, Batch: 790, Training Loss: 0.08087866790592671, LR: 0.010000000000000002
Time, 2019-01-01T22:26:09, Epoch: 20, Batch: 800, Training Loss: 0.04897109419107437, LR: 0.010000000000000002
Time, 2019-01-01T22:26:09, Epoch: 20, Batch: 810, Training Loss: 0.0528822548687458, LR: 0.010000000000000002
Time, 2019-01-01T22:26:10, Epoch: 20, Batch: 820, Training Loss: 0.048418807238340376, LR: 0.010000000000000002
Time, 2019-01-01T22:26:11, Epoch: 20, Batch: 830, Training Loss: 0.07569955810904502, LR: 0.010000000000000002
Time, 2019-01-01T22:26:12, Epoch: 20, Batch: 840, Training Loss: 0.07132207006216049, LR: 0.010000000000000002
Time, 2019-01-01T22:26:13, Epoch: 20, Batch: 850, Training Loss: 0.027341172844171525, LR: 0.010000000000000002
Time, 2019-01-01T22:26:13, Epoch: 20, Batch: 860, Training Loss: 0.05015338435769081, LR: 0.010000000000000002
Time, 2019-01-01T22:26:14, Epoch: 20, Batch: 870, Training Loss: 0.08163352347910405, LR: 0.010000000000000002
Time, 2019-01-01T22:26:15, Epoch: 20, Batch: 880, Training Loss: 0.03648941703140736, LR: 0.010000000000000002
Time, 2019-01-01T22:26:16, Epoch: 20, Batch: 890, Training Loss: 0.04775390177965164, LR: 0.010000000000000002
Time, 2019-01-01T22:26:16, Epoch: 20, Batch: 900, Training Loss: 0.048166967928409576, LR: 0.010000000000000002
Time, 2019-01-01T22:26:17, Epoch: 20, Batch: 910, Training Loss: 0.056328275427222255, LR: 0.010000000000000002
Time, 2019-01-01T22:26:18, Epoch: 20, Batch: 920, Training Loss: 0.038753525167703626, LR: 0.010000000000000002
Time, 2019-01-01T22:26:19, Epoch: 20, Batch: 930, Training Loss: 0.04932354465126991, LR: 0.010000000000000002
Epoch: 20, Validation Top 1 acc: 98.09912109375
Epoch: 20, Validation Top 5 acc: 100.0
Epoch: 20, Validation Set Loss: 0.05925758555531502
Start training epoch 21
Time, 2019-01-01T22:26:25, Epoch: 21, Batch: 10, Training Loss: 0.07167863249778747, LR: 0.010000000000000002
Time, 2019-01-01T22:26:26, Epoch: 21, Batch: 20, Training Loss: 0.038819285854697225, LR: 0.010000000000000002
Time, 2019-01-01T22:26:26, Epoch: 21, Batch: 30, Training Loss: 0.05541834682226181, LR: 0.010000000000000002
Time, 2019-01-01T22:26:27, Epoch: 21, Batch: 40, Training Loss: 0.05269213877618313, LR: 0.010000000000000002
Time, 2019-01-01T22:26:28, Epoch: 21, Batch: 50, Training Loss: 0.0489200159907341, LR: 0.010000000000000002
Time, 2019-01-01T22:26:29, Epoch: 21, Batch: 60, Training Loss: 0.04254655614495277, LR: 0.010000000000000002
Time, 2019-01-01T22:26:30, Epoch: 21, Batch: 70, Training Loss: 0.03694911226630211, LR: 0.010000000000000002
Time, 2019-01-01T22:26:30, Epoch: 21, Batch: 80, Training Loss: 0.05983249805867672, LR: 0.010000000000000002
Time, 2019-01-01T22:26:31, Epoch: 21, Batch: 90, Training Loss: 0.04454643726348877, LR: 0.010000000000000002
Time, 2019-01-01T22:26:32, Epoch: 21, Batch: 100, Training Loss: 0.05058252289891243, LR: 0.010000000000000002
Time, 2019-01-01T22:26:33, Epoch: 21, Batch: 110, Training Loss: 0.035369645804166794, LR: 0.010000000000000002
Time, 2019-01-01T22:26:33, Epoch: 21, Batch: 120, Training Loss: 0.07868476659059524, LR: 0.010000000000000002
Time, 2019-01-01T22:26:34, Epoch: 21, Batch: 130, Training Loss: 0.03944121301174164, LR: 0.010000000000000002
Time, 2019-01-01T22:26:35, Epoch: 21, Batch: 140, Training Loss: 0.045792237669229505, LR: 0.010000000000000002
Time, 2019-01-01T22:26:36, Epoch: 21, Batch: 150, Training Loss: 0.04034259282052517, LR: 0.010000000000000002
Time, 2019-01-01T22:26:37, Epoch: 21, Batch: 160, Training Loss: 0.059669962525367735, LR: 0.010000000000000002
Time, 2019-01-01T22:26:37, Epoch: 21, Batch: 170, Training Loss: 0.03517009951174259, LR: 0.010000000000000002
Time, 2019-01-01T22:26:38, Epoch: 21, Batch: 180, Training Loss: 0.0576076090335846, LR: 0.010000000000000002
Time, 2019-01-01T22:26:39, Epoch: 21, Batch: 190, Training Loss: 0.0361132487654686, LR: 0.010000000000000002
Time, 2019-01-01T22:26:40, Epoch: 21, Batch: 200, Training Loss: 0.05677959695458412, LR: 0.010000000000000002
Time, 2019-01-01T22:26:40, Epoch: 21, Batch: 210, Training Loss: 0.06424996964633464, LR: 0.010000000000000002
Time, 2019-01-01T22:26:41, Epoch: 21, Batch: 220, Training Loss: 0.08426043540239334, LR: 0.010000000000000002
Time, 2019-01-01T22:26:42, Epoch: 21, Batch: 230, Training Loss: 0.02839110866189003, LR: 0.010000000000000002
Time, 2019-01-01T22:26:43, Epoch: 21, Batch: 240, Training Loss: 0.04825597293674946, LR: 0.010000000000000002
Time, 2019-01-01T22:26:44, Epoch: 21, Batch: 250, Training Loss: 0.057951852679252625, LR: 0.010000000000000002
Time, 2019-01-01T22:26:44, Epoch: 21, Batch: 260, Training Loss: 0.04904715120792389, LR: 0.010000000000000002
Time, 2019-01-01T22:26:45, Epoch: 21, Batch: 270, Training Loss: 0.03744082786142826, LR: 0.010000000000000002
Time, 2019-01-01T22:26:46, Epoch: 21, Batch: 280, Training Loss: 0.0828240592032671, LR: 0.010000000000000002
Time, 2019-01-01T22:26:47, Epoch: 21, Batch: 290, Training Loss: 0.0461771983653307, LR: 0.010000000000000002
Time, 2019-01-01T22:26:47, Epoch: 21, Batch: 300, Training Loss: 0.051954235881567, LR: 0.010000000000000002
Time, 2019-01-01T22:26:48, Epoch: 21, Batch: 310, Training Loss: 0.07720093056559563, LR: 0.010000000000000002
Time, 2019-01-01T22:26:49, Epoch: 21, Batch: 320, Training Loss: 0.05067630261182785, LR: 0.010000000000000002
Time, 2019-01-01T22:26:50, Epoch: 21, Batch: 330, Training Loss: 0.06265839114785195, LR: 0.010000000000000002
Time, 2019-01-01T22:26:50, Epoch: 21, Batch: 340, Training Loss: 0.051568218320608136, LR: 0.010000000000000002
Time, 2019-01-01T22:26:51, Epoch: 21, Batch: 350, Training Loss: 0.03281087912619114, LR: 0.010000000000000002
Time, 2019-01-01T22:26:52, Epoch: 21, Batch: 360, Training Loss: 0.04865528643131256, LR: 0.010000000000000002
Time, 2019-01-01T22:26:53, Epoch: 21, Batch: 370, Training Loss: 0.03637699782848358, LR: 0.010000000000000002
Time, 2019-01-01T22:26:54, Epoch: 21, Batch: 380, Training Loss: 0.05469126254320145, LR: 0.010000000000000002
Time, 2019-01-01T22:26:54, Epoch: 21, Batch: 390, Training Loss: 0.056867499649524686, LR: 0.010000000000000002
Time, 2019-01-01T22:26:55, Epoch: 21, Batch: 400, Training Loss: 0.0637157667428255, LR: 0.010000000000000002
Time, 2019-01-01T22:26:56, Epoch: 21, Batch: 410, Training Loss: 0.06169360876083374, LR: 0.010000000000000002
Time, 2019-01-01T22:26:57, Epoch: 21, Batch: 420, Training Loss: 0.055303525552153586, LR: 0.010000000000000002
Time, 2019-01-01T22:26:57, Epoch: 21, Batch: 430, Training Loss: 0.0708755299448967, LR: 0.010000000000000002
Time, 2019-01-01T22:26:58, Epoch: 21, Batch: 440, Training Loss: 0.03991708979010582, LR: 0.010000000000000002
Time, 2019-01-01T22:26:59, Epoch: 21, Batch: 450, Training Loss: 0.0500602163374424, LR: 0.010000000000000002
Time, 2019-01-01T22:27:00, Epoch: 21, Batch: 460, Training Loss: 0.06066316813230514, LR: 0.010000000000000002
Time, 2019-01-01T22:27:01, Epoch: 21, Batch: 470, Training Loss: 0.058730948343873025, LR: 0.010000000000000002
Time, 2019-01-01T22:27:02, Epoch: 21, Batch: 480, Training Loss: 0.04480303414165974, LR: 0.010000000000000002
Time, 2019-01-01T22:27:02, Epoch: 21, Batch: 490, Training Loss: 0.05692565068602562, LR: 0.010000000000000002
Time, 2019-01-01T22:27:03, Epoch: 21, Batch: 500, Training Loss: 0.05243322104215622, LR: 0.010000000000000002
Time, 2019-01-01T22:27:04, Epoch: 21, Batch: 510, Training Loss: 0.0474647518247366, LR: 0.010000000000000002
Time, 2019-01-01T22:27:05, Epoch: 21, Batch: 520, Training Loss: 0.05749612040817738, LR: 0.010000000000000002
Time, 2019-01-01T22:27:06, Epoch: 21, Batch: 530, Training Loss: 0.05061213746666908, LR: 0.010000000000000002
Time, 2019-01-01T22:27:06, Epoch: 21, Batch: 540, Training Loss: 0.08732628226280212, LR: 0.010000000000000002
Time, 2019-01-01T22:27:07, Epoch: 21, Batch: 550, Training Loss: 0.06633075475692748, LR: 0.010000000000000002
Time, 2019-01-01T22:27:08, Epoch: 21, Batch: 560, Training Loss: 0.04756138995289803, LR: 0.010000000000000002
Time, 2019-01-01T22:27:09, Epoch: 21, Batch: 570, Training Loss: 0.028873907029628755, LR: 0.010000000000000002
Time, 2019-01-01T22:27:09, Epoch: 21, Batch: 580, Training Loss: 0.08681595921516419, LR: 0.010000000000000002
Time, 2019-01-01T22:27:10, Epoch: 21, Batch: 590, Training Loss: 0.06189645640552044, LR: 0.010000000000000002
Time, 2019-01-01T22:27:11, Epoch: 21, Batch: 600, Training Loss: 0.04677870497107506, LR: 0.010000000000000002
Time, 2019-01-01T22:27:12, Epoch: 21, Batch: 610, Training Loss: 0.029441767558455467, LR: 0.010000000000000002
Time, 2019-01-01T22:27:13, Epoch: 21, Batch: 620, Training Loss: 0.04931244030594826, LR: 0.010000000000000002
Time, 2019-01-01T22:27:13, Epoch: 21, Batch: 630, Training Loss: 0.07411454245448112, LR: 0.010000000000000002
Time, 2019-01-01T22:27:14, Epoch: 21, Batch: 640, Training Loss: 0.061556365340948105, LR: 0.010000000000000002
Time, 2019-01-01T22:27:15, Epoch: 21, Batch: 650, Training Loss: 0.06000508964061737, LR: 0.010000000000000002
Time, 2019-01-01T22:27:16, Epoch: 21, Batch: 660, Training Loss: 0.06640759482979774, LR: 0.010000000000000002
Time, 2019-01-01T22:27:17, Epoch: 21, Batch: 670, Training Loss: 0.051022469997406006, LR: 0.010000000000000002
Time, 2019-01-01T22:27:17, Epoch: 21, Batch: 680, Training Loss: 0.0753594260662794, LR: 0.010000000000000002
Time, 2019-01-01T22:27:18, Epoch: 21, Batch: 690, Training Loss: 0.04362062811851501, LR: 0.010000000000000002
Time, 2019-01-01T22:27:19, Epoch: 21, Batch: 700, Training Loss: 0.06760354526340961, LR: 0.010000000000000002
Time, 2019-01-01T22:27:20, Epoch: 21, Batch: 710, Training Loss: 0.05228474214673042, LR: 0.010000000000000002
Time, 2019-01-01T22:27:20, Epoch: 21, Batch: 720, Training Loss: 0.05781795270740986, LR: 0.010000000000000002
Time, 2019-01-01T22:27:21, Epoch: 21, Batch: 730, Training Loss: 0.07781643457710744, LR: 0.010000000000000002
Time, 2019-01-01T22:27:22, Epoch: 21, Batch: 740, Training Loss: 0.06223735064268112, LR: 0.010000000000000002
Time, 2019-01-01T22:27:23, Epoch: 21, Batch: 750, Training Loss: 0.045574961230158806, LR: 0.010000000000000002
Time, 2019-01-01T22:27:24, Epoch: 21, Batch: 760, Training Loss: 0.05431213490664959, LR: 0.010000000000000002
Time, 2019-01-01T22:27:24, Epoch: 21, Batch: 770, Training Loss: 0.05587395317852497, LR: 0.010000000000000002
Time, 2019-01-01T22:27:25, Epoch: 21, Batch: 780, Training Loss: 0.04336926676332951, LR: 0.010000000000000002
Time, 2019-01-01T22:27:26, Epoch: 21, Batch: 790, Training Loss: 0.06415046155452728, LR: 0.010000000000000002
Time, 2019-01-01T22:27:27, Epoch: 21, Batch: 800, Training Loss: 0.055862631648778915, LR: 0.010000000000000002
Time, 2019-01-01T22:27:27, Epoch: 21, Batch: 810, Training Loss: 0.05551338903605938, LR: 0.010000000000000002
Time, 2019-01-01T22:27:28, Epoch: 21, Batch: 820, Training Loss: 0.044381958246231076, LR: 0.010000000000000002
Time, 2019-01-01T22:27:29, Epoch: 21, Batch: 830, Training Loss: 0.04679385907948017, LR: 0.010000000000000002
Time, 2019-01-01T22:27:30, Epoch: 21, Batch: 840, Training Loss: 0.059000053256750104, LR: 0.010000000000000002
Time, 2019-01-01T22:27:30, Epoch: 21, Batch: 850, Training Loss: 0.058965523913502696, LR: 0.010000000000000002
Time, 2019-01-01T22:27:31, Epoch: 21, Batch: 860, Training Loss: 0.07349355109035968, LR: 0.010000000000000002
Time, 2019-01-01T22:27:32, Epoch: 21, Batch: 870, Training Loss: 0.06066820174455643, LR: 0.010000000000000002
Time, 2019-01-01T22:27:33, Epoch: 21, Batch: 880, Training Loss: 0.05717948414385319, LR: 0.010000000000000002
Time, 2019-01-01T22:27:33, Epoch: 21, Batch: 890, Training Loss: 0.050123288109898564, LR: 0.010000000000000002
Time, 2019-01-01T22:27:34, Epoch: 21, Batch: 900, Training Loss: 0.0543885525316, LR: 0.010000000000000002
Time, 2019-01-01T22:27:35, Epoch: 21, Batch: 910, Training Loss: 0.0725240983068943, LR: 0.010000000000000002
Time, 2019-01-01T22:27:36, Epoch: 21, Batch: 920, Training Loss: 0.04931148551404476, LR: 0.010000000000000002
Time, 2019-01-01T22:27:36, Epoch: 21, Batch: 930, Training Loss: 0.06924897208809852, LR: 0.010000000000000002
Epoch: 21, Validation Top 1 acc: 98.1886978149414
Epoch: 21, Validation Top 5 acc: 100.0
Epoch: 21, Validation Set Loss: 0.05785122513771057
Start training epoch 22
Time, 2019-01-01T22:27:43, Epoch: 22, Batch: 10, Training Loss: 0.05770579688251019, LR: 0.010000000000000002
Time, 2019-01-01T22:27:44, Epoch: 22, Batch: 20, Training Loss: 0.07622175849974155, LR: 0.010000000000000002
Time, 2019-01-01T22:27:45, Epoch: 22, Batch: 30, Training Loss: 0.06310118176043034, LR: 0.010000000000000002
Time, 2019-01-01T22:27:45, Epoch: 22, Batch: 40, Training Loss: 0.03500362709164619, LR: 0.010000000000000002
Time, 2019-01-01T22:27:46, Epoch: 22, Batch: 50, Training Loss: 0.04685053937137127, LR: 0.010000000000000002
Time, 2019-01-01T22:27:47, Epoch: 22, Batch: 60, Training Loss: 0.0470980454236269, LR: 0.010000000000000002
Time, 2019-01-01T22:27:48, Epoch: 22, Batch: 70, Training Loss: 0.04255422316491604, LR: 0.010000000000000002
Time, 2019-01-01T22:27:49, Epoch: 22, Batch: 80, Training Loss: 0.051954570785164834, LR: 0.010000000000000002
Time, 2019-01-01T22:27:50, Epoch: 22, Batch: 90, Training Loss: 0.05041138678789139, LR: 0.010000000000000002
Time, 2019-01-01T22:27:50, Epoch: 22, Batch: 100, Training Loss: 0.04563875235617161, LR: 0.010000000000000002
Time, 2019-01-01T22:27:51, Epoch: 22, Batch: 110, Training Loss: 0.041483699530363086, LR: 0.010000000000000002
Time, 2019-01-01T22:27:52, Epoch: 22, Batch: 120, Training Loss: 0.0443801287561655, LR: 0.010000000000000002
Time, 2019-01-01T22:27:53, Epoch: 22, Batch: 130, Training Loss: 0.06607331782579422, LR: 0.010000000000000002
Time, 2019-01-01T22:27:54, Epoch: 22, Batch: 140, Training Loss: 0.040257475525140765, LR: 0.010000000000000002
Time, 2019-01-01T22:27:55, Epoch: 22, Batch: 150, Training Loss: 0.059205058962106705, LR: 0.010000000000000002
Time, 2019-01-01T22:27:55, Epoch: 22, Batch: 160, Training Loss: 0.08186279088258744, LR: 0.010000000000000002
Time, 2019-01-01T22:27:56, Epoch: 22, Batch: 170, Training Loss: 0.07731005810201168, LR: 0.010000000000000002
Time, 2019-01-01T22:27:57, Epoch: 22, Batch: 180, Training Loss: 0.053484543785452844, LR: 0.010000000000000002
Time, 2019-01-01T22:27:58, Epoch: 22, Batch: 190, Training Loss: 0.05315658636391163, LR: 0.010000000000000002
Time, 2019-01-01T22:27:59, Epoch: 22, Batch: 200, Training Loss: 0.05313750356435776, LR: 0.010000000000000002
Time, 2019-01-01T22:28:00, Epoch: 22, Batch: 210, Training Loss: 0.07246256694197654, LR: 0.010000000000000002
Time, 2019-01-01T22:28:00, Epoch: 22, Batch: 220, Training Loss: 0.0501457691192627, LR: 0.010000000000000002
Time, 2019-01-01T22:28:01, Epoch: 22, Batch: 230, Training Loss: 0.06685357168316841, LR: 0.010000000000000002
Time, 2019-01-01T22:28:02, Epoch: 22, Batch: 240, Training Loss: 0.06924624741077423, LR: 0.010000000000000002
Time, 2019-01-01T22:28:03, Epoch: 22, Batch: 250, Training Loss: 0.052078866958618165, LR: 0.010000000000000002
Time, 2019-01-01T22:28:04, Epoch: 22, Batch: 260, Training Loss: 0.06049070842564106, LR: 0.010000000000000002
Time, 2019-01-01T22:28:05, Epoch: 22, Batch: 270, Training Loss: 0.06355346851050854, LR: 0.010000000000000002
Time, 2019-01-01T22:28:06, Epoch: 22, Batch: 280, Training Loss: 0.06389760076999665, LR: 0.010000000000000002
Time, 2019-01-01T22:28:07, Epoch: 22, Batch: 290, Training Loss: 0.05929587334394455, LR: 0.010000000000000002
Time, 2019-01-01T22:28:07, Epoch: 22, Batch: 300, Training Loss: 0.05258458256721497, LR: 0.010000000000000002
Time, 2019-01-01T22:28:08, Epoch: 22, Batch: 310, Training Loss: 0.07799157947301864, LR: 0.010000000000000002
Time, 2019-01-01T22:28:09, Epoch: 22, Batch: 320, Training Loss: 0.059516222402453425, LR: 0.010000000000000002
Time, 2019-01-01T22:28:10, Epoch: 22, Batch: 330, Training Loss: 0.05822577849030495, LR: 0.010000000000000002
Time, 2019-01-01T22:28:11, Epoch: 22, Batch: 340, Training Loss: 0.046468807756900786, LR: 0.010000000000000002
Time, 2019-01-01T22:28:12, Epoch: 22, Batch: 350, Training Loss: 0.043711244314908984, LR: 0.010000000000000002
Time, 2019-01-01T22:28:13, Epoch: 22, Batch: 360, Training Loss: 0.05197878293693066, LR: 0.010000000000000002
Time, 2019-01-01T22:28:14, Epoch: 22, Batch: 370, Training Loss: 0.04469673931598663, LR: 0.010000000000000002
Time, 2019-01-01T22:28:14, Epoch: 22, Batch: 380, Training Loss: 0.07291009984910488, LR: 0.010000000000000002
Time, 2019-01-01T22:28:15, Epoch: 22, Batch: 390, Training Loss: 0.09471384324133396, LR: 0.010000000000000002
Time, 2019-01-01T22:28:16, Epoch: 22, Batch: 400, Training Loss: 0.0706833004951477, LR: 0.010000000000000002
Time, 2019-01-01T22:28:17, Epoch: 22, Batch: 410, Training Loss: 0.062177830189466474, LR: 0.010000000000000002
Time, 2019-01-01T22:28:18, Epoch: 22, Batch: 420, Training Loss: 0.054917029291391375, LR: 0.010000000000000002
Time, 2019-01-01T22:28:19, Epoch: 22, Batch: 430, Training Loss: 0.04820214286446571, LR: 0.010000000000000002
Time, 2019-01-01T22:28:19, Epoch: 22, Batch: 440, Training Loss: 0.08087585866451263, LR: 0.010000000000000002
Time, 2019-01-01T22:28:20, Epoch: 22, Batch: 450, Training Loss: 0.059372953698039056, LR: 0.010000000000000002
Time, 2019-01-01T22:28:21, Epoch: 22, Batch: 460, Training Loss: 0.04574480690062046, LR: 0.010000000000000002
Time, 2019-01-01T22:28:22, Epoch: 22, Batch: 470, Training Loss: 0.06782521530985833, LR: 0.010000000000000002
Time, 2019-01-01T22:28:23, Epoch: 22, Batch: 480, Training Loss: 0.0664247278124094, LR: 0.010000000000000002
Time, 2019-01-01T22:28:23, Epoch: 22, Batch: 490, Training Loss: 0.06896400079131126, LR: 0.010000000000000002
Time, 2019-01-01T22:28:24, Epoch: 22, Batch: 500, Training Loss: 0.05791208073496819, LR: 0.010000000000000002
Time, 2019-01-01T22:28:25, Epoch: 22, Batch: 510, Training Loss: 0.04843058362603188, LR: 0.010000000000000002
Time, 2019-01-01T22:28:26, Epoch: 22, Batch: 520, Training Loss: 0.07029185555875302, LR: 0.010000000000000002
Time, 2019-01-01T22:28:27, Epoch: 22, Batch: 530, Training Loss: 0.0740854188799858, LR: 0.010000000000000002
Time, 2019-01-01T22:28:28, Epoch: 22, Batch: 540, Training Loss: 0.048703644797205924, LR: 0.010000000000000002
Time, 2019-01-01T22:28:28, Epoch: 22, Batch: 550, Training Loss: 0.08646149188280106, LR: 0.010000000000000002
Time, 2019-01-01T22:28:29, Epoch: 22, Batch: 560, Training Loss: 0.04611809477210045, LR: 0.010000000000000002
Time, 2019-01-01T22:28:30, Epoch: 22, Batch: 570, Training Loss: 0.05273177400231362, LR: 0.010000000000000002
Time, 2019-01-01T22:28:31, Epoch: 22, Batch: 580, Training Loss: 0.04516014195978642, LR: 0.010000000000000002
Time, 2019-01-01T22:28:32, Epoch: 22, Batch: 590, Training Loss: 0.060733841732144356, LR: 0.010000000000000002
Time, 2019-01-01T22:28:33, Epoch: 22, Batch: 600, Training Loss: 0.04450789205729962, LR: 0.010000000000000002
Time, 2019-01-01T22:28:34, Epoch: 22, Batch: 610, Training Loss: 0.05060463882982731, LR: 0.010000000000000002
Time, 2019-01-01T22:28:34, Epoch: 22, Batch: 620, Training Loss: 0.06594179831445217, LR: 0.010000000000000002
Time, 2019-01-01T22:28:35, Epoch: 22, Batch: 630, Training Loss: 0.06307786628603936, LR: 0.010000000000000002
Time, 2019-01-01T22:28:36, Epoch: 22, Batch: 640, Training Loss: 0.07805443666875363, LR: 0.010000000000000002
Time, 2019-01-01T22:28:37, Epoch: 22, Batch: 650, Training Loss: 0.03918454498052597, LR: 0.010000000000000002
Time, 2019-01-01T22:28:38, Epoch: 22, Batch: 660, Training Loss: 0.05526789799332619, LR: 0.010000000000000002
Time, 2019-01-01T22:28:38, Epoch: 22, Batch: 670, Training Loss: 0.06412070132791996, LR: 0.010000000000000002
Time, 2019-01-01T22:28:39, Epoch: 22, Batch: 680, Training Loss: 0.06977315619587898, LR: 0.010000000000000002
Time, 2019-01-01T22:28:40, Epoch: 22, Batch: 690, Training Loss: 0.0766408957540989, LR: 0.010000000000000002
Time, 2019-01-01T22:28:41, Epoch: 22, Batch: 700, Training Loss: 0.061879422143101694, LR: 0.010000000000000002
Time, 2019-01-01T22:28:42, Epoch: 22, Batch: 710, Training Loss: 0.06434955187141896, LR: 0.010000000000000002
Time, 2019-01-01T22:28:43, Epoch: 22, Batch: 720, Training Loss: 0.04848249480128288, LR: 0.010000000000000002
Time, 2019-01-01T22:28:43, Epoch: 22, Batch: 730, Training Loss: 0.09626567512750625, LR: 0.010000000000000002
Time, 2019-01-01T22:28:44, Epoch: 22, Batch: 740, Training Loss: 0.05181115753948688, LR: 0.010000000000000002
Time, 2019-01-01T22:28:45, Epoch: 22, Batch: 750, Training Loss: 0.05325437970459461, LR: 0.010000000000000002
Time, 2019-01-01T22:28:46, Epoch: 22, Batch: 760, Training Loss: 0.056370675936341284, LR: 0.010000000000000002
Time, 2019-01-01T22:28:47, Epoch: 22, Batch: 770, Training Loss: 0.06888938248157501, LR: 0.010000000000000002
Time, 2019-01-01T22:28:47, Epoch: 22, Batch: 780, Training Loss: 0.05999180413782597, LR: 0.010000000000000002
Time, 2019-01-01T22:28:48, Epoch: 22, Batch: 790, Training Loss: 0.06414425671100617, LR: 0.010000000000000002
Time, 2019-01-01T22:28:49, Epoch: 22, Batch: 800, Training Loss: 0.047670718654990195, LR: 0.010000000000000002
Time, 2019-01-01T22:28:50, Epoch: 22, Batch: 810, Training Loss: 0.059039099141955376, LR: 0.010000000000000002
Time, 2019-01-01T22:28:51, Epoch: 22, Batch: 820, Training Loss: 0.07608130276203155, LR: 0.010000000000000002
Time, 2019-01-01T22:28:51, Epoch: 22, Batch: 830, Training Loss: 0.058530251309275624, LR: 0.010000000000000002
Time, 2019-01-01T22:28:52, Epoch: 22, Batch: 840, Training Loss: 0.04129495322704315, LR: 0.010000000000000002
Time, 2019-01-01T22:28:53, Epoch: 22, Batch: 850, Training Loss: 0.06445090733468532, LR: 0.010000000000000002
Time, 2019-01-01T22:28:54, Epoch: 22, Batch: 860, Training Loss: 0.06503636054694653, LR: 0.010000000000000002
Time, 2019-01-01T22:28:55, Epoch: 22, Batch: 870, Training Loss: 0.04799695536494255, LR: 0.010000000000000002
Time, 2019-01-01T22:28:55, Epoch: 22, Batch: 880, Training Loss: 0.0621108815073967, LR: 0.010000000000000002
Time, 2019-01-01T22:28:56, Epoch: 22, Batch: 890, Training Loss: 0.06069944761693478, LR: 0.010000000000000002
Time, 2019-01-01T22:28:57, Epoch: 22, Batch: 900, Training Loss: 0.059372474998235704, LR: 0.010000000000000002
Time, 2019-01-01T22:28:58, Epoch: 22, Batch: 910, Training Loss: 0.05684165358543396, LR: 0.010000000000000002
Time, 2019-01-01T22:28:59, Epoch: 22, Batch: 920, Training Loss: 0.05534512549638748, LR: 0.010000000000000002
Time, 2019-01-01T22:29:00, Epoch: 22, Batch: 930, Training Loss: 0.077411600202322, LR: 0.010000000000000002
Epoch: 22, Validation Top 1 acc: 98.0195083618164
Epoch: 22, Validation Top 5 acc: 100.0
Epoch: 22, Validation Set Loss: 0.06364002078771591
Start training epoch 23
Time, 2019-01-01T22:29:06, Epoch: 23, Batch: 10, Training Loss: 0.08965187557041646, LR: 0.010000000000000002
Time, 2019-01-01T22:29:07, Epoch: 23, Batch: 20, Training Loss: 0.059626980870962146, LR: 0.010000000000000002
Time, 2019-01-01T22:29:08, Epoch: 23, Batch: 30, Training Loss: 0.0515154741704464, LR: 0.010000000000000002
Time, 2019-01-01T22:29:08, Epoch: 23, Batch: 40, Training Loss: 0.03421122021973133, LR: 0.010000000000000002
Time, 2019-01-01T22:29:09, Epoch: 23, Batch: 50, Training Loss: 0.08111552745103837, LR: 0.010000000000000002
Time, 2019-01-01T22:29:10, Epoch: 23, Batch: 60, Training Loss: 0.05362638980150223, LR: 0.010000000000000002
Time, 2019-01-01T22:29:11, Epoch: 23, Batch: 70, Training Loss: 0.060553088411688806, LR: 0.010000000000000002
Time, 2019-01-01T22:29:12, Epoch: 23, Batch: 80, Training Loss: 0.05423036441206932, LR: 0.010000000000000002
Time, 2019-01-01T22:29:13, Epoch: 23, Batch: 90, Training Loss: 0.06660014912486076, LR: 0.010000000000000002
Time, 2019-01-01T22:29:13, Epoch: 23, Batch: 100, Training Loss: 0.05017266944050789, LR: 0.010000000000000002
Time, 2019-01-01T22:29:14, Epoch: 23, Batch: 110, Training Loss: 0.06689300797879696, LR: 0.010000000000000002
Time, 2019-01-01T22:29:15, Epoch: 23, Batch: 120, Training Loss: 0.06172984689474106, LR: 0.010000000000000002
Time, 2019-01-01T22:29:16, Epoch: 23, Batch: 130, Training Loss: 0.042768140137195584, LR: 0.010000000000000002
Time, 2019-01-01T22:29:17, Epoch: 23, Batch: 140, Training Loss: 0.0540152620524168, LR: 0.010000000000000002
Time, 2019-01-01T22:29:17, Epoch: 23, Batch: 150, Training Loss: 0.0621250756084919, LR: 0.010000000000000002
Time, 2019-01-01T22:29:18, Epoch: 23, Batch: 160, Training Loss: 0.04529813155531883, LR: 0.010000000000000002
Time, 2019-01-01T22:29:19, Epoch: 23, Batch: 170, Training Loss: 0.06487137414515018, LR: 0.010000000000000002
Time, 2019-01-01T22:29:20, Epoch: 23, Batch: 180, Training Loss: 0.04972892105579376, LR: 0.010000000000000002
Time, 2019-01-01T22:29:21, Epoch: 23, Batch: 190, Training Loss: 0.0865325778722763, LR: 0.010000000000000002
Time, 2019-01-01T22:29:21, Epoch: 23, Batch: 200, Training Loss: 0.04316690303385258, LR: 0.010000000000000002
Time, 2019-01-01T22:29:22, Epoch: 23, Batch: 210, Training Loss: 0.05212149135768414, LR: 0.010000000000000002
Time, 2019-01-01T22:29:23, Epoch: 23, Batch: 220, Training Loss: 0.05442210026085377, LR: 0.010000000000000002
Time, 2019-01-01T22:29:24, Epoch: 23, Batch: 230, Training Loss: 0.06373229511082172, LR: 0.010000000000000002
Time, 2019-01-01T22:29:25, Epoch: 23, Batch: 240, Training Loss: 0.0507656142115593, LR: 0.010000000000000002
Time, 2019-01-01T22:29:26, Epoch: 23, Batch: 250, Training Loss: 0.04064783900976181, LR: 0.010000000000000002
Time, 2019-01-01T22:29:27, Epoch: 23, Batch: 260, Training Loss: 0.04493566229939461, LR: 0.010000000000000002
Time, 2019-01-01T22:29:27, Epoch: 23, Batch: 270, Training Loss: 0.05360228195786476, LR: 0.010000000000000002
Time, 2019-01-01T22:29:28, Epoch: 23, Batch: 280, Training Loss: 0.06437496654689312, LR: 0.010000000000000002
Time, 2019-01-01T22:29:29, Epoch: 23, Batch: 290, Training Loss: 0.062122081592679027, LR: 0.010000000000000002
Time, 2019-01-01T22:29:30, Epoch: 23, Batch: 300, Training Loss: 0.04960879683494568, LR: 0.010000000000000002
Time, 2019-01-01T22:29:31, Epoch: 23, Batch: 310, Training Loss: 0.07941478081047534, LR: 0.010000000000000002
Time, 2019-01-01T22:29:31, Epoch: 23, Batch: 320, Training Loss: 0.07700510621070862, LR: 0.010000000000000002
Time, 2019-01-01T22:29:32, Epoch: 23, Batch: 330, Training Loss: 0.04836501888930798, LR: 0.010000000000000002
Time, 2019-01-01T22:29:33, Epoch: 23, Batch: 340, Training Loss: 0.061890432611107826, LR: 0.010000000000000002
Time, 2019-01-01T22:29:34, Epoch: 23, Batch: 350, Training Loss: 0.0685061801224947, LR: 0.010000000000000002
Time, 2019-01-01T22:29:35, Epoch: 23, Batch: 360, Training Loss: 0.037776627019047736, LR: 0.010000000000000002
Time, 2019-01-01T22:29:35, Epoch: 23, Batch: 370, Training Loss: 0.05002011433243751, LR: 0.010000000000000002
Time, 2019-01-01T22:29:36, Epoch: 23, Batch: 380, Training Loss: 0.06547298356890678, LR: 0.010000000000000002
Time, 2019-01-01T22:29:37, Epoch: 23, Batch: 390, Training Loss: 0.04487860500812531, LR: 0.010000000000000002
Time, 2019-01-01T22:29:38, Epoch: 23, Batch: 400, Training Loss: 0.0655344732105732, LR: 0.010000000000000002
Time, 2019-01-01T22:29:39, Epoch: 23, Batch: 410, Training Loss: 0.05754410549998283, LR: 0.010000000000000002
Time, 2019-01-01T22:29:40, Epoch: 23, Batch: 420, Training Loss: 0.04298490695655346, LR: 0.010000000000000002
Time, 2019-01-01T22:29:41, Epoch: 23, Batch: 430, Training Loss: 0.05688062347471714, LR: 0.010000000000000002
Time, 2019-01-01T22:29:42, Epoch: 23, Batch: 440, Training Loss: 0.05797135904431343, LR: 0.010000000000000002
Time, 2019-01-01T22:29:43, Epoch: 23, Batch: 450, Training Loss: 0.05304502733051777, LR: 0.010000000000000002
Time, 2019-01-01T22:29:44, Epoch: 23, Batch: 460, Training Loss: 0.0677214317023754, LR: 0.010000000000000002
Time, 2019-01-01T22:29:44, Epoch: 23, Batch: 470, Training Loss: 0.06695327684283256, LR: 0.010000000000000002
Time, 2019-01-01T22:29:45, Epoch: 23, Batch: 480, Training Loss: 0.05410593152046204, LR: 0.010000000000000002
Time, 2019-01-01T22:29:46, Epoch: 23, Batch: 490, Training Loss: 0.06293647810816765, LR: 0.010000000000000002
Time, 2019-01-01T22:29:47, Epoch: 23, Batch: 500, Training Loss: 0.045224875956773755, LR: 0.010000000000000002
Time, 2019-01-01T22:29:48, Epoch: 23, Batch: 510, Training Loss: 0.06169223189353943, LR: 0.010000000000000002
Time, 2019-01-01T22:29:49, Epoch: 23, Batch: 520, Training Loss: 0.06081306710839272, LR: 0.010000000000000002
Time, 2019-01-01T22:29:50, Epoch: 23, Batch: 530, Training Loss: 0.09976769909262657, LR: 0.010000000000000002
Time, 2019-01-01T22:29:51, Epoch: 23, Batch: 540, Training Loss: 0.06357378847897052, LR: 0.010000000000000002
Time, 2019-01-01T22:29:52, Epoch: 23, Batch: 550, Training Loss: 0.0825288150459528, LR: 0.010000000000000002
Time, 2019-01-01T22:29:53, Epoch: 23, Batch: 560, Training Loss: 0.05353991314768791, LR: 0.010000000000000002
Time, 2019-01-01T22:29:53, Epoch: 23, Batch: 570, Training Loss: 0.04969439767301083, LR: 0.010000000000000002
Time, 2019-01-01T22:29:54, Epoch: 23, Batch: 580, Training Loss: 0.038175341486930844, LR: 0.010000000000000002
Time, 2019-01-01T22:29:55, Epoch: 23, Batch: 590, Training Loss: 0.03936301469802857, LR: 0.010000000000000002
Time, 2019-01-01T22:29:56, Epoch: 23, Batch: 600, Training Loss: 0.04715136997401714, LR: 0.010000000000000002
Time, 2019-01-01T22:29:57, Epoch: 23, Batch: 610, Training Loss: 0.04659474119544029, LR: 0.010000000000000002
Time, 2019-01-01T22:29:57, Epoch: 23, Batch: 620, Training Loss: 0.05148537643253803, LR: 0.010000000000000002
Time, 2019-01-01T22:29:58, Epoch: 23, Batch: 630, Training Loss: 0.04884715713560581, LR: 0.010000000000000002
Time, 2019-01-01T22:29:59, Epoch: 23, Batch: 640, Training Loss: 0.04527920112013817, LR: 0.010000000000000002
Time, 2019-01-01T22:30:00, Epoch: 23, Batch: 650, Training Loss: 0.06793074011802673, LR: 0.010000000000000002
Time, 2019-01-01T22:30:00, Epoch: 23, Batch: 660, Training Loss: 0.059542573615908624, LR: 0.010000000000000002
Time, 2019-01-01T22:30:01, Epoch: 23, Batch: 670, Training Loss: 0.05656751319766044, LR: 0.010000000000000002
Time, 2019-01-01T22:30:02, Epoch: 23, Batch: 680, Training Loss: 0.053665336966514585, LR: 0.010000000000000002
Time, 2019-01-01T22:30:03, Epoch: 23, Batch: 690, Training Loss: 0.038101598620414734, LR: 0.010000000000000002
Time, 2019-01-01T22:30:04, Epoch: 23, Batch: 700, Training Loss: 0.058332991972565654, LR: 0.010000000000000002
Time, 2019-01-01T22:30:04, Epoch: 23, Batch: 710, Training Loss: 0.05136979930102825, LR: 0.010000000000000002
Time, 2019-01-01T22:30:05, Epoch: 23, Batch: 720, Training Loss: 0.0631741501390934, LR: 0.010000000000000002
Time, 2019-01-01T22:30:06, Epoch: 23, Batch: 730, Training Loss: 0.05721171610057354, LR: 0.010000000000000002
Time, 2019-01-01T22:30:07, Epoch: 23, Batch: 740, Training Loss: 0.06521829292178154, LR: 0.010000000000000002
Time, 2019-01-01T22:30:08, Epoch: 23, Batch: 750, Training Loss: 0.06402843296527863, LR: 0.010000000000000002
Time, 2019-01-01T22:30:08, Epoch: 23, Batch: 760, Training Loss: 0.059159614890813825, LR: 0.010000000000000002
Time, 2019-01-01T22:30:09, Epoch: 23, Batch: 770, Training Loss: 0.07050235569477081, LR: 0.010000000000000002
Time, 2019-01-01T22:30:10, Epoch: 23, Batch: 780, Training Loss: 0.04886556603014469, LR: 0.010000000000000002
Time, 2019-01-01T22:30:11, Epoch: 23, Batch: 790, Training Loss: 0.04835525006055832, LR: 0.010000000000000002
Time, 2019-01-01T22:30:12, Epoch: 23, Batch: 800, Training Loss: 0.07083445563912391, LR: 0.010000000000000002
Time, 2019-01-01T22:30:12, Epoch: 23, Batch: 810, Training Loss: 0.05167073979973793, LR: 0.010000000000000002
Time, 2019-01-01T22:30:13, Epoch: 23, Batch: 820, Training Loss: 0.0728262834250927, LR: 0.010000000000000002
Time, 2019-01-01T22:30:14, Epoch: 23, Batch: 830, Training Loss: 0.04630566947162151, LR: 0.010000000000000002
Time, 2019-01-01T22:30:15, Epoch: 23, Batch: 840, Training Loss: 0.054483019188046454, LR: 0.010000000000000002
Time, 2019-01-01T22:30:16, Epoch: 23, Batch: 850, Training Loss: 0.04922558329999447, LR: 0.010000000000000002
Time, 2019-01-01T22:30:16, Epoch: 23, Batch: 860, Training Loss: 0.05315560325980186, LR: 0.010000000000000002
Time, 2019-01-01T22:30:17, Epoch: 23, Batch: 870, Training Loss: 0.04644695818424225, LR: 0.010000000000000002
Time, 2019-01-01T22:30:18, Epoch: 23, Batch: 880, Training Loss: 0.04117579460144043, LR: 0.010000000000000002
Time, 2019-01-01T22:30:19, Epoch: 23, Batch: 890, Training Loss: 0.05893299058079719, LR: 0.010000000000000002
Time, 2019-01-01T22:30:20, Epoch: 23, Batch: 900, Training Loss: 0.05859564878046512, LR: 0.010000000000000002
Time, 2019-01-01T22:30:20, Epoch: 23, Batch: 910, Training Loss: 0.03725009635090828, LR: 0.010000000000000002
Time, 2019-01-01T22:30:21, Epoch: 23, Batch: 920, Training Loss: 0.05922696962952614, LR: 0.010000000000000002
Time, 2019-01-01T22:30:22, Epoch: 23, Batch: 930, Training Loss: 0.07044936418533325, LR: 0.010000000000000002
Epoch: 23, Validation Top 1 acc: 98.22850036621094
Epoch: 23, Validation Top 5 acc: 99.99005126953125
Epoch: 23, Validation Set Loss: 0.05605558678507805
Start training epoch 24
Time, 2019-01-01T22:30:29, Epoch: 24, Batch: 10, Training Loss: 0.06467977054417133, LR: 0.010000000000000002
Time, 2019-01-01T22:30:30, Epoch: 24, Batch: 20, Training Loss: 0.05969258323311806, LR: 0.010000000000000002
Time, 2019-01-01T22:30:30, Epoch: 24, Batch: 30, Training Loss: 0.05787146389484406, LR: 0.010000000000000002
Time, 2019-01-01T22:30:31, Epoch: 24, Batch: 40, Training Loss: 0.06100988462567329, LR: 0.010000000000000002
Time, 2019-01-01T22:30:32, Epoch: 24, Batch: 50, Training Loss: 0.04382668919861317, LR: 0.010000000000000002
Time, 2019-01-01T22:30:33, Epoch: 24, Batch: 60, Training Loss: 0.037367910891771314, LR: 0.010000000000000002
Time, 2019-01-01T22:30:34, Epoch: 24, Batch: 70, Training Loss: 0.05205381028354168, LR: 0.010000000000000002
Time, 2019-01-01T22:30:34, Epoch: 24, Batch: 80, Training Loss: 0.04085861295461655, LR: 0.010000000000000002
Time, 2019-01-01T22:30:35, Epoch: 24, Batch: 90, Training Loss: 0.06682499870657921, LR: 0.010000000000000002
Time, 2019-01-01T22:30:36, Epoch: 24, Batch: 100, Training Loss: 0.042038082331418994, LR: 0.010000000000000002
Time, 2019-01-01T22:30:37, Epoch: 24, Batch: 110, Training Loss: 0.0698410801589489, LR: 0.010000000000000002
Time, 2019-01-01T22:30:37, Epoch: 24, Batch: 120, Training Loss: 0.06858545988798141, LR: 0.010000000000000002
Time, 2019-01-01T22:30:38, Epoch: 24, Batch: 130, Training Loss: 0.05325598455965519, LR: 0.010000000000000002
Time, 2019-01-01T22:30:39, Epoch: 24, Batch: 140, Training Loss: 0.04821585491299629, LR: 0.010000000000000002
Time, 2019-01-01T22:30:40, Epoch: 24, Batch: 150, Training Loss: 0.047331725060939786, LR: 0.010000000000000002
Time, 2019-01-01T22:30:41, Epoch: 24, Batch: 160, Training Loss: 0.055386641249060634, LR: 0.010000000000000002
Time, 2019-01-01T22:30:41, Epoch: 24, Batch: 170, Training Loss: 0.05188216790556908, LR: 0.010000000000000002
Time, 2019-01-01T22:30:42, Epoch: 24, Batch: 180, Training Loss: 0.046064949780702594, LR: 0.010000000000000002
Time, 2019-01-01T22:30:43, Epoch: 24, Batch: 190, Training Loss: 0.057494976371526715, LR: 0.010000000000000002
Time, 2019-01-01T22:30:44, Epoch: 24, Batch: 200, Training Loss: 0.05856183357536793, LR: 0.010000000000000002
Time, 2019-01-01T22:30:44, Epoch: 24, Batch: 210, Training Loss: 0.05154756084084511, LR: 0.010000000000000002
Time, 2019-01-01T22:30:45, Epoch: 24, Batch: 220, Training Loss: 0.060919178649783134, LR: 0.010000000000000002
Time, 2019-01-01T22:30:46, Epoch: 24, Batch: 230, Training Loss: 0.04953503832221031, LR: 0.010000000000000002
Time, 2019-01-01T22:30:47, Epoch: 24, Batch: 240, Training Loss: 0.04763409234583378, LR: 0.010000000000000002
Time, 2019-01-01T22:30:48, Epoch: 24, Batch: 250, Training Loss: 0.06166162639856339, LR: 0.010000000000000002
Time, 2019-01-01T22:30:48, Epoch: 24, Batch: 260, Training Loss: 0.0657130729407072, LR: 0.010000000000000002
Time, 2019-01-01T22:30:49, Epoch: 24, Batch: 270, Training Loss: 0.04570944122970104, LR: 0.010000000000000002
Time, 2019-01-01T22:30:50, Epoch: 24, Batch: 280, Training Loss: 0.04505334347486496, LR: 0.010000000000000002
Time, 2019-01-01T22:30:51, Epoch: 24, Batch: 290, Training Loss: 0.05439866073429585, LR: 0.010000000000000002
Time, 2019-01-01T22:30:52, Epoch: 24, Batch: 300, Training Loss: 0.0632889624685049, LR: 0.010000000000000002
Time, 2019-01-01T22:30:52, Epoch: 24, Batch: 310, Training Loss: 0.044373898580670354, LR: 0.010000000000000002
Time, 2019-01-01T22:30:53, Epoch: 24, Batch: 320, Training Loss: 0.05998498611152172, LR: 0.010000000000000002
Time, 2019-01-01T22:30:54, Epoch: 24, Batch: 330, Training Loss: 0.05536298155784607, LR: 0.010000000000000002
Time, 2019-01-01T22:30:55, Epoch: 24, Batch: 340, Training Loss: 0.046675261855125424, LR: 0.010000000000000002
Time, 2019-01-01T22:30:56, Epoch: 24, Batch: 350, Training Loss: 0.054860270023345946, LR: 0.010000000000000002
Time, 2019-01-01T22:30:57, Epoch: 24, Batch: 360, Training Loss: 0.07364858947694301, LR: 0.010000000000000002
Time, 2019-01-01T22:30:58, Epoch: 24, Batch: 370, Training Loss: 0.04035168439149857, LR: 0.010000000000000002
Time, 2019-01-01T22:30:59, Epoch: 24, Batch: 380, Training Loss: 0.06490405723452568, LR: 0.010000000000000002
Time, 2019-01-01T22:31:00, Epoch: 24, Batch: 390, Training Loss: 0.060822943598031996, LR: 0.010000000000000002
Time, 2019-01-01T22:31:00, Epoch: 24, Batch: 400, Training Loss: 0.0759070485830307, LR: 0.010000000000000002
Time, 2019-01-01T22:31:01, Epoch: 24, Batch: 410, Training Loss: 0.03619797304272652, LR: 0.010000000000000002
Time, 2019-01-01T22:31:02, Epoch: 24, Batch: 420, Training Loss: 0.05298961438238621, LR: 0.010000000000000002
Time, 2019-01-01T22:31:03, Epoch: 24, Batch: 430, Training Loss: 0.06241204664111137, LR: 0.010000000000000002
Time, 2019-01-01T22:31:04, Epoch: 24, Batch: 440, Training Loss: 0.055013582855463025, LR: 0.010000000000000002
Time, 2019-01-01T22:31:05, Epoch: 24, Batch: 450, Training Loss: 0.03950537331402302, LR: 0.010000000000000002
Time, 2019-01-01T22:31:06, Epoch: 24, Batch: 460, Training Loss: 0.06486346796154976, LR: 0.010000000000000002
Time, 2019-01-01T22:31:07, Epoch: 24, Batch: 470, Training Loss: 0.05824449509382248, LR: 0.010000000000000002
Time, 2019-01-01T22:31:07, Epoch: 24, Batch: 480, Training Loss: 0.037714668363332746, LR: 0.010000000000000002
Time, 2019-01-01T22:31:08, Epoch: 24, Batch: 490, Training Loss: 0.06437475420534611, LR: 0.010000000000000002
Time, 2019-01-01T22:31:09, Epoch: 24, Batch: 500, Training Loss: 0.04179924838244915, LR: 0.010000000000000002
Time, 2019-01-01T22:31:10, Epoch: 24, Batch: 510, Training Loss: 0.029669079929590225, LR: 0.010000000000000002
Time, 2019-01-01T22:31:11, Epoch: 24, Batch: 520, Training Loss: 0.04660200700163841, LR: 0.010000000000000002
Time, 2019-01-01T22:31:11, Epoch: 24, Batch: 530, Training Loss: 0.0390397947281599, LR: 0.010000000000000002
Time, 2019-01-01T22:31:12, Epoch: 24, Batch: 540, Training Loss: 0.060138360038399695, LR: 0.010000000000000002
Time, 2019-01-01T22:31:13, Epoch: 24, Batch: 550, Training Loss: 0.05808793418109417, LR: 0.010000000000000002
Time, 2019-01-01T22:31:14, Epoch: 24, Batch: 560, Training Loss: 0.05123702846467495, LR: 0.010000000000000002
Time, 2019-01-01T22:31:15, Epoch: 24, Batch: 570, Training Loss: 0.07684255689382553, LR: 0.010000000000000002
Time, 2019-01-01T22:31:16, Epoch: 24, Batch: 580, Training Loss: 0.05366910994052887, LR: 0.010000000000000002
Time, 2019-01-01T22:31:16, Epoch: 24, Batch: 590, Training Loss: 0.07541663907468318, LR: 0.010000000000000002
Time, 2019-01-01T22:31:17, Epoch: 24, Batch: 600, Training Loss: 0.04148140214383602, LR: 0.010000000000000002
Time, 2019-01-01T22:31:18, Epoch: 24, Batch: 610, Training Loss: 0.06889830976724624, LR: 0.010000000000000002
Time, 2019-01-01T22:31:19, Epoch: 24, Batch: 620, Training Loss: 0.05336855389177799, LR: 0.010000000000000002
Time, 2019-01-01T22:31:19, Epoch: 24, Batch: 630, Training Loss: 0.05804686360061169, LR: 0.010000000000000002
Time, 2019-01-01T22:31:20, Epoch: 24, Batch: 640, Training Loss: 0.056557869911193846, LR: 0.010000000000000002
Time, 2019-01-01T22:31:21, Epoch: 24, Batch: 650, Training Loss: 0.07070311121642589, LR: 0.010000000000000002
Time, 2019-01-01T22:31:22, Epoch: 24, Batch: 660, Training Loss: 0.05904287472367287, LR: 0.010000000000000002
Time, 2019-01-01T22:31:23, Epoch: 24, Batch: 670, Training Loss: 0.05409529879689216, LR: 0.010000000000000002
Time, 2019-01-01T22:31:23, Epoch: 24, Batch: 680, Training Loss: 0.057806534320116044, LR: 0.010000000000000002
Time, 2019-01-01T22:31:24, Epoch: 24, Batch: 690, Training Loss: 0.06256622821092606, LR: 0.010000000000000002
Time, 2019-01-01T22:31:25, Epoch: 24, Batch: 700, Training Loss: 0.08257184401154519, LR: 0.010000000000000002
Time, 2019-01-01T22:31:26, Epoch: 24, Batch: 710, Training Loss: 0.07296469211578369, LR: 0.010000000000000002
Time, 2019-01-01T22:31:27, Epoch: 24, Batch: 720, Training Loss: 0.051063351705670355, LR: 0.010000000000000002
Time, 2019-01-01T22:31:27, Epoch: 24, Batch: 730, Training Loss: 0.06164237931370735, LR: 0.010000000000000002
Time, 2019-01-01T22:31:28, Epoch: 24, Batch: 740, Training Loss: 0.06063192449510098, LR: 0.010000000000000002
Time, 2019-01-01T22:31:29, Epoch: 24, Batch: 750, Training Loss: 0.04244966246187687, LR: 0.010000000000000002
Time, 2019-01-01T22:31:30, Epoch: 24, Batch: 760, Training Loss: 0.047182124480605124, LR: 0.010000000000000002
Time, 2019-01-01T22:31:30, Epoch: 24, Batch: 770, Training Loss: 0.0431559257209301, LR: 0.010000000000000002
Time, 2019-01-01T22:31:31, Epoch: 24, Batch: 780, Training Loss: 0.035851359739899634, LR: 0.010000000000000002
Time, 2019-01-01T22:31:32, Epoch: 24, Batch: 790, Training Loss: 0.032173140347003935, LR: 0.010000000000000002
Time, 2019-01-01T22:31:33, Epoch: 24, Batch: 800, Training Loss: 0.051127494871616365, LR: 0.010000000000000002
Time, 2019-01-01T22:31:34, Epoch: 24, Batch: 810, Training Loss: 0.09161991998553276, LR: 0.010000000000000002
Time, 2019-01-01T22:31:35, Epoch: 24, Batch: 820, Training Loss: 0.05669075660407543, LR: 0.010000000000000002
Time, 2019-01-01T22:31:35, Epoch: 24, Batch: 830, Training Loss: 0.05332685634493828, LR: 0.010000000000000002
Time, 2019-01-01T22:31:36, Epoch: 24, Batch: 840, Training Loss: 0.05751357786357403, LR: 0.010000000000000002
Time, 2019-01-01T22:31:37, Epoch: 24, Batch: 850, Training Loss: 0.0670508973300457, LR: 0.010000000000000002
Time, 2019-01-01T22:31:38, Epoch: 24, Batch: 860, Training Loss: 0.04852977059781551, LR: 0.010000000000000002
Time, 2019-01-01T22:31:39, Epoch: 24, Batch: 870, Training Loss: 0.07873114049434662, LR: 0.010000000000000002
Time, 2019-01-01T22:31:39, Epoch: 24, Batch: 880, Training Loss: 0.07245536223053932, LR: 0.010000000000000002
Time, 2019-01-01T22:31:40, Epoch: 24, Batch: 890, Training Loss: 0.07048487365245819, LR: 0.010000000000000002
Time, 2019-01-01T22:31:41, Epoch: 24, Batch: 900, Training Loss: 0.03867864049971104, LR: 0.010000000000000002
Time, 2019-01-01T22:31:42, Epoch: 24, Batch: 910, Training Loss: 0.0443819060921669, LR: 0.010000000000000002
Time, 2019-01-01T22:31:43, Epoch: 24, Batch: 920, Training Loss: 0.06346329227089882, LR: 0.010000000000000002
Time, 2019-01-01T22:31:43, Epoch: 24, Batch: 930, Training Loss: 0.04317910820245743, LR: 0.010000000000000002
Epoch: 24, Validation Top 1 acc: 98.23845672607422
Epoch: 24, Validation Top 5 acc: 100.0
Epoch: 24, Validation Set Loss: 0.056433603167533875
Start training epoch 25
Time, 2019-01-01T22:31:50, Epoch: 25, Batch: 10, Training Loss: 0.049621643126010896, LR: 0.010000000000000002
Time, 2019-01-01T22:31:51, Epoch: 25, Batch: 20, Training Loss: 0.030294283106923104, LR: 0.010000000000000002
Time, 2019-01-01T22:31:51, Epoch: 25, Batch: 30, Training Loss: 0.027196041122078896, LR: 0.010000000000000002
Time, 2019-01-01T22:31:52, Epoch: 25, Batch: 40, Training Loss: 0.0462911807000637, LR: 0.010000000000000002
Time, 2019-01-01T22:31:53, Epoch: 25, Batch: 50, Training Loss: 0.05119505524635315, LR: 0.010000000000000002
Time, 2019-01-01T22:31:54, Epoch: 25, Batch: 60, Training Loss: 0.042569687590003014, LR: 0.010000000000000002
Time, 2019-01-01T22:31:54, Epoch: 25, Batch: 70, Training Loss: 0.06313797235488891, LR: 0.010000000000000002
Time, 2019-01-01T22:31:55, Epoch: 25, Batch: 80, Training Loss: 0.06163397282361984, LR: 0.010000000000000002
Time, 2019-01-01T22:31:56, Epoch: 25, Batch: 90, Training Loss: 0.05639389231801033, LR: 0.010000000000000002
Time, 2019-01-01T22:31:57, Epoch: 25, Batch: 100, Training Loss: 0.0721854716539383, LR: 0.010000000000000002
Time, 2019-01-01T22:31:58, Epoch: 25, Batch: 110, Training Loss: 0.04353105053305626, LR: 0.010000000000000002
Time, 2019-01-01T22:31:59, Epoch: 25, Batch: 120, Training Loss: 0.05985998436808586, LR: 0.010000000000000002
Time, 2019-01-01T22:31:59, Epoch: 25, Batch: 130, Training Loss: 0.047612877935171126, LR: 0.010000000000000002
Time, 2019-01-01T22:32:00, Epoch: 25, Batch: 140, Training Loss: 0.0690082598477602, LR: 0.010000000000000002
Time, 2019-01-01T22:32:01, Epoch: 25, Batch: 150, Training Loss: 0.06727174296975136, LR: 0.010000000000000002
Time, 2019-01-01T22:32:02, Epoch: 25, Batch: 160, Training Loss: 0.059121739491820334, LR: 0.010000000000000002
Time, 2019-01-01T22:32:03, Epoch: 25, Batch: 170, Training Loss: 0.06663446240127087, LR: 0.010000000000000002
Time, 2019-01-01T22:32:04, Epoch: 25, Batch: 180, Training Loss: 0.07795471325516701, LR: 0.010000000000000002
Time, 2019-01-01T22:32:05, Epoch: 25, Batch: 190, Training Loss: 0.08633523397147655, LR: 0.010000000000000002
Time, 2019-01-01T22:32:05, Epoch: 25, Batch: 200, Training Loss: 0.04889820441603661, LR: 0.010000000000000002
Time, 2019-01-01T22:32:06, Epoch: 25, Batch: 210, Training Loss: 0.05926854908466339, LR: 0.010000000000000002
Time, 2019-01-01T22:32:07, Epoch: 25, Batch: 220, Training Loss: 0.04263183064758778, LR: 0.010000000000000002
Time, 2019-01-01T22:32:08, Epoch: 25, Batch: 230, Training Loss: 0.04705769047141075, LR: 0.010000000000000002
Time, 2019-01-01T22:32:09, Epoch: 25, Batch: 240, Training Loss: 0.06149903945624828, LR: 0.010000000000000002
Time, 2019-01-01T22:32:09, Epoch: 25, Batch: 250, Training Loss: 0.07281525731086731, LR: 0.010000000000000002
Time, 2019-01-01T22:32:10, Epoch: 25, Batch: 260, Training Loss: 0.045985908806324007, LR: 0.010000000000000002
Time, 2019-01-01T22:32:11, Epoch: 25, Batch: 270, Training Loss: 0.05564133375883103, LR: 0.010000000000000002
Time, 2019-01-01T22:32:12, Epoch: 25, Batch: 280, Training Loss: 0.06563658080995083, LR: 0.010000000000000002
Time, 2019-01-01T22:32:12, Epoch: 25, Batch: 290, Training Loss: 0.05412530526518822, LR: 0.010000000000000002
Time, 2019-01-01T22:32:13, Epoch: 25, Batch: 300, Training Loss: 0.05910556316375733, LR: 0.010000000000000002
Time, 2019-01-01T22:32:14, Epoch: 25, Batch: 310, Training Loss: 0.05382361561059952, LR: 0.010000000000000002
Time, 2019-01-01T22:32:15, Epoch: 25, Batch: 320, Training Loss: 0.05634375847876072, LR: 0.010000000000000002
Time, 2019-01-01T22:32:16, Epoch: 25, Batch: 330, Training Loss: 0.07656364440917969, LR: 0.010000000000000002
Time, 2019-01-01T22:32:17, Epoch: 25, Batch: 340, Training Loss: 0.04578851610422134, LR: 0.010000000000000002
Time, 2019-01-01T22:32:18, Epoch: 25, Batch: 350, Training Loss: 0.07126842066645622, LR: 0.010000000000000002
Time, 2019-01-01T22:32:18, Epoch: 25, Batch: 360, Training Loss: 0.05671417079865933, LR: 0.010000000000000002
Time, 2019-01-01T22:32:19, Epoch: 25, Batch: 370, Training Loss: 0.060399473458528516, LR: 0.010000000000000002
Time, 2019-01-01T22:32:20, Epoch: 25, Batch: 380, Training Loss: 0.05064607188105583, LR: 0.010000000000000002
Time, 2019-01-01T22:32:21, Epoch: 25, Batch: 390, Training Loss: 0.05826502814888954, LR: 0.010000000000000002
Time, 2019-01-01T22:32:22, Epoch: 25, Batch: 400, Training Loss: 0.060866962373256686, LR: 0.010000000000000002
Time, 2019-01-01T22:32:23, Epoch: 25, Batch: 410, Training Loss: 0.07188994809985161, LR: 0.010000000000000002
Time, 2019-01-01T22:32:23, Epoch: 25, Batch: 420, Training Loss: 0.05600264742970466, LR: 0.010000000000000002
Time, 2019-01-01T22:32:24, Epoch: 25, Batch: 430, Training Loss: 0.06119014210999012, LR: 0.010000000000000002
Time, 2019-01-01T22:32:25, Epoch: 25, Batch: 440, Training Loss: 0.04909055009484291, LR: 0.010000000000000002
Time, 2019-01-01T22:32:26, Epoch: 25, Batch: 450, Training Loss: 0.06206047534942627, LR: 0.010000000000000002
Time, 2019-01-01T22:32:27, Epoch: 25, Batch: 460, Training Loss: 0.059494155645370486, LR: 0.010000000000000002
Time, 2019-01-01T22:32:28, Epoch: 25, Batch: 470, Training Loss: 0.05427794978022575, LR: 0.010000000000000002
Time, 2019-01-01T22:32:28, Epoch: 25, Batch: 480, Training Loss: 0.0921232771128416, LR: 0.010000000000000002
Time, 2019-01-01T22:32:29, Epoch: 25, Batch: 490, Training Loss: 0.05603647828102112, LR: 0.010000000000000002
Time, 2019-01-01T22:32:30, Epoch: 25, Batch: 500, Training Loss: 0.040558237582445145, LR: 0.010000000000000002
Time, 2019-01-01T22:32:31, Epoch: 25, Batch: 510, Training Loss: 0.04143508076667786, LR: 0.010000000000000002
Time, 2019-01-01T22:32:32, Epoch: 25, Batch: 520, Training Loss: 0.055898481607437135, LR: 0.010000000000000002
Time, 2019-01-01T22:32:33, Epoch: 25, Batch: 530, Training Loss: 0.03778598308563232, LR: 0.010000000000000002
Time, 2019-01-01T22:32:34, Epoch: 25, Batch: 540, Training Loss: 0.059120194613933565, LR: 0.010000000000000002
Time, 2019-01-01T22:32:34, Epoch: 25, Batch: 550, Training Loss: 0.06784132495522499, LR: 0.010000000000000002
Time, 2019-01-01T22:32:35, Epoch: 25, Batch: 560, Training Loss: 0.06779753118753433, LR: 0.010000000000000002
Time, 2019-01-01T22:32:36, Epoch: 25, Batch: 570, Training Loss: 0.059015990048646924, LR: 0.010000000000000002
Time, 2019-01-01T22:32:37, Epoch: 25, Batch: 580, Training Loss: 0.0478091835975647, LR: 0.010000000000000002
Time, 2019-01-01T22:32:38, Epoch: 25, Batch: 590, Training Loss: 0.05916866734623909, LR: 0.010000000000000002
Time, 2019-01-01T22:32:39, Epoch: 25, Batch: 600, Training Loss: 0.07239946350455284, LR: 0.010000000000000002
Time, 2019-01-01T22:32:40, Epoch: 25, Batch: 610, Training Loss: 0.04913831353187561, LR: 0.010000000000000002
Time, 2019-01-01T22:32:40, Epoch: 25, Batch: 620, Training Loss: 0.05604554265737534, LR: 0.010000000000000002
Time, 2019-01-01T22:32:41, Epoch: 25, Batch: 630, Training Loss: 0.053723886236548425, LR: 0.010000000000000002
Time, 2019-01-01T22:32:42, Epoch: 25, Batch: 640, Training Loss: 0.05880604647099972, LR: 0.010000000000000002
Time, 2019-01-01T22:32:43, Epoch: 25, Batch: 650, Training Loss: 0.05966370329260826, LR: 0.010000000000000002
Time, 2019-01-01T22:32:44, Epoch: 25, Batch: 660, Training Loss: 0.049836593121290206, LR: 0.010000000000000002
Time, 2019-01-01T22:32:45, Epoch: 25, Batch: 670, Training Loss: 0.03230798952281475, LR: 0.010000000000000002
Time, 2019-01-01T22:32:46, Epoch: 25, Batch: 680, Training Loss: 0.06938331015408039, LR: 0.010000000000000002
Time, 2019-01-01T22:32:46, Epoch: 25, Batch: 690, Training Loss: 0.043643029406666756, LR: 0.010000000000000002
Time, 2019-01-01T22:32:47, Epoch: 25, Batch: 700, Training Loss: 0.05557283014059067, LR: 0.010000000000000002
Time, 2019-01-01T22:32:48, Epoch: 25, Batch: 710, Training Loss: 0.07400556840002537, LR: 0.010000000000000002
Time, 2019-01-01T22:32:49, Epoch: 25, Batch: 720, Training Loss: 0.05823221504688263, LR: 0.010000000000000002
Time, 2019-01-01T22:32:50, Epoch: 25, Batch: 730, Training Loss: 0.05626439452171326, LR: 0.010000000000000002
Time, 2019-01-01T22:32:51, Epoch: 25, Batch: 740, Training Loss: 0.08434495478868484, LR: 0.010000000000000002
Time, 2019-01-01T22:32:52, Epoch: 25, Batch: 750, Training Loss: 0.04321564808487892, LR: 0.010000000000000002
Time, 2019-01-01T22:32:53, Epoch: 25, Batch: 760, Training Loss: 0.07362034991383552, LR: 0.010000000000000002
Time, 2019-01-01T22:32:53, Epoch: 25, Batch: 770, Training Loss: 0.03410339020192623, LR: 0.010000000000000002
Time, 2019-01-01T22:32:54, Epoch: 25, Batch: 780, Training Loss: 0.05946180820465088, LR: 0.010000000000000002
Time, 2019-01-01T22:32:55, Epoch: 25, Batch: 790, Training Loss: 0.056729703396558764, LR: 0.010000000000000002
Time, 2019-01-01T22:32:56, Epoch: 25, Batch: 800, Training Loss: 0.05557294338941574, LR: 0.010000000000000002
Time, 2019-01-01T22:32:57, Epoch: 25, Batch: 810, Training Loss: 0.06456116251647473, LR: 0.010000000000000002
Time, 2019-01-01T22:32:58, Epoch: 25, Batch: 820, Training Loss: 0.03769812062382698, LR: 0.010000000000000002
Time, 2019-01-01T22:32:58, Epoch: 25, Batch: 830, Training Loss: 0.04426288977265358, LR: 0.010000000000000002
Time, 2019-01-01T22:32:59, Epoch: 25, Batch: 840, Training Loss: 0.08416646607220173, LR: 0.010000000000000002
Time, 2019-01-01T22:33:00, Epoch: 25, Batch: 850, Training Loss: 0.05081463679671287, LR: 0.010000000000000002
Time, 2019-01-01T22:33:01, Epoch: 25, Batch: 860, Training Loss: 0.05867468267679214, LR: 0.010000000000000002
Time, 2019-01-01T22:33:02, Epoch: 25, Batch: 870, Training Loss: 0.060277321934700014, LR: 0.010000000000000002
Time, 2019-01-01T22:33:02, Epoch: 25, Batch: 880, Training Loss: 0.06941344700753689, LR: 0.010000000000000002
Time, 2019-01-01T22:33:03, Epoch: 25, Batch: 890, Training Loss: 0.058196096867322925, LR: 0.010000000000000002
Time, 2019-01-01T22:33:04, Epoch: 25, Batch: 900, Training Loss: 0.06167795732617378, LR: 0.010000000000000002
Time, 2019-01-01T22:33:05, Epoch: 25, Batch: 910, Training Loss: 0.057000283524394034, LR: 0.010000000000000002
Time, 2019-01-01T22:33:06, Epoch: 25, Batch: 920, Training Loss: 0.07072189263999462, LR: 0.010000000000000002
Time, 2019-01-01T22:33:06, Epoch: 25, Batch: 930, Training Loss: 0.0608714122325182, LR: 0.010000000000000002
Epoch: 25, Validation Top 1 acc: 98.268310546875
Epoch: 25, Validation Top 5 acc: 99.99005126953125
Epoch: 25, Validation Set Loss: 0.056300558149814606
Start training epoch 26
Time, 2019-01-01T22:33:13, Epoch: 26, Batch: 10, Training Loss: 0.039970863983035086, LR: 0.010000000000000002
Time, 2019-01-01T22:33:14, Epoch: 26, Batch: 20, Training Loss: 0.049135427176952365, LR: 0.010000000000000002
Time, 2019-01-01T22:33:15, Epoch: 26, Batch: 30, Training Loss: 0.06026845499873161, LR: 0.010000000000000002
Time, 2019-01-01T22:33:15, Epoch: 26, Batch: 40, Training Loss: 0.051352056115865706, LR: 0.010000000000000002
Time, 2019-01-01T22:33:16, Epoch: 26, Batch: 50, Training Loss: 0.04086219631135464, LR: 0.010000000000000002
Time, 2019-01-01T22:33:17, Epoch: 26, Batch: 60, Training Loss: 0.04872600622475147, LR: 0.010000000000000002
Time, 2019-01-01T22:33:18, Epoch: 26, Batch: 70, Training Loss: 0.05476317815482616, LR: 0.010000000000000002
Time, 2019-01-01T22:33:18, Epoch: 26, Batch: 80, Training Loss: 0.053052420541644094, LR: 0.010000000000000002
Time, 2019-01-01T22:33:19, Epoch: 26, Batch: 90, Training Loss: 0.09946033209562302, LR: 0.010000000000000002
Time, 2019-01-01T22:33:20, Epoch: 26, Batch: 100, Training Loss: 0.05256710797548294, LR: 0.010000000000000002
Time, 2019-01-01T22:33:21, Epoch: 26, Batch: 110, Training Loss: 0.058061575144529344, LR: 0.010000000000000002
Time, 2019-01-01T22:33:22, Epoch: 26, Batch: 120, Training Loss: 0.05437561012804508, LR: 0.010000000000000002
Time, 2019-01-01T22:33:22, Epoch: 26, Batch: 130, Training Loss: 0.05788633339107037, LR: 0.010000000000000002
Time, 2019-01-01T22:33:23, Epoch: 26, Batch: 140, Training Loss: 0.043816111609339715, LR: 0.010000000000000002
Time, 2019-01-01T22:33:24, Epoch: 26, Batch: 150, Training Loss: 0.0407130017876625, LR: 0.010000000000000002
Time, 2019-01-01T22:33:25, Epoch: 26, Batch: 160, Training Loss: 0.058125632256269454, LR: 0.010000000000000002
Time, 2019-01-01T22:33:26, Epoch: 26, Batch: 170, Training Loss: 0.05224638767540455, LR: 0.010000000000000002
Time, 2019-01-01T22:33:27, Epoch: 26, Batch: 180, Training Loss: 0.06496168375015259, LR: 0.010000000000000002
Time, 2019-01-01T22:33:28, Epoch: 26, Batch: 190, Training Loss: 0.07123554721474648, LR: 0.010000000000000002
Time, 2019-01-01T22:33:29, Epoch: 26, Batch: 200, Training Loss: 0.037843504920601845, LR: 0.010000000000000002
Time, 2019-01-01T22:33:30, Epoch: 26, Batch: 210, Training Loss: 0.05660303868353367, LR: 0.010000000000000002
Time, 2019-01-01T22:33:31, Epoch: 26, Batch: 220, Training Loss: 0.05501587018370628, LR: 0.010000000000000002
Time, 2019-01-01T22:33:31, Epoch: 26, Batch: 230, Training Loss: 0.04637105166912079, LR: 0.010000000000000002
Time, 2019-01-01T22:33:32, Epoch: 26, Batch: 240, Training Loss: 0.05460610501468181, LR: 0.010000000000000002
Time, 2019-01-01T22:33:33, Epoch: 26, Batch: 250, Training Loss: 0.043714122474193574, LR: 0.010000000000000002
Time, 2019-01-01T22:33:34, Epoch: 26, Batch: 260, Training Loss: 0.05617568120360374, LR: 0.010000000000000002
Time, 2019-01-01T22:33:35, Epoch: 26, Batch: 270, Training Loss: 0.05738027021288872, LR: 0.010000000000000002
Time, 2019-01-01T22:33:35, Epoch: 26, Batch: 280, Training Loss: 0.04199633300304413, LR: 0.010000000000000002
Time, 2019-01-01T22:33:36, Epoch: 26, Batch: 290, Training Loss: 0.046213272213935855, LR: 0.010000000000000002
Time, 2019-01-01T22:33:37, Epoch: 26, Batch: 300, Training Loss: 0.050436243787407874, LR: 0.010000000000000002
Time, 2019-01-01T22:33:38, Epoch: 26, Batch: 310, Training Loss: 0.038534586504101756, LR: 0.010000000000000002
Time, 2019-01-01T22:33:39, Epoch: 26, Batch: 320, Training Loss: 0.0450422003865242, LR: 0.010000000000000002
Time, 2019-01-01T22:33:39, Epoch: 26, Batch: 330, Training Loss: 0.05708427168428898, LR: 0.010000000000000002
Time, 2019-01-01T22:33:40, Epoch: 26, Batch: 340, Training Loss: 0.030615638941526413, LR: 0.010000000000000002
Time, 2019-01-01T22:33:41, Epoch: 26, Batch: 350, Training Loss: 0.06754825189709664, LR: 0.010000000000000002
Time, 2019-01-01T22:33:42, Epoch: 26, Batch: 360, Training Loss: 0.04784943349659443, LR: 0.010000000000000002
Time, 2019-01-01T22:33:42, Epoch: 26, Batch: 370, Training Loss: 0.0731235932558775, LR: 0.010000000000000002
Time, 2019-01-01T22:33:43, Epoch: 26, Batch: 380, Training Loss: 0.05270184352993965, LR: 0.010000000000000002
Time, 2019-01-01T22:33:44, Epoch: 26, Batch: 390, Training Loss: 0.05376975722610951, LR: 0.010000000000000002
Time, 2019-01-01T22:33:45, Epoch: 26, Batch: 400, Training Loss: 0.049592765420675276, LR: 0.010000000000000002
Time, 2019-01-01T22:33:46, Epoch: 26, Batch: 410, Training Loss: 0.05369791500270367, LR: 0.010000000000000002
Time, 2019-01-01T22:33:47, Epoch: 26, Batch: 420, Training Loss: 0.06481608860194683, LR: 0.010000000000000002
Time, 2019-01-01T22:33:47, Epoch: 26, Batch: 430, Training Loss: 0.07583138458430767, LR: 0.010000000000000002
Time, 2019-01-01T22:33:48, Epoch: 26, Batch: 440, Training Loss: 0.03647682406008244, LR: 0.010000000000000002
Time, 2019-01-01T22:33:49, Epoch: 26, Batch: 450, Training Loss: 0.046483608707785604, LR: 0.010000000000000002
Time, 2019-01-01T22:33:50, Epoch: 26, Batch: 460, Training Loss: 0.037298453971743584, LR: 0.010000000000000002
Time, 2019-01-01T22:33:51, Epoch: 26, Batch: 470, Training Loss: 0.049227464199066165, LR: 0.010000000000000002
Time, 2019-01-01T22:33:51, Epoch: 26, Batch: 480, Training Loss: 0.0658726654946804, LR: 0.010000000000000002
Time, 2019-01-01T22:33:52, Epoch: 26, Batch: 490, Training Loss: 0.04634590409696102, LR: 0.010000000000000002
Time, 2019-01-01T22:33:53, Epoch: 26, Batch: 500, Training Loss: 0.059131155535578725, LR: 0.010000000000000002
Time, 2019-01-01T22:33:54, Epoch: 26, Batch: 510, Training Loss: 0.05441561751067638, LR: 0.010000000000000002
Time, 2019-01-01T22:33:55, Epoch: 26, Batch: 520, Training Loss: 0.07760042026638984, LR: 0.010000000000000002
Time, 2019-01-01T22:33:55, Epoch: 26, Batch: 530, Training Loss: 0.046978189051151274, LR: 0.010000000000000002
Time, 2019-01-01T22:33:56, Epoch: 26, Batch: 540, Training Loss: 0.07553120478987693, LR: 0.010000000000000002
Time, 2019-01-01T22:33:57, Epoch: 26, Batch: 550, Training Loss: 0.04838707074522972, LR: 0.010000000000000002
Time, 2019-01-01T22:33:58, Epoch: 26, Batch: 560, Training Loss: 0.04951358549296856, LR: 0.010000000000000002
Time, 2019-01-01T22:33:59, Epoch: 26, Batch: 570, Training Loss: 0.060804664716124536, LR: 0.010000000000000002
Time, 2019-01-01T22:33:59, Epoch: 26, Batch: 580, Training Loss: 0.04595105461776257, LR: 0.010000000000000002
Time, 2019-01-01T22:34:00, Epoch: 26, Batch: 590, Training Loss: 0.07022865265607833, LR: 0.010000000000000002
Time, 2019-01-01T22:34:01, Epoch: 26, Batch: 600, Training Loss: 0.04862087592482567, LR: 0.010000000000000002
Time, 2019-01-01T22:34:02, Epoch: 26, Batch: 610, Training Loss: 0.06995279528200626, LR: 0.010000000000000002
Time, 2019-01-01T22:34:03, Epoch: 26, Batch: 620, Training Loss: 0.050515344738960265, LR: 0.010000000000000002
Time, 2019-01-01T22:34:03, Epoch: 26, Batch: 630, Training Loss: 0.06001068986952305, LR: 0.010000000000000002
Time, 2019-01-01T22:34:04, Epoch: 26, Batch: 640, Training Loss: 0.07152718603610993, LR: 0.010000000000000002
Time, 2019-01-01T22:34:05, Epoch: 26, Batch: 650, Training Loss: 0.05643115118145943, LR: 0.010000000000000002
Time, 2019-01-01T22:34:06, Epoch: 26, Batch: 660, Training Loss: 0.0938506431877613, LR: 0.010000000000000002
Time, 2019-01-01T22:34:07, Epoch: 26, Batch: 670, Training Loss: 0.07207520827651023, LR: 0.010000000000000002
Time, 2019-01-01T22:34:07, Epoch: 26, Batch: 680, Training Loss: 0.08223617672920228, LR: 0.010000000000000002
Time, 2019-01-01T22:34:08, Epoch: 26, Batch: 690, Training Loss: 0.05694548785686493, LR: 0.010000000000000002
Time, 2019-01-01T22:34:09, Epoch: 26, Batch: 700, Training Loss: 0.05872065760195255, LR: 0.010000000000000002
Time, 2019-01-01T22:34:10, Epoch: 26, Batch: 710, Training Loss: 0.06341329216957092, LR: 0.010000000000000002
Time, 2019-01-01T22:34:11, Epoch: 26, Batch: 720, Training Loss: 0.07238772809505463, LR: 0.010000000000000002
Time, 2019-01-01T22:34:11, Epoch: 26, Batch: 730, Training Loss: 0.04689413793385029, LR: 0.010000000000000002
Time, 2019-01-01T22:34:12, Epoch: 26, Batch: 740, Training Loss: 0.07853830270469189, LR: 0.010000000000000002
Time, 2019-01-01T22:34:13, Epoch: 26, Batch: 750, Training Loss: 0.05684064999222756, LR: 0.010000000000000002
Time, 2019-01-01T22:34:14, Epoch: 26, Batch: 760, Training Loss: 0.08207607567310334, LR: 0.010000000000000002
Time, 2019-01-01T22:34:14, Epoch: 26, Batch: 770, Training Loss: 0.06438842192292213, LR: 0.010000000000000002
Time, 2019-01-01T22:34:15, Epoch: 26, Batch: 780, Training Loss: 0.0668517928570509, LR: 0.010000000000000002
Time, 2019-01-01T22:34:16, Epoch: 26, Batch: 790, Training Loss: 0.046618939563632014, LR: 0.010000000000000002
Time, 2019-01-01T22:34:17, Epoch: 26, Batch: 800, Training Loss: 0.04697855934500694, LR: 0.010000000000000002
Time, 2019-01-01T22:34:18, Epoch: 26, Batch: 810, Training Loss: 0.05983000993728638, LR: 0.010000000000000002
Time, 2019-01-01T22:34:18, Epoch: 26, Batch: 820, Training Loss: 0.04321354851126671, LR: 0.010000000000000002
Time, 2019-01-01T22:34:19, Epoch: 26, Batch: 830, Training Loss: 0.06959395073354244, LR: 0.010000000000000002
Time, 2019-01-01T22:34:20, Epoch: 26, Batch: 840, Training Loss: 0.08939198926091194, LR: 0.010000000000000002
Time, 2019-01-01T22:34:21, Epoch: 26, Batch: 850, Training Loss: 0.07352935299277305, LR: 0.010000000000000002
Time, 2019-01-01T22:34:21, Epoch: 26, Batch: 860, Training Loss: 0.04793242476880551, LR: 0.010000000000000002
Time, 2019-01-01T22:34:22, Epoch: 26, Batch: 870, Training Loss: 0.06522952616214753, LR: 0.010000000000000002
Time, 2019-01-01T22:34:23, Epoch: 26, Batch: 880, Training Loss: 0.05299772843718529, LR: 0.010000000000000002
Time, 2019-01-01T22:34:24, Epoch: 26, Batch: 890, Training Loss: 0.057595445588231084, LR: 0.010000000000000002
Time, 2019-01-01T22:34:25, Epoch: 26, Batch: 900, Training Loss: 0.04666455388069153, LR: 0.010000000000000002
Time, 2019-01-01T22:34:25, Epoch: 26, Batch: 910, Training Loss: 0.0610191848129034, LR: 0.010000000000000002
Time, 2019-01-01T22:34:26, Epoch: 26, Batch: 920, Training Loss: 0.04875672422349453, LR: 0.010000000000000002
Time, 2019-01-01T22:34:27, Epoch: 26, Batch: 930, Training Loss: 0.04659136682748795, LR: 0.010000000000000002
Epoch: 26, Validation Top 1 acc: 98.10907745361328
Epoch: 26, Validation Top 5 acc: 99.98009490966797
Epoch: 26, Validation Set Loss: 0.06026017293334007
Start training epoch 27
Time, 2019-01-01T22:34:33, Epoch: 27, Batch: 10, Training Loss: 0.06315931528806687, LR: 0.010000000000000002
Time, 2019-01-01T22:34:34, Epoch: 27, Batch: 20, Training Loss: 0.030674387514591218, LR: 0.010000000000000002
Time, 2019-01-01T22:34:35, Epoch: 27, Batch: 30, Training Loss: 0.05526172295212746, LR: 0.010000000000000002
Time, 2019-01-01T22:34:35, Epoch: 27, Batch: 40, Training Loss: 0.060098426043987276, LR: 0.010000000000000002
Time, 2019-01-01T22:34:36, Epoch: 27, Batch: 50, Training Loss: 0.05683803111314774, LR: 0.010000000000000002
Time, 2019-01-01T22:34:37, Epoch: 27, Batch: 60, Training Loss: 0.04707490019500256, LR: 0.010000000000000002
Time, 2019-01-01T22:34:38, Epoch: 27, Batch: 70, Training Loss: 0.04781679175794125, LR: 0.010000000000000002
Time, 2019-01-01T22:34:39, Epoch: 27, Batch: 80, Training Loss: 0.06345521248877048, LR: 0.010000000000000002
Time, 2019-01-01T22:34:39, Epoch: 27, Batch: 90, Training Loss: 0.05412352681159973, LR: 0.010000000000000002
Time, 2019-01-01T22:34:40, Epoch: 27, Batch: 100, Training Loss: 0.07230806685984134, LR: 0.010000000000000002
Time, 2019-01-01T22:34:41, Epoch: 27, Batch: 110, Training Loss: 0.06183079108595848, LR: 0.010000000000000002
Time, 2019-01-01T22:34:42, Epoch: 27, Batch: 120, Training Loss: 0.04225353822112084, LR: 0.010000000000000002
Time, 2019-01-01T22:34:42, Epoch: 27, Batch: 130, Training Loss: 0.06063798442482948, LR: 0.010000000000000002
Time, 2019-01-01T22:34:43, Epoch: 27, Batch: 140, Training Loss: 0.04891820549964905, LR: 0.010000000000000002
Time, 2019-01-01T22:34:44, Epoch: 27, Batch: 150, Training Loss: 0.09617255218327045, LR: 0.010000000000000002
Time, 2019-01-01T22:34:45, Epoch: 27, Batch: 160, Training Loss: 0.04192296415567398, LR: 0.010000000000000002
Time, 2019-01-01T22:34:45, Epoch: 27, Batch: 170, Training Loss: 0.04693816900253296, LR: 0.010000000000000002
Time, 2019-01-01T22:34:46, Epoch: 27, Batch: 180, Training Loss: 0.038213328644633296, LR: 0.010000000000000002
Time, 2019-01-01T22:34:47, Epoch: 27, Batch: 190, Training Loss: 0.05843794010579586, LR: 0.010000000000000002
Time, 2019-01-01T22:34:48, Epoch: 27, Batch: 200, Training Loss: 0.06600001677870751, LR: 0.010000000000000002
Time, 2019-01-01T22:34:49, Epoch: 27, Batch: 210, Training Loss: 0.048816331475973126, LR: 0.010000000000000002
Time, 2019-01-01T22:34:49, Epoch: 27, Batch: 220, Training Loss: 0.07038766853511333, LR: 0.010000000000000002
Time, 2019-01-01T22:34:50, Epoch: 27, Batch: 230, Training Loss: 0.08033996485173703, LR: 0.010000000000000002
Time, 2019-01-01T22:34:51, Epoch: 27, Batch: 240, Training Loss: 0.05178153961896896, LR: 0.010000000000000002
Time, 2019-01-01T22:34:52, Epoch: 27, Batch: 250, Training Loss: 0.04954406544566155, LR: 0.010000000000000002
Time, 2019-01-01T22:34:52, Epoch: 27, Batch: 260, Training Loss: 0.04716881886124611, LR: 0.010000000000000002
Time, 2019-01-01T22:34:53, Epoch: 27, Batch: 270, Training Loss: 0.051090266555547714, LR: 0.010000000000000002
Time, 2019-01-01T22:34:54, Epoch: 27, Batch: 280, Training Loss: 0.0484434612095356, LR: 0.010000000000000002
Time, 2019-01-01T22:34:55, Epoch: 27, Batch: 290, Training Loss: 0.038064977154135704, LR: 0.010000000000000002
Time, 2019-01-01T22:34:55, Epoch: 27, Batch: 300, Training Loss: 0.05620436407625675, LR: 0.010000000000000002
Time, 2019-01-01T22:34:56, Epoch: 27, Batch: 310, Training Loss: 0.0658386342227459, LR: 0.010000000000000002
Time, 2019-01-01T22:34:57, Epoch: 27, Batch: 320, Training Loss: 0.058924030140042306, LR: 0.010000000000000002
Time, 2019-01-01T22:34:58, Epoch: 27, Batch: 330, Training Loss: 0.06741247065365315, LR: 0.010000000000000002
Time, 2019-01-01T22:34:59, Epoch: 27, Batch: 340, Training Loss: 0.05612577423453331, LR: 0.010000000000000002
Time, 2019-01-01T22:35:00, Epoch: 27, Batch: 350, Training Loss: 0.07635075189173221, LR: 0.010000000000000002
Time, 2019-01-01T22:35:00, Epoch: 27, Batch: 360, Training Loss: 0.06439567729830742, LR: 0.010000000000000002
Time, 2019-01-01T22:35:01, Epoch: 27, Batch: 370, Training Loss: 0.07077482156455517, LR: 0.010000000000000002
Time, 2019-01-01T22:35:02, Epoch: 27, Batch: 380, Training Loss: 0.05409594997763634, LR: 0.010000000000000002
Time, 2019-01-01T22:35:03, Epoch: 27, Batch: 390, Training Loss: 0.05191110298037529, LR: 0.010000000000000002
Time, 2019-01-01T22:35:04, Epoch: 27, Batch: 400, Training Loss: 0.07421087324619294, LR: 0.010000000000000002
Time, 2019-01-01T22:35:04, Epoch: 27, Batch: 410, Training Loss: 0.041700728982686994, LR: 0.010000000000000002
Time, 2019-01-01T22:35:05, Epoch: 27, Batch: 420, Training Loss: 0.06612087860703468, LR: 0.010000000000000002
Time, 2019-01-01T22:35:06, Epoch: 27, Batch: 430, Training Loss: 0.03640123344957828, LR: 0.010000000000000002
Time, 2019-01-01T22:35:07, Epoch: 27, Batch: 440, Training Loss: 0.035967228934168816, LR: 0.010000000000000002
Time, 2019-01-01T22:35:08, Epoch: 27, Batch: 450, Training Loss: 0.06370228976011276, LR: 0.010000000000000002
Time, 2019-01-01T22:35:08, Epoch: 27, Batch: 460, Training Loss: 0.06654933504760266, LR: 0.010000000000000002
Time, 2019-01-01T22:35:09, Epoch: 27, Batch: 470, Training Loss: 0.06583402529358864, LR: 0.010000000000000002
Time, 2019-01-01T22:35:10, Epoch: 27, Batch: 480, Training Loss: 0.06100136563181877, LR: 0.010000000000000002
Time, 2019-01-01T22:35:11, Epoch: 27, Batch: 490, Training Loss: 0.054134077951312067, LR: 0.010000000000000002
Time, 2019-01-01T22:35:12, Epoch: 27, Batch: 500, Training Loss: 0.04792400747537613, LR: 0.010000000000000002
Time, 2019-01-01T22:35:12, Epoch: 27, Batch: 510, Training Loss: 0.06052638031542301, LR: 0.010000000000000002
Time, 2019-01-01T22:35:13, Epoch: 27, Batch: 520, Training Loss: 0.058355452865362166, LR: 0.010000000000000002
Time, 2019-01-01T22:35:14, Epoch: 27, Batch: 530, Training Loss: 0.047197332978248595, LR: 0.010000000000000002
Time, 2019-01-01T22:35:15, Epoch: 27, Batch: 540, Training Loss: 0.06079898476600647, LR: 0.010000000000000002
Time, 2019-01-01T22:35:15, Epoch: 27, Batch: 550, Training Loss: 0.06216211654245853, LR: 0.010000000000000002
Time, 2019-01-01T22:35:16, Epoch: 27, Batch: 560, Training Loss: 0.0541900672018528, LR: 0.010000000000000002
Time, 2019-01-01T22:35:17, Epoch: 27, Batch: 570, Training Loss: 0.08971462473273277, LR: 0.010000000000000002
Time, 2019-01-01T22:35:18, Epoch: 27, Batch: 580, Training Loss: 0.05251733548939228, LR: 0.010000000000000002
Time, 2019-01-01T22:35:19, Epoch: 27, Batch: 590, Training Loss: 0.0480849739164114, LR: 0.010000000000000002
Time, 2019-01-01T22:35:19, Epoch: 27, Batch: 600, Training Loss: 0.0822873778641224, LR: 0.010000000000000002
Time, 2019-01-01T22:35:20, Epoch: 27, Batch: 610, Training Loss: 0.03932970687747002, LR: 0.010000000000000002
Time, 2019-01-01T22:35:21, Epoch: 27, Batch: 620, Training Loss: 0.05151072330772877, LR: 0.010000000000000002
Time, 2019-01-01T22:35:22, Epoch: 27, Batch: 630, Training Loss: 0.04517241008579731, LR: 0.010000000000000002
Time, 2019-01-01T22:35:22, Epoch: 27, Batch: 640, Training Loss: 0.07036952897906304, LR: 0.010000000000000002
Time, 2019-01-01T22:35:23, Epoch: 27, Batch: 650, Training Loss: 0.05563846081495285, LR: 0.010000000000000002
Time, 2019-01-01T22:35:24, Epoch: 27, Batch: 660, Training Loss: 0.07907147333025932, LR: 0.010000000000000002
Time, 2019-01-01T22:35:25, Epoch: 27, Batch: 670, Training Loss: 0.037792662531137465, LR: 0.010000000000000002
Time, 2019-01-01T22:35:25, Epoch: 27, Batch: 680, Training Loss: 0.04355105347931385, LR: 0.010000000000000002
Time, 2019-01-01T22:35:26, Epoch: 27, Batch: 690, Training Loss: 0.0651441503316164, LR: 0.010000000000000002
Time, 2019-01-01T22:35:27, Epoch: 27, Batch: 700, Training Loss: 0.04211581833660603, LR: 0.010000000000000002
Time, 2019-01-01T22:35:28, Epoch: 27, Batch: 710, Training Loss: 0.05549740716814995, LR: 0.010000000000000002
Time, 2019-01-01T22:35:28, Epoch: 27, Batch: 720, Training Loss: 0.055935824289917946, LR: 0.010000000000000002
Time, 2019-01-01T22:35:29, Epoch: 27, Batch: 730, Training Loss: 0.059438887238502505, LR: 0.010000000000000002
Time, 2019-01-01T22:35:30, Epoch: 27, Batch: 740, Training Loss: 0.04907055124640465, LR: 0.010000000000000002
Time, 2019-01-01T22:35:31, Epoch: 27, Batch: 750, Training Loss: 0.04254303947091102, LR: 0.010000000000000002
Time, 2019-01-01T22:35:32, Epoch: 27, Batch: 760, Training Loss: 0.0838006742298603, LR: 0.010000000000000002
Time, 2019-01-01T22:35:32, Epoch: 27, Batch: 770, Training Loss: 0.053506124019622806, LR: 0.010000000000000002
Time, 2019-01-01T22:35:33, Epoch: 27, Batch: 780, Training Loss: 0.04699925072491169, LR: 0.010000000000000002
Time, 2019-01-01T22:35:34, Epoch: 27, Batch: 790, Training Loss: 0.06442842297255993, LR: 0.010000000000000002
Time, 2019-01-01T22:35:35, Epoch: 27, Batch: 800, Training Loss: 0.04646481201052666, LR: 0.010000000000000002
Time, 2019-01-01T22:35:36, Epoch: 27, Batch: 810, Training Loss: 0.09281268194317818, LR: 0.010000000000000002
Time, 2019-01-01T22:35:36, Epoch: 27, Batch: 820, Training Loss: 0.05154568701982498, LR: 0.010000000000000002
Time, 2019-01-01T22:35:37, Epoch: 27, Batch: 830, Training Loss: 0.06320240199565888, LR: 0.010000000000000002
Time, 2019-01-01T22:35:38, Epoch: 27, Batch: 840, Training Loss: 0.045339088514447214, LR: 0.010000000000000002
Time, 2019-01-01T22:35:39, Epoch: 27, Batch: 850, Training Loss: 0.0736497960984707, LR: 0.010000000000000002
Time, 2019-01-01T22:35:40, Epoch: 27, Batch: 860, Training Loss: 0.07642365582287311, LR: 0.010000000000000002
Time, 2019-01-01T22:35:40, Epoch: 27, Batch: 870, Training Loss: 0.05175036154687405, LR: 0.010000000000000002
Time, 2019-01-01T22:35:41, Epoch: 27, Batch: 880, Training Loss: 0.05206417143344879, LR: 0.010000000000000002
Time, 2019-01-01T22:35:42, Epoch: 27, Batch: 890, Training Loss: 0.06734512597322465, LR: 0.010000000000000002
Time, 2019-01-01T22:35:43, Epoch: 27, Batch: 900, Training Loss: 0.04793955907225609, LR: 0.010000000000000002
Time, 2019-01-01T22:35:44, Epoch: 27, Batch: 910, Training Loss: 0.032166996598243715, LR: 0.010000000000000002
Time, 2019-01-01T22:35:44, Epoch: 27, Batch: 920, Training Loss: 0.0754360318183899, LR: 0.010000000000000002
Time, 2019-01-01T22:35:45, Epoch: 27, Batch: 930, Training Loss: 0.06517124325037002, LR: 0.010000000000000002
Epoch: 27, Validation Top 1 acc: 98.27826690673828
Epoch: 27, Validation Top 5 acc: 100.0
Epoch: 27, Validation Set Loss: 0.05751888081431389
Start training epoch 28
Time, 2019-01-01T22:35:52, Epoch: 28, Batch: 10, Training Loss: 0.08724804371595382, LR: 0.010000000000000002
Time, 2019-01-01T22:35:52, Epoch: 28, Batch: 20, Training Loss: 0.05466527752578258, LR: 0.010000000000000002
Time, 2019-01-01T22:35:53, Epoch: 28, Batch: 30, Training Loss: 0.04812260046601295, LR: 0.010000000000000002
Time, 2019-01-01T22:35:54, Epoch: 28, Batch: 40, Training Loss: 0.06532196514308453, LR: 0.010000000000000002
Time, 2019-01-01T22:35:55, Epoch: 28, Batch: 50, Training Loss: 0.05122857019305229, LR: 0.010000000000000002
Time, 2019-01-01T22:35:55, Epoch: 28, Batch: 60, Training Loss: 0.0758814848959446, LR: 0.010000000000000002
Time, 2019-01-01T22:35:56, Epoch: 28, Batch: 70, Training Loss: 0.049396985396742824, LR: 0.010000000000000002
Time, 2019-01-01T22:35:57, Epoch: 28, Batch: 80, Training Loss: 0.048610327392816545, LR: 0.010000000000000002
Time, 2019-01-01T22:35:58, Epoch: 28, Batch: 90, Training Loss: 0.05844451561570167, LR: 0.010000000000000002
Time, 2019-01-01T22:35:59, Epoch: 28, Batch: 100, Training Loss: 0.039999081194400786, LR: 0.010000000000000002
Time, 2019-01-01T22:35:59, Epoch: 28, Batch: 110, Training Loss: 0.048609735816717146, LR: 0.010000000000000002
Time, 2019-01-01T22:36:00, Epoch: 28, Batch: 120, Training Loss: 0.0396679550409317, LR: 0.010000000000000002
Time, 2019-01-01T22:36:01, Epoch: 28, Batch: 130, Training Loss: 0.055724767968058586, LR: 0.010000000000000002
Time, 2019-01-01T22:36:02, Epoch: 28, Batch: 140, Training Loss: 0.08866240233182907, LR: 0.010000000000000002
Time, 2019-01-01T22:36:02, Epoch: 28, Batch: 150, Training Loss: 0.049955471977591516, LR: 0.010000000000000002
Time, 2019-01-01T22:36:03, Epoch: 28, Batch: 160, Training Loss: 0.05380379557609558, LR: 0.010000000000000002
Time, 2019-01-01T22:36:04, Epoch: 28, Batch: 170, Training Loss: 0.06466411165893078, LR: 0.010000000000000002
Time, 2019-01-01T22:36:05, Epoch: 28, Batch: 180, Training Loss: 0.048967461287975314, LR: 0.010000000000000002
Time, 2019-01-01T22:36:05, Epoch: 28, Batch: 190, Training Loss: 0.06494546644389629, LR: 0.010000000000000002
Time, 2019-01-01T22:36:06, Epoch: 28, Batch: 200, Training Loss: 0.06380925476551055, LR: 0.010000000000000002
Time, 2019-01-01T22:36:07, Epoch: 28, Batch: 210, Training Loss: 0.04745248891413212, LR: 0.010000000000000002
Time, 2019-01-01T22:36:08, Epoch: 28, Batch: 220, Training Loss: 0.04444256015121937, LR: 0.010000000000000002
Time, 2019-01-01T22:36:09, Epoch: 28, Batch: 230, Training Loss: 0.04687204547226429, LR: 0.010000000000000002
Time, 2019-01-01T22:36:09, Epoch: 28, Batch: 240, Training Loss: 0.03930933251976967, LR: 0.010000000000000002
Time, 2019-01-01T22:36:10, Epoch: 28, Batch: 250, Training Loss: 0.03965302109718323, LR: 0.010000000000000002
Time, 2019-01-01T22:36:11, Epoch: 28, Batch: 260, Training Loss: 0.057219157367944716, LR: 0.010000000000000002
Time, 2019-01-01T22:36:12, Epoch: 28, Batch: 270, Training Loss: 0.05930948220193386, LR: 0.010000000000000002
Time, 2019-01-01T22:36:13, Epoch: 28, Batch: 280, Training Loss: 0.03194317445158958, LR: 0.010000000000000002
Time, 2019-01-01T22:36:13, Epoch: 28, Batch: 290, Training Loss: 0.04741343185305595, LR: 0.010000000000000002
Time, 2019-01-01T22:36:14, Epoch: 28, Batch: 300, Training Loss: 0.048999154940247536, LR: 0.010000000000000002
Time, 2019-01-01T22:36:15, Epoch: 28, Batch: 310, Training Loss: 0.05818296000361443, LR: 0.010000000000000002
Time, 2019-01-01T22:36:16, Epoch: 28, Batch: 320, Training Loss: 0.08437957465648652, LR: 0.010000000000000002
Time, 2019-01-01T22:36:17, Epoch: 28, Batch: 330, Training Loss: 0.05720105431973934, LR: 0.010000000000000002
Time, 2019-01-01T22:36:17, Epoch: 28, Batch: 340, Training Loss: 0.0740476556122303, LR: 0.010000000000000002
Time, 2019-01-01T22:36:18, Epoch: 28, Batch: 350, Training Loss: 0.06069831475615502, LR: 0.010000000000000002
Time, 2019-01-01T22:36:19, Epoch: 28, Batch: 360, Training Loss: 0.05203661099076271, LR: 0.010000000000000002
Time, 2019-01-01T22:36:20, Epoch: 28, Batch: 370, Training Loss: 0.04213164448738098, LR: 0.010000000000000002
Time, 2019-01-01T22:36:21, Epoch: 28, Batch: 380, Training Loss: 0.061463383585214616, LR: 0.010000000000000002
Time, 2019-01-01T22:36:21, Epoch: 28, Batch: 390, Training Loss: 0.048866591230034825, LR: 0.010000000000000002
Time, 2019-01-01T22:36:22, Epoch: 28, Batch: 400, Training Loss: 0.03261671140789986, LR: 0.010000000000000002
Time, 2019-01-01T22:36:23, Epoch: 28, Batch: 410, Training Loss: 0.047759341076016426, LR: 0.010000000000000002
Time, 2019-01-01T22:36:24, Epoch: 28, Batch: 420, Training Loss: 0.0377721194177866, LR: 0.010000000000000002
Time, 2019-01-01T22:36:24, Epoch: 28, Batch: 430, Training Loss: 0.04098316878080368, LR: 0.010000000000000002
Time, 2019-01-01T22:36:25, Epoch: 28, Batch: 440, Training Loss: 0.05219286754727363, LR: 0.010000000000000002
Time, 2019-01-01T22:36:26, Epoch: 28, Batch: 450, Training Loss: 0.07363383695483208, LR: 0.010000000000000002
Time, 2019-01-01T22:36:27, Epoch: 28, Batch: 460, Training Loss: 0.06419015526771546, LR: 0.010000000000000002
Time, 2019-01-01T22:36:28, Epoch: 28, Batch: 470, Training Loss: 0.0787286926060915, LR: 0.010000000000000002
Time, 2019-01-01T22:36:28, Epoch: 28, Batch: 480, Training Loss: 0.05620490349829197, LR: 0.010000000000000002
Time, 2019-01-01T22:36:29, Epoch: 28, Batch: 490, Training Loss: 0.0557778213173151, LR: 0.010000000000000002
Time, 2019-01-01T22:36:30, Epoch: 28, Batch: 500, Training Loss: 0.0464709933847189, LR: 0.010000000000000002
Time, 2019-01-01T22:36:31, Epoch: 28, Batch: 510, Training Loss: 0.046195730566978455, LR: 0.010000000000000002
Time, 2019-01-01T22:36:31, Epoch: 28, Batch: 520, Training Loss: 0.06056403778493404, LR: 0.010000000000000002
Time, 2019-01-01T22:36:32, Epoch: 28, Batch: 530, Training Loss: 0.06688943319022655, LR: 0.010000000000000002
Time, 2019-01-01T22:36:33, Epoch: 28, Batch: 540, Training Loss: 0.048114529252052306, LR: 0.010000000000000002
Time, 2019-01-01T22:36:34, Epoch: 28, Batch: 550, Training Loss: 0.04870614148676396, LR: 0.010000000000000002
Time, 2019-01-01T22:36:35, Epoch: 28, Batch: 560, Training Loss: 0.060796148329973224, LR: 0.010000000000000002
Time, 2019-01-01T22:36:35, Epoch: 28, Batch: 570, Training Loss: 0.03977481834590435, LR: 0.010000000000000002
Time, 2019-01-01T22:36:36, Epoch: 28, Batch: 580, Training Loss: 0.05448931194841862, LR: 0.010000000000000002
Time, 2019-01-01T22:36:37, Epoch: 28, Batch: 590, Training Loss: 0.04537881352007389, LR: 0.010000000000000002
Time, 2019-01-01T22:36:38, Epoch: 28, Batch: 600, Training Loss: 0.06993170231580734, LR: 0.010000000000000002
Time, 2019-01-01T22:36:39, Epoch: 28, Batch: 610, Training Loss: 0.0414938434958458, LR: 0.010000000000000002
Time, 2019-01-01T22:36:39, Epoch: 28, Batch: 620, Training Loss: 0.05922738090157509, LR: 0.010000000000000002
Time, 2019-01-01T22:36:40, Epoch: 28, Batch: 630, Training Loss: 0.049500614032149316, LR: 0.010000000000000002
Time, 2019-01-01T22:36:41, Epoch: 28, Batch: 640, Training Loss: 0.06650539413094521, LR: 0.010000000000000002
Time, 2019-01-01T22:36:42, Epoch: 28, Batch: 650, Training Loss: 0.032664728537201884, LR: 0.010000000000000002
Time, 2019-01-01T22:36:43, Epoch: 28, Batch: 660, Training Loss: 0.0456851702183485, LR: 0.010000000000000002
Time, 2019-01-01T22:36:43, Epoch: 28, Batch: 670, Training Loss: 0.04340121783316135, LR: 0.010000000000000002
Time, 2019-01-01T22:36:44, Epoch: 28, Batch: 680, Training Loss: 0.04983926117420197, LR: 0.010000000000000002
Time, 2019-01-01T22:36:45, Epoch: 28, Batch: 690, Training Loss: 0.06229556128382683, LR: 0.010000000000000002
Time, 2019-01-01T22:36:46, Epoch: 28, Batch: 700, Training Loss: 0.03818749748170376, LR: 0.010000000000000002
Time, 2019-01-01T22:36:47, Epoch: 28, Batch: 710, Training Loss: 0.04290085919201374, LR: 0.010000000000000002
Time, 2019-01-01T22:36:48, Epoch: 28, Batch: 720, Training Loss: 0.0782209862023592, LR: 0.010000000000000002
Time, 2019-01-01T22:36:49, Epoch: 28, Batch: 730, Training Loss: 0.04170149490237236, LR: 0.010000000000000002
Time, 2019-01-01T22:36:49, Epoch: 28, Batch: 740, Training Loss: 0.07479297854006291, LR: 0.010000000000000002
Time, 2019-01-01T22:36:50, Epoch: 28, Batch: 750, Training Loss: 0.05647571235895157, LR: 0.010000000000000002
Time, 2019-01-01T22:36:51, Epoch: 28, Batch: 760, Training Loss: 0.04225924238562584, LR: 0.010000000000000002
Time, 2019-01-01T22:36:52, Epoch: 28, Batch: 770, Training Loss: 0.07781557813286781, LR: 0.010000000000000002
Time, 2019-01-01T22:36:53, Epoch: 28, Batch: 780, Training Loss: 0.0748929388821125, LR: 0.010000000000000002
Time, 2019-01-01T22:36:53, Epoch: 28, Batch: 790, Training Loss: 0.047443253174424174, LR: 0.010000000000000002
Time, 2019-01-01T22:36:54, Epoch: 28, Batch: 800, Training Loss: 0.05150615274906158, LR: 0.010000000000000002
Time, 2019-01-01T22:36:55, Epoch: 28, Batch: 810, Training Loss: 0.051859389245510104, LR: 0.010000000000000002
Time, 2019-01-01T22:36:56, Epoch: 28, Batch: 820, Training Loss: 0.04586647264659405, LR: 0.010000000000000002
Time, 2019-01-01T22:36:57, Epoch: 28, Batch: 830, Training Loss: 0.04026024602353573, LR: 0.010000000000000002
Time, 2019-01-01T22:36:58, Epoch: 28, Batch: 840, Training Loss: 0.044352857023477556, LR: 0.010000000000000002
Time, 2019-01-01T22:36:59, Epoch: 28, Batch: 850, Training Loss: 0.055785438418388365, LR: 0.010000000000000002
Time, 2019-01-01T22:36:59, Epoch: 28, Batch: 860, Training Loss: 0.07286172211170197, LR: 0.010000000000000002
Time, 2019-01-01T22:37:00, Epoch: 28, Batch: 870, Training Loss: 0.07581443525850773, LR: 0.010000000000000002
Time, 2019-01-01T22:37:01, Epoch: 28, Batch: 880, Training Loss: 0.049722326546907426, LR: 0.010000000000000002
Time, 2019-01-01T22:37:02, Epoch: 28, Batch: 890, Training Loss: 0.060295074433088305, LR: 0.010000000000000002
Time, 2019-01-01T22:37:02, Epoch: 28, Batch: 900, Training Loss: 0.03424001932144165, LR: 0.010000000000000002
Time, 2019-01-01T22:37:03, Epoch: 28, Batch: 910, Training Loss: 0.05319520644843578, LR: 0.010000000000000002
Time, 2019-01-01T22:37:04, Epoch: 28, Batch: 920, Training Loss: 0.03729967474937439, LR: 0.010000000000000002
Time, 2019-01-01T22:37:05, Epoch: 28, Batch: 930, Training Loss: 0.060645238310098645, LR: 0.010000000000000002
Epoch: 28, Validation Top 1 acc: 98.14888763427734
Epoch: 28, Validation Top 5 acc: 100.0
Epoch: 28, Validation Set Loss: 0.058344222605228424
Start training epoch 29
Time, 2019-01-01T22:37:11, Epoch: 29, Batch: 10, Training Loss: 0.04614868015050888, LR: 0.010000000000000002
Time, 2019-01-01T22:37:12, Epoch: 29, Batch: 20, Training Loss: 0.06522973999381065, LR: 0.010000000000000002
Time, 2019-01-01T22:37:12, Epoch: 29, Batch: 30, Training Loss: 0.040384089946746825, LR: 0.010000000000000002
Time, 2019-01-01T22:37:13, Epoch: 29, Batch: 40, Training Loss: 0.053604387864470485, LR: 0.010000000000000002
Time, 2019-01-01T22:37:14, Epoch: 29, Batch: 50, Training Loss: 0.05433305017650127, LR: 0.010000000000000002
Time, 2019-01-01T22:37:15, Epoch: 29, Batch: 60, Training Loss: 0.04513844847679138, LR: 0.010000000000000002
Time, 2019-01-01T22:37:15, Epoch: 29, Batch: 70, Training Loss: 0.035327520221471786, LR: 0.010000000000000002
Time, 2019-01-01T22:37:16, Epoch: 29, Batch: 80, Training Loss: 0.04511639066040516, LR: 0.010000000000000002
Time, 2019-01-01T22:37:17, Epoch: 29, Batch: 90, Training Loss: 0.046668170019984245, LR: 0.010000000000000002
Time, 2019-01-01T22:37:18, Epoch: 29, Batch: 100, Training Loss: 0.043185535445809366, LR: 0.010000000000000002
Time, 2019-01-01T22:37:18, Epoch: 29, Batch: 110, Training Loss: 0.07652305960655212, LR: 0.010000000000000002
Time, 2019-01-01T22:37:19, Epoch: 29, Batch: 120, Training Loss: 0.08199661187827587, LR: 0.010000000000000002
Time, 2019-01-01T22:37:20, Epoch: 29, Batch: 130, Training Loss: 0.03866396248340607, LR: 0.010000000000000002
Time, 2019-01-01T22:37:21, Epoch: 29, Batch: 140, Training Loss: 0.04152784794569016, LR: 0.010000000000000002
Time, 2019-01-01T22:37:22, Epoch: 29, Batch: 150, Training Loss: 0.03920371569693089, LR: 0.010000000000000002
Time, 2019-01-01T22:37:22, Epoch: 29, Batch: 160, Training Loss: 0.04365924187004566, LR: 0.010000000000000002
Time, 2019-01-01T22:37:23, Epoch: 29, Batch: 170, Training Loss: 0.045074979215860365, LR: 0.010000000000000002
Time, 2019-01-01T22:37:24, Epoch: 29, Batch: 180, Training Loss: 0.053685320913791655, LR: 0.010000000000000002
Time, 2019-01-01T22:37:25, Epoch: 29, Batch: 190, Training Loss: 0.055595414340496065, LR: 0.010000000000000002
Time, 2019-01-01T22:37:25, Epoch: 29, Batch: 200, Training Loss: 0.06396436020731926, LR: 0.010000000000000002
Time, 2019-01-01T22:37:26, Epoch: 29, Batch: 210, Training Loss: 0.03449814543128014, LR: 0.010000000000000002
Time, 2019-01-01T22:37:27, Epoch: 29, Batch: 220, Training Loss: 0.05271088071167469, LR: 0.010000000000000002
Time, 2019-01-01T22:37:28, Epoch: 29, Batch: 230, Training Loss: 0.04484243988990784, LR: 0.010000000000000002
Time, 2019-01-01T22:37:29, Epoch: 29, Batch: 240, Training Loss: 0.04585596434772014, LR: 0.010000000000000002
Time, 2019-01-01T22:37:29, Epoch: 29, Batch: 250, Training Loss: 0.06166487447917461, LR: 0.010000000000000002
Time, 2019-01-01T22:37:30, Epoch: 29, Batch: 260, Training Loss: 0.04955768100917339, LR: 0.010000000000000002
Time, 2019-01-01T22:37:31, Epoch: 29, Batch: 270, Training Loss: 0.04257175922393799, LR: 0.010000000000000002
Time, 2019-01-01T22:37:32, Epoch: 29, Batch: 280, Training Loss: 0.03539816290140152, LR: 0.010000000000000002
Time, 2019-01-01T22:37:32, Epoch: 29, Batch: 290, Training Loss: 0.05303707979619503, LR: 0.010000000000000002
Time, 2019-01-01T22:37:33, Epoch: 29, Batch: 300, Training Loss: 0.05650805234909058, LR: 0.010000000000000002
Time, 2019-01-01T22:37:34, Epoch: 29, Batch: 310, Training Loss: 0.04998818710446358, LR: 0.010000000000000002
Time, 2019-01-01T22:37:35, Epoch: 29, Batch: 320, Training Loss: 0.037956973537802696, LR: 0.010000000000000002
Time, 2019-01-01T22:37:35, Epoch: 29, Batch: 330, Training Loss: 0.04603584930300712, LR: 0.010000000000000002
Time, 2019-01-01T22:37:36, Epoch: 29, Batch: 340, Training Loss: 0.06910711489617824, LR: 0.010000000000000002
Time, 2019-01-01T22:37:37, Epoch: 29, Batch: 350, Training Loss: 0.06328395716845989, LR: 0.010000000000000002
Time, 2019-01-01T22:37:38, Epoch: 29, Batch: 360, Training Loss: 0.0463673435151577, LR: 0.010000000000000002
Time, 2019-01-01T22:37:39, Epoch: 29, Batch: 370, Training Loss: 0.06198086440563202, LR: 0.010000000000000002
Time, 2019-01-01T22:37:39, Epoch: 29, Batch: 380, Training Loss: 0.049202000722289085, LR: 0.010000000000000002
Time, 2019-01-01T22:37:40, Epoch: 29, Batch: 390, Training Loss: 0.041427969187498095, LR: 0.010000000000000002
Time, 2019-01-01T22:37:41, Epoch: 29, Batch: 400, Training Loss: 0.05258129984140396, LR: 0.010000000000000002
Time, 2019-01-01T22:37:42, Epoch: 29, Batch: 410, Training Loss: 0.05380104631185532, LR: 0.010000000000000002
Time, 2019-01-01T22:37:43, Epoch: 29, Batch: 420, Training Loss: 0.05764369741082191, LR: 0.010000000000000002
Time, 2019-01-01T22:37:44, Epoch: 29, Batch: 430, Training Loss: 0.050807616859674457, LR: 0.010000000000000002
Time, 2019-01-01T22:37:44, Epoch: 29, Batch: 440, Training Loss: 0.04492236077785492, LR: 0.010000000000000002
Time, 2019-01-01T22:37:45, Epoch: 29, Batch: 450, Training Loss: 0.034829999506473544, LR: 0.010000000000000002
Time, 2019-01-01T22:37:46, Epoch: 29, Batch: 460, Training Loss: 0.06732595935463906, LR: 0.010000000000000002
Time, 2019-01-01T22:37:47, Epoch: 29, Batch: 470, Training Loss: 0.08638513013720513, LR: 0.010000000000000002
Time, 2019-01-01T22:37:48, Epoch: 29, Batch: 480, Training Loss: 0.037947242707014085, LR: 0.010000000000000002
Time, 2019-01-01T22:37:49, Epoch: 29, Batch: 490, Training Loss: 0.04179264232516289, LR: 0.010000000000000002
Time, 2019-01-01T22:37:49, Epoch: 29, Batch: 500, Training Loss: 0.05026839077472687, LR: 0.010000000000000002
Time, 2019-01-01T22:37:50, Epoch: 29, Batch: 510, Training Loss: 0.05018045082688331, LR: 0.010000000000000002
Time, 2019-01-01T22:37:51, Epoch: 29, Batch: 520, Training Loss: 0.07906088568270206, LR: 0.010000000000000002
Time, 2019-01-01T22:37:52, Epoch: 29, Batch: 530, Training Loss: 0.049089439958333966, LR: 0.010000000000000002
Time, 2019-01-01T22:37:52, Epoch: 29, Batch: 540, Training Loss: 0.06190364807844162, LR: 0.010000000000000002
Time, 2019-01-01T22:37:53, Epoch: 29, Batch: 550, Training Loss: 0.052567938715219496, LR: 0.010000000000000002
Time, 2019-01-01T22:37:54, Epoch: 29, Batch: 560, Training Loss: 0.05268019214272499, LR: 0.010000000000000002
Time, 2019-01-01T22:37:55, Epoch: 29, Batch: 570, Training Loss: 0.05420219823718071, LR: 0.010000000000000002
Time, 2019-01-01T22:37:56, Epoch: 29, Batch: 580, Training Loss: 0.035994778573513034, LR: 0.010000000000000002
Time, 2019-01-01T22:37:56, Epoch: 29, Batch: 590, Training Loss: 0.035385172441601755, LR: 0.010000000000000002
Time, 2019-01-01T22:37:57, Epoch: 29, Batch: 600, Training Loss: 0.05625942721962929, LR: 0.010000000000000002
Time, 2019-01-01T22:37:58, Epoch: 29, Batch: 610, Training Loss: 0.04875859543681145, LR: 0.010000000000000002
Time, 2019-01-01T22:37:59, Epoch: 29, Batch: 620, Training Loss: 0.04677336849272251, LR: 0.010000000000000002
Time, 2019-01-01T22:37:59, Epoch: 29, Batch: 630, Training Loss: 0.04323600679636001, LR: 0.010000000000000002
Time, 2019-01-01T22:38:00, Epoch: 29, Batch: 640, Training Loss: 0.03600664138793945, LR: 0.010000000000000002
Time, 2019-01-01T22:38:01, Epoch: 29, Batch: 650, Training Loss: 0.034116458520293234, LR: 0.010000000000000002
Time, 2019-01-01T22:38:02, Epoch: 29, Batch: 660, Training Loss: 0.03318578414618969, LR: 0.010000000000000002
Time, 2019-01-01T22:38:03, Epoch: 29, Batch: 670, Training Loss: 0.047215649485588075, LR: 0.010000000000000002
Time, 2019-01-01T22:38:03, Epoch: 29, Batch: 680, Training Loss: 0.05627204030752182, LR: 0.010000000000000002
Time, 2019-01-01T22:38:04, Epoch: 29, Batch: 690, Training Loss: 0.06285233683884144, LR: 0.010000000000000002
Time, 2019-01-01T22:38:05, Epoch: 29, Batch: 700, Training Loss: 0.07709921374917031, LR: 0.010000000000000002
Time, 2019-01-01T22:38:06, Epoch: 29, Batch: 710, Training Loss: 0.06298046447336673, LR: 0.010000000000000002
Time, 2019-01-01T22:38:07, Epoch: 29, Batch: 720, Training Loss: 0.05948139056563377, LR: 0.010000000000000002
Time, 2019-01-01T22:38:08, Epoch: 29, Batch: 730, Training Loss: 0.04215966798365116, LR: 0.010000000000000002
Time, 2019-01-01T22:38:08, Epoch: 29, Batch: 740, Training Loss: 0.08818231150507927, LR: 0.010000000000000002
Time, 2019-01-01T22:38:09, Epoch: 29, Batch: 750, Training Loss: 0.054962363466620444, LR: 0.010000000000000002
Time, 2019-01-01T22:38:10, Epoch: 29, Batch: 760, Training Loss: 0.029575064405798913, LR: 0.010000000000000002
Time, 2019-01-01T22:38:11, Epoch: 29, Batch: 770, Training Loss: 0.059209151566028594, LR: 0.010000000000000002
Time, 2019-01-01T22:38:12, Epoch: 29, Batch: 780, Training Loss: 0.05812450870871544, LR: 0.010000000000000002
Time, 2019-01-01T22:38:12, Epoch: 29, Batch: 790, Training Loss: 0.04608335979282856, LR: 0.010000000000000002
Time, 2019-01-01T22:38:13, Epoch: 29, Batch: 800, Training Loss: 0.04717588499188423, LR: 0.010000000000000002
Time, 2019-01-01T22:38:14, Epoch: 29, Batch: 810, Training Loss: 0.05992087423801422, LR: 0.010000000000000002
Time, 2019-01-01T22:38:15, Epoch: 29, Batch: 820, Training Loss: 0.03989368043839932, LR: 0.010000000000000002
Time, 2019-01-01T22:38:15, Epoch: 29, Batch: 830, Training Loss: 0.05542300716042518, LR: 0.010000000000000002
Time, 2019-01-01T22:38:16, Epoch: 29, Batch: 840, Training Loss: 0.06797509007155896, LR: 0.010000000000000002
Time, 2019-01-01T22:38:17, Epoch: 29, Batch: 850, Training Loss: 0.06032850407063961, LR: 0.010000000000000002
Time, 2019-01-01T22:38:18, Epoch: 29, Batch: 860, Training Loss: 0.05818879157304764, LR: 0.010000000000000002
Time, 2019-01-01T22:38:18, Epoch: 29, Batch: 870, Training Loss: 0.05625433176755905, LR: 0.010000000000000002
Time, 2019-01-01T22:38:19, Epoch: 29, Batch: 880, Training Loss: 0.05514320656657219, LR: 0.010000000000000002
Time, 2019-01-01T22:38:20, Epoch: 29, Batch: 890, Training Loss: 0.06141259111464024, LR: 0.010000000000000002
Time, 2019-01-01T22:38:21, Epoch: 29, Batch: 900, Training Loss: 0.054045670479536054, LR: 0.010000000000000002
Time, 2019-01-01T22:38:22, Epoch: 29, Batch: 910, Training Loss: 0.06406141221523284, LR: 0.010000000000000002
Time, 2019-01-01T22:38:22, Epoch: 29, Batch: 920, Training Loss: 0.054122262820601466, LR: 0.010000000000000002
Time, 2019-01-01T22:38:23, Epoch: 29, Batch: 930, Training Loss: 0.06269527897238732, LR: 0.010000000000000002
Epoch: 29, Validation Top 1 acc: 98.1886978149414
Epoch: 29, Validation Top 5 acc: 100.0
Epoch: 29, Validation Set Loss: 0.057338375598192215
Start training epoch 30
Time, 2019-01-01T22:38:29, Epoch: 30, Batch: 10, Training Loss: 0.052422162145376205, LR: 0.010000000000000002
Time, 2019-01-01T22:38:30, Epoch: 30, Batch: 20, Training Loss: 0.06260029599070549, LR: 0.010000000000000002
Time, 2019-01-01T22:38:31, Epoch: 30, Batch: 30, Training Loss: 0.04310594126582146, LR: 0.010000000000000002
Time, 2019-01-01T22:38:32, Epoch: 30, Batch: 40, Training Loss: 0.057251456379890445, LR: 0.010000000000000002
Time, 2019-01-01T22:38:33, Epoch: 30, Batch: 50, Training Loss: 0.05865742936730385, LR: 0.010000000000000002
Time, 2019-01-01T22:38:33, Epoch: 30, Batch: 60, Training Loss: 0.027663511410355567, LR: 0.010000000000000002
Time, 2019-01-01T22:38:34, Epoch: 30, Batch: 70, Training Loss: 0.02176658734679222, LR: 0.010000000000000002
Time, 2019-01-01T22:38:35, Epoch: 30, Batch: 80, Training Loss: 0.06634236611425877, LR: 0.010000000000000002
Time, 2019-01-01T22:38:36, Epoch: 30, Batch: 90, Training Loss: 0.06185384690761566, LR: 0.010000000000000002
Time, 2019-01-01T22:38:36, Epoch: 30, Batch: 100, Training Loss: 0.04686204344034195, LR: 0.010000000000000002
Time, 2019-01-01T22:38:37, Epoch: 30, Batch: 110, Training Loss: 0.04427924118936062, LR: 0.010000000000000002
Time, 2019-01-01T22:38:38, Epoch: 30, Batch: 120, Training Loss: 0.046946747601032256, LR: 0.010000000000000002
Time, 2019-01-01T22:38:39, Epoch: 30, Batch: 130, Training Loss: 0.04846611395478249, LR: 0.010000000000000002
Time, 2019-01-01T22:38:40, Epoch: 30, Batch: 140, Training Loss: 0.060162578150629996, LR: 0.010000000000000002
Time, 2019-01-01T22:38:40, Epoch: 30, Batch: 150, Training Loss: 0.0378313235938549, LR: 0.010000000000000002
Time, 2019-01-01T22:38:41, Epoch: 30, Batch: 160, Training Loss: 0.04011033810675144, LR: 0.010000000000000002
Time, 2019-01-01T22:38:42, Epoch: 30, Batch: 170, Training Loss: 0.06485350392758846, LR: 0.010000000000000002
Time, 2019-01-01T22:38:43, Epoch: 30, Batch: 180, Training Loss: 0.048615678772330286, LR: 0.010000000000000002
Time, 2019-01-01T22:38:44, Epoch: 30, Batch: 190, Training Loss: 0.05761160254478455, LR: 0.010000000000000002
Time, 2019-01-01T22:38:44, Epoch: 30, Batch: 200, Training Loss: 0.06543067321181298, LR: 0.010000000000000002
Time, 2019-01-01T22:38:45, Epoch: 30, Batch: 210, Training Loss: 0.05248330235481262, LR: 0.010000000000000002
Time, 2019-01-01T22:38:46, Epoch: 30, Batch: 220, Training Loss: 0.03611683622002602, LR: 0.010000000000000002
Time, 2019-01-01T22:38:47, Epoch: 30, Batch: 230, Training Loss: 0.052777407318353654, LR: 0.010000000000000002
Time, 2019-01-01T22:38:48, Epoch: 30, Batch: 240, Training Loss: 0.05260207913815975, LR: 0.010000000000000002
Time, 2019-01-01T22:38:49, Epoch: 30, Batch: 250, Training Loss: 0.04683341830968857, LR: 0.010000000000000002
Time, 2019-01-01T22:38:50, Epoch: 30, Batch: 260, Training Loss: 0.07629294246435166, LR: 0.010000000000000002
Time, 2019-01-01T22:38:50, Epoch: 30, Batch: 270, Training Loss: 0.03979883305728436, LR: 0.010000000000000002
Time, 2019-01-01T22:38:51, Epoch: 30, Batch: 280, Training Loss: 0.03870207592844963, LR: 0.010000000000000002
Time, 2019-01-01T22:38:52, Epoch: 30, Batch: 290, Training Loss: 0.05105683505535126, LR: 0.010000000000000002
Time, 2019-01-01T22:38:53, Epoch: 30, Batch: 300, Training Loss: 0.03941592946648598, LR: 0.010000000000000002
Time, 2019-01-01T22:38:54, Epoch: 30, Batch: 310, Training Loss: 0.05566722601652145, LR: 0.010000000000000002
Time, 2019-01-01T22:38:55, Epoch: 30, Batch: 320, Training Loss: 0.05888695158064365, LR: 0.010000000000000002
Time, 2019-01-01T22:38:55, Epoch: 30, Batch: 330, Training Loss: 0.07798629552125931, LR: 0.010000000000000002
Time, 2019-01-01T22:38:56, Epoch: 30, Batch: 340, Training Loss: 0.04705934710800648, LR: 0.010000000000000002
Time, 2019-01-01T22:38:57, Epoch: 30, Batch: 350, Training Loss: 0.06411429047584534, LR: 0.010000000000000002
Time, 2019-01-01T22:38:58, Epoch: 30, Batch: 360, Training Loss: 0.048446124792099, LR: 0.010000000000000002
Time, 2019-01-01T22:38:59, Epoch: 30, Batch: 370, Training Loss: 0.05552234873175621, LR: 0.010000000000000002
Time, 2019-01-01T22:39:00, Epoch: 30, Batch: 380, Training Loss: 0.0532393105328083, LR: 0.010000000000000002
Time, 2019-01-01T22:39:00, Epoch: 30, Batch: 390, Training Loss: 0.057724875211715695, LR: 0.010000000000000002
Time, 2019-01-01T22:39:01, Epoch: 30, Batch: 400, Training Loss: 0.05994951613247394, LR: 0.010000000000000002
Time, 2019-01-01T22:39:02, Epoch: 30, Batch: 410, Training Loss: 0.05405985936522484, LR: 0.010000000000000002
Time, 2019-01-01T22:39:03, Epoch: 30, Batch: 420, Training Loss: 0.04182101674377918, LR: 0.010000000000000002
Time, 2019-01-01T22:39:03, Epoch: 30, Batch: 430, Training Loss: 0.04746193289756775, LR: 0.010000000000000002
Time, 2019-01-01T22:39:04, Epoch: 30, Batch: 440, Training Loss: 0.0848505362868309, LR: 0.010000000000000002
Time, 2019-01-01T22:39:05, Epoch: 30, Batch: 450, Training Loss: 0.03985266424715519, LR: 0.010000000000000002
Time, 2019-01-01T22:39:06, Epoch: 30, Batch: 460, Training Loss: 0.05615542903542518, LR: 0.010000000000000002
Time, 2019-01-01T22:39:07, Epoch: 30, Batch: 470, Training Loss: 0.07001697011291981, LR: 0.010000000000000002
Time, 2019-01-01T22:39:07, Epoch: 30, Batch: 480, Training Loss: 0.0629495333880186, LR: 0.010000000000000002
Time, 2019-01-01T22:39:08, Epoch: 30, Batch: 490, Training Loss: 0.06990753039717675, LR: 0.010000000000000002
Time, 2019-01-01T22:39:09, Epoch: 30, Batch: 500, Training Loss: 0.046918858960270884, LR: 0.010000000000000002
Time, 2019-01-01T22:39:10, Epoch: 30, Batch: 510, Training Loss: 0.03053160719573498, LR: 0.010000000000000002
Time, 2019-01-01T22:39:11, Epoch: 30, Batch: 520, Training Loss: 0.045363235846161844, LR: 0.010000000000000002
Time, 2019-01-01T22:39:11, Epoch: 30, Batch: 530, Training Loss: 0.04039521962404251, LR: 0.010000000000000002
Time, 2019-01-01T22:39:12, Epoch: 30, Batch: 540, Training Loss: 0.056345362588763236, LR: 0.010000000000000002
Time, 2019-01-01T22:39:13, Epoch: 30, Batch: 550, Training Loss: 0.05350833423435688, LR: 0.010000000000000002
Time, 2019-01-01T22:39:14, Epoch: 30, Batch: 560, Training Loss: 0.04578777328133583, LR: 0.010000000000000002
Time, 2019-01-01T22:39:15, Epoch: 30, Batch: 570, Training Loss: 0.04720154702663422, LR: 0.010000000000000002
Time, 2019-01-01T22:39:15, Epoch: 30, Batch: 580, Training Loss: 0.06527301631867885, LR: 0.010000000000000002
Time, 2019-01-01T22:39:16, Epoch: 30, Batch: 590, Training Loss: 0.046632020175457, LR: 0.010000000000000002
Time, 2019-01-01T22:39:17, Epoch: 30, Batch: 600, Training Loss: 0.05634998679161072, LR: 0.010000000000000002
Time, 2019-01-01T22:39:18, Epoch: 30, Batch: 610, Training Loss: 0.03599270097911358, LR: 0.010000000000000002
Time, 2019-01-01T22:39:19, Epoch: 30, Batch: 620, Training Loss: 0.06796179935336114, LR: 0.010000000000000002
Time, 2019-01-01T22:39:19, Epoch: 30, Batch: 630, Training Loss: 0.052093584463000296, LR: 0.010000000000000002
Time, 2019-01-01T22:39:20, Epoch: 30, Batch: 640, Training Loss: 0.056132351979613304, LR: 0.010000000000000002
Time, 2019-01-01T22:39:21, Epoch: 30, Batch: 650, Training Loss: 0.054759896174073217, LR: 0.010000000000000002
Time, 2019-01-01T22:39:22, Epoch: 30, Batch: 660, Training Loss: 0.047776288166642186, LR: 0.010000000000000002
Time, 2019-01-01T22:39:23, Epoch: 30, Batch: 670, Training Loss: 0.05376548506319523, LR: 0.010000000000000002
Time, 2019-01-01T22:39:23, Epoch: 30, Batch: 680, Training Loss: 0.03765024393796921, LR: 0.010000000000000002
Time, 2019-01-01T22:39:24, Epoch: 30, Batch: 690, Training Loss: 0.06855937242507934, LR: 0.010000000000000002
Time, 2019-01-01T22:39:25, Epoch: 30, Batch: 700, Training Loss: 0.0515163991600275, LR: 0.010000000000000002
Time, 2019-01-01T22:39:26, Epoch: 30, Batch: 710, Training Loss: 0.07269994653761387, LR: 0.010000000000000002
Time, 2019-01-01T22:39:26, Epoch: 30, Batch: 720, Training Loss: 0.06548278778791428, LR: 0.010000000000000002
Time, 2019-01-01T22:39:27, Epoch: 30, Batch: 730, Training Loss: 0.0629088394343853, LR: 0.010000000000000002
Time, 2019-01-01T22:39:28, Epoch: 30, Batch: 740, Training Loss: 0.03849486000835896, LR: 0.010000000000000002
Time, 2019-01-01T22:39:29, Epoch: 30, Batch: 750, Training Loss: 0.045415505021810534, LR: 0.010000000000000002
Time, 2019-01-01T22:39:30, Epoch: 30, Batch: 760, Training Loss: 0.03181870728731155, LR: 0.010000000000000002
Time, 2019-01-01T22:39:31, Epoch: 30, Batch: 770, Training Loss: 0.0455642431974411, LR: 0.010000000000000002
Time, 2019-01-01T22:39:31, Epoch: 30, Batch: 780, Training Loss: 0.043179091066122055, LR: 0.010000000000000002
Time, 2019-01-01T22:39:32, Epoch: 30, Batch: 790, Training Loss: 0.05464953631162643, LR: 0.010000000000000002
Time, 2019-01-01T22:39:33, Epoch: 30, Batch: 800, Training Loss: 0.06635930426418782, LR: 0.010000000000000002
Time, 2019-01-01T22:39:34, Epoch: 30, Batch: 810, Training Loss: 0.08327325582504272, LR: 0.010000000000000002
Time, 2019-01-01T22:39:35, Epoch: 30, Batch: 820, Training Loss: 0.05543494336307049, LR: 0.010000000000000002
Time, 2019-01-01T22:39:36, Epoch: 30, Batch: 830, Training Loss: 0.06237741038203239, LR: 0.010000000000000002
Time, 2019-01-01T22:39:36, Epoch: 30, Batch: 840, Training Loss: 0.05029366612434387, LR: 0.010000000000000002
Time, 2019-01-01T22:39:37, Epoch: 30, Batch: 850, Training Loss: 0.06451005265116691, LR: 0.010000000000000002
Time, 2019-01-01T22:39:38, Epoch: 30, Batch: 860, Training Loss: 0.03875002637505531, LR: 0.010000000000000002
Time, 2019-01-01T22:39:39, Epoch: 30, Batch: 870, Training Loss: 0.04796470478177071, LR: 0.010000000000000002
Time, 2019-01-01T22:39:40, Epoch: 30, Batch: 880, Training Loss: 0.04327994808554649, LR: 0.010000000000000002
Time, 2019-01-01T22:39:41, Epoch: 30, Batch: 890, Training Loss: 0.049187562242150304, LR: 0.010000000000000002
Time, 2019-01-01T22:39:42, Epoch: 30, Batch: 900, Training Loss: 0.07531029433012008, LR: 0.010000000000000002
Time, 2019-01-01T22:39:43, Epoch: 30, Batch: 910, Training Loss: 0.048750649020075795, LR: 0.010000000000000002
Time, 2019-01-01T22:39:43, Epoch: 30, Batch: 920, Training Loss: 0.04811454936861992, LR: 0.010000000000000002
Time, 2019-01-01T22:39:44, Epoch: 30, Batch: 930, Training Loss: 0.05433219410479069, LR: 0.010000000000000002
Epoch: 30, Validation Top 1 acc: 98.1588363647461
Epoch: 30, Validation Top 5 acc: 99.98009490966797
Epoch: 30, Validation Set Loss: 0.059772055596113205
Start training epoch 31
Time, 2019-01-01T22:39:52, Epoch: 31, Batch: 10, Training Loss: 0.06386927589774131, LR: 0.010000000000000002
Time, 2019-01-01T22:39:53, Epoch: 31, Batch: 20, Training Loss: 0.07842807658016682, LR: 0.010000000000000002
Time, 2019-01-01T22:39:54, Epoch: 31, Batch: 30, Training Loss: 0.06174595132470131, LR: 0.010000000000000002
Time, 2019-01-01T22:39:54, Epoch: 31, Batch: 40, Training Loss: 0.06837826706469059, LR: 0.010000000000000002
Time, 2019-01-01T22:39:55, Epoch: 31, Batch: 50, Training Loss: 0.06054278239607811, LR: 0.010000000000000002
Time, 2019-01-01T22:39:56, Epoch: 31, Batch: 60, Training Loss: 0.060113280266523364, LR: 0.010000000000000002
Time, 2019-01-01T22:39:57, Epoch: 31, Batch: 70, Training Loss: 0.07058787122368812, LR: 0.010000000000000002
Time, 2019-01-01T22:39:58, Epoch: 31, Batch: 80, Training Loss: 0.05823987126350403, LR: 0.010000000000000002
Time, 2019-01-01T22:39:58, Epoch: 31, Batch: 90, Training Loss: 0.06942359060049057, LR: 0.010000000000000002
Time, 2019-01-01T22:39:59, Epoch: 31, Batch: 100, Training Loss: 0.05231219008564949, LR: 0.010000000000000002
Time, 2019-01-01T22:40:00, Epoch: 31, Batch: 110, Training Loss: 0.038686205819249156, LR: 0.010000000000000002
Time, 2019-01-01T22:40:01, Epoch: 31, Batch: 120, Training Loss: 0.08060749247670174, LR: 0.010000000000000002
Time, 2019-01-01T22:40:02, Epoch: 31, Batch: 130, Training Loss: 0.08282845430076122, LR: 0.010000000000000002
Time, 2019-01-01T22:40:02, Epoch: 31, Batch: 140, Training Loss: 0.07754857838153839, LR: 0.010000000000000002
Time, 2019-01-01T22:40:03, Epoch: 31, Batch: 150, Training Loss: 0.06079295203089714, LR: 0.010000000000000002
Time, 2019-01-01T22:40:04, Epoch: 31, Batch: 160, Training Loss: 0.06283560842275619, LR: 0.010000000000000002
Time, 2019-01-01T22:40:05, Epoch: 31, Batch: 170, Training Loss: 0.03516703806817532, LR: 0.010000000000000002
Time, 2019-01-01T22:40:06, Epoch: 31, Batch: 180, Training Loss: 0.05862263143062592, LR: 0.010000000000000002
Time, 2019-01-01T22:40:06, Epoch: 31, Batch: 190, Training Loss: 0.07887187972664833, LR: 0.010000000000000002
Time, 2019-01-01T22:40:07, Epoch: 31, Batch: 200, Training Loss: 0.06230175830423832, LR: 0.010000000000000002
Time, 2019-01-01T22:40:08, Epoch: 31, Batch: 210, Training Loss: 0.04023905247449875, LR: 0.010000000000000002
Time, 2019-01-01T22:40:09, Epoch: 31, Batch: 220, Training Loss: 0.0714149758219719, LR: 0.010000000000000002
Time, 2019-01-01T22:40:10, Epoch: 31, Batch: 230, Training Loss: 0.04174486622214317, LR: 0.010000000000000002
Time, 2019-01-01T22:40:10, Epoch: 31, Batch: 240, Training Loss: 0.05784487947821617, LR: 0.010000000000000002
Time, 2019-01-01T22:40:11, Epoch: 31, Batch: 250, Training Loss: 0.05093813240528107, LR: 0.010000000000000002
Time, 2019-01-01T22:40:12, Epoch: 31, Batch: 260, Training Loss: 0.0653344064950943, LR: 0.010000000000000002
Time, 2019-01-01T22:40:13, Epoch: 31, Batch: 270, Training Loss: 0.04615325704216957, LR: 0.010000000000000002
Time, 2019-01-01T22:40:14, Epoch: 31, Batch: 280, Training Loss: 0.057569637149572375, LR: 0.010000000000000002
Time, 2019-01-01T22:40:15, Epoch: 31, Batch: 290, Training Loss: 0.047410708665847776, LR: 0.010000000000000002
Time, 2019-01-01T22:40:15, Epoch: 31, Batch: 300, Training Loss: 0.042684169858694075, LR: 0.010000000000000002
Time, 2019-01-01T22:40:16, Epoch: 31, Batch: 310, Training Loss: 0.06235583163797855, LR: 0.010000000000000002
Time, 2019-01-01T22:40:17, Epoch: 31, Batch: 320, Training Loss: 0.05892939791083336, LR: 0.010000000000000002
Time, 2019-01-01T22:40:18, Epoch: 31, Batch: 330, Training Loss: 0.04862172193825245, LR: 0.010000000000000002
Time, 2019-01-01T22:40:19, Epoch: 31, Batch: 340, Training Loss: 0.04934549704194069, LR: 0.010000000000000002
Time, 2019-01-01T22:40:19, Epoch: 31, Batch: 350, Training Loss: 0.042016827315092084, LR: 0.010000000000000002
Time, 2019-01-01T22:40:20, Epoch: 31, Batch: 360, Training Loss: 0.052493853867053984, LR: 0.010000000000000002
Time, 2019-01-01T22:40:21, Epoch: 31, Batch: 370, Training Loss: 0.05178904756903648, LR: 0.010000000000000002
Time, 2019-01-01T22:40:22, Epoch: 31, Batch: 380, Training Loss: 0.051795413717627525, LR: 0.010000000000000002
Time, 2019-01-01T22:40:23, Epoch: 31, Batch: 390, Training Loss: 0.049715186655521396, LR: 0.010000000000000002
Time, 2019-01-01T22:40:24, Epoch: 31, Batch: 400, Training Loss: 0.04766374081373215, LR: 0.010000000000000002
Time, 2019-01-01T22:40:24, Epoch: 31, Batch: 410, Training Loss: 0.046345893293619156, LR: 0.010000000000000002
Time, 2019-01-01T22:40:25, Epoch: 31, Batch: 420, Training Loss: 0.05368540957570076, LR: 0.010000000000000002
Time, 2019-01-01T22:40:26, Epoch: 31, Batch: 430, Training Loss: 0.059961859881877896, LR: 0.010000000000000002
Time, 2019-01-01T22:40:27, Epoch: 31, Batch: 440, Training Loss: 0.08663011826574803, LR: 0.010000000000000002
Time, 2019-01-01T22:40:28, Epoch: 31, Batch: 450, Training Loss: 0.059432390704751016, LR: 0.010000000000000002
Time, 2019-01-01T22:40:29, Epoch: 31, Batch: 460, Training Loss: 0.045499487593770024, LR: 0.010000000000000002
Time, 2019-01-01T22:40:29, Epoch: 31, Batch: 470, Training Loss: 0.05497028082609177, LR: 0.010000000000000002
Time, 2019-01-01T22:40:30, Epoch: 31, Batch: 480, Training Loss: 0.07074976600706577, LR: 0.010000000000000002
Time, 2019-01-01T22:40:31, Epoch: 31, Batch: 490, Training Loss: 0.04975923672318459, LR: 0.010000000000000002
Time, 2019-01-01T22:40:32, Epoch: 31, Batch: 500, Training Loss: 0.05291963517665863, LR: 0.010000000000000002
Time, 2019-01-01T22:40:33, Epoch: 31, Batch: 510, Training Loss: 0.06680072322487832, LR: 0.010000000000000002
Time, 2019-01-01T22:40:33, Epoch: 31, Batch: 520, Training Loss: 0.042542175948619844, LR: 0.010000000000000002
Time, 2019-01-01T22:40:34, Epoch: 31, Batch: 530, Training Loss: 0.06955263763666153, LR: 0.010000000000000002
Time, 2019-01-01T22:40:35, Epoch: 31, Batch: 540, Training Loss: 0.04719717763364315, LR: 0.010000000000000002
Time, 2019-01-01T22:40:36, Epoch: 31, Batch: 550, Training Loss: 0.05533687397837639, LR: 0.010000000000000002
Time, 2019-01-01T22:40:37, Epoch: 31, Batch: 560, Training Loss: 0.06412897333502769, LR: 0.010000000000000002
Time, 2019-01-01T22:40:38, Epoch: 31, Batch: 570, Training Loss: 0.06893256828188896, LR: 0.010000000000000002
Time, 2019-01-01T22:40:38, Epoch: 31, Batch: 580, Training Loss: 0.049149299040436745, LR: 0.010000000000000002
Time, 2019-01-01T22:40:39, Epoch: 31, Batch: 590, Training Loss: 0.03947041407227516, LR: 0.010000000000000002
Time, 2019-01-01T22:40:40, Epoch: 31, Batch: 600, Training Loss: 0.04299933910369873, LR: 0.010000000000000002
Time, 2019-01-01T22:40:41, Epoch: 31, Batch: 610, Training Loss: 0.041421600803732875, LR: 0.010000000000000002
Time, 2019-01-01T22:40:42, Epoch: 31, Batch: 620, Training Loss: 0.07547722980380059, LR: 0.010000000000000002
Time, 2019-01-01T22:40:43, Epoch: 31, Batch: 630, Training Loss: 0.0418052114546299, LR: 0.010000000000000002
Time, 2019-01-01T22:40:44, Epoch: 31, Batch: 640, Training Loss: 0.0355484277009964, LR: 0.010000000000000002
Time, 2019-01-01T22:40:44, Epoch: 31, Batch: 650, Training Loss: 0.07053475640714169, LR: 0.010000000000000002
Time, 2019-01-01T22:40:45, Epoch: 31, Batch: 660, Training Loss: 0.06123613528907299, LR: 0.010000000000000002
Time, 2019-01-01T22:40:46, Epoch: 31, Batch: 670, Training Loss: 0.07891788184642792, LR: 0.010000000000000002
Time, 2019-01-01T22:40:47, Epoch: 31, Batch: 680, Training Loss: 0.06632074974477291, LR: 0.010000000000000002
Time, 2019-01-01T22:40:48, Epoch: 31, Batch: 690, Training Loss: 0.042688823863863946, LR: 0.010000000000000002
Time, 2019-01-01T22:40:48, Epoch: 31, Batch: 700, Training Loss: 0.049861778318881986, LR: 0.010000000000000002
Time, 2019-01-01T22:40:49, Epoch: 31, Batch: 710, Training Loss: 0.06602503545582294, LR: 0.010000000000000002
Time, 2019-01-01T22:40:50, Epoch: 31, Batch: 720, Training Loss: 0.05727526769042015, LR: 0.010000000000000002
Time, 2019-01-01T22:40:51, Epoch: 31, Batch: 730, Training Loss: 0.05017508380115032, LR: 0.010000000000000002
Time, 2019-01-01T22:40:52, Epoch: 31, Batch: 740, Training Loss: 0.04307789951562881, LR: 0.010000000000000002
Time, 2019-01-01T22:40:52, Epoch: 31, Batch: 750, Training Loss: 0.036056140437722206, LR: 0.010000000000000002
Time, 2019-01-01T22:40:53, Epoch: 31, Batch: 760, Training Loss: 0.03199685364961624, LR: 0.010000000000000002
Time, 2019-01-01T22:40:54, Epoch: 31, Batch: 770, Training Loss: 0.049433037638664246, LR: 0.010000000000000002
Time, 2019-01-01T22:40:55, Epoch: 31, Batch: 780, Training Loss: 0.04981001541018486, LR: 0.010000000000000002
Time, 2019-01-01T22:40:56, Epoch: 31, Batch: 790, Training Loss: 0.06276991814374924, LR: 0.010000000000000002
Time, 2019-01-01T22:40:57, Epoch: 31, Batch: 800, Training Loss: 0.05150047019124031, LR: 0.010000000000000002
Time, 2019-01-01T22:40:57, Epoch: 31, Batch: 810, Training Loss: 0.03306080400943756, LR: 0.010000000000000002
Time, 2019-01-01T22:40:58, Epoch: 31, Batch: 820, Training Loss: 0.05504860021173954, LR: 0.010000000000000002
Time, 2019-01-01T22:40:59, Epoch: 31, Batch: 830, Training Loss: 0.06575232669711113, LR: 0.010000000000000002
Time, 2019-01-01T22:41:00, Epoch: 31, Batch: 840, Training Loss: 0.06508089490234852, LR: 0.010000000000000002
Time, 2019-01-01T22:41:01, Epoch: 31, Batch: 850, Training Loss: 0.0813869908452034, LR: 0.010000000000000002
Time, 2019-01-01T22:41:02, Epoch: 31, Batch: 860, Training Loss: 0.05287669748067856, LR: 0.010000000000000002
Time, 2019-01-01T22:41:02, Epoch: 31, Batch: 870, Training Loss: 0.045249149203300476, LR: 0.010000000000000002
Time, 2019-01-01T22:41:03, Epoch: 31, Batch: 880, Training Loss: 0.038238628208637236, LR: 0.010000000000000002
Time, 2019-01-01T22:41:04, Epoch: 31, Batch: 890, Training Loss: 0.04985566809773445, LR: 0.010000000000000002
Time, 2019-01-01T22:41:05, Epoch: 31, Batch: 900, Training Loss: 0.027888266369700432, LR: 0.010000000000000002
Time, 2019-01-01T22:41:06, Epoch: 31, Batch: 910, Training Loss: 0.046934160962700845, LR: 0.010000000000000002
Time, 2019-01-01T22:41:06, Epoch: 31, Batch: 920, Training Loss: 0.05104922130703926, LR: 0.010000000000000002
Time, 2019-01-01T22:41:07, Epoch: 31, Batch: 930, Training Loss: 0.07319265641272069, LR: 0.010000000000000002
Epoch: 31, Validation Top 1 acc: 98.39768981933594
Epoch: 31, Validation Top 5 acc: 99.99005126953125
Epoch: 31, Validation Set Loss: 0.05202122777700424
Start training epoch 32
Time, 2019-01-01T22:41:14, Epoch: 32, Batch: 10, Training Loss: 0.04229325503110885, LR: 0.010000000000000002
Time, 2019-01-01T22:41:14, Epoch: 32, Batch: 20, Training Loss: 0.036025892943143845, LR: 0.010000000000000002
Time, 2019-01-01T22:41:15, Epoch: 32, Batch: 30, Training Loss: 0.03739346116781235, LR: 0.010000000000000002
Time, 2019-01-01T22:41:16, Epoch: 32, Batch: 40, Training Loss: 0.061602243408560756, LR: 0.010000000000000002
Time, 2019-01-01T22:41:17, Epoch: 32, Batch: 50, Training Loss: 0.06589334793388843, LR: 0.010000000000000002
Time, 2019-01-01T22:41:18, Epoch: 32, Batch: 60, Training Loss: 0.04786464683711529, LR: 0.010000000000000002
Time, 2019-01-01T22:41:18, Epoch: 32, Batch: 70, Training Loss: 0.051596648246049884, LR: 0.010000000000000002
Time, 2019-01-01T22:41:19, Epoch: 32, Batch: 80, Training Loss: 0.064433953166008, LR: 0.010000000000000002
Time, 2019-01-01T22:41:20, Epoch: 32, Batch: 90, Training Loss: 0.05625033266842365, LR: 0.010000000000000002
Time, 2019-01-01T22:41:21, Epoch: 32, Batch: 100, Training Loss: 0.05372649915516377, LR: 0.010000000000000002
Time, 2019-01-01T22:41:21, Epoch: 32, Batch: 110, Training Loss: 0.043043477833271025, LR: 0.010000000000000002
Time, 2019-01-01T22:41:22, Epoch: 32, Batch: 120, Training Loss: 0.056274119019508365, LR: 0.010000000000000002
Time, 2019-01-01T22:41:23, Epoch: 32, Batch: 130, Training Loss: 0.05656685046851635, LR: 0.010000000000000002
Time, 2019-01-01T22:41:24, Epoch: 32, Batch: 140, Training Loss: 0.05434615984559059, LR: 0.010000000000000002
Time, 2019-01-01T22:41:25, Epoch: 32, Batch: 150, Training Loss: 0.03925874754786492, LR: 0.010000000000000002
Time, 2019-01-01T22:41:25, Epoch: 32, Batch: 160, Training Loss: 0.05681668184697628, LR: 0.010000000000000002
Time, 2019-01-01T22:41:26, Epoch: 32, Batch: 170, Training Loss: 0.058754411339759824, LR: 0.010000000000000002
Time, 2019-01-01T22:41:27, Epoch: 32, Batch: 180, Training Loss: 0.05186250545084477, LR: 0.010000000000000002
Time, 2019-01-01T22:41:28, Epoch: 32, Batch: 190, Training Loss: 0.0722254104912281, LR: 0.010000000000000002
Time, 2019-01-01T22:41:28, Epoch: 32, Batch: 200, Training Loss: 0.03314637839794159, LR: 0.010000000000000002
Time, 2019-01-01T22:41:29, Epoch: 32, Batch: 210, Training Loss: 0.06673815324902535, LR: 0.010000000000000002
Time, 2019-01-01T22:41:30, Epoch: 32, Batch: 220, Training Loss: 0.06158917360007763, LR: 0.010000000000000002
Time, 2019-01-01T22:41:31, Epoch: 32, Batch: 230, Training Loss: 0.05382921695709229, LR: 0.010000000000000002
Time, 2019-01-01T22:41:31, Epoch: 32, Batch: 240, Training Loss: 0.0629073664546013, LR: 0.010000000000000002
Time, 2019-01-01T22:41:32, Epoch: 32, Batch: 250, Training Loss: 0.0473106250166893, LR: 0.010000000000000002
Time, 2019-01-01T22:41:33, Epoch: 32, Batch: 260, Training Loss: 0.051728042960166934, LR: 0.010000000000000002
Time, 2019-01-01T22:41:34, Epoch: 32, Batch: 270, Training Loss: 0.07498477622866631, LR: 0.010000000000000002
Time, 2019-01-01T22:41:34, Epoch: 32, Batch: 280, Training Loss: 0.04009379111230373, LR: 0.010000000000000002
Time, 2019-01-01T22:41:35, Epoch: 32, Batch: 290, Training Loss: 0.056461936235427855, LR: 0.010000000000000002
Time, 2019-01-01T22:41:36, Epoch: 32, Batch: 300, Training Loss: 0.035743828490376474, LR: 0.010000000000000002
Time, 2019-01-01T22:41:37, Epoch: 32, Batch: 310, Training Loss: 0.04804169163107872, LR: 0.010000000000000002
Time, 2019-01-01T22:41:38, Epoch: 32, Batch: 320, Training Loss: 0.06207745559513569, LR: 0.010000000000000002
Time, 2019-01-01T22:41:38, Epoch: 32, Batch: 330, Training Loss: 0.043706240504980086, LR: 0.010000000000000002
Time, 2019-01-01T22:41:39, Epoch: 32, Batch: 340, Training Loss: 0.03407103568315506, LR: 0.010000000000000002
Time, 2019-01-01T22:41:40, Epoch: 32, Batch: 350, Training Loss: 0.0529038455337286, LR: 0.010000000000000002
Time, 2019-01-01T22:41:41, Epoch: 32, Batch: 360, Training Loss: 0.053131797909736635, LR: 0.010000000000000002
Time, 2019-01-01T22:41:41, Epoch: 32, Batch: 370, Training Loss: 0.06758684478700161, LR: 0.010000000000000002
Time, 2019-01-01T22:41:42, Epoch: 32, Batch: 380, Training Loss: 0.0710009891539812, LR: 0.010000000000000002
Time, 2019-01-01T22:41:43, Epoch: 32, Batch: 390, Training Loss: 0.04225888028740883, LR: 0.010000000000000002
Time, 2019-01-01T22:41:44, Epoch: 32, Batch: 400, Training Loss: 0.06518961973488331, LR: 0.010000000000000002
Time, 2019-01-01T22:41:45, Epoch: 32, Batch: 410, Training Loss: 0.055457422882318495, LR: 0.010000000000000002
Time, 2019-01-01T22:41:45, Epoch: 32, Batch: 420, Training Loss: 0.04533465951681137, LR: 0.010000000000000002
Time, 2019-01-01T22:41:46, Epoch: 32, Batch: 430, Training Loss: 0.036830152943730356, LR: 0.010000000000000002
Time, 2019-01-01T22:41:47, Epoch: 32, Batch: 440, Training Loss: 0.05537784844636917, LR: 0.010000000000000002
Time, 2019-01-01T22:41:48, Epoch: 32, Batch: 450, Training Loss: 0.04365541860461235, LR: 0.010000000000000002
Time, 2019-01-01T22:41:49, Epoch: 32, Batch: 460, Training Loss: 0.03606940396130085, LR: 0.010000000000000002
Time, 2019-01-01T22:41:50, Epoch: 32, Batch: 470, Training Loss: 0.04070126488804817, LR: 0.010000000000000002
Time, 2019-01-01T22:41:50, Epoch: 32, Batch: 480, Training Loss: 0.050240236520767215, LR: 0.010000000000000002
Time, 2019-01-01T22:41:51, Epoch: 32, Batch: 490, Training Loss: 0.04294918291270733, LR: 0.010000000000000002
Time, 2019-01-01T22:41:52, Epoch: 32, Batch: 500, Training Loss: 0.04064333140850067, LR: 0.010000000000000002
Time, 2019-01-01T22:41:53, Epoch: 32, Batch: 510, Training Loss: 0.043907107412815095, LR: 0.010000000000000002
Time, 2019-01-01T22:41:54, Epoch: 32, Batch: 520, Training Loss: 0.06288422495126725, LR: 0.010000000000000002
Time, 2019-01-01T22:41:54, Epoch: 32, Batch: 530, Training Loss: 0.042753638327121736, LR: 0.010000000000000002
Time, 2019-01-01T22:41:55, Epoch: 32, Batch: 540, Training Loss: 0.05167092084884643, LR: 0.010000000000000002
Time, 2019-01-01T22:41:56, Epoch: 32, Batch: 550, Training Loss: 0.042410173639655116, LR: 0.010000000000000002
Time, 2019-01-01T22:41:57, Epoch: 32, Batch: 560, Training Loss: 0.038951902836561206, LR: 0.010000000000000002
Time, 2019-01-01T22:41:58, Epoch: 32, Batch: 570, Training Loss: 0.059226854145526885, LR: 0.010000000000000002
Time, 2019-01-01T22:41:58, Epoch: 32, Batch: 580, Training Loss: 0.04054150879383087, LR: 0.010000000000000002
Time, 2019-01-01T22:41:59, Epoch: 32, Batch: 590, Training Loss: 0.07954747304320335, LR: 0.010000000000000002
Time, 2019-01-01T22:42:00, Epoch: 32, Batch: 600, Training Loss: 0.04578404501080513, LR: 0.010000000000000002
Time, 2019-01-01T22:42:01, Epoch: 32, Batch: 610, Training Loss: 0.05275061950087547, LR: 0.010000000000000002
Time, 2019-01-01T22:42:02, Epoch: 32, Batch: 620, Training Loss: 0.061815845966339114, LR: 0.010000000000000002
Time, 2019-01-01T22:42:02, Epoch: 32, Batch: 630, Training Loss: 0.06094730086624622, LR: 0.010000000000000002
Time, 2019-01-01T22:42:03, Epoch: 32, Batch: 640, Training Loss: 0.04364664331078529, LR: 0.010000000000000002
Time, 2019-01-01T22:42:04, Epoch: 32, Batch: 650, Training Loss: 0.04581134878098965, LR: 0.010000000000000002
Time, 2019-01-01T22:42:05, Epoch: 32, Batch: 660, Training Loss: 0.05449245497584343, LR: 0.010000000000000002
Time, 2019-01-01T22:42:05, Epoch: 32, Batch: 670, Training Loss: 0.048377041891217235, LR: 0.010000000000000002
Time, 2019-01-01T22:42:06, Epoch: 32, Batch: 680, Training Loss: 0.09590214565396309, LR: 0.010000000000000002
Time, 2019-01-01T22:42:07, Epoch: 32, Batch: 690, Training Loss: 0.057880721613764764, LR: 0.010000000000000002
Time, 2019-01-01T22:42:08, Epoch: 32, Batch: 700, Training Loss: 0.029522331058979036, LR: 0.010000000000000002
Time, 2019-01-01T22:42:09, Epoch: 32, Batch: 710, Training Loss: 0.07093689516186714, LR: 0.010000000000000002
Time, 2019-01-01T22:42:10, Epoch: 32, Batch: 720, Training Loss: 0.07210373319685459, LR: 0.010000000000000002
Time, 2019-01-01T22:42:11, Epoch: 32, Batch: 730, Training Loss: 0.057060257345438, LR: 0.010000000000000002
Time, 2019-01-01T22:42:12, Epoch: 32, Batch: 740, Training Loss: 0.07687766589224339, LR: 0.010000000000000002
Time, 2019-01-01T22:42:13, Epoch: 32, Batch: 750, Training Loss: 0.05020671896636486, LR: 0.010000000000000002
Time, 2019-01-01T22:42:13, Epoch: 32, Batch: 760, Training Loss: 0.046679404377937314, LR: 0.010000000000000002
Time, 2019-01-01T22:42:14, Epoch: 32, Batch: 770, Training Loss: 0.04177759289741516, LR: 0.010000000000000002
Time, 2019-01-01T22:42:15, Epoch: 32, Batch: 780, Training Loss: 0.04387876205146313, LR: 0.010000000000000002
Time, 2019-01-01T22:42:16, Epoch: 32, Batch: 790, Training Loss: 0.07153791449964046, LR: 0.010000000000000002
Time, 2019-01-01T22:42:17, Epoch: 32, Batch: 800, Training Loss: 0.08231926783919334, LR: 0.010000000000000002
Time, 2019-01-01T22:42:18, Epoch: 32, Batch: 810, Training Loss: 0.06779263690114021, LR: 0.010000000000000002
Time, 2019-01-01T22:42:18, Epoch: 32, Batch: 820, Training Loss: 0.04880110137164593, LR: 0.010000000000000002
Time, 2019-01-01T22:42:19, Epoch: 32, Batch: 830, Training Loss: 0.0655370008200407, LR: 0.010000000000000002
Time, 2019-01-01T22:42:20, Epoch: 32, Batch: 840, Training Loss: 0.053017988055944446, LR: 0.010000000000000002
Time, 2019-01-01T22:42:21, Epoch: 32, Batch: 850, Training Loss: 0.05042207166552544, LR: 0.010000000000000002
Time, 2019-01-01T22:42:22, Epoch: 32, Batch: 860, Training Loss: 0.045369521528482434, LR: 0.010000000000000002
Time, 2019-01-01T22:42:22, Epoch: 32, Batch: 870, Training Loss: 0.07803914621472359, LR: 0.010000000000000002
Time, 2019-01-01T22:42:23, Epoch: 32, Batch: 880, Training Loss: 0.05128796622157097, LR: 0.010000000000000002
Time, 2019-01-01T22:42:24, Epoch: 32, Batch: 890, Training Loss: 0.056190763413906095, LR: 0.010000000000000002
Time, 2019-01-01T22:42:25, Epoch: 32, Batch: 900, Training Loss: 0.08369933515787124, LR: 0.010000000000000002
Time, 2019-01-01T22:42:26, Epoch: 32, Batch: 910, Training Loss: 0.06408413797616959, LR: 0.010000000000000002
Time, 2019-01-01T22:42:26, Epoch: 32, Batch: 920, Training Loss: 0.060294263437390326, LR: 0.010000000000000002
Time, 2019-01-01T22:42:27, Epoch: 32, Batch: 930, Training Loss: 0.09584718346595764, LR: 0.010000000000000002
Epoch: 32, Validation Top 1 acc: 97.91998291015625
Epoch: 32, Validation Top 5 acc: 100.0
Epoch: 32, Validation Set Loss: 0.06305602937936783
Start training epoch 33
Time, 2019-01-01T22:42:34, Epoch: 33, Batch: 10, Training Loss: 0.05625230483710766, LR: 0.010000000000000002
Time, 2019-01-01T22:42:35, Epoch: 33, Batch: 20, Training Loss: 0.07709874585270882, LR: 0.010000000000000002
Time, 2019-01-01T22:42:36, Epoch: 33, Batch: 30, Training Loss: 0.06078176312148571, LR: 0.010000000000000002
Time, 2019-01-01T22:42:37, Epoch: 33, Batch: 40, Training Loss: 0.0523043405264616, LR: 0.010000000000000002
Time, 2019-01-01T22:42:38, Epoch: 33, Batch: 50, Training Loss: 0.047556573525071144, LR: 0.010000000000000002
Time, 2019-01-01T22:42:39, Epoch: 33, Batch: 60, Training Loss: 0.055763741955161095, LR: 0.010000000000000002
Time, 2019-01-01T22:42:40, Epoch: 33, Batch: 70, Training Loss: 0.06344840712845326, LR: 0.010000000000000002
Time, 2019-01-01T22:42:41, Epoch: 33, Batch: 80, Training Loss: 0.05503981895744801, LR: 0.010000000000000002
Time, 2019-01-01T22:42:42, Epoch: 33, Batch: 90, Training Loss: 0.06284552216529846, LR: 0.010000000000000002
Time, 2019-01-01T22:42:43, Epoch: 33, Batch: 100, Training Loss: 0.05157027021050453, LR: 0.010000000000000002
Time, 2019-01-01T22:42:44, Epoch: 33, Batch: 110, Training Loss: 0.06688454002141953, LR: 0.010000000000000002
Time, 2019-01-01T22:42:44, Epoch: 33, Batch: 120, Training Loss: 0.05696874223649502, LR: 0.010000000000000002
Time, 2019-01-01T22:42:45, Epoch: 33, Batch: 130, Training Loss: 0.06194661259651184, LR: 0.010000000000000002
Time, 2019-01-01T22:42:46, Epoch: 33, Batch: 140, Training Loss: 0.04847217276692391, LR: 0.010000000000000002
Time, 2019-01-01T22:42:47, Epoch: 33, Batch: 150, Training Loss: 0.051123013347387315, LR: 0.010000000000000002
Time, 2019-01-01T22:42:48, Epoch: 33, Batch: 160, Training Loss: 0.05696430802345276, LR: 0.010000000000000002
Time, 2019-01-01T22:42:49, Epoch: 33, Batch: 170, Training Loss: 0.04462006278336048, LR: 0.010000000000000002
Time, 2019-01-01T22:42:50, Epoch: 33, Batch: 180, Training Loss: 0.040924111008644105, LR: 0.010000000000000002
Time, 2019-01-01T22:42:50, Epoch: 33, Batch: 190, Training Loss: 0.055932801216840744, LR: 0.010000000000000002
Time, 2019-01-01T22:42:51, Epoch: 33, Batch: 200, Training Loss: 0.0501293458044529, LR: 0.010000000000000002
Time, 2019-01-01T22:42:52, Epoch: 33, Batch: 210, Training Loss: 0.05513488613069058, LR: 0.010000000000000002
Time, 2019-01-01T22:42:53, Epoch: 33, Batch: 220, Training Loss: 0.03731848672032356, LR: 0.010000000000000002
Time, 2019-01-01T22:42:54, Epoch: 33, Batch: 230, Training Loss: 0.04274306371808052, LR: 0.010000000000000002
Time, 2019-01-01T22:42:54, Epoch: 33, Batch: 240, Training Loss: 0.0488422304391861, LR: 0.010000000000000002
Time, 2019-01-01T22:42:55, Epoch: 33, Batch: 250, Training Loss: 0.04223342277109623, LR: 0.010000000000000002
Time, 2019-01-01T22:42:56, Epoch: 33, Batch: 260, Training Loss: 0.053413217142224315, LR: 0.010000000000000002
Time, 2019-01-01T22:42:57, Epoch: 33, Batch: 270, Training Loss: 0.07170183807611466, LR: 0.010000000000000002
Time, 2019-01-01T22:42:58, Epoch: 33, Batch: 280, Training Loss: 0.044248273223638536, LR: 0.010000000000000002
Time, 2019-01-01T22:42:59, Epoch: 33, Batch: 290, Training Loss: 0.0503415372222662, LR: 0.010000000000000002
Time, 2019-01-01T22:42:59, Epoch: 33, Batch: 300, Training Loss: 0.057752900198101996, LR: 0.010000000000000002
Time, 2019-01-01T22:43:00, Epoch: 33, Batch: 310, Training Loss: 0.08992943726480007, LR: 0.010000000000000002
Time, 2019-01-01T22:43:01, Epoch: 33, Batch: 320, Training Loss: 0.045903145149350164, LR: 0.010000000000000002
Time, 2019-01-01T22:43:02, Epoch: 33, Batch: 330, Training Loss: 0.05551611185073853, LR: 0.010000000000000002
Time, 2019-01-01T22:43:03, Epoch: 33, Batch: 340, Training Loss: 0.055031473934650424, LR: 0.010000000000000002
Time, 2019-01-01T22:43:03, Epoch: 33, Batch: 350, Training Loss: 0.044488827139139174, LR: 0.010000000000000002
Time, 2019-01-01T22:43:04, Epoch: 33, Batch: 360, Training Loss: 0.05339736305177212, LR: 0.010000000000000002
Time, 2019-01-01T22:43:05, Epoch: 33, Batch: 370, Training Loss: 0.0632833156734705, LR: 0.010000000000000002
Time, 2019-01-01T22:43:06, Epoch: 33, Batch: 380, Training Loss: 0.05000055059790611, LR: 0.010000000000000002
Time, 2019-01-01T22:43:06, Epoch: 33, Batch: 390, Training Loss: 0.05626312643289566, LR: 0.010000000000000002
Time, 2019-01-01T22:43:07, Epoch: 33, Batch: 400, Training Loss: 0.07054471112787723, LR: 0.010000000000000002
Time, 2019-01-01T22:43:08, Epoch: 33, Batch: 410, Training Loss: 0.06637797355651856, LR: 0.010000000000000002
Time, 2019-01-01T22:43:09, Epoch: 33, Batch: 420, Training Loss: 0.05289310775697231, LR: 0.010000000000000002
Time, 2019-01-01T22:43:10, Epoch: 33, Batch: 430, Training Loss: 0.05007244758307934, LR: 0.010000000000000002
Time, 2019-01-01T22:43:11, Epoch: 33, Batch: 440, Training Loss: 0.047314050048589705, LR: 0.010000000000000002
Time, 2019-01-01T22:43:11, Epoch: 33, Batch: 450, Training Loss: 0.05041835494339466, LR: 0.010000000000000002
Time, 2019-01-01T22:43:12, Epoch: 33, Batch: 460, Training Loss: 0.04682773128151894, LR: 0.010000000000000002
Time, 2019-01-01T22:43:13, Epoch: 33, Batch: 470, Training Loss: 0.040846963226795194, LR: 0.010000000000000002
Time, 2019-01-01T22:43:14, Epoch: 33, Batch: 480, Training Loss: 0.0438923254609108, LR: 0.010000000000000002
Time, 2019-01-01T22:43:14, Epoch: 33, Batch: 490, Training Loss: 0.05585246905684471, LR: 0.010000000000000002
Time, 2019-01-01T22:43:15, Epoch: 33, Batch: 500, Training Loss: 0.04361708238720894, LR: 0.010000000000000002
Time, 2019-01-01T22:43:16, Epoch: 33, Batch: 510, Training Loss: 0.06478735134005546, LR: 0.010000000000000002
Time, 2019-01-01T22:43:17, Epoch: 33, Batch: 520, Training Loss: 0.058731671422719955, LR: 0.010000000000000002
Time, 2019-01-01T22:43:18, Epoch: 33, Batch: 530, Training Loss: 0.04722226783633232, LR: 0.010000000000000002
Time, 2019-01-01T22:43:19, Epoch: 33, Batch: 540, Training Loss: 0.05536590702831745, LR: 0.010000000000000002
Time, 2019-01-01T22:43:19, Epoch: 33, Batch: 550, Training Loss: 0.049135951697826384, LR: 0.010000000000000002
Time, 2019-01-01T22:43:20, Epoch: 33, Batch: 560, Training Loss: 0.03621700629591942, LR: 0.010000000000000002
Time, 2019-01-01T22:43:21, Epoch: 33, Batch: 570, Training Loss: 0.03480016924440861, LR: 0.010000000000000002
Time, 2019-01-01T22:43:22, Epoch: 33, Batch: 580, Training Loss: 0.07864990308880807, LR: 0.010000000000000002
Time, 2019-01-01T22:43:22, Epoch: 33, Batch: 590, Training Loss: 0.05336095616221428, LR: 0.010000000000000002
Time, 2019-01-01T22:43:23, Epoch: 33, Batch: 600, Training Loss: 0.07120742797851562, LR: 0.010000000000000002
Time, 2019-01-01T22:43:24, Epoch: 33, Batch: 610, Training Loss: 0.06733809560537338, LR: 0.010000000000000002
Time, 2019-01-01T22:43:25, Epoch: 33, Batch: 620, Training Loss: 0.04518093466758728, LR: 0.010000000000000002
Time, 2019-01-01T22:43:26, Epoch: 33, Batch: 630, Training Loss: 0.061452941969037056, LR: 0.010000000000000002
Time, 2019-01-01T22:43:27, Epoch: 33, Batch: 640, Training Loss: 0.047149880602955815, LR: 0.010000000000000002
Time, 2019-01-01T22:43:27, Epoch: 33, Batch: 650, Training Loss: 0.042030643299221994, LR: 0.010000000000000002
Time, 2019-01-01T22:43:28, Epoch: 33, Batch: 660, Training Loss: 0.04751896485686302, LR: 0.010000000000000002
Time, 2019-01-01T22:43:29, Epoch: 33, Batch: 670, Training Loss: 0.07522649466991424, LR: 0.010000000000000002
Time, 2019-01-01T22:43:30, Epoch: 33, Batch: 680, Training Loss: 0.04222112111747265, LR: 0.010000000000000002
Time, 2019-01-01T22:43:31, Epoch: 33, Batch: 690, Training Loss: 0.04248413071036339, LR: 0.010000000000000002
Time, 2019-01-01T22:43:31, Epoch: 33, Batch: 700, Training Loss: 0.050109833106398584, LR: 0.010000000000000002
Time, 2019-01-01T22:43:32, Epoch: 33, Batch: 710, Training Loss: 0.07439477182924747, LR: 0.010000000000000002
Time, 2019-01-01T22:43:33, Epoch: 33, Batch: 720, Training Loss: 0.0568755466490984, LR: 0.010000000000000002
Time, 2019-01-01T22:43:34, Epoch: 33, Batch: 730, Training Loss: 0.07172999233007431, LR: 0.010000000000000002
Time, 2019-01-01T22:43:35, Epoch: 33, Batch: 740, Training Loss: 0.04480563141405582, LR: 0.010000000000000002
Time, 2019-01-01T22:43:35, Epoch: 33, Batch: 750, Training Loss: 0.05938273034989834, LR: 0.010000000000000002
Time, 2019-01-01T22:43:36, Epoch: 33, Batch: 760, Training Loss: 0.0532976932823658, LR: 0.010000000000000002
Time, 2019-01-01T22:43:37, Epoch: 33, Batch: 770, Training Loss: 0.0475618302822113, LR: 0.010000000000000002
Time, 2019-01-01T22:43:38, Epoch: 33, Batch: 780, Training Loss: 0.04804819002747536, LR: 0.010000000000000002
Time, 2019-01-01T22:43:38, Epoch: 33, Batch: 790, Training Loss: 0.05211332961916924, LR: 0.010000000000000002
Time, 2019-01-01T22:43:39, Epoch: 33, Batch: 800, Training Loss: 0.0440347857773304, LR: 0.010000000000000002
Time, 2019-01-01T22:43:40, Epoch: 33, Batch: 810, Training Loss: 0.06327315159142018, LR: 0.010000000000000002
Time, 2019-01-01T22:43:41, Epoch: 33, Batch: 820, Training Loss: 0.06294457837939263, LR: 0.010000000000000002
Time, 2019-01-01T22:43:42, Epoch: 33, Batch: 830, Training Loss: 0.07475746795535088, LR: 0.010000000000000002
Time, 2019-01-01T22:43:43, Epoch: 33, Batch: 840, Training Loss: 0.0640797458589077, LR: 0.010000000000000002
Time, 2019-01-01T22:43:43, Epoch: 33, Batch: 850, Training Loss: 0.058543532714247705, LR: 0.010000000000000002
Time, 2019-01-01T22:43:44, Epoch: 33, Batch: 860, Training Loss: 0.07026950158178806, LR: 0.010000000000000002
Time, 2019-01-01T22:43:45, Epoch: 33, Batch: 870, Training Loss: 0.03731165304780006, LR: 0.010000000000000002
Time, 2019-01-01T22:43:46, Epoch: 33, Batch: 880, Training Loss: 0.05325508341193199, LR: 0.010000000000000002
Time, 2019-01-01T22:43:47, Epoch: 33, Batch: 890, Training Loss: 0.05395754687488079, LR: 0.010000000000000002
Time, 2019-01-01T22:43:47, Epoch: 33, Batch: 900, Training Loss: 0.05467897653579712, LR: 0.010000000000000002
Time, 2019-01-01T22:43:48, Epoch: 33, Batch: 910, Training Loss: 0.06751722432672977, LR: 0.010000000000000002
Time, 2019-01-01T22:43:49, Epoch: 33, Batch: 920, Training Loss: 0.0688378430902958, LR: 0.010000000000000002
Time, 2019-01-01T22:43:50, Epoch: 33, Batch: 930, Training Loss: 0.048860976472496986, LR: 0.010000000000000002
Epoch: 33, Validation Top 1 acc: 98.1588363647461
Epoch: 33, Validation Top 5 acc: 100.0
Epoch: 33, Validation Set Loss: 0.05646951496601105
Start training epoch 34
Time, 2019-01-01T22:43:56, Epoch: 34, Batch: 10, Training Loss: 0.034777387604117395, LR: 0.010000000000000002
Time, 2019-01-01T22:43:57, Epoch: 34, Batch: 20, Training Loss: 0.05269306488335133, LR: 0.010000000000000002
Time, 2019-01-01T22:43:58, Epoch: 34, Batch: 30, Training Loss: 0.05399866811931133, LR: 0.010000000000000002
Time, 2019-01-01T22:43:59, Epoch: 34, Batch: 40, Training Loss: 0.051426255702972413, LR: 0.010000000000000002
Time, 2019-01-01T22:43:59, Epoch: 34, Batch: 50, Training Loss: 0.04645911939442158, LR: 0.010000000000000002
Time, 2019-01-01T22:44:00, Epoch: 34, Batch: 60, Training Loss: 0.050160642340779306, LR: 0.010000000000000002
Time, 2019-01-01T22:44:01, Epoch: 34, Batch: 70, Training Loss: 0.05278666540980339, LR: 0.010000000000000002
Time, 2019-01-01T22:44:02, Epoch: 34, Batch: 80, Training Loss: 0.05952277034521103, LR: 0.010000000000000002
Time, 2019-01-01T22:44:03, Epoch: 34, Batch: 90, Training Loss: 0.04227709583938122, LR: 0.010000000000000002
Time, 2019-01-01T22:44:03, Epoch: 34, Batch: 100, Training Loss: 0.045654069632291794, LR: 0.010000000000000002
Time, 2019-01-01T22:44:04, Epoch: 34, Batch: 110, Training Loss: 0.03029797300696373, LR: 0.010000000000000002
Time, 2019-01-01T22:44:05, Epoch: 34, Batch: 120, Training Loss: 0.03607433885335922, LR: 0.010000000000000002
Time, 2019-01-01T22:44:06, Epoch: 34, Batch: 130, Training Loss: 0.06083873957395554, LR: 0.010000000000000002
Time, 2019-01-01T22:44:07, Epoch: 34, Batch: 140, Training Loss: 0.03797580972313881, LR: 0.010000000000000002
Time, 2019-01-01T22:44:07, Epoch: 34, Batch: 150, Training Loss: 0.049528637528419496, LR: 0.010000000000000002
Time, 2019-01-01T22:44:08, Epoch: 34, Batch: 160, Training Loss: 0.04048928506672382, LR: 0.010000000000000002
Time, 2019-01-01T22:44:09, Epoch: 34, Batch: 170, Training Loss: 0.05086260139942169, LR: 0.010000000000000002
Time, 2019-01-01T22:44:10, Epoch: 34, Batch: 180, Training Loss: 0.051471445336937906, LR: 0.010000000000000002
Time, 2019-01-01T22:44:11, Epoch: 34, Batch: 190, Training Loss: 0.06305923089385032, LR: 0.010000000000000002
Time, 2019-01-01T22:44:12, Epoch: 34, Batch: 200, Training Loss: 0.06235799789428711, LR: 0.010000000000000002
Time, 2019-01-01T22:44:12, Epoch: 34, Batch: 210, Training Loss: 0.08180894739925862, LR: 0.010000000000000002
Time, 2019-01-01T22:44:14, Epoch: 34, Batch: 220, Training Loss: 0.06167019791901111, LR: 0.010000000000000002
Time, 2019-01-01T22:44:15, Epoch: 34, Batch: 230, Training Loss: 0.07419183738529682, LR: 0.010000000000000002
Time, 2019-01-01T22:44:16, Epoch: 34, Batch: 240, Training Loss: 0.031205024942755698, LR: 0.010000000000000002
Time, 2019-01-01T22:44:16, Epoch: 34, Batch: 250, Training Loss: 0.06025368869304657, LR: 0.010000000000000002
Time, 2019-01-01T22:44:17, Epoch: 34, Batch: 260, Training Loss: 0.04494679011404514, LR: 0.010000000000000002
Time, 2019-01-01T22:44:18, Epoch: 34, Batch: 270, Training Loss: 0.039269689470529556, LR: 0.010000000000000002
Time, 2019-01-01T22:44:19, Epoch: 34, Batch: 280, Training Loss: 0.057846691086888315, LR: 0.010000000000000002
Time, 2019-01-01T22:44:19, Epoch: 34, Batch: 290, Training Loss: 0.06298297941684723, LR: 0.010000000000000002
Time, 2019-01-01T22:44:20, Epoch: 34, Batch: 300, Training Loss: 0.09544269815087318, LR: 0.010000000000000002
Time, 2019-01-01T22:44:21, Epoch: 34, Batch: 310, Training Loss: 0.06429536715149879, LR: 0.010000000000000002
Time, 2019-01-01T22:44:22, Epoch: 34, Batch: 320, Training Loss: 0.05293269641697407, LR: 0.010000000000000002
Time, 2019-01-01T22:44:22, Epoch: 34, Batch: 330, Training Loss: 0.05246282629668712, LR: 0.010000000000000002
Time, 2019-01-01T22:44:23, Epoch: 34, Batch: 340, Training Loss: 0.056742338091135026, LR: 0.010000000000000002
Time, 2019-01-01T22:44:24, Epoch: 34, Batch: 350, Training Loss: 0.037494389712810515, LR: 0.010000000000000002
Time, 2019-01-01T22:44:25, Epoch: 34, Batch: 360, Training Loss: 0.05419121608138085, LR: 0.010000000000000002
Time, 2019-01-01T22:44:26, Epoch: 34, Batch: 370, Training Loss: 0.06469521820545196, LR: 0.010000000000000002
Time, 2019-01-01T22:44:26, Epoch: 34, Batch: 380, Training Loss: 0.056443119794130324, LR: 0.010000000000000002
Time, 2019-01-01T22:44:27, Epoch: 34, Batch: 390, Training Loss: 0.03813615925610066, LR: 0.010000000000000002
Time, 2019-01-01T22:44:28, Epoch: 34, Batch: 400, Training Loss: 0.030795521661639215, LR: 0.010000000000000002
Time, 2019-01-01T22:44:29, Epoch: 34, Batch: 410, Training Loss: 0.058497438207268715, LR: 0.010000000000000002
Time, 2019-01-01T22:44:29, Epoch: 34, Batch: 420, Training Loss: 0.07079657092690468, LR: 0.010000000000000002
Time, 2019-01-01T22:44:30, Epoch: 34, Batch: 430, Training Loss: 0.049820613116025925, LR: 0.010000000000000002
Time, 2019-01-01T22:44:31, Epoch: 34, Batch: 440, Training Loss: 0.06382595896720886, LR: 0.010000000000000002
Time, 2019-01-01T22:44:32, Epoch: 34, Batch: 450, Training Loss: 0.0572528176009655, LR: 0.010000000000000002
Time, 2019-01-01T22:44:32, Epoch: 34, Batch: 460, Training Loss: 0.07576344758272172, LR: 0.010000000000000002
Time, 2019-01-01T22:44:33, Epoch: 34, Batch: 470, Training Loss: 0.043114426732063296, LR: 0.010000000000000002
Time, 2019-01-01T22:44:34, Epoch: 34, Batch: 480, Training Loss: 0.04547217935323715, LR: 0.010000000000000002
Time, 2019-01-01T22:44:35, Epoch: 34, Batch: 490, Training Loss: 0.04344135373830795, LR: 0.010000000000000002
Time, 2019-01-01T22:44:36, Epoch: 34, Batch: 500, Training Loss: 0.047208815813064575, LR: 0.010000000000000002
Time, 2019-01-01T22:44:36, Epoch: 34, Batch: 510, Training Loss: 0.05184837616980076, LR: 0.010000000000000002
Time, 2019-01-01T22:44:37, Epoch: 34, Batch: 520, Training Loss: 0.05113660059869289, LR: 0.010000000000000002
Time, 2019-01-01T22:44:38, Epoch: 34, Batch: 530, Training Loss: 0.07072995863854885, LR: 0.010000000000000002
Time, 2019-01-01T22:44:39, Epoch: 34, Batch: 540, Training Loss: 0.042995793744921684, LR: 0.010000000000000002
Time, 2019-01-01T22:44:40, Epoch: 34, Batch: 550, Training Loss: 0.05980459973216057, LR: 0.010000000000000002
Time, 2019-01-01T22:44:40, Epoch: 34, Batch: 560, Training Loss: 0.030124632641673088, LR: 0.010000000000000002
Time, 2019-01-01T22:44:41, Epoch: 34, Batch: 570, Training Loss: 0.0457868006080389, LR: 0.010000000000000002
Time, 2019-01-01T22:44:42, Epoch: 34, Batch: 580, Training Loss: 0.04924337565898895, LR: 0.010000000000000002
Time, 2019-01-01T22:44:43, Epoch: 34, Batch: 590, Training Loss: 0.04710007160902023, LR: 0.010000000000000002
Time, 2019-01-01T22:44:43, Epoch: 34, Batch: 600, Training Loss: 0.04098438285291195, LR: 0.010000000000000002
Time, 2019-01-01T22:44:44, Epoch: 34, Batch: 610, Training Loss: 0.048113252967596054, LR: 0.010000000000000002
Time, 2019-01-01T22:44:45, Epoch: 34, Batch: 620, Training Loss: 0.05359058156609535, LR: 0.010000000000000002
Time, 2019-01-01T22:44:46, Epoch: 34, Batch: 630, Training Loss: 0.05149239189922809, LR: 0.010000000000000002
Time, 2019-01-01T22:44:46, Epoch: 34, Batch: 640, Training Loss: 0.044583377242088315, LR: 0.010000000000000002
Time, 2019-01-01T22:44:47, Epoch: 34, Batch: 650, Training Loss: 0.04232945516705513, LR: 0.010000000000000002
Time, 2019-01-01T22:44:48, Epoch: 34, Batch: 660, Training Loss: 0.04891137704253197, LR: 0.010000000000000002
Time, 2019-01-01T22:44:49, Epoch: 34, Batch: 670, Training Loss: 0.049550920724868774, LR: 0.010000000000000002
Time, 2019-01-01T22:44:49, Epoch: 34, Batch: 680, Training Loss: 0.04549303501844406, LR: 0.010000000000000002
Time, 2019-01-01T22:44:50, Epoch: 34, Batch: 690, Training Loss: 0.049377201870083806, LR: 0.010000000000000002
Time, 2019-01-01T22:44:51, Epoch: 34, Batch: 700, Training Loss: 0.055337734147906305, LR: 0.010000000000000002
Time, 2019-01-01T22:44:52, Epoch: 34, Batch: 710, Training Loss: 0.06056952849030495, LR: 0.010000000000000002
Time, 2019-01-01T22:44:53, Epoch: 34, Batch: 720, Training Loss: 0.04668572694063187, LR: 0.010000000000000002
Time, 2019-01-01T22:44:54, Epoch: 34, Batch: 730, Training Loss: 0.0679557390511036, LR: 0.010000000000000002
Time, 2019-01-01T22:44:55, Epoch: 34, Batch: 740, Training Loss: 0.03593999370932579, LR: 0.010000000000000002
Time, 2019-01-01T22:44:56, Epoch: 34, Batch: 750, Training Loss: 0.03430059403181076, LR: 0.010000000000000002
Time, 2019-01-01T22:44:57, Epoch: 34, Batch: 760, Training Loss: 0.03192655816674232, LR: 0.010000000000000002
Time, 2019-01-01T22:44:57, Epoch: 34, Batch: 770, Training Loss: 0.04507792070508003, LR: 0.010000000000000002
Time, 2019-01-01T22:44:58, Epoch: 34, Batch: 780, Training Loss: 0.05262702405452728, LR: 0.010000000000000002
Time, 2019-01-01T22:44:59, Epoch: 34, Batch: 790, Training Loss: 0.028125163167715073, LR: 0.010000000000000002
Time, 2019-01-01T22:45:00, Epoch: 34, Batch: 800, Training Loss: 0.05511660315096378, LR: 0.010000000000000002
Time, 2019-01-01T22:45:01, Epoch: 34, Batch: 810, Training Loss: 0.04782306812703609, LR: 0.010000000000000002
Time, 2019-01-01T22:45:01, Epoch: 34, Batch: 820, Training Loss: 0.06533150300383568, LR: 0.010000000000000002
Time, 2019-01-01T22:45:02, Epoch: 34, Batch: 830, Training Loss: 0.04970412105321884, LR: 0.010000000000000002
Time, 2019-01-01T22:45:03, Epoch: 34, Batch: 840, Training Loss: 0.06592121720314026, LR: 0.010000000000000002
Time, 2019-01-01T22:45:04, Epoch: 34, Batch: 850, Training Loss: 0.036381836608052255, LR: 0.010000000000000002
Time, 2019-01-01T22:45:05, Epoch: 34, Batch: 860, Training Loss: 0.06509372591972351, LR: 0.010000000000000002
Time, 2019-01-01T22:45:05, Epoch: 34, Batch: 870, Training Loss: 0.04619774632155895, LR: 0.010000000000000002
Time, 2019-01-01T22:45:06, Epoch: 34, Batch: 880, Training Loss: 0.03197845220565796, LR: 0.010000000000000002
Time, 2019-01-01T22:45:07, Epoch: 34, Batch: 890, Training Loss: 0.035386472940444946, LR: 0.010000000000000002
Time, 2019-01-01T22:45:08, Epoch: 34, Batch: 900, Training Loss: 0.04014058858156204, LR: 0.010000000000000002
Time, 2019-01-01T22:45:09, Epoch: 34, Batch: 910, Training Loss: 0.045360177755355835, LR: 0.010000000000000002
Time, 2019-01-01T22:45:09, Epoch: 34, Batch: 920, Training Loss: 0.04722301997244358, LR: 0.010000000000000002
Time, 2019-01-01T22:45:10, Epoch: 34, Batch: 930, Training Loss: 0.05009337291121483, LR: 0.010000000000000002
Epoch: 34, Validation Top 1 acc: 98.57682800292969
Epoch: 34, Validation Top 5 acc: 100.0
Epoch: 34, Validation Set Loss: 0.04956776276230812
Start training epoch 35
Time, 2019-01-01T22:45:17, Epoch: 35, Batch: 10, Training Loss: 0.04156147316098213, LR: 0.010000000000000002
Time, 2019-01-01T22:45:17, Epoch: 35, Batch: 20, Training Loss: 0.04228269085288048, LR: 0.010000000000000002
Time, 2019-01-01T22:45:18, Epoch: 35, Batch: 30, Training Loss: 0.05576783195137978, LR: 0.010000000000000002
Time, 2019-01-01T22:45:19, Epoch: 35, Batch: 40, Training Loss: 0.04584796503186226, LR: 0.010000000000000002
Time, 2019-01-01T22:45:20, Epoch: 35, Batch: 50, Training Loss: 0.052057061716914176, LR: 0.010000000000000002
Time, 2019-01-01T22:45:21, Epoch: 35, Batch: 60, Training Loss: 0.04012388847768307, LR: 0.010000000000000002
Time, 2019-01-01T22:45:21, Epoch: 35, Batch: 70, Training Loss: 0.05684010311961174, LR: 0.010000000000000002
Time, 2019-01-01T22:45:22, Epoch: 35, Batch: 80, Training Loss: 0.05678915195167065, LR: 0.010000000000000002
Time, 2019-01-01T22:45:23, Epoch: 35, Batch: 90, Training Loss: 0.03821020908653736, LR: 0.010000000000000002
Time, 2019-01-01T22:45:24, Epoch: 35, Batch: 100, Training Loss: 0.04831067472696304, LR: 0.010000000000000002
Time, 2019-01-01T22:45:25, Epoch: 35, Batch: 110, Training Loss: 0.03701615259051323, LR: 0.010000000000000002
Time, 2019-01-01T22:45:25, Epoch: 35, Batch: 120, Training Loss: 0.04612768553197384, LR: 0.010000000000000002
Time, 2019-01-01T22:45:26, Epoch: 35, Batch: 130, Training Loss: 0.07596143074333668, LR: 0.010000000000000002
Time, 2019-01-01T22:45:27, Epoch: 35, Batch: 140, Training Loss: 0.049130068719387056, LR: 0.010000000000000002
Time, 2019-01-01T22:45:28, Epoch: 35, Batch: 150, Training Loss: 0.04827225022017956, LR: 0.010000000000000002
Time, 2019-01-01T22:45:29, Epoch: 35, Batch: 160, Training Loss: 0.04969329871237278, LR: 0.010000000000000002
Time, 2019-01-01T22:45:29, Epoch: 35, Batch: 170, Training Loss: 0.05255984663963318, LR: 0.010000000000000002
Time, 2019-01-01T22:45:30, Epoch: 35, Batch: 180, Training Loss: 0.06227129325270653, LR: 0.010000000000000002
Time, 2019-01-01T22:45:31, Epoch: 35, Batch: 190, Training Loss: 0.06304754391312599, LR: 0.010000000000000002
Time, 2019-01-01T22:45:32, Epoch: 35, Batch: 200, Training Loss: 0.02946726717054844, LR: 0.010000000000000002
Time, 2019-01-01T22:45:33, Epoch: 35, Batch: 210, Training Loss: 0.07076879777014256, LR: 0.010000000000000002
Time, 2019-01-01T22:45:33, Epoch: 35, Batch: 220, Training Loss: 0.042704223096370696, LR: 0.010000000000000002
Time, 2019-01-01T22:45:34, Epoch: 35, Batch: 230, Training Loss: 0.05198299996554852, LR: 0.010000000000000002
Time, 2019-01-01T22:45:35, Epoch: 35, Batch: 240, Training Loss: 0.061448191106319425, LR: 0.010000000000000002
Time, 2019-01-01T22:45:36, Epoch: 35, Batch: 250, Training Loss: 0.046114084869623186, LR: 0.010000000000000002
Time, 2019-01-01T22:45:36, Epoch: 35, Batch: 260, Training Loss: 0.06506293043494224, LR: 0.010000000000000002
Time, 2019-01-01T22:45:37, Epoch: 35, Batch: 270, Training Loss: 0.04561277367174625, LR: 0.010000000000000002
Time, 2019-01-01T22:45:38, Epoch: 35, Batch: 280, Training Loss: 0.038157705962657926, LR: 0.010000000000000002
Time, 2019-01-01T22:45:39, Epoch: 35, Batch: 290, Training Loss: 0.06808111518621444, LR: 0.010000000000000002
Time, 2019-01-01T22:45:39, Epoch: 35, Batch: 300, Training Loss: 0.04903060421347618, LR: 0.010000000000000002
Time, 2019-01-01T22:45:40, Epoch: 35, Batch: 310, Training Loss: 0.03374854102730751, LR: 0.010000000000000002
Time, 2019-01-01T22:45:41, Epoch: 35, Batch: 320, Training Loss: 0.0629915475845337, LR: 0.010000000000000002
Time, 2019-01-01T22:45:42, Epoch: 35, Batch: 330, Training Loss: 0.037024271488189694, LR: 0.010000000000000002
Time, 2019-01-01T22:45:43, Epoch: 35, Batch: 340, Training Loss: 0.0677365642040968, LR: 0.010000000000000002
Time, 2019-01-01T22:45:43, Epoch: 35, Batch: 350, Training Loss: 0.06720373928546905, LR: 0.010000000000000002
Time, 2019-01-01T22:45:44, Epoch: 35, Batch: 360, Training Loss: 0.05417694002389908, LR: 0.010000000000000002
Time, 2019-01-01T22:45:45, Epoch: 35, Batch: 370, Training Loss: 0.05589102618396282, LR: 0.010000000000000002
Time, 2019-01-01T22:45:46, Epoch: 35, Batch: 380, Training Loss: 0.07846827171742916, LR: 0.010000000000000002
Time, 2019-01-01T22:45:47, Epoch: 35, Batch: 390, Training Loss: 0.06425082199275493, LR: 0.010000000000000002
Time, 2019-01-01T22:45:48, Epoch: 35, Batch: 400, Training Loss: 0.04342258423566818, LR: 0.010000000000000002
Time, 2019-01-01T22:45:48, Epoch: 35, Batch: 410, Training Loss: 0.05248988345265389, LR: 0.010000000000000002
Time, 2019-01-01T22:45:49, Epoch: 35, Batch: 420, Training Loss: 0.0632544469088316, LR: 0.010000000000000002
Time, 2019-01-01T22:45:50, Epoch: 35, Batch: 430, Training Loss: 0.053924450650811195, LR: 0.010000000000000002
Time, 2019-01-01T22:45:51, Epoch: 35, Batch: 440, Training Loss: 0.05858784727752209, LR: 0.010000000000000002
Time, 2019-01-01T22:45:51, Epoch: 35, Batch: 450, Training Loss: 0.05192408412694931, LR: 0.010000000000000002
Time, 2019-01-01T22:45:52, Epoch: 35, Batch: 460, Training Loss: 0.061713945120573044, LR: 0.010000000000000002
Time, 2019-01-01T22:45:53, Epoch: 35, Batch: 470, Training Loss: 0.060671316832304, LR: 0.010000000000000002
Time, 2019-01-01T22:45:54, Epoch: 35, Batch: 480, Training Loss: 0.061072219535708426, LR: 0.010000000000000002
Time, 2019-01-01T22:45:55, Epoch: 35, Batch: 490, Training Loss: 0.055300944671034816, LR: 0.010000000000000002
Time, 2019-01-01T22:45:55, Epoch: 35, Batch: 500, Training Loss: 0.04541362375020981, LR: 0.010000000000000002
Time, 2019-01-01T22:45:56, Epoch: 35, Batch: 510, Training Loss: 0.049834339320659636, LR: 0.010000000000000002
Time, 2019-01-01T22:45:57, Epoch: 35, Batch: 520, Training Loss: 0.04118155017495155, LR: 0.010000000000000002
Time, 2019-01-01T22:45:58, Epoch: 35, Batch: 530, Training Loss: 0.034792691096663476, LR: 0.010000000000000002
Time, 2019-01-01T22:45:58, Epoch: 35, Batch: 540, Training Loss: 0.06273023821413518, LR: 0.010000000000000002
Time, 2019-01-01T22:45:59, Epoch: 35, Batch: 550, Training Loss: 0.047755106911063196, LR: 0.010000000000000002
Time, 2019-01-01T22:46:00, Epoch: 35, Batch: 560, Training Loss: 0.0485771045088768, LR: 0.010000000000000002
Time, 2019-01-01T22:46:01, Epoch: 35, Batch: 570, Training Loss: 0.04448903389275074, LR: 0.010000000000000002
Time, 2019-01-01T22:46:02, Epoch: 35, Batch: 580, Training Loss: 0.043005666509270665, LR: 0.010000000000000002
Time, 2019-01-01T22:46:02, Epoch: 35, Batch: 590, Training Loss: 0.028799331560730935, LR: 0.010000000000000002
Time, 2019-01-01T22:46:03, Epoch: 35, Batch: 600, Training Loss: 0.03373676426708698, LR: 0.010000000000000002
Time, 2019-01-01T22:46:04, Epoch: 35, Batch: 610, Training Loss: 0.06227567046880722, LR: 0.010000000000000002
Time, 2019-01-01T22:46:05, Epoch: 35, Batch: 620, Training Loss: 0.06971808895468712, LR: 0.010000000000000002
Time, 2019-01-01T22:46:05, Epoch: 35, Batch: 630, Training Loss: 0.04547016061842442, LR: 0.010000000000000002
Time, 2019-01-01T22:46:06, Epoch: 35, Batch: 640, Training Loss: 0.046680798381567, LR: 0.010000000000000002
Time, 2019-01-01T22:46:07, Epoch: 35, Batch: 650, Training Loss: 0.04139895811676979, LR: 0.010000000000000002
Time, 2019-01-01T22:46:08, Epoch: 35, Batch: 660, Training Loss: 0.05340436473488808, LR: 0.010000000000000002
Time, 2019-01-01T22:46:09, Epoch: 35, Batch: 670, Training Loss: 0.062313730269670485, LR: 0.010000000000000002
Time, 2019-01-01T22:46:10, Epoch: 35, Batch: 680, Training Loss: 0.04084871374070644, LR: 0.010000000000000002
Time, 2019-01-01T22:46:11, Epoch: 35, Batch: 690, Training Loss: 0.06260189116001129, LR: 0.010000000000000002
Time, 2019-01-01T22:46:12, Epoch: 35, Batch: 700, Training Loss: 0.06954171024262905, LR: 0.010000000000000002
Time, 2019-01-01T22:46:13, Epoch: 35, Batch: 710, Training Loss: 0.06024559661746025, LR: 0.010000000000000002
Time, 2019-01-01T22:46:14, Epoch: 35, Batch: 720, Training Loss: 0.07341457046568393, LR: 0.010000000000000002
Time, 2019-01-01T22:46:14, Epoch: 35, Batch: 730, Training Loss: 0.06622974798083306, LR: 0.010000000000000002
Time, 2019-01-01T22:46:15, Epoch: 35, Batch: 740, Training Loss: 0.06397960186004639, LR: 0.010000000000000002
Time, 2019-01-01T22:46:16, Epoch: 35, Batch: 750, Training Loss: 0.07573900185525417, LR: 0.010000000000000002
Time, 2019-01-01T22:46:17, Epoch: 35, Batch: 760, Training Loss: 0.05109920091927052, LR: 0.010000000000000002
Time, 2019-01-01T22:46:18, Epoch: 35, Batch: 770, Training Loss: 0.06017129197716713, LR: 0.010000000000000002
Time, 2019-01-01T22:46:19, Epoch: 35, Batch: 780, Training Loss: 0.04373845048248768, LR: 0.010000000000000002
Time, 2019-01-01T22:46:19, Epoch: 35, Batch: 790, Training Loss: 0.07293767035007477, LR: 0.010000000000000002
Time, 2019-01-01T22:46:20, Epoch: 35, Batch: 800, Training Loss: 0.07735629715025424, LR: 0.010000000000000002
Time, 2019-01-01T22:46:21, Epoch: 35, Batch: 810, Training Loss: 0.06656360998749733, LR: 0.010000000000000002
Time, 2019-01-01T22:46:22, Epoch: 35, Batch: 820, Training Loss: 0.051191965118050575, LR: 0.010000000000000002
Time, 2019-01-01T22:46:23, Epoch: 35, Batch: 830, Training Loss: 0.06746971942484378, LR: 0.010000000000000002
Time, 2019-01-01T22:46:23, Epoch: 35, Batch: 840, Training Loss: 0.07239431142807007, LR: 0.010000000000000002
Time, 2019-01-01T22:46:24, Epoch: 35, Batch: 850, Training Loss: 0.049214818328619, LR: 0.010000000000000002
Time, 2019-01-01T22:46:25, Epoch: 35, Batch: 860, Training Loss: 0.04346331171691418, LR: 0.010000000000000002
Time, 2019-01-01T22:46:26, Epoch: 35, Batch: 870, Training Loss: 0.05296688824892044, LR: 0.010000000000000002
Time, 2019-01-01T22:46:27, Epoch: 35, Batch: 880, Training Loss: 0.06396791636943817, LR: 0.010000000000000002
Time, 2019-01-01T22:46:27, Epoch: 35, Batch: 890, Training Loss: 0.05463100224733353, LR: 0.010000000000000002
Time, 2019-01-01T22:46:28, Epoch: 35, Batch: 900, Training Loss: 0.07159609049558639, LR: 0.010000000000000002
Time, 2019-01-01T22:46:29, Epoch: 35, Batch: 910, Training Loss: 0.048286080732941626, LR: 0.010000000000000002
Time, 2019-01-01T22:46:30, Epoch: 35, Batch: 920, Training Loss: 0.07131679393351079, LR: 0.010000000000000002
Time, 2019-01-01T22:46:31, Epoch: 35, Batch: 930, Training Loss: 0.0635176956653595, LR: 0.010000000000000002
Epoch: 35, Validation Top 1 acc: 98.3280258178711
Epoch: 35, Validation Top 5 acc: 99.97014617919922
Epoch: 35, Validation Set Loss: 0.05722557380795479
Start training epoch 36
Time, 2019-01-01T22:46:37, Epoch: 36, Batch: 10, Training Loss: 0.05843140706419945, LR: 0.010000000000000002
Time, 2019-01-01T22:46:38, Epoch: 36, Batch: 20, Training Loss: 0.05932674780488014, LR: 0.010000000000000002
Time, 2019-01-01T22:46:39, Epoch: 36, Batch: 30, Training Loss: 0.052116684243083, LR: 0.010000000000000002
Time, 2019-01-01T22:46:40, Epoch: 36, Batch: 40, Training Loss: 0.05498265437781811, LR: 0.010000000000000002
Time, 2019-01-01T22:46:41, Epoch: 36, Batch: 50, Training Loss: 0.06520176939666271, LR: 0.010000000000000002
Time, 2019-01-01T22:46:42, Epoch: 36, Batch: 60, Training Loss: 0.03596804961562157, LR: 0.010000000000000002
Time, 2019-01-01T22:46:43, Epoch: 36, Batch: 70, Training Loss: 0.06620095893740655, LR: 0.010000000000000002
Time, 2019-01-01T22:46:44, Epoch: 36, Batch: 80, Training Loss: 0.04157278388738632, LR: 0.010000000000000002
Time, 2019-01-01T22:46:44, Epoch: 36, Batch: 90, Training Loss: 0.05223439112305641, LR: 0.010000000000000002
Time, 2019-01-01T22:46:45, Epoch: 36, Batch: 100, Training Loss: 0.04661396779119968, LR: 0.010000000000000002
Time, 2019-01-01T22:46:46, Epoch: 36, Batch: 110, Training Loss: 0.05948188193142414, LR: 0.010000000000000002
Time, 2019-01-01T22:46:47, Epoch: 36, Batch: 120, Training Loss: 0.03599784448742867, LR: 0.010000000000000002
Time, 2019-01-01T22:46:47, Epoch: 36, Batch: 130, Training Loss: 0.05126418359577656, LR: 0.010000000000000002
Time, 2019-01-01T22:46:48, Epoch: 36, Batch: 140, Training Loss: 0.039517955482006074, LR: 0.010000000000000002
Time, 2019-01-01T22:46:49, Epoch: 36, Batch: 150, Training Loss: 0.06280942112207413, LR: 0.010000000000000002
Time, 2019-01-01T22:46:50, Epoch: 36, Batch: 160, Training Loss: 0.05341904908418656, LR: 0.010000000000000002
Time, 2019-01-01T22:46:51, Epoch: 36, Batch: 170, Training Loss: 0.05639774650335312, LR: 0.010000000000000002
Time, 2019-01-01T22:46:51, Epoch: 36, Batch: 180, Training Loss: 0.0628193236887455, LR: 0.010000000000000002
Time, 2019-01-01T22:46:52, Epoch: 36, Batch: 190, Training Loss: 0.058142322674393655, LR: 0.010000000000000002
Time, 2019-01-01T22:46:53, Epoch: 36, Batch: 200, Training Loss: 0.05212042294442654, LR: 0.010000000000000002
Time, 2019-01-01T22:46:54, Epoch: 36, Batch: 210, Training Loss: 0.05083833783864975, LR: 0.010000000000000002
Time, 2019-01-01T22:46:54, Epoch: 36, Batch: 220, Training Loss: 0.06628491915762424, LR: 0.010000000000000002
Time, 2019-01-01T22:46:55, Epoch: 36, Batch: 230, Training Loss: 0.051978422701358794, LR: 0.010000000000000002
Time, 2019-01-01T22:46:56, Epoch: 36, Batch: 240, Training Loss: 0.05316499769687653, LR: 0.010000000000000002
Time, 2019-01-01T22:46:57, Epoch: 36, Batch: 250, Training Loss: 0.0353045828640461, LR: 0.010000000000000002
Time, 2019-01-01T22:46:58, Epoch: 36, Batch: 260, Training Loss: 0.053787050768733025, LR: 0.010000000000000002
Time, 2019-01-01T22:46:58, Epoch: 36, Batch: 270, Training Loss: 0.04742344096302986, LR: 0.010000000000000002
Time, 2019-01-01T22:46:59, Epoch: 36, Batch: 280, Training Loss: 0.047879341244697574, LR: 0.010000000000000002
Time, 2019-01-01T22:47:00, Epoch: 36, Batch: 290, Training Loss: 0.06711205840110779, LR: 0.010000000000000002
Time, 2019-01-01T22:47:01, Epoch: 36, Batch: 300, Training Loss: 0.05429392270743847, LR: 0.010000000000000002
Time, 2019-01-01T22:47:01, Epoch: 36, Batch: 310, Training Loss: 0.06519687995314598, LR: 0.010000000000000002
Time, 2019-01-01T22:47:02, Epoch: 36, Batch: 320, Training Loss: 0.063737079128623, LR: 0.010000000000000002
Time, 2019-01-01T22:47:03, Epoch: 36, Batch: 330, Training Loss: 0.0684329092502594, LR: 0.010000000000000002
Time, 2019-01-01T22:47:04, Epoch: 36, Batch: 340, Training Loss: 0.04981626085937023, LR: 0.010000000000000002
Time, 2019-01-01T22:47:05, Epoch: 36, Batch: 350, Training Loss: 0.03231653049588203, LR: 0.010000000000000002
Time, 2019-01-01T22:47:05, Epoch: 36, Batch: 360, Training Loss: 0.058219240978360176, LR: 0.010000000000000002
Time, 2019-01-01T22:47:06, Epoch: 36, Batch: 370, Training Loss: 0.057897507399320605, LR: 0.010000000000000002
Time, 2019-01-01T22:47:07, Epoch: 36, Batch: 380, Training Loss: 0.04180256687104702, LR: 0.010000000000000002
Time, 2019-01-01T22:47:08, Epoch: 36, Batch: 390, Training Loss: 0.05394492149353027, LR: 0.010000000000000002
Time, 2019-01-01T22:47:09, Epoch: 36, Batch: 400, Training Loss: 0.06030800975859165, LR: 0.010000000000000002
Time, 2019-01-01T22:47:09, Epoch: 36, Batch: 410, Training Loss: 0.04012877345085144, LR: 0.010000000000000002
Time, 2019-01-01T22:47:10, Epoch: 36, Batch: 420, Training Loss: 0.05166126117110252, LR: 0.010000000000000002
Time, 2019-01-01T22:47:11, Epoch: 36, Batch: 430, Training Loss: 0.04696061983704567, LR: 0.010000000000000002
Time, 2019-01-01T22:47:12, Epoch: 36, Batch: 440, Training Loss: 0.05070222988724708, LR: 0.010000000000000002
Time, 2019-01-01T22:47:13, Epoch: 36, Batch: 450, Training Loss: 0.060618865117430684, LR: 0.010000000000000002
Time, 2019-01-01T22:47:14, Epoch: 36, Batch: 460, Training Loss: 0.04058099910616875, LR: 0.010000000000000002
Time, 2019-01-01T22:47:15, Epoch: 36, Batch: 470, Training Loss: 0.05558917857706547, LR: 0.010000000000000002
Time, 2019-01-01T22:47:16, Epoch: 36, Batch: 480, Training Loss: 0.05154753513634205, LR: 0.010000000000000002
Time, 2019-01-01T22:47:17, Epoch: 36, Batch: 490, Training Loss: 0.05728437080979347, LR: 0.010000000000000002
Time, 2019-01-01T22:47:18, Epoch: 36, Batch: 500, Training Loss: 0.06891994252800941, LR: 0.010000000000000002
Time, 2019-01-01T22:47:19, Epoch: 36, Batch: 510, Training Loss: 0.035812470316886905, LR: 0.010000000000000002
Time, 2019-01-01T22:47:20, Epoch: 36, Batch: 520, Training Loss: 0.057305017858743666, LR: 0.010000000000000002
Time, 2019-01-01T22:47:21, Epoch: 36, Batch: 530, Training Loss: 0.051478660106658934, LR: 0.010000000000000002
Time, 2019-01-01T22:47:22, Epoch: 36, Batch: 540, Training Loss: 0.0357216939330101, LR: 0.010000000000000002
Time, 2019-01-01T22:47:23, Epoch: 36, Batch: 550, Training Loss: 0.04342647269368172, LR: 0.010000000000000002
Time, 2019-01-01T22:47:24, Epoch: 36, Batch: 560, Training Loss: 0.03192331679165363, LR: 0.010000000000000002
Time, 2019-01-01T22:47:25, Epoch: 36, Batch: 570, Training Loss: 0.04701879471540451, LR: 0.010000000000000002
Time, 2019-01-01T22:47:26, Epoch: 36, Batch: 580, Training Loss: 0.035629214346408845, LR: 0.010000000000000002
Time, 2019-01-01T22:47:27, Epoch: 36, Batch: 590, Training Loss: 0.037348149716854094, LR: 0.010000000000000002
Time, 2019-01-01T22:47:27, Epoch: 36, Batch: 600, Training Loss: 0.058975710347294805, LR: 0.010000000000000002
Time, 2019-01-01T22:47:28, Epoch: 36, Batch: 610, Training Loss: 0.03717564940452576, LR: 0.010000000000000002
Time, 2019-01-01T22:47:29, Epoch: 36, Batch: 620, Training Loss: 0.029372377693653105, LR: 0.010000000000000002
Time, 2019-01-01T22:47:30, Epoch: 36, Batch: 630, Training Loss: 0.0655262365937233, LR: 0.010000000000000002
Time, 2019-01-01T22:47:31, Epoch: 36, Batch: 640, Training Loss: 0.03504765853285789, LR: 0.010000000000000002
Time, 2019-01-01T22:47:32, Epoch: 36, Batch: 650, Training Loss: 0.04770255163311958, LR: 0.010000000000000002
Time, 2019-01-01T22:47:32, Epoch: 36, Batch: 660, Training Loss: 0.05209563486278057, LR: 0.010000000000000002
Time, 2019-01-01T22:47:33, Epoch: 36, Batch: 670, Training Loss: 0.0740981262177229, LR: 0.010000000000000002
Time, 2019-01-01T22:47:34, Epoch: 36, Batch: 680, Training Loss: 0.06589830741286277, LR: 0.010000000000000002
Time, 2019-01-01T22:47:35, Epoch: 36, Batch: 690, Training Loss: 0.053790847212076186, LR: 0.010000000000000002
Time, 2019-01-01T22:47:36, Epoch: 36, Batch: 700, Training Loss: 0.05439271964132786, LR: 0.010000000000000002
Time, 2019-01-01T22:47:37, Epoch: 36, Batch: 710, Training Loss: 0.0651935238391161, LR: 0.010000000000000002
Time, 2019-01-01T22:47:37, Epoch: 36, Batch: 720, Training Loss: 0.05928460024297237, LR: 0.010000000000000002
Time, 2019-01-01T22:47:38, Epoch: 36, Batch: 730, Training Loss: 0.060403015464544296, LR: 0.010000000000000002
Time, 2019-01-01T22:47:39, Epoch: 36, Batch: 740, Training Loss: 0.040032995492219926, LR: 0.010000000000000002
Time, 2019-01-01T22:47:40, Epoch: 36, Batch: 750, Training Loss: 0.05242504999041557, LR: 0.010000000000000002
Time, 2019-01-01T22:47:41, Epoch: 36, Batch: 760, Training Loss: 0.027261538058519365, LR: 0.010000000000000002
Time, 2019-01-01T22:47:41, Epoch: 36, Batch: 770, Training Loss: 0.06082294471561909, LR: 0.010000000000000002
Time, 2019-01-01T22:47:42, Epoch: 36, Batch: 780, Training Loss: 0.05967566072940826, LR: 0.010000000000000002
Time, 2019-01-01T22:47:43, Epoch: 36, Batch: 790, Training Loss: 0.06996453143656253, LR: 0.010000000000000002
Time, 2019-01-01T22:47:44, Epoch: 36, Batch: 800, Training Loss: 0.04121448397636414, LR: 0.010000000000000002
Time, 2019-01-01T22:47:45, Epoch: 36, Batch: 810, Training Loss: 0.04560493789613247, LR: 0.010000000000000002
Time, 2019-01-01T22:47:45, Epoch: 36, Batch: 820, Training Loss: 0.04444282203912735, LR: 0.010000000000000002
Time, 2019-01-01T22:47:46, Epoch: 36, Batch: 830, Training Loss: 0.05318514779210091, LR: 0.010000000000000002
Time, 2019-01-01T22:47:47, Epoch: 36, Batch: 840, Training Loss: 0.03788424879312515, LR: 0.010000000000000002
Time, 2019-01-01T22:47:48, Epoch: 36, Batch: 850, Training Loss: 0.05800276696681976, LR: 0.010000000000000002
Time, 2019-01-01T22:47:48, Epoch: 36, Batch: 860, Training Loss: 0.05604081377387047, LR: 0.010000000000000002
Time, 2019-01-01T22:47:49, Epoch: 36, Batch: 870, Training Loss: 0.05664866641163826, LR: 0.010000000000000002
Time, 2019-01-01T22:47:50, Epoch: 36, Batch: 880, Training Loss: 0.04606139287352562, LR: 0.010000000000000002
Time, 2019-01-01T22:47:51, Epoch: 36, Batch: 890, Training Loss: 0.05355421453714371, LR: 0.010000000000000002
Time, 2019-01-01T22:47:52, Epoch: 36, Batch: 900, Training Loss: 0.03663289211690426, LR: 0.010000000000000002
Time, 2019-01-01T22:47:52, Epoch: 36, Batch: 910, Training Loss: 0.051698746532201766, LR: 0.010000000000000002
Time, 2019-01-01T22:47:53, Epoch: 36, Batch: 920, Training Loss: 0.04351213201880455, LR: 0.010000000000000002
Time, 2019-01-01T22:47:54, Epoch: 36, Batch: 930, Training Loss: 0.05571735575795174, LR: 0.010000000000000002
Epoch: 36, Validation Top 1 acc: 98.45740509033203
Epoch: 36, Validation Top 5 acc: 99.99005126953125
Epoch: 36, Validation Set Loss: 0.05336669087409973
Start training epoch 37
Time, 2019-01-01T22:48:01, Epoch: 37, Batch: 10, Training Loss: 0.04940028451383114, LR: 0.010000000000000002
Time, 2019-01-01T22:48:01, Epoch: 37, Batch: 20, Training Loss: 0.05110007636249066, LR: 0.010000000000000002
Time, 2019-01-01T22:48:02, Epoch: 37, Batch: 30, Training Loss: 0.08059267178177834, LR: 0.010000000000000002
Time, 2019-01-01T22:48:03, Epoch: 37, Batch: 40, Training Loss: 0.03694064579904079, LR: 0.010000000000000002
Time, 2019-01-01T22:48:04, Epoch: 37, Batch: 50, Training Loss: 0.05392133146524429, LR: 0.010000000000000002
Time, 2019-01-01T22:48:05, Epoch: 37, Batch: 60, Training Loss: 0.035896758735179904, LR: 0.010000000000000002
Time, 2019-01-01T22:48:05, Epoch: 37, Batch: 70, Training Loss: 0.07223803177475929, LR: 0.010000000000000002
Time, 2019-01-01T22:48:06, Epoch: 37, Batch: 80, Training Loss: 0.06337492987513542, LR: 0.010000000000000002
Time, 2019-01-01T22:48:07, Epoch: 37, Batch: 90, Training Loss: 0.053435125201940534, LR: 0.010000000000000002
Time, 2019-01-01T22:48:08, Epoch: 37, Batch: 100, Training Loss: 0.04122045300900936, LR: 0.010000000000000002
Time, 2019-01-01T22:48:09, Epoch: 37, Batch: 110, Training Loss: 0.05161837302148342, LR: 0.010000000000000002
Time, 2019-01-01T22:48:10, Epoch: 37, Batch: 120, Training Loss: 0.07114649266004562, LR: 0.010000000000000002
Time, 2019-01-01T22:48:11, Epoch: 37, Batch: 130, Training Loss: 0.046702992916107175, LR: 0.010000000000000002
Time, 2019-01-01T22:48:12, Epoch: 37, Batch: 140, Training Loss: 0.05645859688520431, LR: 0.010000000000000002
Time, 2019-01-01T22:48:12, Epoch: 37, Batch: 150, Training Loss: 0.046164200827479365, LR: 0.010000000000000002
Time, 2019-01-01T22:48:13, Epoch: 37, Batch: 160, Training Loss: 0.05655856765806675, LR: 0.010000000000000002
Time, 2019-01-01T22:48:14, Epoch: 37, Batch: 170, Training Loss: 0.056194431707262994, LR: 0.010000000000000002
Time, 2019-01-01T22:48:15, Epoch: 37, Batch: 180, Training Loss: 0.06843834593892098, LR: 0.010000000000000002
Time, 2019-01-01T22:48:16, Epoch: 37, Batch: 190, Training Loss: 0.06063679829239845, LR: 0.010000000000000002
Time, 2019-01-01T22:48:16, Epoch: 37, Batch: 200, Training Loss: 0.0758853442966938, LR: 0.010000000000000002
Time, 2019-01-01T22:48:17, Epoch: 37, Batch: 210, Training Loss: 0.055855126678943635, LR: 0.010000000000000002
Time, 2019-01-01T22:48:18, Epoch: 37, Batch: 220, Training Loss: 0.042604034394025804, LR: 0.010000000000000002
Time, 2019-01-01T22:48:19, Epoch: 37, Batch: 230, Training Loss: 0.05114966928958893, LR: 0.010000000000000002
Time, 2019-01-01T22:48:20, Epoch: 37, Batch: 240, Training Loss: 0.05211419574916363, LR: 0.010000000000000002
Time, 2019-01-01T22:48:20, Epoch: 37, Batch: 250, Training Loss: 0.04004943482577801, LR: 0.010000000000000002
Time, 2019-01-01T22:48:21, Epoch: 37, Batch: 260, Training Loss: 0.0691649667918682, LR: 0.010000000000000002
Time, 2019-01-01T22:48:22, Epoch: 37, Batch: 270, Training Loss: 0.048216086253523825, LR: 0.010000000000000002
Time, 2019-01-01T22:48:23, Epoch: 37, Batch: 280, Training Loss: 0.039803390577435496, LR: 0.010000000000000002
Time, 2019-01-01T22:48:24, Epoch: 37, Batch: 290, Training Loss: 0.047021569311618806, LR: 0.010000000000000002
Time, 2019-01-01T22:48:24, Epoch: 37, Batch: 300, Training Loss: 0.046482385322451594, LR: 0.010000000000000002
Time, 2019-01-01T22:48:25, Epoch: 37, Batch: 310, Training Loss: 0.060432561486959455, LR: 0.010000000000000002
Time, 2019-01-01T22:48:26, Epoch: 37, Batch: 320, Training Loss: 0.04750848896801472, LR: 0.010000000000000002
Time, 2019-01-01T22:48:27, Epoch: 37, Batch: 330, Training Loss: 0.05628843978047371, LR: 0.010000000000000002
Time, 2019-01-01T22:48:27, Epoch: 37, Batch: 340, Training Loss: 0.07199894227087497, LR: 0.010000000000000002
Time, 2019-01-01T22:48:28, Epoch: 37, Batch: 350, Training Loss: 0.06477339267730713, LR: 0.010000000000000002
Time, 2019-01-01T22:48:29, Epoch: 37, Batch: 360, Training Loss: 0.03425761759281158, LR: 0.010000000000000002
Time, 2019-01-01T22:48:30, Epoch: 37, Batch: 370, Training Loss: 0.04259568527340889, LR: 0.010000000000000002
Time, 2019-01-01T22:48:31, Epoch: 37, Batch: 380, Training Loss: 0.06807857528328895, LR: 0.010000000000000002
Time, 2019-01-01T22:48:31, Epoch: 37, Batch: 390, Training Loss: 0.03922731839120388, LR: 0.010000000000000002
Time, 2019-01-01T22:48:32, Epoch: 37, Batch: 400, Training Loss: 0.055357464402914044, LR: 0.010000000000000002
Time, 2019-01-01T22:48:33, Epoch: 37, Batch: 410, Training Loss: 0.058803006634116174, LR: 0.010000000000000002
Time, 2019-01-01T22:48:34, Epoch: 37, Batch: 420, Training Loss: 0.05480566248297691, LR: 0.010000000000000002
Time, 2019-01-01T22:48:35, Epoch: 37, Batch: 430, Training Loss: 0.05820061415433884, LR: 0.010000000000000002
Time, 2019-01-01T22:48:36, Epoch: 37, Batch: 440, Training Loss: 0.06632259041070938, LR: 0.010000000000000002
Time, 2019-01-01T22:48:37, Epoch: 37, Batch: 450, Training Loss: 0.03964824229478836, LR: 0.010000000000000002
Time, 2019-01-01T22:48:37, Epoch: 37, Batch: 460, Training Loss: 0.05470799207687378, LR: 0.010000000000000002
Time, 2019-01-01T22:48:38, Epoch: 37, Batch: 470, Training Loss: 0.08042761459946632, LR: 0.010000000000000002
Time, 2019-01-01T22:48:39, Epoch: 37, Batch: 480, Training Loss: 0.06509986668825149, LR: 0.010000000000000002
Time, 2019-01-01T22:48:40, Epoch: 37, Batch: 490, Training Loss: 0.045373450592160226, LR: 0.010000000000000002
Time, 2019-01-01T22:48:41, Epoch: 37, Batch: 500, Training Loss: 0.06966048292815685, LR: 0.010000000000000002
Time, 2019-01-01T22:48:42, Epoch: 37, Batch: 510, Training Loss: 0.05540076158940792, LR: 0.010000000000000002
Time, 2019-01-01T22:48:42, Epoch: 37, Batch: 520, Training Loss: 0.05730708204209804, LR: 0.010000000000000002
Time, 2019-01-01T22:48:43, Epoch: 37, Batch: 530, Training Loss: 0.03329209238290787, LR: 0.010000000000000002
Time, 2019-01-01T22:48:44, Epoch: 37, Batch: 540, Training Loss: 0.041792653128504755, LR: 0.010000000000000002
Time, 2019-01-01T22:48:45, Epoch: 37, Batch: 550, Training Loss: 0.07070523500442505, LR: 0.010000000000000002
Time, 2019-01-01T22:48:45, Epoch: 37, Batch: 560, Training Loss: 0.04027408324182034, LR: 0.010000000000000002
Time, 2019-01-01T22:48:46, Epoch: 37, Batch: 570, Training Loss: 0.03991949111223221, LR: 0.010000000000000002
Time, 2019-01-01T22:48:47, Epoch: 37, Batch: 580, Training Loss: 0.0712286714464426, LR: 0.010000000000000002
Time, 2019-01-01T22:48:48, Epoch: 37, Batch: 590, Training Loss: 0.048469866439700124, LR: 0.010000000000000002
Time, 2019-01-01T22:48:49, Epoch: 37, Batch: 600, Training Loss: 0.07188176587224007, LR: 0.010000000000000002
Time, 2019-01-01T22:48:49, Epoch: 37, Batch: 610, Training Loss: 0.06474021822214127, LR: 0.010000000000000002
Time, 2019-01-01T22:48:50, Epoch: 37, Batch: 620, Training Loss: 0.04215220920741558, LR: 0.010000000000000002
Time, 2019-01-01T22:48:51, Epoch: 37, Batch: 630, Training Loss: 0.050579913333058356, LR: 0.010000000000000002
Time, 2019-01-01T22:48:52, Epoch: 37, Batch: 640, Training Loss: 0.053016170114278796, LR: 0.010000000000000002
Time, 2019-01-01T22:48:52, Epoch: 37, Batch: 650, Training Loss: 0.05422002151608467, LR: 0.010000000000000002
Time, 2019-01-01T22:48:53, Epoch: 37, Batch: 660, Training Loss: 0.04903692975640297, LR: 0.010000000000000002
Time, 2019-01-01T22:48:54, Epoch: 37, Batch: 670, Training Loss: 0.04797356054186821, LR: 0.010000000000000002
Time, 2019-01-01T22:48:55, Epoch: 37, Batch: 680, Training Loss: 0.031611443310976026, LR: 0.010000000000000002
Time, 2019-01-01T22:48:55, Epoch: 37, Batch: 690, Training Loss: 0.046832235902547835, LR: 0.010000000000000002
Time, 2019-01-01T22:48:56, Epoch: 37, Batch: 700, Training Loss: 0.049283887073397634, LR: 0.010000000000000002
Time, 2019-01-01T22:48:57, Epoch: 37, Batch: 710, Training Loss: 0.061555242910981176, LR: 0.010000000000000002
Time, 2019-01-01T22:48:58, Epoch: 37, Batch: 720, Training Loss: 0.06093908064067364, LR: 0.010000000000000002
Time, 2019-01-01T22:48:58, Epoch: 37, Batch: 730, Training Loss: 0.062289484590291974, LR: 0.010000000000000002
Time, 2019-01-01T22:48:59, Epoch: 37, Batch: 740, Training Loss: 0.04535093046724796, LR: 0.010000000000000002
Time, 2019-01-01T22:49:00, Epoch: 37, Batch: 750, Training Loss: 0.05124755837023258, LR: 0.010000000000000002
Time, 2019-01-01T22:49:01, Epoch: 37, Batch: 760, Training Loss: 0.043658099323511126, LR: 0.010000000000000002
Time, 2019-01-01T22:49:02, Epoch: 37, Batch: 770, Training Loss: 0.0550258569419384, LR: 0.010000000000000002
Time, 2019-01-01T22:49:02, Epoch: 37, Batch: 780, Training Loss: 0.0494560532271862, LR: 0.010000000000000002
Time, 2019-01-01T22:49:03, Epoch: 37, Batch: 790, Training Loss: 0.0587005153298378, LR: 0.010000000000000002
Time, 2019-01-01T22:49:04, Epoch: 37, Batch: 800, Training Loss: 0.04316605925559998, LR: 0.010000000000000002
Time, 2019-01-01T22:49:05, Epoch: 37, Batch: 810, Training Loss: 0.052233627438545226, LR: 0.010000000000000002
Time, 2019-01-01T22:49:05, Epoch: 37, Batch: 820, Training Loss: 0.03572384789586067, LR: 0.010000000000000002
Time, 2019-01-01T22:49:06, Epoch: 37, Batch: 830, Training Loss: 0.04052439518272877, LR: 0.010000000000000002
Time, 2019-01-01T22:49:07, Epoch: 37, Batch: 840, Training Loss: 0.03127811923623085, LR: 0.010000000000000002
Time, 2019-01-01T22:49:08, Epoch: 37, Batch: 850, Training Loss: 0.04354925006628037, LR: 0.010000000000000002
Time, 2019-01-01T22:49:08, Epoch: 37, Batch: 860, Training Loss: 0.03513364195823669, LR: 0.010000000000000002
Time, 2019-01-01T22:49:09, Epoch: 37, Batch: 870, Training Loss: 0.03819524459540844, LR: 0.010000000000000002
Time, 2019-01-01T22:49:10, Epoch: 37, Batch: 880, Training Loss: 0.05470407344400883, LR: 0.010000000000000002
Time, 2019-01-01T22:49:11, Epoch: 37, Batch: 890, Training Loss: 0.04576456248760223, LR: 0.010000000000000002
Time, 2019-01-01T22:49:11, Epoch: 37, Batch: 900, Training Loss: 0.0437775231897831, LR: 0.010000000000000002
Time, 2019-01-01T22:49:12, Epoch: 37, Batch: 910, Training Loss: 0.05410962961614132, LR: 0.010000000000000002
Time, 2019-01-01T22:49:13, Epoch: 37, Batch: 920, Training Loss: 0.05192643478512764, LR: 0.010000000000000002
Time, 2019-01-01T22:49:14, Epoch: 37, Batch: 930, Training Loss: 0.058232901617884636, LR: 0.010000000000000002
Epoch: 37, Validation Top 1 acc: 98.19864654541016
Epoch: 37, Validation Top 5 acc: 100.0
Epoch: 37, Validation Set Loss: 0.05567744001746178
Start training epoch 38
Time, 2019-01-01T22:49:20, Epoch: 38, Batch: 10, Training Loss: 0.059175359830260274, LR: 0.010000000000000002
Time, 2019-01-01T22:49:21, Epoch: 38, Batch: 20, Training Loss: 0.05439007505774498, LR: 0.010000000000000002
Time, 2019-01-01T22:49:22, Epoch: 38, Batch: 30, Training Loss: 0.047993230447173116, LR: 0.010000000000000002
Time, 2019-01-01T22:49:23, Epoch: 38, Batch: 40, Training Loss: 0.045348352193832396, LR: 0.010000000000000002
Time, 2019-01-01T22:49:24, Epoch: 38, Batch: 50, Training Loss: 0.04165562614798546, LR: 0.010000000000000002
Time, 2019-01-01T22:49:25, Epoch: 38, Batch: 60, Training Loss: 0.03850103765726089, LR: 0.010000000000000002
Time, 2019-01-01T22:49:26, Epoch: 38, Batch: 70, Training Loss: 0.04075546152889729, LR: 0.010000000000000002
Time, 2019-01-01T22:49:27, Epoch: 38, Batch: 80, Training Loss: 0.03302387706935406, LR: 0.010000000000000002
Time, 2019-01-01T22:49:28, Epoch: 38, Batch: 90, Training Loss: 0.038548418879508974, LR: 0.010000000000000002
Time, 2019-01-01T22:49:28, Epoch: 38, Batch: 100, Training Loss: 0.06409094892442227, LR: 0.010000000000000002
Time, 2019-01-01T22:49:29, Epoch: 38, Batch: 110, Training Loss: 0.06992806866765022, LR: 0.010000000000000002
Time, 2019-01-01T22:49:30, Epoch: 38, Batch: 120, Training Loss: 0.044362864643335345, LR: 0.010000000000000002
Time, 2019-01-01T22:49:31, Epoch: 38, Batch: 130, Training Loss: 0.0388232734054327, LR: 0.010000000000000002
Time, 2019-01-01T22:49:32, Epoch: 38, Batch: 140, Training Loss: 0.047236958518624306, LR: 0.010000000000000002
Time, 2019-01-01T22:49:33, Epoch: 38, Batch: 150, Training Loss: 0.05156137645244598, LR: 0.010000000000000002
Time, 2019-01-01T22:49:34, Epoch: 38, Batch: 160, Training Loss: 0.05308095626533031, LR: 0.010000000000000002
Time, 2019-01-01T22:49:34, Epoch: 38, Batch: 170, Training Loss: 0.04688101708889007, LR: 0.010000000000000002
Time, 2019-01-01T22:49:35, Epoch: 38, Batch: 180, Training Loss: 0.03317151442170143, LR: 0.010000000000000002
Time, 2019-01-01T22:49:36, Epoch: 38, Batch: 190, Training Loss: 0.046982820332050326, LR: 0.010000000000000002
Time, 2019-01-01T22:49:37, Epoch: 38, Batch: 200, Training Loss: 0.05585445836186409, LR: 0.010000000000000002
Time, 2019-01-01T22:49:38, Epoch: 38, Batch: 210, Training Loss: 0.06393407061696052, LR: 0.010000000000000002
Time, 2019-01-01T22:49:39, Epoch: 38, Batch: 220, Training Loss: 0.04284146428108215, LR: 0.010000000000000002
Time, 2019-01-01T22:49:39, Epoch: 38, Batch: 230, Training Loss: 0.027114469185471536, LR: 0.010000000000000002
Time, 2019-01-01T22:49:40, Epoch: 38, Batch: 240, Training Loss: 0.041953802853822705, LR: 0.010000000000000002
Time, 2019-01-01T22:49:41, Epoch: 38, Batch: 250, Training Loss: 0.04313962757587433, LR: 0.010000000000000002
Time, 2019-01-01T22:49:42, Epoch: 38, Batch: 260, Training Loss: 0.060130995512008664, LR: 0.010000000000000002
Time, 2019-01-01T22:49:43, Epoch: 38, Batch: 270, Training Loss: 0.03684878423810005, LR: 0.010000000000000002
Time, 2019-01-01T22:49:44, Epoch: 38, Batch: 280, Training Loss: 0.060302609577775, LR: 0.010000000000000002
Time, 2019-01-01T22:49:44, Epoch: 38, Batch: 290, Training Loss: 0.030524594709277153, LR: 0.010000000000000002
Time, 2019-01-01T22:49:45, Epoch: 38, Batch: 300, Training Loss: 0.04153263866901398, LR: 0.010000000000000002
Time, 2019-01-01T22:49:46, Epoch: 38, Batch: 310, Training Loss: 0.055169084668159486, LR: 0.010000000000000002
Time, 2019-01-01T22:49:47, Epoch: 38, Batch: 320, Training Loss: 0.05378977507352829, LR: 0.010000000000000002
Time, 2019-01-01T22:49:48, Epoch: 38, Batch: 330, Training Loss: 0.04389970377087593, LR: 0.010000000000000002
Time, 2019-01-01T22:49:48, Epoch: 38, Batch: 340, Training Loss: 0.06007673963904381, LR: 0.010000000000000002
Time, 2019-01-01T22:49:49, Epoch: 38, Batch: 350, Training Loss: 0.04983378872275353, LR: 0.010000000000000002
Time, 2019-01-01T22:49:50, Epoch: 38, Batch: 360, Training Loss: 0.05417599380016327, LR: 0.010000000000000002
Time, 2019-01-01T22:49:51, Epoch: 38, Batch: 370, Training Loss: 0.048633887246251103, LR: 0.010000000000000002
Time, 2019-01-01T22:49:51, Epoch: 38, Batch: 380, Training Loss: 0.04454934000968933, LR: 0.010000000000000002
Time, 2019-01-01T22:49:52, Epoch: 38, Batch: 390, Training Loss: 0.06119096651673317, LR: 0.010000000000000002
Time, 2019-01-01T22:49:53, Epoch: 38, Batch: 400, Training Loss: 0.0569329559803009, LR: 0.010000000000000002
Time, 2019-01-01T22:49:54, Epoch: 38, Batch: 410, Training Loss: 0.05295096263289452, LR: 0.010000000000000002
Time, 2019-01-01T22:49:55, Epoch: 38, Batch: 420, Training Loss: 0.07124434746801853, LR: 0.010000000000000002
Time, 2019-01-01T22:49:55, Epoch: 38, Batch: 430, Training Loss: 0.04895412996411323, LR: 0.010000000000000002
Time, 2019-01-01T22:49:56, Epoch: 38, Batch: 440, Training Loss: 0.04598970636725426, LR: 0.010000000000000002
Time, 2019-01-01T22:49:57, Epoch: 38, Batch: 450, Training Loss: 0.03249205201864243, LR: 0.010000000000000002
Time, 2019-01-01T22:49:58, Epoch: 38, Batch: 460, Training Loss: 0.04956198781728745, LR: 0.010000000000000002
Time, 2019-01-01T22:49:59, Epoch: 38, Batch: 470, Training Loss: 0.05279455408453941, LR: 0.010000000000000002
Time, 2019-01-01T22:49:59, Epoch: 38, Batch: 480, Training Loss: 0.059938213974237445, LR: 0.010000000000000002
Time, 2019-01-01T22:50:00, Epoch: 38, Batch: 490, Training Loss: 0.03471943810582161, LR: 0.010000000000000002
Time, 2019-01-01T22:50:01, Epoch: 38, Batch: 500, Training Loss: 0.06066832914948463, LR: 0.010000000000000002
Time, 2019-01-01T22:50:02, Epoch: 38, Batch: 510, Training Loss: 0.04436796009540558, LR: 0.010000000000000002
Time, 2019-01-01T22:50:02, Epoch: 38, Batch: 520, Training Loss: 0.049228445068001746, LR: 0.010000000000000002
Time, 2019-01-01T22:50:03, Epoch: 38, Batch: 530, Training Loss: 0.03757434748113155, LR: 0.010000000000000002
Time, 2019-01-01T22:50:04, Epoch: 38, Batch: 540, Training Loss: 0.04845350310206413, LR: 0.010000000000000002
Time, 2019-01-01T22:50:05, Epoch: 38, Batch: 550, Training Loss: 0.054597148299217226, LR: 0.010000000000000002
Time, 2019-01-01T22:50:05, Epoch: 38, Batch: 560, Training Loss: 0.045615588501095775, LR: 0.010000000000000002
Time, 2019-01-01T22:50:06, Epoch: 38, Batch: 570, Training Loss: 0.0404626090079546, LR: 0.010000000000000002
Time, 2019-01-01T22:50:07, Epoch: 38, Batch: 580, Training Loss: 0.03422486335039139, LR: 0.010000000000000002
Time, 2019-01-01T22:50:08, Epoch: 38, Batch: 590, Training Loss: 0.04007003642618656, LR: 0.010000000000000002
Time, 2019-01-01T22:50:09, Epoch: 38, Batch: 600, Training Loss: 0.043429940566420557, LR: 0.010000000000000002
Time, 2019-01-01T22:50:09, Epoch: 38, Batch: 610, Training Loss: 0.04264174290001392, LR: 0.010000000000000002
Time, 2019-01-01T22:50:10, Epoch: 38, Batch: 620, Training Loss: 0.08330093510448933, LR: 0.010000000000000002
Time, 2019-01-01T22:50:11, Epoch: 38, Batch: 630, Training Loss: 0.06765596941113472, LR: 0.010000000000000002
Time, 2019-01-01T22:50:12, Epoch: 38, Batch: 640, Training Loss: 0.03488149344921112, LR: 0.010000000000000002
Time, 2019-01-01T22:50:12, Epoch: 38, Batch: 650, Training Loss: 0.06270024068653583, LR: 0.010000000000000002
Time, 2019-01-01T22:50:13, Epoch: 38, Batch: 660, Training Loss: 0.042512234300374985, LR: 0.010000000000000002
Time, 2019-01-01T22:50:14, Epoch: 38, Batch: 670, Training Loss: 0.048186234384775165, LR: 0.010000000000000002
Time, 2019-01-01T22:50:15, Epoch: 38, Batch: 680, Training Loss: 0.06043836176395416, LR: 0.010000000000000002
Time, 2019-01-01T22:50:15, Epoch: 38, Batch: 690, Training Loss: 0.050014305114746097, LR: 0.010000000000000002
Time, 2019-01-01T22:50:16, Epoch: 38, Batch: 700, Training Loss: 0.04756500832736492, LR: 0.010000000000000002
Time, 2019-01-01T22:50:17, Epoch: 38, Batch: 710, Training Loss: 0.042840860411524774, LR: 0.010000000000000002
Time, 2019-01-01T22:50:18, Epoch: 38, Batch: 720, Training Loss: 0.06073702499270439, LR: 0.010000000000000002
Time, 2019-01-01T22:50:19, Epoch: 38, Batch: 730, Training Loss: 0.03298580534756183, LR: 0.010000000000000002
Time, 2019-01-01T22:50:19, Epoch: 38, Batch: 740, Training Loss: 0.05060274600982666, LR: 0.010000000000000002
Time, 2019-01-01T22:50:20, Epoch: 38, Batch: 750, Training Loss: 0.036473114788532254, LR: 0.010000000000000002
Time, 2019-01-01T22:50:21, Epoch: 38, Batch: 760, Training Loss: 0.076735457777977, LR: 0.010000000000000002
Time, 2019-01-01T22:50:22, Epoch: 38, Batch: 770, Training Loss: 0.03811383284628391, LR: 0.010000000000000002
Time, 2019-01-01T22:50:22, Epoch: 38, Batch: 780, Training Loss: 0.058471182361245155, LR: 0.010000000000000002
Time, 2019-01-01T22:50:23, Epoch: 38, Batch: 790, Training Loss: 0.06157951354980469, LR: 0.010000000000000002
Time, 2019-01-01T22:50:24, Epoch: 38, Batch: 800, Training Loss: 0.08104853108525276, LR: 0.010000000000000002
Time, 2019-01-01T22:50:25, Epoch: 38, Batch: 810, Training Loss: 0.053941229730844496, LR: 0.010000000000000002
Time, 2019-01-01T22:50:26, Epoch: 38, Batch: 820, Training Loss: 0.05817181169986725, LR: 0.010000000000000002
Time, 2019-01-01T22:50:26, Epoch: 38, Batch: 830, Training Loss: 0.06477938294410705, LR: 0.010000000000000002
Time, 2019-01-01T22:50:27, Epoch: 38, Batch: 840, Training Loss: 0.039311351627111434, LR: 0.010000000000000002
Time, 2019-01-01T22:50:28, Epoch: 38, Batch: 850, Training Loss: 0.06771755889058113, LR: 0.010000000000000002
Time, 2019-01-01T22:50:29, Epoch: 38, Batch: 860, Training Loss: 0.030150366574525835, LR: 0.010000000000000002
Time, 2019-01-01T22:50:29, Epoch: 38, Batch: 870, Training Loss: 0.06808796972036361, LR: 0.010000000000000002
Time, 2019-01-01T22:50:30, Epoch: 38, Batch: 880, Training Loss: 0.03246887996792793, LR: 0.010000000000000002
Time, 2019-01-01T22:50:31, Epoch: 38, Batch: 890, Training Loss: 0.08710886836051941, LR: 0.010000000000000002
Time, 2019-01-01T22:50:32, Epoch: 38, Batch: 900, Training Loss: 0.04400107115507126, LR: 0.010000000000000002
Time, 2019-01-01T22:50:32, Epoch: 38, Batch: 910, Training Loss: 0.04161902479827404, LR: 0.010000000000000002
Time, 2019-01-01T22:50:33, Epoch: 38, Batch: 920, Training Loss: 0.07592438943684102, LR: 0.010000000000000002
Time, 2019-01-01T22:50:34, Epoch: 38, Batch: 930, Training Loss: 0.03732904307544231, LR: 0.010000000000000002
Epoch: 38, Validation Top 1 acc: 98.3777847290039
Epoch: 38, Validation Top 5 acc: 99.99005126953125
Epoch: 38, Validation Set Loss: 0.049766696989536285
Start training epoch 39
Time, 2019-01-01T22:50:40, Epoch: 39, Batch: 10, Training Loss: 0.061686625331640245, LR: 0.010000000000000002
Time, 2019-01-01T22:50:41, Epoch: 39, Batch: 20, Training Loss: 0.05115709789097309, LR: 0.010000000000000002
Time, 2019-01-01T22:50:42, Epoch: 39, Batch: 30, Training Loss: 0.05433597192168236, LR: 0.010000000000000002
Time, 2019-01-01T22:50:43, Epoch: 39, Batch: 40, Training Loss: 0.046589213982224466, LR: 0.010000000000000002
Time, 2019-01-01T22:50:44, Epoch: 39, Batch: 50, Training Loss: 0.043367502838373186, LR: 0.010000000000000002
Time, 2019-01-01T22:50:44, Epoch: 39, Batch: 60, Training Loss: 0.04310612976551056, LR: 0.010000000000000002
Time, 2019-01-01T22:50:45, Epoch: 39, Batch: 70, Training Loss: 0.04787526652216911, LR: 0.010000000000000002
Time, 2019-01-01T22:50:46, Epoch: 39, Batch: 80, Training Loss: 0.06471618227660655, LR: 0.010000000000000002
Time, 2019-01-01T22:50:47, Epoch: 39, Batch: 90, Training Loss: 0.06007172092795372, LR: 0.010000000000000002
Time, 2019-01-01T22:50:47, Epoch: 39, Batch: 100, Training Loss: 0.05046158544719219, LR: 0.010000000000000002
Time, 2019-01-01T22:50:48, Epoch: 39, Batch: 110, Training Loss: 0.07511577978730202, LR: 0.010000000000000002
Time, 2019-01-01T22:50:49, Epoch: 39, Batch: 120, Training Loss: 0.04320264048874378, LR: 0.010000000000000002
Time, 2019-01-01T22:50:50, Epoch: 39, Batch: 130, Training Loss: 0.05907784029841423, LR: 0.010000000000000002
Time, 2019-01-01T22:50:50, Epoch: 39, Batch: 140, Training Loss: 0.04358127526938915, LR: 0.010000000000000002
Time, 2019-01-01T22:50:51, Epoch: 39, Batch: 150, Training Loss: 0.05505012013018131, LR: 0.010000000000000002
Time, 2019-01-01T22:50:52, Epoch: 39, Batch: 160, Training Loss: 0.04878889732062817, LR: 0.010000000000000002
Time, 2019-01-01T22:50:53, Epoch: 39, Batch: 170, Training Loss: 0.0383037380874157, LR: 0.010000000000000002
Time, 2019-01-01T22:50:53, Epoch: 39, Batch: 180, Training Loss: 0.04165535792708397, LR: 0.010000000000000002
Time, 2019-01-01T22:50:54, Epoch: 39, Batch: 190, Training Loss: 0.03882628306746483, LR: 0.010000000000000002
Time, 2019-01-01T22:50:55, Epoch: 39, Batch: 200, Training Loss: 0.037947824224829674, LR: 0.010000000000000002
Time, 2019-01-01T22:50:56, Epoch: 39, Batch: 210, Training Loss: 0.06148803159594536, LR: 0.010000000000000002
Time, 2019-01-01T22:50:56, Epoch: 39, Batch: 220, Training Loss: 0.0792405664920807, LR: 0.010000000000000002
Time, 2019-01-01T22:50:57, Epoch: 39, Batch: 230, Training Loss: 0.04822456240653992, LR: 0.010000000000000002
Time, 2019-01-01T22:50:58, Epoch: 39, Batch: 240, Training Loss: 0.05769130252301693, LR: 0.010000000000000002
Time, 2019-01-01T22:50:59, Epoch: 39, Batch: 250, Training Loss: 0.04815212227404118, LR: 0.010000000000000002
Time, 2019-01-01T22:50:59, Epoch: 39, Batch: 260, Training Loss: 0.08361940160393715, LR: 0.010000000000000002
Time, 2019-01-01T22:51:00, Epoch: 39, Batch: 270, Training Loss: 0.05158945024013519, LR: 0.010000000000000002
Time, 2019-01-01T22:51:01, Epoch: 39, Batch: 280, Training Loss: 0.06295523755252361, LR: 0.010000000000000002
Time, 2019-01-01T22:51:02, Epoch: 39, Batch: 290, Training Loss: 0.058310913667082784, LR: 0.010000000000000002
Time, 2019-01-01T22:51:02, Epoch: 39, Batch: 300, Training Loss: 0.0527246318757534, LR: 0.010000000000000002
Time, 2019-01-01T22:51:03, Epoch: 39, Batch: 310, Training Loss: 0.027512522414326668, LR: 0.010000000000000002
Time, 2019-01-01T22:51:04, Epoch: 39, Batch: 320, Training Loss: 0.07446302473545074, LR: 0.010000000000000002
Time, 2019-01-01T22:51:05, Epoch: 39, Batch: 330, Training Loss: 0.06680183857679367, LR: 0.010000000000000002
Time, 2019-01-01T22:51:06, Epoch: 39, Batch: 340, Training Loss: 0.03917692340910435, LR: 0.010000000000000002
Time, 2019-01-01T22:51:06, Epoch: 39, Batch: 350, Training Loss: 0.06728036664426326, LR: 0.010000000000000002
Time, 2019-01-01T22:51:07, Epoch: 39, Batch: 360, Training Loss: 0.03808067366480827, LR: 0.010000000000000002
Time, 2019-01-01T22:51:08, Epoch: 39, Batch: 370, Training Loss: 0.044907551258802414, LR: 0.010000000000000002
Time, 2019-01-01T22:51:09, Epoch: 39, Batch: 380, Training Loss: 0.042275092005729674, LR: 0.010000000000000002
Time, 2019-01-01T22:51:09, Epoch: 39, Batch: 390, Training Loss: 0.05504192002117634, LR: 0.010000000000000002
Time, 2019-01-01T22:51:10, Epoch: 39, Batch: 400, Training Loss: 0.05136498436331749, LR: 0.010000000000000002
Time, 2019-01-01T22:51:11, Epoch: 39, Batch: 410, Training Loss: 0.08460124880075455, LR: 0.010000000000000002
Time, 2019-01-01T22:51:12, Epoch: 39, Batch: 420, Training Loss: 0.06198652945458889, LR: 0.010000000000000002
Time, 2019-01-01T22:51:12, Epoch: 39, Batch: 430, Training Loss: 0.049297289177775386, LR: 0.010000000000000002
Time, 2019-01-01T22:51:13, Epoch: 39, Batch: 440, Training Loss: 0.04529653638601303, LR: 0.010000000000000002
Time, 2019-01-01T22:51:14, Epoch: 39, Batch: 450, Training Loss: 0.06882690526545047, LR: 0.010000000000000002
Time, 2019-01-01T22:51:15, Epoch: 39, Batch: 460, Training Loss: 0.04085295498371124, LR: 0.010000000000000002
Time, 2019-01-01T22:51:15, Epoch: 39, Batch: 470, Training Loss: 0.06484013795852661, LR: 0.010000000000000002
Time, 2019-01-01T22:51:16, Epoch: 39, Batch: 480, Training Loss: 0.06003344617784023, LR: 0.010000000000000002
Time, 2019-01-01T22:51:17, Epoch: 39, Batch: 490, Training Loss: 0.03785378858447075, LR: 0.010000000000000002
Time, 2019-01-01T22:51:18, Epoch: 39, Batch: 500, Training Loss: 0.06263183429837227, LR: 0.010000000000000002
Time, 2019-01-01T22:51:18, Epoch: 39, Batch: 510, Training Loss: 0.03397250920534134, LR: 0.010000000000000002
Time, 2019-01-01T22:51:19, Epoch: 39, Batch: 520, Training Loss: 0.05787585377693176, LR: 0.010000000000000002
Time, 2019-01-01T22:51:20, Epoch: 39, Batch: 530, Training Loss: 0.06642450019717216, LR: 0.010000000000000002
Time, 2019-01-01T22:51:21, Epoch: 39, Batch: 540, Training Loss: 0.044510668516159056, LR: 0.010000000000000002
Time, 2019-01-01T22:51:22, Epoch: 39, Batch: 550, Training Loss: 0.050925707817077635, LR: 0.010000000000000002
Time, 2019-01-01T22:51:23, Epoch: 39, Batch: 560, Training Loss: 0.062063901126384734, LR: 0.010000000000000002
Time, 2019-01-01T22:51:24, Epoch: 39, Batch: 570, Training Loss: 0.049080466851592064, LR: 0.010000000000000002
Time, 2019-01-01T22:51:25, Epoch: 39, Batch: 580, Training Loss: 0.056604232639074326, LR: 0.010000000000000002
Time, 2019-01-01T22:51:26, Epoch: 39, Batch: 590, Training Loss: 0.055248528718948364, LR: 0.010000000000000002
Time, 2019-01-01T22:51:26, Epoch: 39, Batch: 600, Training Loss: 0.04048902206122875, LR: 0.010000000000000002
Time, 2019-01-01T22:51:27, Epoch: 39, Batch: 610, Training Loss: 0.045708883181214334, LR: 0.010000000000000002
Time, 2019-01-01T22:51:28, Epoch: 39, Batch: 620, Training Loss: 0.029841528087854386, LR: 0.010000000000000002
Time, 2019-01-01T22:51:29, Epoch: 39, Batch: 630, Training Loss: 0.033243486657738686, LR: 0.010000000000000002
Time, 2019-01-01T22:51:30, Epoch: 39, Batch: 640, Training Loss: 0.04886422120034695, LR: 0.010000000000000002
Time, 2019-01-01T22:51:31, Epoch: 39, Batch: 650, Training Loss: 0.0448067594319582, LR: 0.010000000000000002
Time, 2019-01-01T22:51:31, Epoch: 39, Batch: 660, Training Loss: 0.04085473716259003, LR: 0.010000000000000002
Time, 2019-01-01T22:51:32, Epoch: 39, Batch: 670, Training Loss: 0.047069748491048814, LR: 0.010000000000000002
Time, 2019-01-01T22:51:33, Epoch: 39, Batch: 680, Training Loss: 0.039741355925798416, LR: 0.010000000000000002
Time, 2019-01-01T22:51:34, Epoch: 39, Batch: 690, Training Loss: 0.04546439424157143, LR: 0.010000000000000002
Time, 2019-01-01T22:51:36, Epoch: 39, Batch: 700, Training Loss: 0.049575204774737355, LR: 0.010000000000000002
Time, 2019-01-01T22:51:37, Epoch: 39, Batch: 710, Training Loss: 0.05286952778697014, LR: 0.010000000000000002
Time, 2019-01-01T22:51:38, Epoch: 39, Batch: 720, Training Loss: 0.04644165560603142, LR: 0.010000000000000002
Time, 2019-01-01T22:51:39, Epoch: 39, Batch: 730, Training Loss: 0.0399576935917139, LR: 0.010000000000000002
Time, 2019-01-01T22:51:40, Epoch: 39, Batch: 740, Training Loss: 0.043665891885757445, LR: 0.010000000000000002
Time, 2019-01-01T22:51:42, Epoch: 39, Batch: 750, Training Loss: 0.042737574130296704, LR: 0.010000000000000002
Time, 2019-01-01T22:51:43, Epoch: 39, Batch: 760, Training Loss: 0.05343462564051151, LR: 0.010000000000000002
Time, 2019-01-01T22:51:44, Epoch: 39, Batch: 770, Training Loss: 0.0725435197353363, LR: 0.010000000000000002
Time, 2019-01-01T22:51:45, Epoch: 39, Batch: 780, Training Loss: 0.0539711944758892, LR: 0.010000000000000002
Time, 2019-01-01T22:51:46, Epoch: 39, Batch: 790, Training Loss: 0.032522959262132646, LR: 0.010000000000000002
Time, 2019-01-01T22:51:47, Epoch: 39, Batch: 800, Training Loss: 0.0685932531952858, LR: 0.010000000000000002
Time, 2019-01-01T22:51:47, Epoch: 39, Batch: 810, Training Loss: 0.06257284730672837, LR: 0.010000000000000002
Time, 2019-01-01T22:51:48, Epoch: 39, Batch: 820, Training Loss: 0.053261926770210265, LR: 0.010000000000000002
Time, 2019-01-01T22:51:49, Epoch: 39, Batch: 830, Training Loss: 0.05448438860476017, LR: 0.010000000000000002
Time, 2019-01-01T22:51:50, Epoch: 39, Batch: 840, Training Loss: 0.05098188370466232, LR: 0.010000000000000002
Time, 2019-01-01T22:51:51, Epoch: 39, Batch: 850, Training Loss: 0.03583983257412911, LR: 0.010000000000000002
Time, 2019-01-01T22:51:51, Epoch: 39, Batch: 860, Training Loss: 0.06282689310610294, LR: 0.010000000000000002
Time, 2019-01-01T22:51:52, Epoch: 39, Batch: 870, Training Loss: 0.04773263782262802, LR: 0.010000000000000002
Time, 2019-01-01T22:51:53, Epoch: 39, Batch: 880, Training Loss: 0.060413869097828865, LR: 0.010000000000000002
Time, 2019-01-01T22:51:54, Epoch: 39, Batch: 890, Training Loss: 0.05231124758720398, LR: 0.010000000000000002
Time, 2019-01-01T22:51:55, Epoch: 39, Batch: 900, Training Loss: 0.0602891605347395, LR: 0.010000000000000002
Time, 2019-01-01T22:51:55, Epoch: 39, Batch: 910, Training Loss: 0.059363147243857384, LR: 0.010000000000000002
Time, 2019-01-01T22:51:56, Epoch: 39, Batch: 920, Training Loss: 0.0452009379863739, LR: 0.010000000000000002
Time, 2019-01-01T22:51:57, Epoch: 39, Batch: 930, Training Loss: 0.039520947635173796, LR: 0.010000000000000002
Epoch: 39, Validation Top 1 acc: 98.3280258178711
Epoch: 39, Validation Top 5 acc: 99.99005126953125
Epoch: 39, Validation Set Loss: 0.05326114222407341
Start training epoch 40
Time, 2019-01-01T22:52:03, Epoch: 40, Batch: 10, Training Loss: 0.046951497346162795, LR: 0.010000000000000002
Time, 2019-01-01T22:52:04, Epoch: 40, Batch: 20, Training Loss: 0.049346385151147844, LR: 0.010000000000000002
Time, 2019-01-01T22:52:05, Epoch: 40, Batch: 30, Training Loss: 0.06063649095594883, LR: 0.010000000000000002
Time, 2019-01-01T22:52:06, Epoch: 40, Batch: 40, Training Loss: 0.06206126250326634, LR: 0.010000000000000002
Time, 2019-01-01T22:52:06, Epoch: 40, Batch: 50, Training Loss: 0.043442470207810405, LR: 0.010000000000000002
Time, 2019-01-01T22:52:07, Epoch: 40, Batch: 60, Training Loss: 0.055866127088665965, LR: 0.010000000000000002
Time, 2019-01-01T22:52:08, Epoch: 40, Batch: 70, Training Loss: 0.04978083074092865, LR: 0.010000000000000002
Time, 2019-01-01T22:52:09, Epoch: 40, Batch: 80, Training Loss: 0.054043916240334514, LR: 0.010000000000000002
Time, 2019-01-01T22:52:09, Epoch: 40, Batch: 90, Training Loss: 0.047911965474486354, LR: 0.010000000000000002
Time, 2019-01-01T22:52:10, Epoch: 40, Batch: 100, Training Loss: 0.03758830279111862, LR: 0.010000000000000002
Time, 2019-01-01T22:52:11, Epoch: 40, Batch: 110, Training Loss: 0.05111490450799465, LR: 0.010000000000000002
Time, 2019-01-01T22:52:12, Epoch: 40, Batch: 120, Training Loss: 0.05585235357284546, LR: 0.010000000000000002
Time, 2019-01-01T22:52:12, Epoch: 40, Batch: 130, Training Loss: 0.05267217569053173, LR: 0.010000000000000002
Time, 2019-01-01T22:52:13, Epoch: 40, Batch: 140, Training Loss: 0.05837428942322731, LR: 0.010000000000000002
Time, 2019-01-01T22:52:14, Epoch: 40, Batch: 150, Training Loss: 0.03840021416544914, LR: 0.010000000000000002
Time, 2019-01-01T22:52:15, Epoch: 40, Batch: 160, Training Loss: 0.06472811549901962, LR: 0.010000000000000002
Time, 2019-01-01T22:52:15, Epoch: 40, Batch: 170, Training Loss: 0.05491189137101173, LR: 0.010000000000000002
Time, 2019-01-01T22:52:16, Epoch: 40, Batch: 180, Training Loss: 0.04878184087574482, LR: 0.010000000000000002
Time, 2019-01-01T22:52:17, Epoch: 40, Batch: 190, Training Loss: 0.043617236986756326, LR: 0.010000000000000002
Time, 2019-01-01T22:52:18, Epoch: 40, Batch: 200, Training Loss: 0.05298673138022423, LR: 0.010000000000000002
Time, 2019-01-01T22:52:19, Epoch: 40, Batch: 210, Training Loss: 0.07436270192265511, LR: 0.010000000000000002
Time, 2019-01-01T22:52:19, Epoch: 40, Batch: 220, Training Loss: 0.04333297535777092, LR: 0.010000000000000002
Time, 2019-01-01T22:52:20, Epoch: 40, Batch: 230, Training Loss: 0.031194385141134262, LR: 0.010000000000000002
Time, 2019-01-01T22:52:21, Epoch: 40, Batch: 240, Training Loss: 0.06103399097919464, LR: 0.010000000000000002
Time, 2019-01-01T22:52:22, Epoch: 40, Batch: 250, Training Loss: 0.037181201949715616, LR: 0.010000000000000002
Time, 2019-01-01T22:52:22, Epoch: 40, Batch: 260, Training Loss: 0.08999766781926155, LR: 0.010000000000000002
Time, 2019-01-01T22:52:23, Epoch: 40, Batch: 270, Training Loss: 0.07060203440487385, LR: 0.010000000000000002
Time, 2019-01-01T22:52:24, Epoch: 40, Batch: 280, Training Loss: 0.03975629769265652, LR: 0.010000000000000002
Time, 2019-01-01T22:52:25, Epoch: 40, Batch: 290, Training Loss: 0.034561491757631305, LR: 0.010000000000000002
Time, 2019-01-01T22:52:26, Epoch: 40, Batch: 300, Training Loss: 0.05507887341082096, LR: 0.010000000000000002
Time, 2019-01-01T22:52:27, Epoch: 40, Batch: 310, Training Loss: 0.059857485443353654, LR: 0.010000000000000002
Time, 2019-01-01T22:52:28, Epoch: 40, Batch: 320, Training Loss: 0.050896212458610535, LR: 0.010000000000000002
Time, 2019-01-01T22:52:29, Epoch: 40, Batch: 330, Training Loss: 0.03803093247115612, LR: 0.010000000000000002
Time, 2019-01-01T22:52:29, Epoch: 40, Batch: 340, Training Loss: 0.03965001553297043, LR: 0.010000000000000002
Time, 2019-01-01T22:52:30, Epoch: 40, Batch: 350, Training Loss: 0.03829836919903755, LR: 0.010000000000000002
Time, 2019-01-01T22:52:31, Epoch: 40, Batch: 360, Training Loss: 0.05765296667814255, LR: 0.010000000000000002
Time, 2019-01-01T22:52:32, Epoch: 40, Batch: 370, Training Loss: 0.046758301183581354, LR: 0.010000000000000002
Time, 2019-01-01T22:52:32, Epoch: 40, Batch: 380, Training Loss: 0.05755892619490623, LR: 0.010000000000000002
Time, 2019-01-01T22:52:33, Epoch: 40, Batch: 390, Training Loss: 0.056856078654527666, LR: 0.010000000000000002
Time, 2019-01-01T22:52:34, Epoch: 40, Batch: 400, Training Loss: 0.05805525369942188, LR: 0.010000000000000002
Time, 2019-01-01T22:52:35, Epoch: 40, Batch: 410, Training Loss: 0.05435849353671074, LR: 0.010000000000000002
Time, 2019-01-01T22:52:35, Epoch: 40, Batch: 420, Training Loss: 0.05348456837236881, LR: 0.010000000000000002
Time, 2019-01-01T22:52:36, Epoch: 40, Batch: 430, Training Loss: 0.06592235341668129, LR: 0.010000000000000002
Time, 2019-01-01T22:52:37, Epoch: 40, Batch: 440, Training Loss: 0.04282759502530098, LR: 0.010000000000000002
Time, 2019-01-01T22:52:38, Epoch: 40, Batch: 450, Training Loss: 0.04761900380253792, LR: 0.010000000000000002
Time, 2019-01-01T22:52:39, Epoch: 40, Batch: 460, Training Loss: 0.034490158781409264, LR: 0.010000000000000002
Time, 2019-01-01T22:52:39, Epoch: 40, Batch: 470, Training Loss: 0.05196109227836132, LR: 0.010000000000000002
Time, 2019-01-01T22:52:40, Epoch: 40, Batch: 480, Training Loss: 0.03257840387523174, LR: 0.010000000000000002
Time, 2019-01-01T22:52:41, Epoch: 40, Batch: 490, Training Loss: 0.049354609847068784, LR: 0.010000000000000002
Time, 2019-01-01T22:52:41, Epoch: 40, Batch: 500, Training Loss: 0.03534261099994183, LR: 0.010000000000000002
Time, 2019-01-01T22:52:42, Epoch: 40, Batch: 510, Training Loss: 0.07604410126805305, LR: 0.010000000000000002
Time, 2019-01-01T22:52:43, Epoch: 40, Batch: 520, Training Loss: 0.04496317692101002, LR: 0.010000000000000002
Time, 2019-01-01T22:52:44, Epoch: 40, Batch: 530, Training Loss: 0.05043258257210255, LR: 0.010000000000000002
Time, 2019-01-01T22:52:45, Epoch: 40, Batch: 540, Training Loss: 0.03475867956876755, LR: 0.010000000000000002
Time, 2019-01-01T22:52:45, Epoch: 40, Batch: 550, Training Loss: 0.04202637001872063, LR: 0.010000000000000002
Time, 2019-01-01T22:52:46, Epoch: 40, Batch: 560, Training Loss: 0.04159732721745968, LR: 0.010000000000000002
Time, 2019-01-01T22:52:47, Epoch: 40, Batch: 570, Training Loss: 0.05151931867003441, LR: 0.010000000000000002
Time, 2019-01-01T22:52:48, Epoch: 40, Batch: 580, Training Loss: 0.04276529215276241, LR: 0.010000000000000002
Time, 2019-01-01T22:52:48, Epoch: 40, Batch: 590, Training Loss: 0.06465336158871651, LR: 0.010000000000000002
Time, 2019-01-01T22:52:49, Epoch: 40, Batch: 600, Training Loss: 0.031913488358259204, LR: 0.010000000000000002
Time, 2019-01-01T22:52:50, Epoch: 40, Batch: 610, Training Loss: 0.06069540679454803, LR: 0.010000000000000002
Time, 2019-01-01T22:52:51, Epoch: 40, Batch: 620, Training Loss: 0.062426330521702766, LR: 0.010000000000000002
Time, 2019-01-01T22:52:51, Epoch: 40, Batch: 630, Training Loss: 0.037660516798496246, LR: 0.010000000000000002
Time, 2019-01-01T22:52:52, Epoch: 40, Batch: 640, Training Loss: 0.048594719171524046, LR: 0.010000000000000002
Time, 2019-01-01T22:52:53, Epoch: 40, Batch: 650, Training Loss: 0.07644941247999668, LR: 0.010000000000000002
Time, 2019-01-01T22:52:54, Epoch: 40, Batch: 660, Training Loss: 0.06664812751114368, LR: 0.010000000000000002
Time, 2019-01-01T22:52:54, Epoch: 40, Batch: 670, Training Loss: 0.054603736847639084, LR: 0.010000000000000002
Time, 2019-01-01T22:52:55, Epoch: 40, Batch: 680, Training Loss: 0.03680874928832054, LR: 0.010000000000000002
Time, 2019-01-01T22:52:56, Epoch: 40, Batch: 690, Training Loss: 0.046349947527050975, LR: 0.010000000000000002
Time, 2019-01-01T22:52:57, Epoch: 40, Batch: 700, Training Loss: 0.058382710069417955, LR: 0.010000000000000002
Time, 2019-01-01T22:52:57, Epoch: 40, Batch: 710, Training Loss: 0.05055061392486095, LR: 0.010000000000000002
Time, 2019-01-01T22:52:58, Epoch: 40, Batch: 720, Training Loss: 0.047176876291632655, LR: 0.010000000000000002
Time, 2019-01-01T22:52:59, Epoch: 40, Batch: 730, Training Loss: 0.043641646206378934, LR: 0.010000000000000002
Time, 2019-01-01T22:53:00, Epoch: 40, Batch: 740, Training Loss: 0.05302948318421841, LR: 0.010000000000000002
Time, 2019-01-01T22:53:00, Epoch: 40, Batch: 750, Training Loss: 0.04766169413924217, LR: 0.010000000000000002
Time, 2019-01-01T22:53:01, Epoch: 40, Batch: 760, Training Loss: 0.053964338079094884, LR: 0.010000000000000002
Time, 2019-01-01T22:53:02, Epoch: 40, Batch: 770, Training Loss: 0.0515969954431057, LR: 0.010000000000000002
Time, 2019-01-01T22:53:03, Epoch: 40, Batch: 780, Training Loss: 0.05671771429479122, LR: 0.010000000000000002
Time, 2019-01-01T22:53:03, Epoch: 40, Batch: 790, Training Loss: 0.040164948999881746, LR: 0.010000000000000002
Time, 2019-01-01T22:53:04, Epoch: 40, Batch: 800, Training Loss: 0.06019938252866268, LR: 0.010000000000000002
Time, 2019-01-01T22:53:05, Epoch: 40, Batch: 810, Training Loss: 0.05173083059489727, LR: 0.010000000000000002
Time, 2019-01-01T22:53:06, Epoch: 40, Batch: 820, Training Loss: 0.06026037223637104, LR: 0.010000000000000002
Time, 2019-01-01T22:53:06, Epoch: 40, Batch: 830, Training Loss: 0.05021263211965561, LR: 0.010000000000000002
Time, 2019-01-01T22:53:07, Epoch: 40, Batch: 840, Training Loss: 0.04281759969890118, LR: 0.010000000000000002
Time, 2019-01-01T22:53:08, Epoch: 40, Batch: 850, Training Loss: 0.03766970336437225, LR: 0.010000000000000002
Time, 2019-01-01T22:53:09, Epoch: 40, Batch: 860, Training Loss: 0.05571562051773071, LR: 0.010000000000000002
Time, 2019-01-01T22:53:09, Epoch: 40, Batch: 870, Training Loss: 0.03247361294925213, LR: 0.010000000000000002
Time, 2019-01-01T22:53:10, Epoch: 40, Batch: 880, Training Loss: 0.05267813205718994, LR: 0.010000000000000002
Time, 2019-01-01T22:53:11, Epoch: 40, Batch: 890, Training Loss: 0.057120722532272336, LR: 0.010000000000000002
Time, 2019-01-01T22:53:12, Epoch: 40, Batch: 900, Training Loss: 0.10040735527873039, LR: 0.010000000000000002
Time, 2019-01-01T22:53:12, Epoch: 40, Batch: 910, Training Loss: 0.055270463973283765, LR: 0.010000000000000002
Time, 2019-01-01T22:53:13, Epoch: 40, Batch: 920, Training Loss: 0.05269493125379086, LR: 0.010000000000000002
Time, 2019-01-01T22:53:14, Epoch: 40, Batch: 930, Training Loss: 0.033347851037979125, LR: 0.010000000000000002
Epoch: 40, Validation Top 1 acc: 98.52706909179688
Epoch: 40, Validation Top 5 acc: 100.0
Epoch: 40, Validation Set Loss: 0.04881991073489189
Start training epoch 41
Time, 2019-01-01T22:53:20, Epoch: 41, Batch: 10, Training Loss: 0.033050963655114174, LR: 0.010000000000000002
Time, 2019-01-01T22:53:21, Epoch: 41, Batch: 20, Training Loss: 0.03839479498565197, LR: 0.010000000000000002
Time, 2019-01-01T22:53:21, Epoch: 41, Batch: 30, Training Loss: 0.045240328460931775, LR: 0.010000000000000002
Time, 2019-01-01T22:53:22, Epoch: 41, Batch: 40, Training Loss: 0.0588312178850174, LR: 0.010000000000000002
Time, 2019-01-01T22:53:23, Epoch: 41, Batch: 50, Training Loss: 0.05196758173406124, LR: 0.010000000000000002
Time, 2019-01-01T22:53:24, Epoch: 41, Batch: 60, Training Loss: 0.0515724640339613, LR: 0.010000000000000002
Time, 2019-01-01T22:53:25, Epoch: 41, Batch: 70, Training Loss: 0.042579208686947825, LR: 0.010000000000000002
Time, 2019-01-01T22:53:25, Epoch: 41, Batch: 80, Training Loss: 0.060551319271326065, LR: 0.010000000000000002
Time, 2019-01-01T22:53:26, Epoch: 41, Batch: 90, Training Loss: 0.06536469273269177, LR: 0.010000000000000002
Time, 2019-01-01T22:53:27, Epoch: 41, Batch: 100, Training Loss: 0.08114519976079464, LR: 0.010000000000000002
Time, 2019-01-01T22:53:28, Epoch: 41, Batch: 110, Training Loss: 0.06190228872001171, LR: 0.010000000000000002
Time, 2019-01-01T22:53:28, Epoch: 41, Batch: 120, Training Loss: 0.05484592355787754, LR: 0.010000000000000002
Time, 2019-01-01T22:53:29, Epoch: 41, Batch: 130, Training Loss: 0.050542304664850234, LR: 0.010000000000000002
Time, 2019-01-01T22:53:30, Epoch: 41, Batch: 140, Training Loss: 0.04395991750061512, LR: 0.010000000000000002
Time, 2019-01-01T22:53:31, Epoch: 41, Batch: 150, Training Loss: 0.036133294180035594, LR: 0.010000000000000002
Time, 2019-01-01T22:53:32, Epoch: 41, Batch: 160, Training Loss: 0.07410618811845779, LR: 0.010000000000000002
Time, 2019-01-01T22:53:32, Epoch: 41, Batch: 170, Training Loss: 0.036064985767006875, LR: 0.010000000000000002
Time, 2019-01-01T22:53:33, Epoch: 41, Batch: 180, Training Loss: 0.06355133354663849, LR: 0.010000000000000002
Time, 2019-01-01T22:53:34, Epoch: 41, Batch: 190, Training Loss: 0.05243588984012604, LR: 0.010000000000000002
Time, 2019-01-01T22:53:35, Epoch: 41, Batch: 200, Training Loss: 0.04392741955816746, LR: 0.010000000000000002
Time, 2019-01-01T22:53:35, Epoch: 41, Batch: 210, Training Loss: 0.08334740549325943, LR: 0.010000000000000002
Time, 2019-01-01T22:53:36, Epoch: 41, Batch: 220, Training Loss: 0.04197562634944916, LR: 0.010000000000000002
Time, 2019-01-01T22:53:37, Epoch: 41, Batch: 230, Training Loss: 0.057812072709202765, LR: 0.010000000000000002
Time, 2019-01-01T22:53:38, Epoch: 41, Batch: 240, Training Loss: 0.03423566408455372, LR: 0.010000000000000002
Time, 2019-01-01T22:53:39, Epoch: 41, Batch: 250, Training Loss: 0.07142278514802455, LR: 0.010000000000000002
Time, 2019-01-01T22:53:39, Epoch: 41, Batch: 260, Training Loss: 0.04788588583469391, LR: 0.010000000000000002
Time, 2019-01-01T22:53:40, Epoch: 41, Batch: 270, Training Loss: 0.04771180152893066, LR: 0.010000000000000002
Time, 2019-01-01T22:53:41, Epoch: 41, Batch: 280, Training Loss: 0.04495983496308327, LR: 0.010000000000000002
Time, 2019-01-01T22:53:42, Epoch: 41, Batch: 290, Training Loss: 0.04560988172888756, LR: 0.010000000000000002
Time, 2019-01-01T22:53:43, Epoch: 41, Batch: 300, Training Loss: 0.06551283709704876, LR: 0.010000000000000002
Time, 2019-01-01T22:53:43, Epoch: 41, Batch: 310, Training Loss: 0.0501400001347065, LR: 0.010000000000000002
Time, 2019-01-01T22:53:44, Epoch: 41, Batch: 320, Training Loss: 0.05170498751103878, LR: 0.010000000000000002
Time, 2019-01-01T22:53:45, Epoch: 41, Batch: 330, Training Loss: 0.0395809255540371, LR: 0.010000000000000002
Time, 2019-01-01T22:53:46, Epoch: 41, Batch: 340, Training Loss: 0.04904564134776592, LR: 0.010000000000000002
Time, 2019-01-01T22:53:46, Epoch: 41, Batch: 350, Training Loss: 0.04726299867033958, LR: 0.010000000000000002
Time, 2019-01-01T22:53:47, Epoch: 41, Batch: 360, Training Loss: 0.04243162199854851, LR: 0.010000000000000002
Time, 2019-01-01T22:53:48, Epoch: 41, Batch: 370, Training Loss: 0.05352837555110455, LR: 0.010000000000000002
Time, 2019-01-01T22:53:49, Epoch: 41, Batch: 380, Training Loss: 0.03900430165231228, LR: 0.010000000000000002
Time, 2019-01-01T22:53:50, Epoch: 41, Batch: 390, Training Loss: 0.03917848728597164, LR: 0.010000000000000002
Time, 2019-01-01T22:53:50, Epoch: 41, Batch: 400, Training Loss: 0.03887026011943817, LR: 0.010000000000000002
Time, 2019-01-01T22:53:51, Epoch: 41, Batch: 410, Training Loss: 0.043057316169142726, LR: 0.010000000000000002
Time, 2019-01-01T22:53:52, Epoch: 41, Batch: 420, Training Loss: 0.07283859476447105, LR: 0.010000000000000002
Time, 2019-01-01T22:53:53, Epoch: 41, Batch: 430, Training Loss: 0.047819149866700175, LR: 0.010000000000000002
Time, 2019-01-01T22:53:54, Epoch: 41, Batch: 440, Training Loss: 0.06653800867497921, LR: 0.010000000000000002
Time, 2019-01-01T22:53:55, Epoch: 41, Batch: 450, Training Loss: 0.061930596083402636, LR: 0.010000000000000002
Time, 2019-01-01T22:53:55, Epoch: 41, Batch: 460, Training Loss: 0.05504051931202412, LR: 0.010000000000000002
Time, 2019-01-01T22:53:56, Epoch: 41, Batch: 470, Training Loss: 0.057186519354581834, LR: 0.010000000000000002
Time, 2019-01-01T22:53:57, Epoch: 41, Batch: 480, Training Loss: 0.04886506423354149, LR: 0.010000000000000002
Time, 2019-01-01T22:53:58, Epoch: 41, Batch: 490, Training Loss: 0.046789690107107165, LR: 0.010000000000000002
Time, 2019-01-01T22:53:58, Epoch: 41, Batch: 500, Training Loss: 0.03179004117846489, LR: 0.010000000000000002
Time, 2019-01-01T22:53:59, Epoch: 41, Batch: 510, Training Loss: 0.06603893861174584, LR: 0.010000000000000002
Time, 2019-01-01T22:54:00, Epoch: 41, Batch: 520, Training Loss: 0.045256137102842334, LR: 0.010000000000000002
Time, 2019-01-01T22:54:01, Epoch: 41, Batch: 530, Training Loss: 0.050547894462943076, LR: 0.010000000000000002
Time, 2019-01-01T22:54:01, Epoch: 41, Batch: 540, Training Loss: 0.04072852917015553, LR: 0.010000000000000002
Time, 2019-01-01T22:54:02, Epoch: 41, Batch: 550, Training Loss: 0.03910453394055367, LR: 0.010000000000000002
Time, 2019-01-01T22:54:03, Epoch: 41, Batch: 560, Training Loss: 0.02698197029531002, LR: 0.010000000000000002
Time, 2019-01-01T22:54:04, Epoch: 41, Batch: 570, Training Loss: 0.045306574553251266, LR: 0.010000000000000002
Time, 2019-01-01T22:54:05, Epoch: 41, Batch: 580, Training Loss: 0.06591230258345604, LR: 0.010000000000000002
Time, 2019-01-01T22:54:06, Epoch: 41, Batch: 590, Training Loss: 0.04735898971557617, LR: 0.010000000000000002
Time, 2019-01-01T22:54:06, Epoch: 41, Batch: 600, Training Loss: 0.05499102547764778, LR: 0.010000000000000002
Time, 2019-01-01T22:54:07, Epoch: 41, Batch: 610, Training Loss: 0.054377080500125886, LR: 0.010000000000000002
Time, 2019-01-01T22:54:08, Epoch: 41, Batch: 620, Training Loss: 0.0660193469375372, LR: 0.010000000000000002
Time, 2019-01-01T22:54:09, Epoch: 41, Batch: 630, Training Loss: 0.06002648808062076, LR: 0.010000000000000002
Time, 2019-01-01T22:54:10, Epoch: 41, Batch: 640, Training Loss: 0.06300435774028301, LR: 0.010000000000000002
Time, 2019-01-01T22:54:11, Epoch: 41, Batch: 650, Training Loss: 0.05080373659729957, LR: 0.010000000000000002
Time, 2019-01-01T22:54:12, Epoch: 41, Batch: 660, Training Loss: 0.055398262664675714, LR: 0.010000000000000002
Time, 2019-01-01T22:54:13, Epoch: 41, Batch: 670, Training Loss: 0.06513297408819199, LR: 0.010000000000000002
Time, 2019-01-01T22:54:14, Epoch: 41, Batch: 680, Training Loss: 0.04985023364424705, LR: 0.010000000000000002
Time, 2019-01-01T22:54:15, Epoch: 41, Batch: 690, Training Loss: 0.05311170555651188, LR: 0.010000000000000002
Time, 2019-01-01T22:54:15, Epoch: 41, Batch: 700, Training Loss: 0.0481712631881237, LR: 0.010000000000000002
Time, 2019-01-01T22:54:16, Epoch: 41, Batch: 710, Training Loss: 0.062035415321588516, LR: 0.010000000000000002
Time, 2019-01-01T22:54:17, Epoch: 41, Batch: 720, Training Loss: 0.05219223648309708, LR: 0.010000000000000002
Time, 2019-01-01T22:54:18, Epoch: 41, Batch: 730, Training Loss: 0.05357097014784813, LR: 0.010000000000000002
Time, 2019-01-01T22:54:18, Epoch: 41, Batch: 740, Training Loss: 0.0460829246789217, LR: 0.010000000000000002
Time, 2019-01-01T22:54:19, Epoch: 41, Batch: 750, Training Loss: 0.06510740742087365, LR: 0.010000000000000002
Time, 2019-01-01T22:54:20, Epoch: 41, Batch: 760, Training Loss: 0.042561302334070204, LR: 0.010000000000000002
Time, 2019-01-01T22:54:21, Epoch: 41, Batch: 770, Training Loss: 0.05444118045270443, LR: 0.010000000000000002
Time, 2019-01-01T22:54:22, Epoch: 41, Batch: 780, Training Loss: 0.05055390782654286, LR: 0.010000000000000002
Time, 2019-01-01T22:54:22, Epoch: 41, Batch: 790, Training Loss: 0.029315735772252084, LR: 0.010000000000000002
Time, 2019-01-01T22:54:23, Epoch: 41, Batch: 800, Training Loss: 0.034116290509700775, LR: 0.010000000000000002
Time, 2019-01-01T22:54:24, Epoch: 41, Batch: 810, Training Loss: 0.051568308100104335, LR: 0.010000000000000002
Time, 2019-01-01T22:54:25, Epoch: 41, Batch: 820, Training Loss: 0.04106923155486584, LR: 0.010000000000000002
Time, 2019-01-01T22:54:25, Epoch: 41, Batch: 830, Training Loss: 0.05842508003115654, LR: 0.010000000000000002
Time, 2019-01-01T22:54:26, Epoch: 41, Batch: 840, Training Loss: 0.03685998879373074, LR: 0.010000000000000002
Time, 2019-01-01T22:54:27, Epoch: 41, Batch: 850, Training Loss: 0.03747454583644867, LR: 0.010000000000000002
Time, 2019-01-01T22:54:27, Epoch: 41, Batch: 860, Training Loss: 0.0316256795078516, LR: 0.010000000000000002
Time, 2019-01-01T22:54:28, Epoch: 41, Batch: 870, Training Loss: 0.06882911287248135, LR: 0.010000000000000002
Time, 2019-01-01T22:54:29, Epoch: 41, Batch: 880, Training Loss: 0.05427171252667904, LR: 0.010000000000000002
Time, 2019-01-01T22:54:30, Epoch: 41, Batch: 890, Training Loss: 0.06667261496186257, LR: 0.010000000000000002
Time, 2019-01-01T22:54:31, Epoch: 41, Batch: 900, Training Loss: 0.03987245224416256, LR: 0.010000000000000002
Time, 2019-01-01T22:54:31, Epoch: 41, Batch: 910, Training Loss: 0.04143519699573517, LR: 0.010000000000000002
Time, 2019-01-01T22:54:32, Epoch: 41, Batch: 920, Training Loss: 0.04645442552864552, LR: 0.010000000000000002
Time, 2019-01-01T22:54:33, Epoch: 41, Batch: 930, Training Loss: 0.04769706577062607, LR: 0.010000000000000002
Epoch: 41, Validation Top 1 acc: 98.1588363647461
Epoch: 41, Validation Top 5 acc: 100.0
Epoch: 41, Validation Set Loss: 0.0559583343565464
Start training epoch 42
Time, 2019-01-01T22:54:39, Epoch: 42, Batch: 10, Training Loss: 0.04823544174432755, LR: 0.010000000000000002
Time, 2019-01-01T22:54:40, Epoch: 42, Batch: 20, Training Loss: 0.064200534299016, LR: 0.010000000000000002
Time, 2019-01-01T22:54:40, Epoch: 42, Batch: 30, Training Loss: 0.06336454525589943, LR: 0.010000000000000002
Time, 2019-01-01T22:54:41, Epoch: 42, Batch: 40, Training Loss: 0.047909687459468844, LR: 0.010000000000000002
Time, 2019-01-01T22:54:42, Epoch: 42, Batch: 50, Training Loss: 0.05210065618157387, LR: 0.010000000000000002
Time, 2019-01-01T22:54:42, Epoch: 42, Batch: 60, Training Loss: 0.04564955942332745, LR: 0.010000000000000002
Time, 2019-01-01T22:54:43, Epoch: 42, Batch: 70, Training Loss: 0.057399533689022064, LR: 0.010000000000000002
Time, 2019-01-01T22:54:44, Epoch: 42, Batch: 80, Training Loss: 0.0527607798576355, LR: 0.010000000000000002
Time, 2019-01-01T22:54:45, Epoch: 42, Batch: 90, Training Loss: 0.05124063268303871, LR: 0.010000000000000002
Time, 2019-01-01T22:54:45, Epoch: 42, Batch: 100, Training Loss: 0.0446298748254776, LR: 0.010000000000000002
Time, 2019-01-01T22:54:46, Epoch: 42, Batch: 110, Training Loss: 0.03344193547964096, LR: 0.010000000000000002
Time, 2019-01-01T22:54:47, Epoch: 42, Batch: 120, Training Loss: 0.04151589311659336, LR: 0.010000000000000002
Time, 2019-01-01T22:54:48, Epoch: 42, Batch: 130, Training Loss: 0.03075937442481518, LR: 0.010000000000000002
Time, 2019-01-01T22:54:48, Epoch: 42, Batch: 140, Training Loss: 0.04554135873913765, LR: 0.010000000000000002
Time, 2019-01-01T22:54:49, Epoch: 42, Batch: 150, Training Loss: 0.05761687234044075, LR: 0.010000000000000002
Time, 2019-01-01T22:54:50, Epoch: 42, Batch: 160, Training Loss: 0.0738797053694725, LR: 0.010000000000000002
Time, 2019-01-01T22:54:51, Epoch: 42, Batch: 170, Training Loss: 0.06993789747357368, LR: 0.010000000000000002
Time, 2019-01-01T22:54:51, Epoch: 42, Batch: 180, Training Loss: 0.04889996387064457, LR: 0.010000000000000002
Time, 2019-01-01T22:54:52, Epoch: 42, Batch: 190, Training Loss: 0.04712844640016556, LR: 0.010000000000000002
Time, 2019-01-01T22:54:53, Epoch: 42, Batch: 200, Training Loss: 0.051494868099689485, LR: 0.010000000000000002
Time, 2019-01-01T22:54:54, Epoch: 42, Batch: 210, Training Loss: 0.032094373181462285, LR: 0.010000000000000002
Time, 2019-01-01T22:54:55, Epoch: 42, Batch: 220, Training Loss: 0.04425343871116638, LR: 0.010000000000000002
Time, 2019-01-01T22:54:55, Epoch: 42, Batch: 230, Training Loss: 0.07847583442926406, LR: 0.010000000000000002
Time, 2019-01-01T22:54:56, Epoch: 42, Batch: 240, Training Loss: 0.07036836110055447, LR: 0.010000000000000002
Time, 2019-01-01T22:54:57, Epoch: 42, Batch: 250, Training Loss: 0.04705074615776539, LR: 0.010000000000000002
Time, 2019-01-01T22:54:58, Epoch: 42, Batch: 260, Training Loss: 0.04924445226788521, LR: 0.010000000000000002
Time, 2019-01-01T22:54:59, Epoch: 42, Batch: 270, Training Loss: 0.04784972220659256, LR: 0.010000000000000002
Time, 2019-01-01T22:54:59, Epoch: 42, Batch: 280, Training Loss: 0.059040633216500285, LR: 0.010000000000000002
Time, 2019-01-01T22:55:00, Epoch: 42, Batch: 290, Training Loss: 0.06260532662272453, LR: 0.010000000000000002
Time, 2019-01-01T22:55:01, Epoch: 42, Batch: 300, Training Loss: 0.07274099849164486, LR: 0.010000000000000002
Time, 2019-01-01T22:55:02, Epoch: 42, Batch: 310, Training Loss: 0.060743920132517816, LR: 0.010000000000000002
Time, 2019-01-01T22:55:02, Epoch: 42, Batch: 320, Training Loss: 0.0529139555990696, LR: 0.010000000000000002
Time, 2019-01-01T22:55:03, Epoch: 42, Batch: 330, Training Loss: 0.0545765645802021, LR: 0.010000000000000002
Time, 2019-01-01T22:55:04, Epoch: 42, Batch: 340, Training Loss: 0.05576270632445812, LR: 0.010000000000000002
Time, 2019-01-01T22:55:05, Epoch: 42, Batch: 350, Training Loss: 0.07516890913248062, LR: 0.010000000000000002
Time, 2019-01-01T22:55:06, Epoch: 42, Batch: 360, Training Loss: 0.05901425369083881, LR: 0.010000000000000002
Time, 2019-01-01T22:55:06, Epoch: 42, Batch: 370, Training Loss: 0.037821778655052186, LR: 0.010000000000000002
Time, 2019-01-01T22:55:07, Epoch: 42, Batch: 380, Training Loss: 0.058136246725916864, LR: 0.010000000000000002
Time, 2019-01-01T22:55:08, Epoch: 42, Batch: 390, Training Loss: 0.05474008992314339, LR: 0.010000000000000002
Time, 2019-01-01T22:55:09, Epoch: 42, Batch: 400, Training Loss: 0.05949995815753937, LR: 0.010000000000000002
Time, 2019-01-01T22:55:09, Epoch: 42, Batch: 410, Training Loss: 0.04494797140359878, LR: 0.010000000000000002
Time, 2019-01-01T22:55:10, Epoch: 42, Batch: 420, Training Loss: 0.046541818603873256, LR: 0.010000000000000002
Time, 2019-01-01T22:55:11, Epoch: 42, Batch: 430, Training Loss: 0.05995993204414844, LR: 0.010000000000000002
Time, 2019-01-01T22:55:12, Epoch: 42, Batch: 440, Training Loss: 0.06662945114076138, LR: 0.010000000000000002
Time, 2019-01-01T22:55:12, Epoch: 42, Batch: 450, Training Loss: 0.0632870189845562, LR: 0.010000000000000002
Time, 2019-01-01T22:55:13, Epoch: 42, Batch: 460, Training Loss: 0.07869694642722606, LR: 0.010000000000000002
Time, 2019-01-01T22:55:14, Epoch: 42, Batch: 470, Training Loss: 0.054249219596385956, LR: 0.010000000000000002
Time, 2019-01-01T22:55:15, Epoch: 42, Batch: 480, Training Loss: 0.04639933407306671, LR: 0.010000000000000002
Time, 2019-01-01T22:55:15, Epoch: 42, Batch: 490, Training Loss: 0.04633921794593334, LR: 0.010000000000000002
Time, 2019-01-01T22:55:16, Epoch: 42, Batch: 500, Training Loss: 0.029804759100079535, LR: 0.010000000000000002
Time, 2019-01-01T22:55:17, Epoch: 42, Batch: 510, Training Loss: 0.05304880030453205, LR: 0.010000000000000002
Time, 2019-01-01T22:55:18, Epoch: 42, Batch: 520, Training Loss: 0.0337778840214014, LR: 0.010000000000000002
Time, 2019-01-01T22:55:19, Epoch: 42, Batch: 530, Training Loss: 0.07970950789749623, LR: 0.010000000000000002
Time, 2019-01-01T22:55:19, Epoch: 42, Batch: 540, Training Loss: 0.048718686774373056, LR: 0.010000000000000002
Time, 2019-01-01T22:55:20, Epoch: 42, Batch: 550, Training Loss: 0.04120585657656193, LR: 0.010000000000000002
Time, 2019-01-01T22:55:21, Epoch: 42, Batch: 560, Training Loss: 0.06515876203775406, LR: 0.010000000000000002
Time, 2019-01-01T22:55:22, Epoch: 42, Batch: 570, Training Loss: 0.0726843811571598, LR: 0.010000000000000002
Time, 2019-01-01T22:55:22, Epoch: 42, Batch: 580, Training Loss: 0.06944320797920227, LR: 0.010000000000000002
Time, 2019-01-01T22:55:23, Epoch: 42, Batch: 590, Training Loss: 0.06498877927660943, LR: 0.010000000000000002
Time, 2019-01-01T22:55:24, Epoch: 42, Batch: 600, Training Loss: 0.05532689355313778, LR: 0.010000000000000002
Time, 2019-01-01T22:55:25, Epoch: 42, Batch: 610, Training Loss: 0.06469614766538143, LR: 0.010000000000000002
Time, 2019-01-01T22:55:25, Epoch: 42, Batch: 620, Training Loss: 0.05966880284249783, LR: 0.010000000000000002
Time, 2019-01-01T22:55:26, Epoch: 42, Batch: 630, Training Loss: 0.0519463799893856, LR: 0.010000000000000002
Time, 2019-01-01T22:55:27, Epoch: 42, Batch: 640, Training Loss: 0.04462165161967278, LR: 0.010000000000000002
Time, 2019-01-01T22:55:28, Epoch: 42, Batch: 650, Training Loss: 0.05946749076247215, LR: 0.010000000000000002
Time, 2019-01-01T22:55:29, Epoch: 42, Batch: 660, Training Loss: 0.07455992177128792, LR: 0.010000000000000002
Time, 2019-01-01T22:55:29, Epoch: 42, Batch: 670, Training Loss: 0.05511341281235218, LR: 0.010000000000000002
Time, 2019-01-01T22:55:30, Epoch: 42, Batch: 680, Training Loss: 0.06514500007033348, LR: 0.010000000000000002
Time, 2019-01-01T22:55:31, Epoch: 42, Batch: 690, Training Loss: 0.039867163822054866, LR: 0.010000000000000002
Time, 2019-01-01T22:55:32, Epoch: 42, Batch: 700, Training Loss: 0.048554349690675735, LR: 0.010000000000000002
Time, 2019-01-01T22:55:32, Epoch: 42, Batch: 710, Training Loss: 0.06270330995321274, LR: 0.010000000000000002
Time, 2019-01-01T22:55:33, Epoch: 42, Batch: 720, Training Loss: 0.03903602585196495, LR: 0.010000000000000002
Time, 2019-01-01T22:55:34, Epoch: 42, Batch: 730, Training Loss: 0.04315583147108555, LR: 0.010000000000000002
Time, 2019-01-01T22:55:35, Epoch: 42, Batch: 740, Training Loss: 0.07766309641301632, LR: 0.010000000000000002
Time, 2019-01-01T22:55:35, Epoch: 42, Batch: 750, Training Loss: 0.052459583058953284, LR: 0.010000000000000002
Time, 2019-01-01T22:55:36, Epoch: 42, Batch: 760, Training Loss: 0.04819643460214138, LR: 0.010000000000000002
Time, 2019-01-01T22:55:37, Epoch: 42, Batch: 770, Training Loss: 0.05401906259357929, LR: 0.010000000000000002
Time, 2019-01-01T22:55:38, Epoch: 42, Batch: 780, Training Loss: 0.05550753250718117, LR: 0.010000000000000002
Time, 2019-01-01T22:55:38, Epoch: 42, Batch: 790, Training Loss: 0.06837322786450387, LR: 0.010000000000000002
Time, 2019-01-01T22:55:39, Epoch: 42, Batch: 800, Training Loss: 0.06796933971345424, LR: 0.010000000000000002
Time, 2019-01-01T22:55:40, Epoch: 42, Batch: 810, Training Loss: 0.04916133806109428, LR: 0.010000000000000002
Time, 2019-01-01T22:55:41, Epoch: 42, Batch: 820, Training Loss: 0.05007970593869686, LR: 0.010000000000000002
Time, 2019-01-01T22:55:42, Epoch: 42, Batch: 830, Training Loss: 0.04050994887948036, LR: 0.010000000000000002
Time, 2019-01-01T22:55:42, Epoch: 42, Batch: 840, Training Loss: 0.06915359795093537, LR: 0.010000000000000002
Time, 2019-01-01T22:55:43, Epoch: 42, Batch: 850, Training Loss: 0.04293319880962372, LR: 0.010000000000000002
Time, 2019-01-01T22:55:44, Epoch: 42, Batch: 860, Training Loss: 0.059900561720132826, LR: 0.010000000000000002
Time, 2019-01-01T22:55:45, Epoch: 42, Batch: 870, Training Loss: 0.049383874982595444, LR: 0.010000000000000002
Time, 2019-01-01T22:55:45, Epoch: 42, Batch: 880, Training Loss: 0.058634287863969806, LR: 0.010000000000000002
Time, 2019-01-01T22:55:46, Epoch: 42, Batch: 890, Training Loss: 0.04164871275424957, LR: 0.010000000000000002
Time, 2019-01-01T22:55:47, Epoch: 42, Batch: 900, Training Loss: 0.0423978928476572, LR: 0.010000000000000002
Time, 2019-01-01T22:55:48, Epoch: 42, Batch: 910, Training Loss: 0.05003959201276302, LR: 0.010000000000000002
Time, 2019-01-01T22:55:48, Epoch: 42, Batch: 920, Training Loss: 0.04383025579154491, LR: 0.010000000000000002
Time, 2019-01-01T22:55:49, Epoch: 42, Batch: 930, Training Loss: 0.04760783761739731, LR: 0.010000000000000002
Epoch: 42, Validation Top 1 acc: 98.3280258178711
Epoch: 42, Validation Top 5 acc: 99.99005126953125
Epoch: 42, Validation Set Loss: 0.053890276700258255
Start training epoch 43
Time, 2019-01-01T22:55:56, Epoch: 43, Batch: 10, Training Loss: 0.04991096742451191, LR: 0.010000000000000002
Time, 2019-01-01T22:55:56, Epoch: 43, Batch: 20, Training Loss: 0.05352865383028984, LR: 0.010000000000000002
Time, 2019-01-01T22:55:57, Epoch: 43, Batch: 30, Training Loss: 0.0437328927218914, LR: 0.010000000000000002
Time, 2019-01-01T22:55:58, Epoch: 43, Batch: 40, Training Loss: 0.03709062971174717, LR: 0.010000000000000002
Time, 2019-01-01T22:55:59, Epoch: 43, Batch: 50, Training Loss: 0.0363639771938324, LR: 0.010000000000000002
Time, 2019-01-01T22:56:00, Epoch: 43, Batch: 60, Training Loss: 0.04155750349164009, LR: 0.010000000000000002
Time, 2019-01-01T22:56:00, Epoch: 43, Batch: 70, Training Loss: 0.06619612351059914, LR: 0.010000000000000002
Time, 2019-01-01T22:56:01, Epoch: 43, Batch: 80, Training Loss: 0.057360653206706046, LR: 0.010000000000000002
Time, 2019-01-01T22:56:02, Epoch: 43, Batch: 90, Training Loss: 0.05047200806438923, LR: 0.010000000000000002
Time, 2019-01-01T22:56:03, Epoch: 43, Batch: 100, Training Loss: 0.05943981446325779, LR: 0.010000000000000002
Time, 2019-01-01T22:56:03, Epoch: 43, Batch: 110, Training Loss: 0.05670600682497025, LR: 0.010000000000000002
Time, 2019-01-01T22:56:04, Epoch: 43, Batch: 120, Training Loss: 0.051729346811771396, LR: 0.010000000000000002
Time, 2019-01-01T22:56:05, Epoch: 43, Batch: 130, Training Loss: 0.07552015036344528, LR: 0.010000000000000002
Time, 2019-01-01T22:56:06, Epoch: 43, Batch: 140, Training Loss: 0.05757594220340252, LR: 0.010000000000000002
Time, 2019-01-01T22:56:06, Epoch: 43, Batch: 150, Training Loss: 0.0698792278766632, LR: 0.010000000000000002
Time, 2019-01-01T22:56:07, Epoch: 43, Batch: 160, Training Loss: 0.050906192138791086, LR: 0.010000000000000002
Time, 2019-01-01T22:56:08, Epoch: 43, Batch: 170, Training Loss: 0.05397154986858368, LR: 0.010000000000000002
Time, 2019-01-01T22:56:09, Epoch: 43, Batch: 180, Training Loss: 0.07251711934804916, LR: 0.010000000000000002
Time, 2019-01-01T22:56:10, Epoch: 43, Batch: 190, Training Loss: 0.027927272766828538, LR: 0.010000000000000002
Time, 2019-01-01T22:56:10, Epoch: 43, Batch: 200, Training Loss: 0.057125870138406754, LR: 0.010000000000000002
Time, 2019-01-01T22:56:11, Epoch: 43, Batch: 210, Training Loss: 0.029210944101214407, LR: 0.010000000000000002
Time, 2019-01-01T22:56:12, Epoch: 43, Batch: 220, Training Loss: 0.046212711557745936, LR: 0.010000000000000002
Time, 2019-01-01T22:56:13, Epoch: 43, Batch: 230, Training Loss: 0.03169707842171192, LR: 0.010000000000000002
Time, 2019-01-01T22:56:13, Epoch: 43, Batch: 240, Training Loss: 0.04539876617491245, LR: 0.010000000000000002
Time, 2019-01-01T22:56:14, Epoch: 43, Batch: 250, Training Loss: 0.06344014964997768, LR: 0.010000000000000002
Time, 2019-01-01T22:56:15, Epoch: 43, Batch: 260, Training Loss: 0.0702703446149826, LR: 0.010000000000000002
Time, 2019-01-01T22:56:16, Epoch: 43, Batch: 270, Training Loss: 0.05719631835818291, LR: 0.010000000000000002
Time, 2019-01-01T22:56:16, Epoch: 43, Batch: 280, Training Loss: 0.06893253847956657, LR: 0.010000000000000002
Time, 2019-01-01T22:56:17, Epoch: 43, Batch: 290, Training Loss: 0.04422157183289528, LR: 0.010000000000000002
Time, 2019-01-01T22:56:18, Epoch: 43, Batch: 300, Training Loss: 0.06625031158328057, LR: 0.010000000000000002
Time, 2019-01-01T22:56:19, Epoch: 43, Batch: 310, Training Loss: 0.03695016950368881, LR: 0.010000000000000002
Time, 2019-01-01T22:56:20, Epoch: 43, Batch: 320, Training Loss: 0.04445747993886471, LR: 0.010000000000000002
Time, 2019-01-01T22:56:21, Epoch: 43, Batch: 330, Training Loss: 0.04218434207141399, LR: 0.010000000000000002
Time, 2019-01-01T22:56:21, Epoch: 43, Batch: 340, Training Loss: 0.0622520960867405, LR: 0.010000000000000002
Time, 2019-01-01T22:56:22, Epoch: 43, Batch: 350, Training Loss: 0.08197501301765442, LR: 0.010000000000000002
Time, 2019-01-01T22:56:23, Epoch: 43, Batch: 360, Training Loss: 0.05231539979577064, LR: 0.010000000000000002
Time, 2019-01-01T22:56:24, Epoch: 43, Batch: 370, Training Loss: 0.06090081930160522, LR: 0.010000000000000002
Time, 2019-01-01T22:56:25, Epoch: 43, Batch: 380, Training Loss: 0.03734547831118107, LR: 0.010000000000000002
Time, 2019-01-01T22:56:25, Epoch: 43, Batch: 390, Training Loss: 0.06747624725103378, LR: 0.010000000000000002
Time, 2019-01-01T22:56:26, Epoch: 43, Batch: 400, Training Loss: 0.059071211516857146, LR: 0.010000000000000002
Time, 2019-01-01T22:56:27, Epoch: 43, Batch: 410, Training Loss: 0.06453412733972072, LR: 0.010000000000000002
Time, 2019-01-01T22:56:28, Epoch: 43, Batch: 420, Training Loss: 0.06642933934926987, LR: 0.010000000000000002
Time, 2019-01-01T22:56:28, Epoch: 43, Batch: 430, Training Loss: 0.042281844094395636, LR: 0.010000000000000002
Time, 2019-01-01T22:56:29, Epoch: 43, Batch: 440, Training Loss: 0.041740096360445025, LR: 0.010000000000000002
Time, 2019-01-01T22:56:30, Epoch: 43, Batch: 450, Training Loss: 0.06769132986664772, LR: 0.010000000000000002
Time, 2019-01-01T22:56:31, Epoch: 43, Batch: 460, Training Loss: 0.050514907017350195, LR: 0.010000000000000002
Time, 2019-01-01T22:56:32, Epoch: 43, Batch: 470, Training Loss: 0.06340588256716728, LR: 0.010000000000000002
Time, 2019-01-01T22:56:32, Epoch: 43, Batch: 480, Training Loss: 0.03697496391832829, LR: 0.010000000000000002
Time, 2019-01-01T22:56:33, Epoch: 43, Batch: 490, Training Loss: 0.045670853555202486, LR: 0.010000000000000002
Time, 2019-01-01T22:56:34, Epoch: 43, Batch: 500, Training Loss: 0.051457101851701735, LR: 0.010000000000000002
Time, 2019-01-01T22:56:35, Epoch: 43, Batch: 510, Training Loss: 0.06115797385573387, LR: 0.010000000000000002
Time, 2019-01-01T22:56:36, Epoch: 43, Batch: 520, Training Loss: 0.06962538883090019, LR: 0.010000000000000002
Time, 2019-01-01T22:56:36, Epoch: 43, Batch: 530, Training Loss: 0.05910224691033363, LR: 0.010000000000000002
Time, 2019-01-01T22:56:37, Epoch: 43, Batch: 540, Training Loss: 0.06488939896225929, LR: 0.010000000000000002
Time, 2019-01-01T22:56:38, Epoch: 43, Batch: 550, Training Loss: 0.08005868792533874, LR: 0.010000000000000002
Time, 2019-01-01T22:56:39, Epoch: 43, Batch: 560, Training Loss: 0.0639222264289856, LR: 0.010000000000000002
Time, 2019-01-01T22:56:39, Epoch: 43, Batch: 570, Training Loss: 0.03206150010228157, LR: 0.010000000000000002
Time, 2019-01-01T22:56:40, Epoch: 43, Batch: 580, Training Loss: 0.042337342351675036, LR: 0.010000000000000002
Time, 2019-01-01T22:56:41, Epoch: 43, Batch: 590, Training Loss: 0.07272980399429799, LR: 0.010000000000000002
Time, 2019-01-01T22:56:42, Epoch: 43, Batch: 600, Training Loss: 0.022108500450849534, LR: 0.010000000000000002
Time, 2019-01-01T22:56:42, Epoch: 43, Batch: 610, Training Loss: 0.049132928252220154, LR: 0.010000000000000002
Time, 2019-01-01T22:56:43, Epoch: 43, Batch: 620, Training Loss: 0.03818482533097267, LR: 0.010000000000000002
Time, 2019-01-01T22:56:44, Epoch: 43, Batch: 630, Training Loss: 0.045684884488582614, LR: 0.010000000000000002
Time, 2019-01-01T22:56:45, Epoch: 43, Batch: 640, Training Loss: 0.04293472431600094, LR: 0.010000000000000002
Time, 2019-01-01T22:56:46, Epoch: 43, Batch: 650, Training Loss: 0.05355854444205761, LR: 0.010000000000000002
Time, 2019-01-01T22:56:46, Epoch: 43, Batch: 660, Training Loss: 0.060375820472836496, LR: 0.010000000000000002
Time, 2019-01-01T22:56:47, Epoch: 43, Batch: 670, Training Loss: 0.04866096973419189, LR: 0.010000000000000002
Time, 2019-01-01T22:56:48, Epoch: 43, Batch: 680, Training Loss: 0.058266331627964976, LR: 0.010000000000000002
Time, 2019-01-01T22:56:49, Epoch: 43, Batch: 690, Training Loss: 0.048135538771748546, LR: 0.010000000000000002
Time, 2019-01-01T22:56:49, Epoch: 43, Batch: 700, Training Loss: 0.06006150096654892, LR: 0.010000000000000002
Time, 2019-01-01T22:56:50, Epoch: 43, Batch: 710, Training Loss: 0.04408459216356277, LR: 0.010000000000000002
Time, 2019-01-01T22:56:51, Epoch: 43, Batch: 720, Training Loss: 0.06710650771856308, LR: 0.010000000000000002
Time, 2019-01-01T22:56:52, Epoch: 43, Batch: 730, Training Loss: 0.05659145191311836, LR: 0.010000000000000002
Time, 2019-01-01T22:56:52, Epoch: 43, Batch: 740, Training Loss: 0.05985029600560665, LR: 0.010000000000000002
Time, 2019-01-01T22:56:53, Epoch: 43, Batch: 750, Training Loss: 0.05975622572004795, LR: 0.010000000000000002
Time, 2019-01-01T22:56:54, Epoch: 43, Batch: 760, Training Loss: 0.060297942534089086, LR: 0.010000000000000002
Time, 2019-01-01T22:56:55, Epoch: 43, Batch: 770, Training Loss: 0.045211241394281385, LR: 0.010000000000000002
Time, 2019-01-01T22:56:55, Epoch: 43, Batch: 780, Training Loss: 0.05795876979827881, LR: 0.010000000000000002
Time, 2019-01-01T22:56:56, Epoch: 43, Batch: 790, Training Loss: 0.04721731469035149, LR: 0.010000000000000002
Time, 2019-01-01T22:56:57, Epoch: 43, Batch: 800, Training Loss: 0.04231451153755188, LR: 0.010000000000000002
Time, 2019-01-01T22:56:58, Epoch: 43, Batch: 810, Training Loss: 0.06262392699718475, LR: 0.010000000000000002
Time, 2019-01-01T22:56:58, Epoch: 43, Batch: 820, Training Loss: 0.034599251300096515, LR: 0.010000000000000002
Time, 2019-01-01T22:56:59, Epoch: 43, Batch: 830, Training Loss: 0.03064733073115349, LR: 0.010000000000000002
Time, 2019-01-01T22:57:00, Epoch: 43, Batch: 840, Training Loss: 0.051867784932255745, LR: 0.010000000000000002
Time, 2019-01-01T22:57:01, Epoch: 43, Batch: 850, Training Loss: 0.04207356497645378, LR: 0.010000000000000002
Time, 2019-01-01T22:57:02, Epoch: 43, Batch: 860, Training Loss: 0.04039011858403683, LR: 0.010000000000000002
Time, 2019-01-01T22:57:02, Epoch: 43, Batch: 870, Training Loss: 0.05831194519996643, LR: 0.010000000000000002
Time, 2019-01-01T22:57:03, Epoch: 43, Batch: 880, Training Loss: 0.048982493579387665, LR: 0.010000000000000002
Time, 2019-01-01T22:57:04, Epoch: 43, Batch: 890, Training Loss: 0.057367877662181856, LR: 0.010000000000000002
Time, 2019-01-01T22:57:05, Epoch: 43, Batch: 900, Training Loss: 0.04815973229706287, LR: 0.010000000000000002
Time, 2019-01-01T22:57:06, Epoch: 43, Batch: 910, Training Loss: 0.06460803784430028, LR: 0.010000000000000002
Time, 2019-01-01T22:57:06, Epoch: 43, Batch: 920, Training Loss: 0.054551821947097776, LR: 0.010000000000000002
Time, 2019-01-01T22:57:07, Epoch: 43, Batch: 930, Training Loss: 0.08853204771876336, LR: 0.010000000000000002
Epoch: 43, Validation Top 1 acc: 98.35787963867188
Epoch: 43, Validation Top 5 acc: 99.99005126953125
Epoch: 43, Validation Set Loss: 0.052450813353061676
Start training epoch 44
Time, 2019-01-01T22:57:13, Epoch: 44, Batch: 10, Training Loss: 0.04048871994018555, LR: 0.010000000000000002
Time, 2019-01-01T22:57:14, Epoch: 44, Batch: 20, Training Loss: 0.057802888378500936, LR: 0.010000000000000002
Time, 2019-01-01T22:57:15, Epoch: 44, Batch: 30, Training Loss: 0.02850772812962532, LR: 0.010000000000000002
Time, 2019-01-01T22:57:15, Epoch: 44, Batch: 40, Training Loss: 0.049172629788517955, LR: 0.010000000000000002
Time, 2019-01-01T22:57:16, Epoch: 44, Batch: 50, Training Loss: 0.04157114252448082, LR: 0.010000000000000002
Time, 2019-01-01T22:57:17, Epoch: 44, Batch: 60, Training Loss: 0.05773619599640369, LR: 0.010000000000000002
Time, 2019-01-01T22:57:18, Epoch: 44, Batch: 70, Training Loss: 0.05210103839635849, LR: 0.010000000000000002
Time, 2019-01-01T22:57:19, Epoch: 44, Batch: 80, Training Loss: 0.0361545629799366, LR: 0.010000000000000002
Time, 2019-01-01T22:57:19, Epoch: 44, Batch: 90, Training Loss: 0.03533122316002846, LR: 0.010000000000000002
Time, 2019-01-01T22:57:20, Epoch: 44, Batch: 100, Training Loss: 0.04724423550069332, LR: 0.010000000000000002
Time, 2019-01-01T22:57:21, Epoch: 44, Batch: 110, Training Loss: 0.062282494455575946, LR: 0.010000000000000002
Time, 2019-01-01T22:57:22, Epoch: 44, Batch: 120, Training Loss: 0.0520681980997324, LR: 0.010000000000000002
Time, 2019-01-01T22:57:22, Epoch: 44, Batch: 130, Training Loss: 0.041751591861248015, LR: 0.010000000000000002
Time, 2019-01-01T22:57:23, Epoch: 44, Batch: 140, Training Loss: 0.03958888649940491, LR: 0.010000000000000002
Time, 2019-01-01T22:57:24, Epoch: 44, Batch: 150, Training Loss: 0.03267412148416042, LR: 0.010000000000000002
Time, 2019-01-01T22:57:25, Epoch: 44, Batch: 160, Training Loss: 0.0610038049519062, LR: 0.010000000000000002
Time, 2019-01-01T22:57:26, Epoch: 44, Batch: 170, Training Loss: 0.04949491769075394, LR: 0.010000000000000002
Time, 2019-01-01T22:57:26, Epoch: 44, Batch: 180, Training Loss: 0.050068479403853415, LR: 0.010000000000000002
Time, 2019-01-01T22:57:27, Epoch: 44, Batch: 190, Training Loss: 0.06881211064755917, LR: 0.010000000000000002
Time, 2019-01-01T22:57:28, Epoch: 44, Batch: 200, Training Loss: 0.06807890757918358, LR: 0.010000000000000002
Time, 2019-01-01T22:57:29, Epoch: 44, Batch: 210, Training Loss: 0.06121552214026451, LR: 0.010000000000000002
Time, 2019-01-01T22:57:30, Epoch: 44, Batch: 220, Training Loss: 0.07313775755465031, LR: 0.010000000000000002
Time, 2019-01-01T22:57:30, Epoch: 44, Batch: 230, Training Loss: 0.0805018924176693, LR: 0.010000000000000002
Time, 2019-01-01T22:57:31, Epoch: 44, Batch: 240, Training Loss: 0.05007576495409012, LR: 0.010000000000000002
Time, 2019-01-01T22:57:32, Epoch: 44, Batch: 250, Training Loss: 0.04817991778254509, LR: 0.010000000000000002
Time, 2019-01-01T22:57:33, Epoch: 44, Batch: 260, Training Loss: 0.05251956544816494, LR: 0.010000000000000002
Time, 2019-01-01T22:57:34, Epoch: 44, Batch: 270, Training Loss: 0.06581222787499427, LR: 0.010000000000000002
Time, 2019-01-01T22:57:34, Epoch: 44, Batch: 280, Training Loss: 0.058022087439894676, LR: 0.010000000000000002
Time, 2019-01-01T22:57:35, Epoch: 44, Batch: 290, Training Loss: 0.043490467220544816, LR: 0.010000000000000002
Time, 2019-01-01T22:57:36, Epoch: 44, Batch: 300, Training Loss: 0.055654465407133105, LR: 0.010000000000000002
Time, 2019-01-01T22:57:37, Epoch: 44, Batch: 310, Training Loss: 0.07475132867693901, LR: 0.010000000000000002
Time, 2019-01-01T22:57:38, Epoch: 44, Batch: 320, Training Loss: 0.07435233630239964, LR: 0.010000000000000002
Time, 2019-01-01T22:57:38, Epoch: 44, Batch: 330, Training Loss: 0.07174127884209155, LR: 0.010000000000000002
Time, 2019-01-01T22:57:39, Epoch: 44, Batch: 340, Training Loss: 0.046617531776428224, LR: 0.010000000000000002
Time, 2019-01-01T22:57:40, Epoch: 44, Batch: 350, Training Loss: 0.06904989965260029, LR: 0.010000000000000002
Time, 2019-01-01T22:57:41, Epoch: 44, Batch: 360, Training Loss: 0.06840841509401799, LR: 0.010000000000000002
Time, 2019-01-01T22:57:42, Epoch: 44, Batch: 370, Training Loss: 0.07132233008742332, LR: 0.010000000000000002
Time, 2019-01-01T22:57:43, Epoch: 44, Batch: 380, Training Loss: 0.062488092482089995, LR: 0.010000000000000002
Time, 2019-01-01T22:57:43, Epoch: 44, Batch: 390, Training Loss: 0.06403983607888222, LR: 0.010000000000000002
Time, 2019-01-01T22:57:44, Epoch: 44, Batch: 400, Training Loss: 0.06129913702607155, LR: 0.010000000000000002
Time, 2019-01-01T22:57:45, Epoch: 44, Batch: 410, Training Loss: 0.0615774642676115, LR: 0.010000000000000002
Time, 2019-01-01T22:57:46, Epoch: 44, Batch: 420, Training Loss: 0.06707589477300643, LR: 0.010000000000000002
Time, 2019-01-01T22:57:47, Epoch: 44, Batch: 430, Training Loss: 0.05913841426372528, LR: 0.010000000000000002
Time, 2019-01-01T22:57:48, Epoch: 44, Batch: 440, Training Loss: 0.039375039935112, LR: 0.010000000000000002
Time, 2019-01-01T22:57:48, Epoch: 44, Batch: 450, Training Loss: 0.06906287372112274, LR: 0.010000000000000002
Time, 2019-01-01T22:57:49, Epoch: 44, Batch: 460, Training Loss: 0.052437427639961245, LR: 0.010000000000000002
Time, 2019-01-01T22:57:50, Epoch: 44, Batch: 470, Training Loss: 0.030759792774915695, LR: 0.010000000000000002
Time, 2019-01-01T22:57:52, Epoch: 44, Batch: 480, Training Loss: 0.0565673403441906, LR: 0.010000000000000002
Time, 2019-01-01T22:57:53, Epoch: 44, Batch: 490, Training Loss: 0.0621898427605629, LR: 0.010000000000000002
Time, 2019-01-01T22:57:54, Epoch: 44, Batch: 500, Training Loss: 0.05580550506711006, LR: 0.010000000000000002
Time, 2019-01-01T22:57:55, Epoch: 44, Batch: 510, Training Loss: 0.07970145419239998, LR: 0.010000000000000002
Time, 2019-01-01T22:57:56, Epoch: 44, Batch: 520, Training Loss: 0.05102324932813644, LR: 0.010000000000000002
Time, 2019-01-01T22:57:56, Epoch: 44, Batch: 530, Training Loss: 0.05860577113926411, LR: 0.010000000000000002
Time, 2019-01-01T22:57:57, Epoch: 44, Batch: 540, Training Loss: 0.059711140021681784, LR: 0.010000000000000002
Time, 2019-01-01T22:57:58, Epoch: 44, Batch: 550, Training Loss: 0.04153652042150498, LR: 0.010000000000000002
Time, 2019-01-01T22:57:59, Epoch: 44, Batch: 560, Training Loss: 0.06109280064702034, LR: 0.010000000000000002
Time, 2019-01-01T22:58:00, Epoch: 44, Batch: 570, Training Loss: 0.05702640637755394, LR: 0.010000000000000002
Time, 2019-01-01T22:58:01, Epoch: 44, Batch: 580, Training Loss: 0.05372430756688118, LR: 0.010000000000000002
Time, 2019-01-01T22:58:02, Epoch: 44, Batch: 590, Training Loss: 0.04795149601995945, LR: 0.010000000000000002
Time, 2019-01-01T22:58:02, Epoch: 44, Batch: 600, Training Loss: 0.047665201127529144, LR: 0.010000000000000002
Time, 2019-01-01T22:58:03, Epoch: 44, Batch: 610, Training Loss: 0.06665153056383133, LR: 0.010000000000000002
Time, 2019-01-01T22:58:04, Epoch: 44, Batch: 620, Training Loss: 0.03850697390735149, LR: 0.010000000000000002
Time, 2019-01-01T22:58:05, Epoch: 44, Batch: 630, Training Loss: 0.06139566153287888, LR: 0.010000000000000002
Time, 2019-01-01T22:58:06, Epoch: 44, Batch: 640, Training Loss: 0.06222758144140243, LR: 0.010000000000000002
Time, 2019-01-01T22:58:06, Epoch: 44, Batch: 650, Training Loss: 0.04241547919809818, LR: 0.010000000000000002
Time, 2019-01-01T22:58:07, Epoch: 44, Batch: 660, Training Loss: 0.04026603177189827, LR: 0.010000000000000002
Time, 2019-01-01T22:58:08, Epoch: 44, Batch: 670, Training Loss: 0.05875607505440712, LR: 0.010000000000000002
Time, 2019-01-01T22:58:09, Epoch: 44, Batch: 680, Training Loss: 0.03280176445841789, LR: 0.010000000000000002
Time, 2019-01-01T22:58:10, Epoch: 44, Batch: 690, Training Loss: 0.05042235217988491, LR: 0.010000000000000002
Time, 2019-01-01T22:58:10, Epoch: 44, Batch: 700, Training Loss: 0.04883890151977539, LR: 0.010000000000000002
Time, 2019-01-01T22:58:11, Epoch: 44, Batch: 710, Training Loss: 0.03642837665975094, LR: 0.010000000000000002
Time, 2019-01-01T22:58:12, Epoch: 44, Batch: 720, Training Loss: 0.04970133118331432, LR: 0.010000000000000002
Time, 2019-01-01T22:58:13, Epoch: 44, Batch: 730, Training Loss: 0.050550540909171104, LR: 0.010000000000000002
Time, 2019-01-01T22:58:14, Epoch: 44, Batch: 740, Training Loss: 0.043231171742081645, LR: 0.010000000000000002
Time, 2019-01-01T22:58:14, Epoch: 44, Batch: 750, Training Loss: 0.05948021486401558, LR: 0.010000000000000002
Time, 2019-01-01T22:58:15, Epoch: 44, Batch: 760, Training Loss: 0.07516540475189686, LR: 0.010000000000000002
Time, 2019-01-01T22:58:16, Epoch: 44, Batch: 770, Training Loss: 0.0469796221703291, LR: 0.010000000000000002
Time, 2019-01-01T22:58:17, Epoch: 44, Batch: 780, Training Loss: 0.04762841984629631, LR: 0.010000000000000002
Time, 2019-01-01T22:58:17, Epoch: 44, Batch: 790, Training Loss: 0.044076553732156756, LR: 0.010000000000000002
Time, 2019-01-01T22:58:18, Epoch: 44, Batch: 800, Training Loss: 0.05223365724086761, LR: 0.010000000000000002
Time, 2019-01-01T22:58:19, Epoch: 44, Batch: 810, Training Loss: 0.05953209139406681, LR: 0.010000000000000002
Time, 2019-01-01T22:58:20, Epoch: 44, Batch: 820, Training Loss: 0.04389393664896488, LR: 0.010000000000000002
Time, 2019-01-01T22:58:20, Epoch: 44, Batch: 830, Training Loss: 0.05721764899790287, LR: 0.010000000000000002
Time, 2019-01-01T22:58:21, Epoch: 44, Batch: 840, Training Loss: 0.05858574882149696, LR: 0.010000000000000002
Time, 2019-01-01T22:58:22, Epoch: 44, Batch: 850, Training Loss: 0.05509803928434849, LR: 0.010000000000000002
Time, 2019-01-01T22:58:23, Epoch: 44, Batch: 860, Training Loss: 0.05897071398794651, LR: 0.010000000000000002
Time, 2019-01-01T22:58:24, Epoch: 44, Batch: 870, Training Loss: 0.06576968580484391, LR: 0.010000000000000002
Time, 2019-01-01T22:58:24, Epoch: 44, Batch: 880, Training Loss: 0.06170747354626656, LR: 0.010000000000000002
Time, 2019-01-01T22:58:25, Epoch: 44, Batch: 890, Training Loss: 0.060983110964298246, LR: 0.010000000000000002
Time, 2019-01-01T22:58:26, Epoch: 44, Batch: 900, Training Loss: 0.03688080571591854, LR: 0.010000000000000002
Time, 2019-01-01T22:58:27, Epoch: 44, Batch: 910, Training Loss: 0.03642897382378578, LR: 0.010000000000000002
Time, 2019-01-01T22:58:27, Epoch: 44, Batch: 920, Training Loss: 0.06519459709525108, LR: 0.010000000000000002
Time, 2019-01-01T22:58:28, Epoch: 44, Batch: 930, Training Loss: 0.05606530010700226, LR: 0.010000000000000002
Epoch: 44, Validation Top 1 acc: 98.16879272460938
Epoch: 44, Validation Top 5 acc: 99.99005126953125
Epoch: 44, Validation Set Loss: 0.05542707070708275
Start training epoch 45
Time, 2019-01-01T22:58:34, Epoch: 45, Batch: 10, Training Loss: 0.040269292145967486, LR: 0.010000000000000002
Time, 2019-01-01T22:58:35, Epoch: 45, Batch: 20, Training Loss: 0.04997165761888027, LR: 0.010000000000000002
Time, 2019-01-01T22:58:36, Epoch: 45, Batch: 30, Training Loss: 0.041064941138029096, LR: 0.010000000000000002
Time, 2019-01-01T22:58:36, Epoch: 45, Batch: 40, Training Loss: 0.0338195938616991, LR: 0.010000000000000002
Time, 2019-01-01T22:58:37, Epoch: 45, Batch: 50, Training Loss: 0.05766229070723057, LR: 0.010000000000000002
Time, 2019-01-01T22:58:38, Epoch: 45, Batch: 60, Training Loss: 0.02709837071597576, LR: 0.010000000000000002
Time, 2019-01-01T22:58:39, Epoch: 45, Batch: 70, Training Loss: 0.050051814317703246, LR: 0.010000000000000002
Time, 2019-01-01T22:58:39, Epoch: 45, Batch: 80, Training Loss: 0.03288455978035927, LR: 0.010000000000000002
Time, 2019-01-01T22:58:40, Epoch: 45, Batch: 90, Training Loss: 0.047226143255829814, LR: 0.010000000000000002
Time, 2019-01-01T22:58:41, Epoch: 45, Batch: 100, Training Loss: 0.03178526423871517, LR: 0.010000000000000002
Time, 2019-01-01T22:58:42, Epoch: 45, Batch: 110, Training Loss: 0.04602270796895027, LR: 0.010000000000000002
Time, 2019-01-01T22:58:42, Epoch: 45, Batch: 120, Training Loss: 0.0451700247824192, LR: 0.010000000000000002
Time, 2019-01-01T22:58:43, Epoch: 45, Batch: 130, Training Loss: 0.07060464471578598, LR: 0.010000000000000002
Time, 2019-01-01T22:58:44, Epoch: 45, Batch: 140, Training Loss: 0.051959282159805296, LR: 0.010000000000000002
Time, 2019-01-01T22:58:45, Epoch: 45, Batch: 150, Training Loss: 0.03689340725541115, LR: 0.010000000000000002
Time, 2019-01-01T22:58:45, Epoch: 45, Batch: 160, Training Loss: 0.0409636776894331, LR: 0.010000000000000002
Time, 2019-01-01T22:58:46, Epoch: 45, Batch: 170, Training Loss: 0.06467226259410382, LR: 0.010000000000000002
Time, 2019-01-01T22:58:47, Epoch: 45, Batch: 180, Training Loss: 0.034140271693468095, LR: 0.010000000000000002
Time, 2019-01-01T22:58:48, Epoch: 45, Batch: 190, Training Loss: 0.041699232161045076, LR: 0.010000000000000002
Time, 2019-01-01T22:58:49, Epoch: 45, Batch: 200, Training Loss: 0.04861828126013279, LR: 0.010000000000000002
Time, 2019-01-01T22:58:49, Epoch: 45, Batch: 210, Training Loss: 0.061684976890683176, LR: 0.010000000000000002
Time, 2019-01-01T22:58:50, Epoch: 45, Batch: 220, Training Loss: 0.03432895839214325, LR: 0.010000000000000002
Time, 2019-01-01T22:58:51, Epoch: 45, Batch: 230, Training Loss: 0.05626381374895573, LR: 0.010000000000000002
Time, 2019-01-01T22:58:52, Epoch: 45, Batch: 240, Training Loss: 0.06733201891183853, LR: 0.010000000000000002
Time, 2019-01-01T22:58:53, Epoch: 45, Batch: 250, Training Loss: 0.0436507772654295, LR: 0.010000000000000002
Time, 2019-01-01T22:58:53, Epoch: 45, Batch: 260, Training Loss: 0.03897568508982659, LR: 0.010000000000000002
Time, 2019-01-01T22:58:54, Epoch: 45, Batch: 270, Training Loss: 0.04568340256810188, LR: 0.010000000000000002
Time, 2019-01-01T22:58:55, Epoch: 45, Batch: 280, Training Loss: 0.07238254882395267, LR: 0.010000000000000002
Time, 2019-01-01T22:58:56, Epoch: 45, Batch: 290, Training Loss: 0.04806830026209354, LR: 0.010000000000000002
Time, 2019-01-01T22:58:56, Epoch: 45, Batch: 300, Training Loss: 0.02993806414306164, LR: 0.010000000000000002
Time, 2019-01-01T22:58:57, Epoch: 45, Batch: 310, Training Loss: 0.0506372481584549, LR: 0.010000000000000002
Time, 2019-01-01T22:58:58, Epoch: 45, Batch: 320, Training Loss: 0.047729586437344554, LR: 0.010000000000000002
Time, 2019-01-01T22:58:59, Epoch: 45, Batch: 330, Training Loss: 0.04275979474186897, LR: 0.010000000000000002
Time, 2019-01-01T22:59:00, Epoch: 45, Batch: 340, Training Loss: 0.06396905183792115, LR: 0.010000000000000002
Time, 2019-01-01T22:59:00, Epoch: 45, Batch: 350, Training Loss: 0.04320664368569851, LR: 0.010000000000000002
Time, 2019-01-01T22:59:01, Epoch: 45, Batch: 360, Training Loss: 0.05281109996140003, LR: 0.010000000000000002
Time, 2019-01-01T22:59:02, Epoch: 45, Batch: 370, Training Loss: 0.041123007982969285, LR: 0.010000000000000002
Time, 2019-01-01T22:59:03, Epoch: 45, Batch: 380, Training Loss: 0.06670961789786815, LR: 0.010000000000000002
Time, 2019-01-01T22:59:03, Epoch: 45, Batch: 390, Training Loss: 0.04034336432814598, LR: 0.010000000000000002
Time, 2019-01-01T22:59:04, Epoch: 45, Batch: 400, Training Loss: 0.04359496012330055, LR: 0.010000000000000002
Time, 2019-01-01T22:59:05, Epoch: 45, Batch: 410, Training Loss: 0.03862640038132668, LR: 0.010000000000000002
Time, 2019-01-01T22:59:06, Epoch: 45, Batch: 420, Training Loss: 0.04382041804492474, LR: 0.010000000000000002
Time, 2019-01-01T22:59:06, Epoch: 45, Batch: 430, Training Loss: 0.03647105321288109, LR: 0.010000000000000002
Time, 2019-01-01T22:59:07, Epoch: 45, Batch: 440, Training Loss: 0.05577659718692303, LR: 0.010000000000000002
Time, 2019-01-01T22:59:08, Epoch: 45, Batch: 450, Training Loss: 0.06445354893803597, LR: 0.010000000000000002
Time, 2019-01-01T22:59:09, Epoch: 45, Batch: 460, Training Loss: 0.05643528923392296, LR: 0.010000000000000002
Time, 2019-01-01T22:59:09, Epoch: 45, Batch: 470, Training Loss: 0.04884711615741253, LR: 0.010000000000000002
Time, 2019-01-01T22:59:10, Epoch: 45, Batch: 480, Training Loss: 0.058135582506656645, LR: 0.010000000000000002
Time, 2019-01-01T22:59:11, Epoch: 45, Batch: 490, Training Loss: 0.05730382353067398, LR: 0.010000000000000002
Time, 2019-01-01T22:59:12, Epoch: 45, Batch: 500, Training Loss: 0.04233090355992317, LR: 0.010000000000000002
Time, 2019-01-01T22:59:12, Epoch: 45, Batch: 510, Training Loss: 0.04075261764228344, LR: 0.010000000000000002
Time, 2019-01-01T22:59:13, Epoch: 45, Batch: 520, Training Loss: 0.03193480521440506, LR: 0.010000000000000002
Time, 2019-01-01T22:59:14, Epoch: 45, Batch: 530, Training Loss: 0.05122470781207085, LR: 0.010000000000000002
Time, 2019-01-01T22:59:15, Epoch: 45, Batch: 540, Training Loss: 0.0413705937564373, LR: 0.010000000000000002
Time, 2019-01-01T22:59:16, Epoch: 45, Batch: 550, Training Loss: 0.07134825363755226, LR: 0.010000000000000002
Time, 2019-01-01T22:59:16, Epoch: 45, Batch: 560, Training Loss: 0.036318638175725934, LR: 0.010000000000000002
Time, 2019-01-01T22:59:17, Epoch: 45, Batch: 570, Training Loss: 0.04746899604797363, LR: 0.010000000000000002
Time, 2019-01-01T22:59:18, Epoch: 45, Batch: 580, Training Loss: 0.05789549872279167, LR: 0.010000000000000002
Time, 2019-01-01T22:59:19, Epoch: 45, Batch: 590, Training Loss: 0.04983602985739708, LR: 0.010000000000000002
Time, 2019-01-01T22:59:19, Epoch: 45, Batch: 600, Training Loss: 0.04452315755188465, LR: 0.010000000000000002
Time, 2019-01-01T22:59:20, Epoch: 45, Batch: 610, Training Loss: 0.03360977657139301, LR: 0.010000000000000002
Time, 2019-01-01T22:59:21, Epoch: 45, Batch: 620, Training Loss: 0.04354320764541626, LR: 0.010000000000000002
Time, 2019-01-01T22:59:22, Epoch: 45, Batch: 630, Training Loss: 0.04987269751727581, LR: 0.010000000000000002
Time, 2019-01-01T22:59:22, Epoch: 45, Batch: 640, Training Loss: 0.05315000340342522, LR: 0.010000000000000002
Time, 2019-01-01T22:59:23, Epoch: 45, Batch: 650, Training Loss: 0.05488208308815956, LR: 0.010000000000000002
Time, 2019-01-01T22:59:24, Epoch: 45, Batch: 660, Training Loss: 0.03099484443664551, LR: 0.010000000000000002
Time, 2019-01-01T22:59:25, Epoch: 45, Batch: 670, Training Loss: 0.028372156992554666, LR: 0.010000000000000002
Time, 2019-01-01T22:59:25, Epoch: 45, Batch: 680, Training Loss: 0.05176246240735054, LR: 0.010000000000000002
Time, 2019-01-01T22:59:26, Epoch: 45, Batch: 690, Training Loss: 0.05179606340825558, LR: 0.010000000000000002
Time, 2019-01-01T22:59:27, Epoch: 45, Batch: 700, Training Loss: 0.0698270007967949, LR: 0.010000000000000002
Time, 2019-01-01T22:59:28, Epoch: 45, Batch: 710, Training Loss: 0.059684839099645615, LR: 0.010000000000000002
Time, 2019-01-01T22:59:28, Epoch: 45, Batch: 720, Training Loss: 0.04183608517050743, LR: 0.010000000000000002
Time, 2019-01-01T22:59:29, Epoch: 45, Batch: 730, Training Loss: 0.08031427338719369, LR: 0.010000000000000002
Time, 2019-01-01T22:59:30, Epoch: 45, Batch: 740, Training Loss: 0.0463819358497858, LR: 0.010000000000000002
Time, 2019-01-01T22:59:31, Epoch: 45, Batch: 750, Training Loss: 0.041181841120123866, LR: 0.010000000000000002
Time, 2019-01-01T22:59:31, Epoch: 45, Batch: 760, Training Loss: 0.03425513692200184, LR: 0.010000000000000002
Time, 2019-01-01T22:59:32, Epoch: 45, Batch: 770, Training Loss: 0.03596362210810185, LR: 0.010000000000000002
Time, 2019-01-01T22:59:33, Epoch: 45, Batch: 780, Training Loss: 0.06089044995605945, LR: 0.010000000000000002
Time, 2019-01-01T22:59:34, Epoch: 45, Batch: 790, Training Loss: 0.06027490571141243, LR: 0.010000000000000002
Time, 2019-01-01T22:59:34, Epoch: 45, Batch: 800, Training Loss: 0.044608233496546745, LR: 0.010000000000000002
Time, 2019-01-01T22:59:35, Epoch: 45, Batch: 810, Training Loss: 0.05436518006026745, LR: 0.010000000000000002
Time, 2019-01-01T22:59:36, Epoch: 45, Batch: 820, Training Loss: 0.037382112443447114, LR: 0.010000000000000002
Time, 2019-01-01T22:59:37, Epoch: 45, Batch: 830, Training Loss: 0.05285674296319485, LR: 0.010000000000000002
Time, 2019-01-01T22:59:37, Epoch: 45, Batch: 840, Training Loss: 0.06621132530272007, LR: 0.010000000000000002
Time, 2019-01-01T22:59:38, Epoch: 45, Batch: 850, Training Loss: 0.05229510478675366, LR: 0.010000000000000002
Time, 2019-01-01T22:59:39, Epoch: 45, Batch: 860, Training Loss: 0.06482811048626899, LR: 0.010000000000000002
Time, 2019-01-01T22:59:40, Epoch: 45, Batch: 870, Training Loss: 0.05578630417585373, LR: 0.010000000000000002
Time, 2019-01-01T22:59:41, Epoch: 45, Batch: 880, Training Loss: 0.049008240923285484, LR: 0.010000000000000002
Time, 2019-01-01T22:59:41, Epoch: 45, Batch: 890, Training Loss: 0.029173722490668297, LR: 0.010000000000000002
Time, 2019-01-01T22:59:42, Epoch: 45, Batch: 900, Training Loss: 0.06474904045462608, LR: 0.010000000000000002
Time, 2019-01-01T22:59:43, Epoch: 45, Batch: 910, Training Loss: 0.037534261122345924, LR: 0.010000000000000002
Time, 2019-01-01T22:59:44, Epoch: 45, Batch: 920, Training Loss: 0.04442900009453297, LR: 0.010000000000000002
Time, 2019-01-01T22:59:44, Epoch: 45, Batch: 930, Training Loss: 0.05772166028618812, LR: 0.010000000000000002
Epoch: 45, Validation Top 1 acc: 98.17874145507812
Epoch: 45, Validation Top 5 acc: 100.0
Epoch: 45, Validation Set Loss: 0.05955295264720917
Start training epoch 46
Time, 2019-01-01T22:59:50, Epoch: 46, Batch: 10, Training Loss: 0.04646782986819744, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:51, Epoch: 46, Batch: 20, Training Loss: 0.04249798245728016, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:52, Epoch: 46, Batch: 30, Training Loss: 0.05608494207262993, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:52, Epoch: 46, Batch: 40, Training Loss: 0.043732475489377975, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:53, Epoch: 46, Batch: 50, Training Loss: 0.04679411873221397, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:54, Epoch: 46, Batch: 60, Training Loss: 0.029980893060564996, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:54, Epoch: 46, Batch: 70, Training Loss: 0.03245711028575897, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:55, Epoch: 46, Batch: 80, Training Loss: 0.042851772904396054, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:56, Epoch: 46, Batch: 90, Training Loss: 0.03435385078191757, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:57, Epoch: 46, Batch: 100, Training Loss: 0.032542822510004045, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:57, Epoch: 46, Batch: 110, Training Loss: 0.026897353678941728, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:58, Epoch: 46, Batch: 120, Training Loss: 0.04910434521734715, LR: 0.0010000000000000002
Time, 2019-01-01T22:59:59, Epoch: 46, Batch: 130, Training Loss: 0.03592349588871002, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:00, Epoch: 46, Batch: 140, Training Loss: 0.03574926368892193, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:00, Epoch: 46, Batch: 150, Training Loss: 0.05782408267259598, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:01, Epoch: 46, Batch: 160, Training Loss: 0.05296638160943985, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:02, Epoch: 46, Batch: 170, Training Loss: 0.06089613512158394, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:03, Epoch: 46, Batch: 180, Training Loss: 0.044229303300380704, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:03, Epoch: 46, Batch: 190, Training Loss: 0.04270907714962959, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:04, Epoch: 46, Batch: 200, Training Loss: 0.05320154763758182, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:05, Epoch: 46, Batch: 210, Training Loss: 0.04186094515025616, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:06, Epoch: 46, Batch: 220, Training Loss: 0.05539279021322727, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:06, Epoch: 46, Batch: 230, Training Loss: 0.04291796013712883, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:07, Epoch: 46, Batch: 240, Training Loss: 0.031683916598558425, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:08, Epoch: 46, Batch: 250, Training Loss: 0.04382256828248501, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:08, Epoch: 46, Batch: 260, Training Loss: 0.04944359436631203, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:09, Epoch: 46, Batch: 270, Training Loss: 0.06339684464037418, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:10, Epoch: 46, Batch: 280, Training Loss: 0.04691569171845913, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:11, Epoch: 46, Batch: 290, Training Loss: 0.056467875465750696, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:11, Epoch: 46, Batch: 300, Training Loss: 0.04264569170773029, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:12, Epoch: 46, Batch: 310, Training Loss: 0.039400779828429225, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:13, Epoch: 46, Batch: 320, Training Loss: 0.04806692376732826, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:14, Epoch: 46, Batch: 330, Training Loss: 0.05997037850320339, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:14, Epoch: 46, Batch: 340, Training Loss: 0.04806633926928043, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:15, Epoch: 46, Batch: 350, Training Loss: 0.03179977647960186, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:16, Epoch: 46, Batch: 360, Training Loss: 0.040007703378796576, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:17, Epoch: 46, Batch: 370, Training Loss: 0.05195793882012367, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:17, Epoch: 46, Batch: 380, Training Loss: 0.051974967867136, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:18, Epoch: 46, Batch: 390, Training Loss: 0.03781433887779713, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:19, Epoch: 46, Batch: 400, Training Loss: 0.04324292615056038, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:20, Epoch: 46, Batch: 410, Training Loss: 0.044641489163041115, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:20, Epoch: 46, Batch: 420, Training Loss: 0.048374570161104205, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:21, Epoch: 46, Batch: 430, Training Loss: 0.043679036945104596, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:22, Epoch: 46, Batch: 440, Training Loss: 0.03344004601240158, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:22, Epoch: 46, Batch: 450, Training Loss: 0.03896714225411415, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:23, Epoch: 46, Batch: 460, Training Loss: 0.04908054694533348, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:24, Epoch: 46, Batch: 470, Training Loss: 0.028790786862373352, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:25, Epoch: 46, Batch: 480, Training Loss: 0.03931348137557507, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:25, Epoch: 46, Batch: 490, Training Loss: 0.03249565288424492, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:26, Epoch: 46, Batch: 500, Training Loss: 0.059482358396053314, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:27, Epoch: 46, Batch: 510, Training Loss: 0.04490206316113472, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:28, Epoch: 46, Batch: 520, Training Loss: 0.041069041937589645, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:28, Epoch: 46, Batch: 530, Training Loss: 0.0530688650906086, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:29, Epoch: 46, Batch: 540, Training Loss: 0.04001156091690063, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:30, Epoch: 46, Batch: 550, Training Loss: 0.040013037621974945, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:31, Epoch: 46, Batch: 560, Training Loss: 0.05609624162316322, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:31, Epoch: 46, Batch: 570, Training Loss: 0.041855264082551004, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:32, Epoch: 46, Batch: 580, Training Loss: 0.048620561882853505, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:33, Epoch: 46, Batch: 590, Training Loss: 0.04245670400559902, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:34, Epoch: 46, Batch: 600, Training Loss: 0.05611709766089916, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:34, Epoch: 46, Batch: 610, Training Loss: 0.040493269264698026, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:35, Epoch: 46, Batch: 620, Training Loss: 0.0398422971367836, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:36, Epoch: 46, Batch: 630, Training Loss: 0.05576573014259338, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:37, Epoch: 46, Batch: 640, Training Loss: 0.043488851934671405, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:37, Epoch: 46, Batch: 650, Training Loss: 0.03855176344513893, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:38, Epoch: 46, Batch: 660, Training Loss: 0.05315536484122276, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:39, Epoch: 46, Batch: 670, Training Loss: 0.05100569911301136, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:40, Epoch: 46, Batch: 680, Training Loss: 0.037900153920054434, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:40, Epoch: 46, Batch: 690, Training Loss: 0.03971325159072876, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:41, Epoch: 46, Batch: 700, Training Loss: 0.04066348224878311, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:42, Epoch: 46, Batch: 710, Training Loss: 0.027786242589354516, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:43, Epoch: 46, Batch: 720, Training Loss: 0.060618549957871436, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:43, Epoch: 46, Batch: 730, Training Loss: 0.03444375321269035, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:44, Epoch: 46, Batch: 740, Training Loss: 0.029191245883703233, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:45, Epoch: 46, Batch: 750, Training Loss: 0.03480069041252136, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:46, Epoch: 46, Batch: 760, Training Loss: 0.056128207594156265, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:47, Epoch: 46, Batch: 770, Training Loss: 0.05631403625011444, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:47, Epoch: 46, Batch: 780, Training Loss: 0.02501213438808918, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:48, Epoch: 46, Batch: 790, Training Loss: 0.027531952410936356, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:49, Epoch: 46, Batch: 800, Training Loss: 0.056410381197929384, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:50, Epoch: 46, Batch: 810, Training Loss: 0.03801867067813873, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:51, Epoch: 46, Batch: 820, Training Loss: 0.04516035094857216, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:52, Epoch: 46, Batch: 830, Training Loss: 0.048286973312497136, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:52, Epoch: 46, Batch: 840, Training Loss: 0.03894547857344151, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:53, Epoch: 46, Batch: 850, Training Loss: 0.0755379043519497, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:54, Epoch: 46, Batch: 860, Training Loss: 0.038863498345017435, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:55, Epoch: 46, Batch: 870, Training Loss: 0.052548124268651006, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:55, Epoch: 46, Batch: 880, Training Loss: 0.04858954139053821, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:56, Epoch: 46, Batch: 890, Training Loss: 0.030574027448892593, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:57, Epoch: 46, Batch: 900, Training Loss: 0.05747140161693096, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:58, Epoch: 46, Batch: 910, Training Loss: 0.03888854086399078, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:58, Epoch: 46, Batch: 920, Training Loss: 0.033364716172218326, LR: 0.0010000000000000002
Time, 2019-01-01T23:00:59, Epoch: 46, Batch: 930, Training Loss: 0.02623356059193611, LR: 0.0010000000000000002
Epoch: 46, Validation Top 1 acc: 98.5469741821289
Epoch: 46, Validation Top 5 acc: 100.0
Epoch: 46, Validation Set Loss: 0.0466344952583313
Start training epoch 47
Time, 2019-01-01T23:01:05, Epoch: 47, Batch: 10, Training Loss: 0.0423282541334629, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:06, Epoch: 47, Batch: 20, Training Loss: 0.02942579984664917, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:06, Epoch: 47, Batch: 30, Training Loss: 0.033417532593011855, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:07, Epoch: 47, Batch: 40, Training Loss: 0.033320581540465355, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:08, Epoch: 47, Batch: 50, Training Loss: 0.05216883160173893, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:09, Epoch: 47, Batch: 60, Training Loss: 0.035731210559606555, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:09, Epoch: 47, Batch: 70, Training Loss: 0.04484877251088619, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:10, Epoch: 47, Batch: 80, Training Loss: 0.05317895896732807, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:11, Epoch: 47, Batch: 90, Training Loss: 0.045480279996991155, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:12, Epoch: 47, Batch: 100, Training Loss: 0.038229770585894586, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:12, Epoch: 47, Batch: 110, Training Loss: 0.025055433809757232, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:13, Epoch: 47, Batch: 120, Training Loss: 0.05372842736542225, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:14, Epoch: 47, Batch: 130, Training Loss: 0.05461125932633877, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:15, Epoch: 47, Batch: 140, Training Loss: 0.035245006904006004, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:15, Epoch: 47, Batch: 150, Training Loss: 0.03593786172568798, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:16, Epoch: 47, Batch: 160, Training Loss: 0.041266569122672084, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:17, Epoch: 47, Batch: 170, Training Loss: 0.05047040805220604, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:18, Epoch: 47, Batch: 180, Training Loss: 0.03566749766469002, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:18, Epoch: 47, Batch: 190, Training Loss: 0.05627623498439789, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:19, Epoch: 47, Batch: 200, Training Loss: 0.04044695608317852, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:20, Epoch: 47, Batch: 210, Training Loss: 0.017977892607450485, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:21, Epoch: 47, Batch: 220, Training Loss: 0.04385116659104824, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:21, Epoch: 47, Batch: 230, Training Loss: 0.03508074283599853, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:22, Epoch: 47, Batch: 240, Training Loss: 0.04587944895029068, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:23, Epoch: 47, Batch: 250, Training Loss: 0.039551857858896255, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:24, Epoch: 47, Batch: 260, Training Loss: 0.041862019896507265, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:24, Epoch: 47, Batch: 270, Training Loss: 0.0357286736369133, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:25, Epoch: 47, Batch: 280, Training Loss: 0.06447306536138057, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:26, Epoch: 47, Batch: 290, Training Loss: 0.04701225534081459, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:27, Epoch: 47, Batch: 300, Training Loss: 0.028227713704109193, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:27, Epoch: 47, Batch: 310, Training Loss: 0.033348501473665235, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:28, Epoch: 47, Batch: 320, Training Loss: 0.04410049244761467, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:29, Epoch: 47, Batch: 330, Training Loss: 0.0567954495549202, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:30, Epoch: 47, Batch: 340, Training Loss: 0.036450835317373274, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:30, Epoch: 47, Batch: 350, Training Loss: 0.024957390129566194, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:31, Epoch: 47, Batch: 360, Training Loss: 0.03885647505521774, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:32, Epoch: 47, Batch: 370, Training Loss: 0.04013165235519409, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:33, Epoch: 47, Batch: 380, Training Loss: 0.05402719117701053, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:33, Epoch: 47, Batch: 390, Training Loss: 0.04009020589292049, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:34, Epoch: 47, Batch: 400, Training Loss: 0.03511533737182617, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:35, Epoch: 47, Batch: 410, Training Loss: 0.04231571927666664, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:36, Epoch: 47, Batch: 420, Training Loss: 0.04701012521982193, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:37, Epoch: 47, Batch: 430, Training Loss: 0.04140515923500061, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:37, Epoch: 47, Batch: 440, Training Loss: 0.025534385442733766, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:38, Epoch: 47, Batch: 450, Training Loss: 0.05082344487309456, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:39, Epoch: 47, Batch: 460, Training Loss: 0.01956678070127964, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:40, Epoch: 47, Batch: 470, Training Loss: 0.05437744185328484, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:40, Epoch: 47, Batch: 480, Training Loss: 0.04478616341948509, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:41, Epoch: 47, Batch: 490, Training Loss: 0.05759485699236393, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:42, Epoch: 47, Batch: 500, Training Loss: 0.03827594369649887, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:43, Epoch: 47, Batch: 510, Training Loss: 0.025022893399000167, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:43, Epoch: 47, Batch: 520, Training Loss: 0.050408169627189636, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:44, Epoch: 47, Batch: 530, Training Loss: 0.0525961272418499, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:45, Epoch: 47, Batch: 540, Training Loss: 0.042377207428216934, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:46, Epoch: 47, Batch: 550, Training Loss: 0.049570585042238234, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:46, Epoch: 47, Batch: 560, Training Loss: 0.02700640633702278, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:47, Epoch: 47, Batch: 570, Training Loss: 0.027567318826913833, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:48, Epoch: 47, Batch: 580, Training Loss: 0.03406813852488995, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:49, Epoch: 47, Batch: 590, Training Loss: 0.03312404714524746, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:49, Epoch: 47, Batch: 600, Training Loss: 0.03172489069402218, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:50, Epoch: 47, Batch: 610, Training Loss: 0.041192546114325525, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:51, Epoch: 47, Batch: 620, Training Loss: 0.06791746281087399, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:52, Epoch: 47, Batch: 630, Training Loss: 0.029064865410327913, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:52, Epoch: 47, Batch: 640, Training Loss: 0.0473546177148819, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:53, Epoch: 47, Batch: 650, Training Loss: 0.05577574819326401, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:54, Epoch: 47, Batch: 660, Training Loss: 0.04614526927471161, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:55, Epoch: 47, Batch: 670, Training Loss: 0.038361915946006776, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:55, Epoch: 47, Batch: 680, Training Loss: 0.06636836975812913, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:56, Epoch: 47, Batch: 690, Training Loss: 0.03469223752617836, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:57, Epoch: 47, Batch: 700, Training Loss: 0.03157544806599617, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:58, Epoch: 47, Batch: 710, Training Loss: 0.06389773301780224, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:58, Epoch: 47, Batch: 720, Training Loss: 0.028142786771059036, LR: 0.0010000000000000002
Time, 2019-01-01T23:01:59, Epoch: 47, Batch: 730, Training Loss: 0.04028167277574539, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:00, Epoch: 47, Batch: 740, Training Loss: 0.05611790455877781, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:01, Epoch: 47, Batch: 750, Training Loss: 0.05199174806475639, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:01, Epoch: 47, Batch: 760, Training Loss: 0.04311394356191158, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:02, Epoch: 47, Batch: 770, Training Loss: 0.03276356086134911, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:03, Epoch: 47, Batch: 780, Training Loss: 0.043306111544370654, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:03, Epoch: 47, Batch: 790, Training Loss: 0.03541287891566754, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:04, Epoch: 47, Batch: 800, Training Loss: 0.04023662805557251, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:05, Epoch: 47, Batch: 810, Training Loss: 0.04119055792689323, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:06, Epoch: 47, Batch: 820, Training Loss: 0.04448282048106193, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:06, Epoch: 47, Batch: 830, Training Loss: 0.06050146482884884, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:07, Epoch: 47, Batch: 840, Training Loss: 0.03095192015171051, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:08, Epoch: 47, Batch: 850, Training Loss: 0.04270820766687393, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:09, Epoch: 47, Batch: 860, Training Loss: 0.04119946360588074, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:09, Epoch: 47, Batch: 870, Training Loss: 0.039217110723257065, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:10, Epoch: 47, Batch: 880, Training Loss: 0.04317091777920723, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:11, Epoch: 47, Batch: 890, Training Loss: 0.047725212574005124, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:12, Epoch: 47, Batch: 900, Training Loss: 0.04528553485870361, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:12, Epoch: 47, Batch: 910, Training Loss: 0.028290852904319763, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:13, Epoch: 47, Batch: 920, Training Loss: 0.035783526673913005, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:14, Epoch: 47, Batch: 930, Training Loss: 0.05260004922747612, LR: 0.0010000000000000002
Epoch: 47, Validation Top 1 acc: 98.606689453125
Epoch: 47, Validation Top 5 acc: 100.0
Epoch: 47, Validation Set Loss: 0.045775532722473145
Start training epoch 48
Time, 2019-01-01T23:02:20, Epoch: 48, Batch: 10, Training Loss: 0.03303943276405334, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:21, Epoch: 48, Batch: 20, Training Loss: 0.0344156451523304, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:21, Epoch: 48, Batch: 30, Training Loss: 0.040204350650310514, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:22, Epoch: 48, Batch: 40, Training Loss: 0.030252788215875626, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:23, Epoch: 48, Batch: 50, Training Loss: 0.05178721249103546, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:24, Epoch: 48, Batch: 60, Training Loss: 0.03475634828209877, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:25, Epoch: 48, Batch: 70, Training Loss: 0.028696494922041894, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:25, Epoch: 48, Batch: 80, Training Loss: 0.029011107236146926, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:26, Epoch: 48, Batch: 90, Training Loss: 0.03674786761403084, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:27, Epoch: 48, Batch: 100, Training Loss: 0.04160993881523609, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:28, Epoch: 48, Batch: 110, Training Loss: 0.03334474116563797, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:28, Epoch: 48, Batch: 120, Training Loss: 0.02430327981710434, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:29, Epoch: 48, Batch: 130, Training Loss: 0.041699722036719324, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:30, Epoch: 48, Batch: 140, Training Loss: 0.03931984007358551, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:31, Epoch: 48, Batch: 150, Training Loss: 0.04855523481965065, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:31, Epoch: 48, Batch: 160, Training Loss: 0.07181185260415077, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:32, Epoch: 48, Batch: 170, Training Loss: 0.03338325321674347, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:33, Epoch: 48, Batch: 180, Training Loss: 0.0578410442918539, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:34, Epoch: 48, Batch: 190, Training Loss: 0.0411944393068552, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:34, Epoch: 48, Batch: 200, Training Loss: 0.041116205602884294, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:35, Epoch: 48, Batch: 210, Training Loss: 0.0424156665802002, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:36, Epoch: 48, Batch: 220, Training Loss: 0.03655321635305882, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:37, Epoch: 48, Batch: 230, Training Loss: 0.029114604368805884, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:38, Epoch: 48, Batch: 240, Training Loss: 0.03424757048487663, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:39, Epoch: 48, Batch: 250, Training Loss: 0.035910486429929736, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:39, Epoch: 48, Batch: 260, Training Loss: 0.047645439952611925, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:40, Epoch: 48, Batch: 270, Training Loss: 0.028434757515788077, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:41, Epoch: 48, Batch: 280, Training Loss: 0.042318827286362645, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:42, Epoch: 48, Batch: 290, Training Loss: 0.05365862362086773, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:43, Epoch: 48, Batch: 300, Training Loss: 0.04857460297644138, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:44, Epoch: 48, Batch: 310, Training Loss: 0.03825613297522068, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:44, Epoch: 48, Batch: 320, Training Loss: 0.041714951395988464, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:45, Epoch: 48, Batch: 330, Training Loss: 0.034847109019756316, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:46, Epoch: 48, Batch: 340, Training Loss: 0.04622399881482124, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:47, Epoch: 48, Batch: 350, Training Loss: 0.04272644072771072, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:47, Epoch: 48, Batch: 360, Training Loss: 0.04151122495532036, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:48, Epoch: 48, Batch: 370, Training Loss: 0.0435741476714611, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:49, Epoch: 48, Batch: 380, Training Loss: 0.04240378886461258, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:50, Epoch: 48, Batch: 390, Training Loss: 0.030704075843095778, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:50, Epoch: 48, Batch: 400, Training Loss: 0.04233136251568794, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:51, Epoch: 48, Batch: 410, Training Loss: 0.05652289465069771, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:52, Epoch: 48, Batch: 420, Training Loss: 0.03740703240036965, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:53, Epoch: 48, Batch: 430, Training Loss: 0.03949729911983013, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:54, Epoch: 48, Batch: 440, Training Loss: 0.057375024259090426, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:54, Epoch: 48, Batch: 450, Training Loss: 0.034088924154639245, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:55, Epoch: 48, Batch: 460, Training Loss: 0.05269425101578236, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:56, Epoch: 48, Batch: 470, Training Loss: 0.03296527452766895, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:57, Epoch: 48, Batch: 480, Training Loss: 0.036324620619416235, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:58, Epoch: 48, Batch: 490, Training Loss: 0.04823848344385624, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:58, Epoch: 48, Batch: 500, Training Loss: 0.04663383066654205, LR: 0.0010000000000000002
Time, 2019-01-01T23:02:59, Epoch: 48, Batch: 510, Training Loss: 0.04191635735332966, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:00, Epoch: 48, Batch: 520, Training Loss: 0.027734622359275818, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:00, Epoch: 48, Batch: 530, Training Loss: 0.03547825515270233, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:01, Epoch: 48, Batch: 540, Training Loss: 0.033180462941527364, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:02, Epoch: 48, Batch: 550, Training Loss: 0.05449000895023346, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:03, Epoch: 48, Batch: 560, Training Loss: 0.051389056444168094, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:03, Epoch: 48, Batch: 570, Training Loss: 0.0314034815877676, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:04, Epoch: 48, Batch: 580, Training Loss: 0.04466156624257565, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:05, Epoch: 48, Batch: 590, Training Loss: 0.0495457798242569, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:06, Epoch: 48, Batch: 600, Training Loss: 0.03899557776749134, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:06, Epoch: 48, Batch: 610, Training Loss: 0.04110870100557804, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:07, Epoch: 48, Batch: 620, Training Loss: 0.03715480044484139, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:08, Epoch: 48, Batch: 630, Training Loss: 0.04302320741117001, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:09, Epoch: 48, Batch: 640, Training Loss: 0.0330192219465971, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:09, Epoch: 48, Batch: 650, Training Loss: 0.03294138200581074, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:10, Epoch: 48, Batch: 660, Training Loss: 0.04843977987766266, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:11, Epoch: 48, Batch: 670, Training Loss: 0.06855043470859527, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:12, Epoch: 48, Batch: 680, Training Loss: 0.05211577340960503, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:12, Epoch: 48, Batch: 690, Training Loss: 0.04755039252340794, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:13, Epoch: 48, Batch: 700, Training Loss: 0.021127934008836745, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:14, Epoch: 48, Batch: 710, Training Loss: 0.032914165034890176, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:15, Epoch: 48, Batch: 720, Training Loss: 0.04425987154245377, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:15, Epoch: 48, Batch: 730, Training Loss: 0.04021973460912705, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:16, Epoch: 48, Batch: 740, Training Loss: 0.04746657647192478, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:17, Epoch: 48, Batch: 750, Training Loss: 0.02302076928317547, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:18, Epoch: 48, Batch: 760, Training Loss: 0.04207940064370632, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:18, Epoch: 48, Batch: 770, Training Loss: 0.03604797013103962, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:19, Epoch: 48, Batch: 780, Training Loss: 0.04011132605373859, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:20, Epoch: 48, Batch: 790, Training Loss: 0.04616201370954513, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:21, Epoch: 48, Batch: 800, Training Loss: 0.029860082268714904, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:21, Epoch: 48, Batch: 810, Training Loss: 0.04115712605416775, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:22, Epoch: 48, Batch: 820, Training Loss: 0.06104378253221512, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:23, Epoch: 48, Batch: 830, Training Loss: 0.06389874219894409, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:24, Epoch: 48, Batch: 840, Training Loss: 0.049646387621760366, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:24, Epoch: 48, Batch: 850, Training Loss: 0.03627967536449432, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:25, Epoch: 48, Batch: 860, Training Loss: 0.05447316467761994, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:26, Epoch: 48, Batch: 870, Training Loss: 0.04118214398622513, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:27, Epoch: 48, Batch: 880, Training Loss: 0.035183293372392656, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:27, Epoch: 48, Batch: 890, Training Loss: 0.04545804411172867, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:28, Epoch: 48, Batch: 900, Training Loss: 0.050153513252735135, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:29, Epoch: 48, Batch: 910, Training Loss: 0.05203516930341721, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:30, Epoch: 48, Batch: 920, Training Loss: 0.03081745281815529, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:30, Epoch: 48, Batch: 930, Training Loss: 0.04279547706246376, LR: 0.0010000000000000002
Epoch: 48, Validation Top 1 acc: 98.57682800292969
Epoch: 48, Validation Top 5 acc: 100.0
Epoch: 48, Validation Set Loss: 0.044949475675821304
Start training epoch 49
Time, 2019-01-01T23:03:36, Epoch: 49, Batch: 10, Training Loss: 0.036295737326145175, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:37, Epoch: 49, Batch: 20, Training Loss: 0.050023337826132774, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:38, Epoch: 49, Batch: 30, Training Loss: 0.0462879192084074, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:39, Epoch: 49, Batch: 40, Training Loss: 0.040880268812179564, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:39, Epoch: 49, Batch: 50, Training Loss: 0.04969953261315822, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:40, Epoch: 49, Batch: 60, Training Loss: 0.054219605028629304, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:41, Epoch: 49, Batch: 70, Training Loss: 0.05059307441115379, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:42, Epoch: 49, Batch: 80, Training Loss: 0.061451032757759094, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:42, Epoch: 49, Batch: 90, Training Loss: 0.05336507558822632, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:43, Epoch: 49, Batch: 100, Training Loss: 0.05225263498723507, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:44, Epoch: 49, Batch: 110, Training Loss: 0.03735676258802414, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:45, Epoch: 49, Batch: 120, Training Loss: 0.051625219732522966, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:45, Epoch: 49, Batch: 130, Training Loss: 0.037966611236333846, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:46, Epoch: 49, Batch: 140, Training Loss: 0.047082720696926116, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:47, Epoch: 49, Batch: 150, Training Loss: 0.025967563316226006, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:48, Epoch: 49, Batch: 160, Training Loss: 0.04444140680134297, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:48, Epoch: 49, Batch: 170, Training Loss: 0.047784759849309924, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:49, Epoch: 49, Batch: 180, Training Loss: 0.03988409824669361, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:50, Epoch: 49, Batch: 190, Training Loss: 0.04296023473143577, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:51, Epoch: 49, Batch: 200, Training Loss: 0.032125961408019064, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:51, Epoch: 49, Batch: 210, Training Loss: 0.04965831078588963, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:52, Epoch: 49, Batch: 220, Training Loss: 0.037959179282188414, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:53, Epoch: 49, Batch: 230, Training Loss: 0.04158499166369438, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:54, Epoch: 49, Batch: 240, Training Loss: 0.0470144011080265, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:54, Epoch: 49, Batch: 250, Training Loss: 0.04101612120866775, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:55, Epoch: 49, Batch: 260, Training Loss: 0.04129604324698448, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:56, Epoch: 49, Batch: 270, Training Loss: 0.0647322803735733, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:57, Epoch: 49, Batch: 280, Training Loss: 0.06779314689338208, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:58, Epoch: 49, Batch: 290, Training Loss: 0.04667118899524212, LR: 0.0010000000000000002
Time, 2019-01-01T23:03:59, Epoch: 49, Batch: 300, Training Loss: 0.03329090736806393, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:00, Epoch: 49, Batch: 310, Training Loss: 0.03278741352260113, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:00, Epoch: 49, Batch: 320, Training Loss: 0.03260553516447544, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:01, Epoch: 49, Batch: 330, Training Loss: 0.04858230501413345, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:02, Epoch: 49, Batch: 340, Training Loss: 0.043863532692193986, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:03, Epoch: 49, Batch: 350, Training Loss: 0.028596919402480124, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:04, Epoch: 49, Batch: 360, Training Loss: 0.04129097536206246, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:05, Epoch: 49, Batch: 370, Training Loss: 0.028995963186025618, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:06, Epoch: 49, Batch: 380, Training Loss: 0.049888289719820025, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:06, Epoch: 49, Batch: 390, Training Loss: 0.043999938666820525, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:07, Epoch: 49, Batch: 400, Training Loss: 0.03214699812233448, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:08, Epoch: 49, Batch: 410, Training Loss: 0.05151214525103569, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:09, Epoch: 49, Batch: 420, Training Loss: 0.036886004731059074, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:09, Epoch: 49, Batch: 430, Training Loss: 0.036522071436047555, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:10, Epoch: 49, Batch: 440, Training Loss: 0.028950998187065126, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:11, Epoch: 49, Batch: 450, Training Loss: 0.034515947103500366, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:12, Epoch: 49, Batch: 460, Training Loss: 0.04325622245669365, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:12, Epoch: 49, Batch: 470, Training Loss: 0.05123971924185753, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:13, Epoch: 49, Batch: 480, Training Loss: 0.04624550193548203, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:14, Epoch: 49, Batch: 490, Training Loss: 0.026363836601376534, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:15, Epoch: 49, Batch: 500, Training Loss: 0.03817916475236416, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:15, Epoch: 49, Batch: 510, Training Loss: 0.028644057363271712, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:16, Epoch: 49, Batch: 520, Training Loss: 0.024259132146835328, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:17, Epoch: 49, Batch: 530, Training Loss: 0.0332725390791893, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:17, Epoch: 49, Batch: 540, Training Loss: 0.05632988139986992, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:18, Epoch: 49, Batch: 550, Training Loss: 0.03859078101813793, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:19, Epoch: 49, Batch: 560, Training Loss: 0.04359510615468025, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:20, Epoch: 49, Batch: 570, Training Loss: 0.05130490884184837, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:21, Epoch: 49, Batch: 580, Training Loss: 0.021955849975347518, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:22, Epoch: 49, Batch: 590, Training Loss: 0.043935471773147584, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:22, Epoch: 49, Batch: 600, Training Loss: 0.04207112416625023, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:23, Epoch: 49, Batch: 610, Training Loss: 0.05838468782603741, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:24, Epoch: 49, Batch: 620, Training Loss: 0.038104018196463585, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:25, Epoch: 49, Batch: 630, Training Loss: 0.051527092978358266, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:26, Epoch: 49, Batch: 640, Training Loss: 0.04820910096168518, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:26, Epoch: 49, Batch: 650, Training Loss: 0.02975896969437599, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:27, Epoch: 49, Batch: 660, Training Loss: 0.026327114924788474, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:28, Epoch: 49, Batch: 670, Training Loss: 0.040254606306552886, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:29, Epoch: 49, Batch: 680, Training Loss: 0.029570139199495315, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:29, Epoch: 49, Batch: 690, Training Loss: 0.03252338096499443, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:30, Epoch: 49, Batch: 700, Training Loss: 0.041481399163603785, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:31, Epoch: 49, Batch: 710, Training Loss: 0.048252058774232866, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:32, Epoch: 49, Batch: 720, Training Loss: 0.02948491871356964, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:32, Epoch: 49, Batch: 730, Training Loss: 0.06255859211087227, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:33, Epoch: 49, Batch: 740, Training Loss: 0.03029452860355377, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:34, Epoch: 49, Batch: 750, Training Loss: 0.027233010903000832, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:35, Epoch: 49, Batch: 760, Training Loss: 0.0557385191321373, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:35, Epoch: 49, Batch: 770, Training Loss: 0.03440206050872803, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:36, Epoch: 49, Batch: 780, Training Loss: 0.0415501493960619, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:37, Epoch: 49, Batch: 790, Training Loss: 0.036503180861473083, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:38, Epoch: 49, Batch: 800, Training Loss: 0.03121511749923229, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:38, Epoch: 49, Batch: 810, Training Loss: 0.029445169121026994, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:39, Epoch: 49, Batch: 820, Training Loss: 0.029795711860060692, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:40, Epoch: 49, Batch: 830, Training Loss: 0.037166401743888855, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:41, Epoch: 49, Batch: 840, Training Loss: 0.04040123298764229, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:42, Epoch: 49, Batch: 850, Training Loss: 0.03647662959992885, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:42, Epoch: 49, Batch: 860, Training Loss: 0.0397975318133831, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:43, Epoch: 49, Batch: 870, Training Loss: 0.03943540975451469, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:44, Epoch: 49, Batch: 880, Training Loss: 0.04432845488190651, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:45, Epoch: 49, Batch: 890, Training Loss: 0.03281224817037583, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:45, Epoch: 49, Batch: 900, Training Loss: 0.03306101970374584, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:46, Epoch: 49, Batch: 910, Training Loss: 0.03372482284903526, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:47, Epoch: 49, Batch: 920, Training Loss: 0.026200990378856658, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:48, Epoch: 49, Batch: 930, Training Loss: 0.030227577313780785, LR: 0.0010000000000000002
Epoch: 49, Validation Top 1 acc: 98.57682800292969
Epoch: 49, Validation Top 5 acc: 100.0
Epoch: 49, Validation Set Loss: 0.044598739594221115
Start training epoch 50
Time, 2019-01-01T23:04:54, Epoch: 50, Batch: 10, Training Loss: 0.03322666361927986, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:54, Epoch: 50, Batch: 20, Training Loss: 0.04975246712565422, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:55, Epoch: 50, Batch: 30, Training Loss: 0.04246701896190643, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:56, Epoch: 50, Batch: 40, Training Loss: 0.02585969716310501, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:57, Epoch: 50, Batch: 50, Training Loss: 0.03179252967238426, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:57, Epoch: 50, Batch: 60, Training Loss: 0.03594279661774635, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:58, Epoch: 50, Batch: 70, Training Loss: 0.04364897198975086, LR: 0.0010000000000000002
Time, 2019-01-01T23:04:59, Epoch: 50, Batch: 80, Training Loss: 0.028714662417769432, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:00, Epoch: 50, Batch: 90, Training Loss: 0.03509748540818691, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:00, Epoch: 50, Batch: 100, Training Loss: 0.05818029046058655, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:01, Epoch: 50, Batch: 110, Training Loss: 0.03969322144985199, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:02, Epoch: 50, Batch: 120, Training Loss: 0.03116985857486725, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:03, Epoch: 50, Batch: 130, Training Loss: 0.03423374928534031, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:03, Epoch: 50, Batch: 140, Training Loss: 0.032371538877487185, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:04, Epoch: 50, Batch: 150, Training Loss: 0.022179659083485605, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:05, Epoch: 50, Batch: 160, Training Loss: 0.03393978253006935, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:06, Epoch: 50, Batch: 170, Training Loss: 0.03975832127034664, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:06, Epoch: 50, Batch: 180, Training Loss: 0.05288248211145401, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:07, Epoch: 50, Batch: 190, Training Loss: 0.03393063992261887, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:08, Epoch: 50, Batch: 200, Training Loss: 0.04267275370657444, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:09, Epoch: 50, Batch: 210, Training Loss: 0.04413013868033886, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:09, Epoch: 50, Batch: 220, Training Loss: 0.03615722954273224, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:10, Epoch: 50, Batch: 230, Training Loss: 0.04404413476586342, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:11, Epoch: 50, Batch: 240, Training Loss: 0.03327326253056526, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:12, Epoch: 50, Batch: 250, Training Loss: 0.05732676535844803, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:12, Epoch: 50, Batch: 260, Training Loss: 0.03554786518216133, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:13, Epoch: 50, Batch: 270, Training Loss: 0.031035974621772766, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:14, Epoch: 50, Batch: 280, Training Loss: 0.037194178253412244, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:15, Epoch: 50, Batch: 290, Training Loss: 0.026601892709732056, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:15, Epoch: 50, Batch: 300, Training Loss: 0.04755984880030155, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:16, Epoch: 50, Batch: 310, Training Loss: 0.03401885963976383, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:17, Epoch: 50, Batch: 320, Training Loss: 0.04721455350518226, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:18, Epoch: 50, Batch: 330, Training Loss: 0.04351476095616817, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:18, Epoch: 50, Batch: 340, Training Loss: 0.048041611164808276, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:19, Epoch: 50, Batch: 350, Training Loss: 0.03566752821207046, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:20, Epoch: 50, Batch: 360, Training Loss: 0.04803090952336788, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:21, Epoch: 50, Batch: 370, Training Loss: 0.031239693611860277, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:21, Epoch: 50, Batch: 380, Training Loss: 0.04754853323101997, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:22, Epoch: 50, Batch: 390, Training Loss: 0.03770568892359734, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:23, Epoch: 50, Batch: 400, Training Loss: 0.0403145182877779, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:24, Epoch: 50, Batch: 410, Training Loss: 0.054773806035518645, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:24, Epoch: 50, Batch: 420, Training Loss: 0.04362515658140183, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:25, Epoch: 50, Batch: 430, Training Loss: 0.07300015315413474, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:26, Epoch: 50, Batch: 440, Training Loss: 0.04582887478172779, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:27, Epoch: 50, Batch: 450, Training Loss: 0.03284820280969143, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:27, Epoch: 50, Batch: 460, Training Loss: 0.041827726364135745, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:28, Epoch: 50, Batch: 470, Training Loss: 0.038196080923080446, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:29, Epoch: 50, Batch: 480, Training Loss: 0.04194580093026161, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:30, Epoch: 50, Batch: 490, Training Loss: 0.036352986842393874, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:30, Epoch: 50, Batch: 500, Training Loss: 0.03871544450521469, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:31, Epoch: 50, Batch: 510, Training Loss: 0.022857316583395005, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:32, Epoch: 50, Batch: 520, Training Loss: 0.05247772075235844, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:33, Epoch: 50, Batch: 530, Training Loss: 0.043600814789533614, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:33, Epoch: 50, Batch: 540, Training Loss: 0.047543079033493994, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:34, Epoch: 50, Batch: 550, Training Loss: 0.02943795658648014, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:35, Epoch: 50, Batch: 560, Training Loss: 0.026678790524601936, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:36, Epoch: 50, Batch: 570, Training Loss: 0.03828714862465858, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:36, Epoch: 50, Batch: 580, Training Loss: 0.03519986569881439, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:37, Epoch: 50, Batch: 590, Training Loss: 0.025534959509968758, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:38, Epoch: 50, Batch: 600, Training Loss: 0.0444773655384779, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:39, Epoch: 50, Batch: 610, Training Loss: 0.031296716257929805, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:39, Epoch: 50, Batch: 620, Training Loss: 0.08469205126166343, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:40, Epoch: 50, Batch: 630, Training Loss: 0.062259625643491745, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:41, Epoch: 50, Batch: 640, Training Loss: 0.0366323996335268, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:42, Epoch: 50, Batch: 650, Training Loss: 0.043311852216720584, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:42, Epoch: 50, Batch: 660, Training Loss: 0.042396989092230795, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:43, Epoch: 50, Batch: 670, Training Loss: 0.04121008962392807, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:44, Epoch: 50, Batch: 680, Training Loss: 0.047104374319314954, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:45, Epoch: 50, Batch: 690, Training Loss: 0.031923065334558486, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:45, Epoch: 50, Batch: 700, Training Loss: 0.045270568877458575, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:46, Epoch: 50, Batch: 710, Training Loss: 0.04088009782135486, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:47, Epoch: 50, Batch: 720, Training Loss: 0.052264747023582456, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:48, Epoch: 50, Batch: 730, Training Loss: 0.045469877868890764, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:48, Epoch: 50, Batch: 740, Training Loss: 0.02741111218929291, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:49, Epoch: 50, Batch: 750, Training Loss: 0.035526356846094134, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:50, Epoch: 50, Batch: 760, Training Loss: 0.01917169950902462, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:51, Epoch: 50, Batch: 770, Training Loss: 0.03576978035271168, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:51, Epoch: 50, Batch: 780, Training Loss: 0.04852317608892918, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:52, Epoch: 50, Batch: 790, Training Loss: 0.04805562123656273, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:53, Epoch: 50, Batch: 800, Training Loss: 0.037714728713035585, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:53, Epoch: 50, Batch: 810, Training Loss: 0.03287730775773525, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:54, Epoch: 50, Batch: 820, Training Loss: 0.03740283921360969, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:55, Epoch: 50, Batch: 830, Training Loss: 0.03137301653623581, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:56, Epoch: 50, Batch: 840, Training Loss: 0.048374640941619876, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:57, Epoch: 50, Batch: 850, Training Loss: 0.03362867310643196, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:57, Epoch: 50, Batch: 860, Training Loss: 0.04748264849185944, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:58, Epoch: 50, Batch: 870, Training Loss: 0.037184471264481544, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:59, Epoch: 50, Batch: 880, Training Loss: 0.05771985650062561, LR: 0.0010000000000000002
Time, 2019-01-01T23:05:59, Epoch: 50, Batch: 890, Training Loss: 0.03386399671435356, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:00, Epoch: 50, Batch: 900, Training Loss: 0.03753831908106804, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:01, Epoch: 50, Batch: 910, Training Loss: 0.027973784506320952, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:02, Epoch: 50, Batch: 920, Training Loss: 0.029720132052898408, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:02, Epoch: 50, Batch: 930, Training Loss: 0.039361102133989335, LR: 0.0010000000000000002
Epoch: 50, Validation Top 1 acc: 98.59673309326172
Epoch: 50, Validation Top 5 acc: 100.0
Epoch: 50, Validation Set Loss: 0.0441889651119709
Start training epoch 51
Time, 2019-01-01T23:06:08, Epoch: 51, Batch: 10, Training Loss: 0.04816788733005524, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:09, Epoch: 51, Batch: 20, Training Loss: 0.027592656761407854, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:10, Epoch: 51, Batch: 30, Training Loss: 0.0396725133061409, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:11, Epoch: 51, Batch: 40, Training Loss: 0.05291137285530567, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:11, Epoch: 51, Batch: 50, Training Loss: 0.0499695174396038, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:12, Epoch: 51, Batch: 60, Training Loss: 0.049161184951663016, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:13, Epoch: 51, Batch: 70, Training Loss: 0.03297450877726078, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:14, Epoch: 51, Batch: 80, Training Loss: 0.04227425679564476, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:14, Epoch: 51, Batch: 90, Training Loss: 0.052285116910934445, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:15, Epoch: 51, Batch: 100, Training Loss: 0.039914308860898015, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:16, Epoch: 51, Batch: 110, Training Loss: 0.03069637604057789, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:17, Epoch: 51, Batch: 120, Training Loss: 0.025154079124331475, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:17, Epoch: 51, Batch: 130, Training Loss: 0.045389996096491816, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:18, Epoch: 51, Batch: 140, Training Loss: 0.037133775651454926, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:19, Epoch: 51, Batch: 150, Training Loss: 0.04041563682258129, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:20, Epoch: 51, Batch: 160, Training Loss: 0.03190348818898201, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:20, Epoch: 51, Batch: 170, Training Loss: 0.0319064911454916, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:21, Epoch: 51, Batch: 180, Training Loss: 0.03071685992181301, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:22, Epoch: 51, Batch: 190, Training Loss: 0.028222069889307023, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:23, Epoch: 51, Batch: 200, Training Loss: 0.053871817886829376, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:23, Epoch: 51, Batch: 210, Training Loss: 0.03073451966047287, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:24, Epoch: 51, Batch: 220, Training Loss: 0.05265928134322166, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:25, Epoch: 51, Batch: 230, Training Loss: 0.046570132672786715, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:26, Epoch: 51, Batch: 240, Training Loss: 0.038542382046580316, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:26, Epoch: 51, Batch: 250, Training Loss: 0.04654033109545708, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:27, Epoch: 51, Batch: 260, Training Loss: 0.04505114257335663, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:28, Epoch: 51, Batch: 270, Training Loss: 0.028597962483763693, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:29, Epoch: 51, Batch: 280, Training Loss: 0.03443226665258407, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:29, Epoch: 51, Batch: 290, Training Loss: 0.04505662731826306, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:30, Epoch: 51, Batch: 300, Training Loss: 0.045739563554525374, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:31, Epoch: 51, Batch: 310, Training Loss: 0.03292971588671208, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:32, Epoch: 51, Batch: 320, Training Loss: 0.03085681311786175, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:32, Epoch: 51, Batch: 330, Training Loss: 0.036284114420413974, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:33, Epoch: 51, Batch: 340, Training Loss: 0.02995098941028118, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:34, Epoch: 51, Batch: 350, Training Loss: 0.040753574669361116, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:35, Epoch: 51, Batch: 360, Training Loss: 0.06705633513629436, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:35, Epoch: 51, Batch: 370, Training Loss: 0.0443441066890955, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:36, Epoch: 51, Batch: 380, Training Loss: 0.03341736383736134, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:37, Epoch: 51, Batch: 390, Training Loss: 0.048769193887710574, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:37, Epoch: 51, Batch: 400, Training Loss: 0.04726868309080601, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:38, Epoch: 51, Batch: 410, Training Loss: 0.03138776682317257, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:39, Epoch: 51, Batch: 420, Training Loss: 0.037866855040192604, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:40, Epoch: 51, Batch: 430, Training Loss: 0.03573451153934002, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:40, Epoch: 51, Batch: 440, Training Loss: 0.04466565176844597, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:41, Epoch: 51, Batch: 450, Training Loss: 0.04097312241792679, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:42, Epoch: 51, Batch: 460, Training Loss: 0.048307597264647485, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:43, Epoch: 51, Batch: 470, Training Loss: 0.03600769974291325, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:43, Epoch: 51, Batch: 480, Training Loss: 0.04534388408064842, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:44, Epoch: 51, Batch: 490, Training Loss: 0.056688230112195016, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:45, Epoch: 51, Batch: 500, Training Loss: 0.04398347325623035, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:46, Epoch: 51, Batch: 510, Training Loss: 0.023553435504436494, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:46, Epoch: 51, Batch: 520, Training Loss: 0.03047281764447689, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:47, Epoch: 51, Batch: 530, Training Loss: 0.06499891206622124, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:48, Epoch: 51, Batch: 540, Training Loss: 0.055173120275139806, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:49, Epoch: 51, Batch: 550, Training Loss: 0.04347504749894142, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:49, Epoch: 51, Batch: 560, Training Loss: 0.029326914250850676, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:50, Epoch: 51, Batch: 570, Training Loss: 0.042149027436971666, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:51, Epoch: 51, Batch: 580, Training Loss: 0.03218275159597397, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:52, Epoch: 51, Batch: 590, Training Loss: 0.03106527253985405, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:53, Epoch: 51, Batch: 600, Training Loss: 0.047879526019096376, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:53, Epoch: 51, Batch: 610, Training Loss: 0.0255879707634449, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:54, Epoch: 51, Batch: 620, Training Loss: 0.03983759209513664, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:55, Epoch: 51, Batch: 630, Training Loss: 0.04076482579112053, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:56, Epoch: 51, Batch: 640, Training Loss: 0.04726754166185856, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:56, Epoch: 51, Batch: 650, Training Loss: 0.038288135454058646, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:57, Epoch: 51, Batch: 660, Training Loss: 0.032603596523404124, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:58, Epoch: 51, Batch: 670, Training Loss: 0.03173161149024963, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:59, Epoch: 51, Batch: 680, Training Loss: 0.04628130607306957, LR: 0.0010000000000000002
Time, 2019-01-01T23:06:59, Epoch: 51, Batch: 690, Training Loss: 0.02414069212973118, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:00, Epoch: 51, Batch: 700, Training Loss: 0.02880270332098007, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:01, Epoch: 51, Batch: 710, Training Loss: 0.025878826901316643, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:02, Epoch: 51, Batch: 720, Training Loss: 0.04347976669669151, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:02, Epoch: 51, Batch: 730, Training Loss: 0.03838791623711586, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:03, Epoch: 51, Batch: 740, Training Loss: 0.035001667216420174, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:04, Epoch: 51, Batch: 750, Training Loss: 0.02685730196535587, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:05, Epoch: 51, Batch: 760, Training Loss: 0.04731247127056122, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:06, Epoch: 51, Batch: 770, Training Loss: 0.04895023703575134, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:06, Epoch: 51, Batch: 780, Training Loss: 0.042468040436506274, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:07, Epoch: 51, Batch: 790, Training Loss: 0.04808838814496994, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:08, Epoch: 51, Batch: 800, Training Loss: 0.04750769697129727, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:09, Epoch: 51, Batch: 810, Training Loss: 0.038629936054348946, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:09, Epoch: 51, Batch: 820, Training Loss: 0.0436651386320591, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:10, Epoch: 51, Batch: 830, Training Loss: 0.031134646385908127, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:11, Epoch: 51, Batch: 840, Training Loss: 0.039210231974720955, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:12, Epoch: 51, Batch: 850, Training Loss: 0.04404090195894241, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:12, Epoch: 51, Batch: 860, Training Loss: 0.03336098156869412, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:13, Epoch: 51, Batch: 870, Training Loss: 0.04262200221419334, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:14, Epoch: 51, Batch: 880, Training Loss: 0.03908613957464695, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:15, Epoch: 51, Batch: 890, Training Loss: 0.047052647173404696, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:15, Epoch: 51, Batch: 900, Training Loss: 0.050055010989308354, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:16, Epoch: 51, Batch: 910, Training Loss: 0.03216155618429184, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:17, Epoch: 51, Batch: 920, Training Loss: 0.03111409693956375, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:18, Epoch: 51, Batch: 930, Training Loss: 0.03803334496915341, LR: 0.0010000000000000002
Epoch: 51, Validation Top 1 acc: 98.53702545166016
Epoch: 51, Validation Top 5 acc: 100.0
Epoch: 51, Validation Set Loss: 0.045005422085523605
Start training epoch 52
Time, 2019-01-01T23:07:24, Epoch: 52, Batch: 10, Training Loss: 0.04688674807548523, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:24, Epoch: 52, Batch: 20, Training Loss: 0.037126343324780466, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:25, Epoch: 52, Batch: 30, Training Loss: 0.01775330975651741, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:26, Epoch: 52, Batch: 40, Training Loss: 0.0465420201420784, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:27, Epoch: 52, Batch: 50, Training Loss: 0.045113885030150416, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:27, Epoch: 52, Batch: 60, Training Loss: 0.04231509603559971, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:28, Epoch: 52, Batch: 70, Training Loss: 0.0516272209584713, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:29, Epoch: 52, Batch: 80, Training Loss: 0.04415374770760536, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:30, Epoch: 52, Batch: 90, Training Loss: 0.03578452803194523, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:30, Epoch: 52, Batch: 100, Training Loss: 0.03526108115911484, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:31, Epoch: 52, Batch: 110, Training Loss: 0.04628572948276997, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:32, Epoch: 52, Batch: 120, Training Loss: 0.03647013120353222, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:33, Epoch: 52, Batch: 130, Training Loss: 0.041651790216565135, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:34, Epoch: 52, Batch: 140, Training Loss: 0.03830080106854439, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:35, Epoch: 52, Batch: 150, Training Loss: 0.055567356571555135, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:35, Epoch: 52, Batch: 160, Training Loss: 0.042213235050439835, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:36, Epoch: 52, Batch: 170, Training Loss: 0.03434183672070503, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:37, Epoch: 52, Batch: 180, Training Loss: 0.04670435637235641, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:38, Epoch: 52, Batch: 190, Training Loss: 0.04381144717335701, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:39, Epoch: 52, Batch: 200, Training Loss: 0.025865305960178376, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:39, Epoch: 52, Batch: 210, Training Loss: 0.03274111859500408, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:40, Epoch: 52, Batch: 220, Training Loss: 0.031190834194421767, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:41, Epoch: 52, Batch: 230, Training Loss: 0.05493181198835373, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:42, Epoch: 52, Batch: 240, Training Loss: 0.0328867107629776, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:42, Epoch: 52, Batch: 250, Training Loss: 0.06598532646894455, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:43, Epoch: 52, Batch: 260, Training Loss: 0.04246768839657307, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:44, Epoch: 52, Batch: 270, Training Loss: 0.03134653270244599, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:45, Epoch: 52, Batch: 280, Training Loss: 0.02918282672762871, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:45, Epoch: 52, Batch: 290, Training Loss: 0.029973744973540307, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:46, Epoch: 52, Batch: 300, Training Loss: 0.03715264499187469, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:47, Epoch: 52, Batch: 310, Training Loss: 0.04482538439333439, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:48, Epoch: 52, Batch: 320, Training Loss: 0.031196676194667816, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:49, Epoch: 52, Batch: 330, Training Loss: 0.03018380142748356, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:50, Epoch: 52, Batch: 340, Training Loss: 0.0516583651304245, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:50, Epoch: 52, Batch: 350, Training Loss: 0.049112017080187795, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:51, Epoch: 52, Batch: 360, Training Loss: 0.029376518353819846, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:52, Epoch: 52, Batch: 370, Training Loss: 0.08139596879482269, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:53, Epoch: 52, Batch: 380, Training Loss: 0.029234461858868598, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:53, Epoch: 52, Batch: 390, Training Loss: 0.03406390137970448, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:54, Epoch: 52, Batch: 400, Training Loss: 0.026347456127405168, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:55, Epoch: 52, Batch: 410, Training Loss: 0.03435292914509773, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:56, Epoch: 52, Batch: 420, Training Loss: 0.03809061273932457, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:56, Epoch: 52, Batch: 430, Training Loss: 0.02860097326338291, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:57, Epoch: 52, Batch: 440, Training Loss: 0.03284326307475567, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:58, Epoch: 52, Batch: 450, Training Loss: 0.048628146201372145, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:59, Epoch: 52, Batch: 460, Training Loss: 0.04853512048721313, LR: 0.0010000000000000002
Time, 2019-01-01T23:07:59, Epoch: 52, Batch: 470, Training Loss: 0.03737155497074127, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:00, Epoch: 52, Batch: 480, Training Loss: 0.03594639003276825, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:01, Epoch: 52, Batch: 490, Training Loss: 0.03901495672762394, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:02, Epoch: 52, Batch: 500, Training Loss: 0.04120853766798973, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:03, Epoch: 52, Batch: 510, Training Loss: 0.037576328963041306, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:03, Epoch: 52, Batch: 520, Training Loss: 0.048896561935544015, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:04, Epoch: 52, Batch: 530, Training Loss: 0.06239967979490757, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:05, Epoch: 52, Batch: 540, Training Loss: 0.033330609649419786, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:06, Epoch: 52, Batch: 550, Training Loss: 0.03798501491546631, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:06, Epoch: 52, Batch: 560, Training Loss: 0.03514784425497055, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:07, Epoch: 52, Batch: 570, Training Loss: 0.030758795887231828, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:08, Epoch: 52, Batch: 580, Training Loss: 0.05307379551231861, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:09, Epoch: 52, Batch: 590, Training Loss: 0.04452612586319447, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:09, Epoch: 52, Batch: 600, Training Loss: 0.03376448377966881, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:10, Epoch: 52, Batch: 610, Training Loss: 0.03155502527952194, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:11, Epoch: 52, Batch: 620, Training Loss: 0.03211534284055233, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:12, Epoch: 52, Batch: 630, Training Loss: 0.04758361466228962, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:12, Epoch: 52, Batch: 640, Training Loss: 0.04418249614536762, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:13, Epoch: 52, Batch: 650, Training Loss: 0.05466028079390526, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:14, Epoch: 52, Batch: 660, Training Loss: 0.03454929143190384, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:15, Epoch: 52, Batch: 670, Training Loss: 0.019768736138939858, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:15, Epoch: 52, Batch: 680, Training Loss: 0.03822814226150513, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:16, Epoch: 52, Batch: 690, Training Loss: 0.034258521348237994, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:17, Epoch: 52, Batch: 700, Training Loss: 0.021950963139533996, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:18, Epoch: 52, Batch: 710, Training Loss: 0.03467646576464176, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:18, Epoch: 52, Batch: 720, Training Loss: 0.03439053259789944, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:19, Epoch: 52, Batch: 730, Training Loss: 0.06364829912781715, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:20, Epoch: 52, Batch: 740, Training Loss: 0.03156235516071319, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:21, Epoch: 52, Batch: 750, Training Loss: 0.030786192044615746, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:21, Epoch: 52, Batch: 760, Training Loss: 0.025243406370282172, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:22, Epoch: 52, Batch: 770, Training Loss: 0.06010017693042755, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:23, Epoch: 52, Batch: 780, Training Loss: 0.0323070339858532, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:24, Epoch: 52, Batch: 790, Training Loss: 0.04248454868793487, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:25, Epoch: 52, Batch: 800, Training Loss: 0.04086836725473404, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:25, Epoch: 52, Batch: 810, Training Loss: 0.03040038123726845, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:26, Epoch: 52, Batch: 820, Training Loss: 0.03681880533695221, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:27, Epoch: 52, Batch: 830, Training Loss: 0.03421618267893791, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:28, Epoch: 52, Batch: 840, Training Loss: 0.045721353590488435, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:28, Epoch: 52, Batch: 850, Training Loss: 0.04286685064435005, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:29, Epoch: 52, Batch: 860, Training Loss: 0.02796456925570965, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:30, Epoch: 52, Batch: 870, Training Loss: 0.04238443151116371, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:31, Epoch: 52, Batch: 880, Training Loss: 0.03232405371963978, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:31, Epoch: 52, Batch: 890, Training Loss: 0.04000653624534607, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:32, Epoch: 52, Batch: 900, Training Loss: 0.04234717190265656, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:33, Epoch: 52, Batch: 910, Training Loss: 0.04982511773705482, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:34, Epoch: 52, Batch: 920, Training Loss: 0.05554169975221157, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:35, Epoch: 52, Batch: 930, Training Loss: 0.042372196167707446, LR: 0.0010000000000000002
Epoch: 52, Validation Top 1 acc: 98.59673309326172
Epoch: 52, Validation Top 5 acc: 100.0
Epoch: 52, Validation Set Loss: 0.04446963220834732
Start training epoch 53
Time, 2019-01-01T23:08:41, Epoch: 53, Batch: 10, Training Loss: 0.03869386240839958, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:41, Epoch: 53, Batch: 20, Training Loss: 0.03546108901500702, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:42, Epoch: 53, Batch: 30, Training Loss: 0.03244023732841015, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:43, Epoch: 53, Batch: 40, Training Loss: 0.049146365746855734, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:44, Epoch: 53, Batch: 50, Training Loss: 0.03209487870335579, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:44, Epoch: 53, Batch: 60, Training Loss: 0.03925044946372509, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:45, Epoch: 53, Batch: 70, Training Loss: 0.047239334881305696, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:46, Epoch: 53, Batch: 80, Training Loss: 0.0187321525067091, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:47, Epoch: 53, Batch: 90, Training Loss: 0.02979264222085476, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:47, Epoch: 53, Batch: 100, Training Loss: 0.03882447890937328, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:48, Epoch: 53, Batch: 110, Training Loss: 0.04075120091438293, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:49, Epoch: 53, Batch: 120, Training Loss: 0.02353549301624298, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:50, Epoch: 53, Batch: 130, Training Loss: 0.04628287851810455, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:50, Epoch: 53, Batch: 140, Training Loss: 0.03697425127029419, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:51, Epoch: 53, Batch: 150, Training Loss: 0.024991701170802117, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:52, Epoch: 53, Batch: 160, Training Loss: 0.036769576370716095, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:53, Epoch: 53, Batch: 170, Training Loss: 0.028288592398166657, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:53, Epoch: 53, Batch: 180, Training Loss: 0.059674908965826036, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:54, Epoch: 53, Batch: 190, Training Loss: 0.04726170562207699, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:55, Epoch: 53, Batch: 200, Training Loss: 0.03992725387215614, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:56, Epoch: 53, Batch: 210, Training Loss: 0.02788490504026413, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:56, Epoch: 53, Batch: 220, Training Loss: 0.04545283392071724, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:57, Epoch: 53, Batch: 230, Training Loss: 0.04395691566169262, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:58, Epoch: 53, Batch: 240, Training Loss: 0.04608305767178535, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:59, Epoch: 53, Batch: 250, Training Loss: 0.026771946996450424, LR: 0.0010000000000000002
Time, 2019-01-01T23:08:59, Epoch: 53, Batch: 260, Training Loss: 0.0425863116979599, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:00, Epoch: 53, Batch: 270, Training Loss: 0.0665848433971405, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:01, Epoch: 53, Batch: 280, Training Loss: 0.03692291229963303, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:02, Epoch: 53, Batch: 290, Training Loss: 0.038371116295456885, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:03, Epoch: 53, Batch: 300, Training Loss: 0.03508070595562458, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:03, Epoch: 53, Batch: 310, Training Loss: 0.03553910329937935, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:04, Epoch: 53, Batch: 320, Training Loss: 0.03999010920524597, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:05, Epoch: 53, Batch: 330, Training Loss: 0.05610029734671116, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:06, Epoch: 53, Batch: 340, Training Loss: 0.04325481727719307, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:06, Epoch: 53, Batch: 350, Training Loss: 0.0341414175927639, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:07, Epoch: 53, Batch: 360, Training Loss: 0.033194076642394064, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:08, Epoch: 53, Batch: 370, Training Loss: 0.04197208248078823, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:09, Epoch: 53, Batch: 380, Training Loss: 0.027255402505397798, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:09, Epoch: 53, Batch: 390, Training Loss: 0.029139215499162673, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:10, Epoch: 53, Batch: 400, Training Loss: 0.04259763360023498, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:11, Epoch: 53, Batch: 410, Training Loss: 0.030306311696767806, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:12, Epoch: 53, Batch: 420, Training Loss: 0.05525785535573959, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:12, Epoch: 53, Batch: 430, Training Loss: 0.028662386536598205, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:13, Epoch: 53, Batch: 440, Training Loss: 0.03794923834502697, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:14, Epoch: 53, Batch: 450, Training Loss: 0.05006396695971489, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:15, Epoch: 53, Batch: 460, Training Loss: 0.035769379884004596, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:15, Epoch: 53, Batch: 470, Training Loss: 0.029208270460367204, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:16, Epoch: 53, Batch: 480, Training Loss: 0.03651488721370697, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:17, Epoch: 53, Batch: 490, Training Loss: 0.04394964575767517, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:18, Epoch: 53, Batch: 500, Training Loss: 0.04037733152508736, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:18, Epoch: 53, Batch: 510, Training Loss: 0.03966168165206909, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:19, Epoch: 53, Batch: 520, Training Loss: 0.038755085319280624, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:20, Epoch: 53, Batch: 530, Training Loss: 0.04785975366830826, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:21, Epoch: 53, Batch: 540, Training Loss: 0.029748644679784775, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:21, Epoch: 53, Batch: 550, Training Loss: 0.0578066062182188, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:22, Epoch: 53, Batch: 560, Training Loss: 0.040418701246380806, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:23, Epoch: 53, Batch: 570, Training Loss: 0.036097553372383115, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:24, Epoch: 53, Batch: 580, Training Loss: 0.03968639969825745, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:24, Epoch: 53, Batch: 590, Training Loss: 0.041291558742523195, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:25, Epoch: 53, Batch: 600, Training Loss: 0.042208469659090045, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:26, Epoch: 53, Batch: 610, Training Loss: 0.031221773102879526, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:27, Epoch: 53, Batch: 620, Training Loss: 0.050866348296403886, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:27, Epoch: 53, Batch: 630, Training Loss: 0.041227970644831656, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:28, Epoch: 53, Batch: 640, Training Loss: 0.026719234883785248, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:29, Epoch: 53, Batch: 650, Training Loss: 0.03839047290384769, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:30, Epoch: 53, Batch: 660, Training Loss: 0.036655201390385625, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:30, Epoch: 53, Batch: 670, Training Loss: 0.0342593140900135, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:31, Epoch: 53, Batch: 680, Training Loss: 0.04239219203591347, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:32, Epoch: 53, Batch: 690, Training Loss: 0.027661997824907303, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:33, Epoch: 53, Batch: 700, Training Loss: 0.03531240969896317, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:33, Epoch: 53, Batch: 710, Training Loss: 0.04256962314248085, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:34, Epoch: 53, Batch: 720, Training Loss: 0.018478233739733695, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:35, Epoch: 53, Batch: 730, Training Loss: 0.028339871019124985, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:36, Epoch: 53, Batch: 740, Training Loss: 0.04474495686590672, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:36, Epoch: 53, Batch: 750, Training Loss: 0.04505674801766872, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:37, Epoch: 53, Batch: 760, Training Loss: 0.03990803435444832, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:38, Epoch: 53, Batch: 770, Training Loss: 0.026359861716628075, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:38, Epoch: 53, Batch: 780, Training Loss: 0.03754206225275993, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:39, Epoch: 53, Batch: 790, Training Loss: 0.029119399562478065, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:40, Epoch: 53, Batch: 800, Training Loss: 0.05707491412758827, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:41, Epoch: 53, Batch: 810, Training Loss: 0.05526778846979141, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:41, Epoch: 53, Batch: 820, Training Loss: 0.03517735712230206, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:42, Epoch: 53, Batch: 830, Training Loss: 0.059707450494170186, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:43, Epoch: 53, Batch: 840, Training Loss: 0.053504695743322374, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:44, Epoch: 53, Batch: 850, Training Loss: 0.03949440121650696, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:44, Epoch: 53, Batch: 860, Training Loss: 0.060381734371185304, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:45, Epoch: 53, Batch: 870, Training Loss: 0.06263677775859833, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:46, Epoch: 53, Batch: 880, Training Loss: 0.035772880911827086, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:47, Epoch: 53, Batch: 890, Training Loss: 0.055850930884480475, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:47, Epoch: 53, Batch: 900, Training Loss: 0.03899612836539745, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:48, Epoch: 53, Batch: 910, Training Loss: 0.030735480785369872, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:49, Epoch: 53, Batch: 920, Training Loss: 0.03881385028362274, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:50, Epoch: 53, Batch: 930, Training Loss: 0.052264227718114856, LR: 0.0010000000000000002
Epoch: 53, Validation Top 1 acc: 98.64649963378906
Epoch: 53, Validation Top 5 acc: 100.0
Epoch: 53, Validation Set Loss: 0.044874340295791626
Start training epoch 54
Time, 2019-01-01T23:09:55, Epoch: 54, Batch: 10, Training Loss: 0.03631178252398968, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:56, Epoch: 54, Batch: 20, Training Loss: 0.035857859253883365, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:57, Epoch: 54, Batch: 30, Training Loss: 0.04823934733867645, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:58, Epoch: 54, Batch: 40, Training Loss: 0.04310754612088204, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:59, Epoch: 54, Batch: 50, Training Loss: 0.039873677492141726, LR: 0.0010000000000000002
Time, 2019-01-01T23:09:59, Epoch: 54, Batch: 60, Training Loss: 0.04055810756981373, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:00, Epoch: 54, Batch: 70, Training Loss: 0.0433661088347435, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:01, Epoch: 54, Batch: 80, Training Loss: 0.04072025083005428, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:02, Epoch: 54, Batch: 90, Training Loss: 0.03876905292272568, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:03, Epoch: 54, Batch: 100, Training Loss: 0.04400711543858051, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:04, Epoch: 54, Batch: 110, Training Loss: 0.03729439340531826, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:04, Epoch: 54, Batch: 120, Training Loss: 0.04397166185081005, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:05, Epoch: 54, Batch: 130, Training Loss: 0.04697755798697471, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:06, Epoch: 54, Batch: 140, Training Loss: 0.04433136582374573, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:07, Epoch: 54, Batch: 150, Training Loss: 0.04119179807603359, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:07, Epoch: 54, Batch: 160, Training Loss: 0.029524584114551545, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:08, Epoch: 54, Batch: 170, Training Loss: 0.04917837604880333, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:09, Epoch: 54, Batch: 180, Training Loss: 0.06276418566703797, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:10, Epoch: 54, Batch: 190, Training Loss: 0.033725183457136154, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:10, Epoch: 54, Batch: 200, Training Loss: 0.03554573580622673, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:11, Epoch: 54, Batch: 210, Training Loss: 0.04638928398489952, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:12, Epoch: 54, Batch: 220, Training Loss: 0.022879475355148317, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:13, Epoch: 54, Batch: 230, Training Loss: 0.04618589878082276, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:13, Epoch: 54, Batch: 240, Training Loss: 0.04201549589633942, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:14, Epoch: 54, Batch: 250, Training Loss: 0.043101533129811284, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:15, Epoch: 54, Batch: 260, Training Loss: 0.041314441710710526, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:16, Epoch: 54, Batch: 270, Training Loss: 0.03810323514044285, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:16, Epoch: 54, Batch: 280, Training Loss: 0.034109370037913325, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:17, Epoch: 54, Batch: 290, Training Loss: 0.04615829214453697, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:18, Epoch: 54, Batch: 300, Training Loss: 0.02713831216096878, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:19, Epoch: 54, Batch: 310, Training Loss: 0.04260408468544483, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:19, Epoch: 54, Batch: 320, Training Loss: 0.022557516768574714, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:20, Epoch: 54, Batch: 330, Training Loss: 0.06717942915856838, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:21, Epoch: 54, Batch: 340, Training Loss: 0.04785257242619991, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:22, Epoch: 54, Batch: 350, Training Loss: 0.043829991668462756, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:22, Epoch: 54, Batch: 360, Training Loss: 0.02729571871459484, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:23, Epoch: 54, Batch: 370, Training Loss: 0.03466674238443375, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:24, Epoch: 54, Batch: 380, Training Loss: 0.03602995201945305, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:25, Epoch: 54, Batch: 390, Training Loss: 0.03538400381803512, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:25, Epoch: 54, Batch: 400, Training Loss: 0.04257207401096821, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:26, Epoch: 54, Batch: 410, Training Loss: 0.03657000735402107, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:27, Epoch: 54, Batch: 420, Training Loss: 0.03163150399923324, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:28, Epoch: 54, Batch: 430, Training Loss: 0.03366152197122574, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:28, Epoch: 54, Batch: 440, Training Loss: 0.0349100612103939, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:29, Epoch: 54, Batch: 450, Training Loss: 0.044816937297582626, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:30, Epoch: 54, Batch: 460, Training Loss: 0.0347485214471817, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:31, Epoch: 54, Batch: 470, Training Loss: 0.056026653945446016, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:31, Epoch: 54, Batch: 480, Training Loss: 0.0428775705397129, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:32, Epoch: 54, Batch: 490, Training Loss: 0.04935481436550617, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:33, Epoch: 54, Batch: 500, Training Loss: 0.030595533549785614, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:34, Epoch: 54, Batch: 510, Training Loss: 0.024768251180648803, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:34, Epoch: 54, Batch: 520, Training Loss: 0.05934462323784828, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:35, Epoch: 54, Batch: 530, Training Loss: 0.02692994996905327, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:36, Epoch: 54, Batch: 540, Training Loss: 0.029417696222662926, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:37, Epoch: 54, Batch: 550, Training Loss: 0.032780534401535985, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:38, Epoch: 54, Batch: 560, Training Loss: 0.038528769835829736, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:38, Epoch: 54, Batch: 570, Training Loss: 0.03028484731912613, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:39, Epoch: 54, Batch: 580, Training Loss: 0.0385909765958786, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:40, Epoch: 54, Batch: 590, Training Loss: 0.03271365016698837, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:41, Epoch: 54, Batch: 600, Training Loss: 0.026164048910140993, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:42, Epoch: 54, Batch: 610, Training Loss: 0.05114625170826912, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:43, Epoch: 54, Batch: 620, Training Loss: 0.03223957419395447, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:43, Epoch: 54, Batch: 630, Training Loss: 0.04444780386984348, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:44, Epoch: 54, Batch: 640, Training Loss: 0.03664236664772034, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:45, Epoch: 54, Batch: 650, Training Loss: 0.025819400697946547, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:46, Epoch: 54, Batch: 660, Training Loss: 0.03484639562666416, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:46, Epoch: 54, Batch: 670, Training Loss: 0.0369259100407362, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:47, Epoch: 54, Batch: 680, Training Loss: 0.028208956122398376, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:48, Epoch: 54, Batch: 690, Training Loss: 0.03731805086135864, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:49, Epoch: 54, Batch: 700, Training Loss: 0.02670465111732483, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:49, Epoch: 54, Batch: 710, Training Loss: 0.03795401230454445, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:50, Epoch: 54, Batch: 720, Training Loss: 0.03427790999412537, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:51, Epoch: 54, Batch: 730, Training Loss: 0.06004433184862137, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:52, Epoch: 54, Batch: 740, Training Loss: 0.04006840735673904, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:52, Epoch: 54, Batch: 750, Training Loss: 0.04400397539138794, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:53, Epoch: 54, Batch: 760, Training Loss: 0.03888932913541794, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:54, Epoch: 54, Batch: 770, Training Loss: 0.039210200682282446, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:55, Epoch: 54, Batch: 780, Training Loss: 0.047913220897316935, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:55, Epoch: 54, Batch: 790, Training Loss: 0.04513142928481102, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:56, Epoch: 54, Batch: 800, Training Loss: 0.033688300475478175, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:57, Epoch: 54, Batch: 810, Training Loss: 0.04564455859363079, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:58, Epoch: 54, Batch: 820, Training Loss: 0.03344641625881195, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:59, Epoch: 54, Batch: 830, Training Loss: 0.026968900486826895, LR: 0.0010000000000000002
Time, 2019-01-01T23:10:59, Epoch: 54, Batch: 840, Training Loss: 0.0557997927069664, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:00, Epoch: 54, Batch: 850, Training Loss: 0.07252996899187565, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:01, Epoch: 54, Batch: 860, Training Loss: 0.034995755180716515, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:02, Epoch: 54, Batch: 870, Training Loss: 0.034824030473828316, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:03, Epoch: 54, Batch: 880, Training Loss: 0.03795997947454453, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:03, Epoch: 54, Batch: 890, Training Loss: 0.040893709659576415, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:04, Epoch: 54, Batch: 900, Training Loss: 0.04116260036826134, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:05, Epoch: 54, Batch: 910, Training Loss: 0.03632811382412911, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:06, Epoch: 54, Batch: 920, Training Loss: 0.04132542684674263, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:06, Epoch: 54, Batch: 930, Training Loss: 0.039904982596635816, LR: 0.0010000000000000002
Epoch: 54, Validation Top 1 acc: 98.62659454345703
Epoch: 54, Validation Top 5 acc: 100.0
Epoch: 54, Validation Set Loss: 0.04438498616218567
Start training epoch 55
Time, 2019-01-01T23:11:12, Epoch: 55, Batch: 10, Training Loss: 0.03719201162457466, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:13, Epoch: 55, Batch: 20, Training Loss: 0.023639630526304245, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:14, Epoch: 55, Batch: 30, Training Loss: 0.04020109921693802, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:15, Epoch: 55, Batch: 40, Training Loss: 0.04330405183136463, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:15, Epoch: 55, Batch: 50, Training Loss: 0.031439848244190216, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:16, Epoch: 55, Batch: 60, Training Loss: 0.04245578199625015, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:17, Epoch: 55, Batch: 70, Training Loss: 0.02203548699617386, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:18, Epoch: 55, Batch: 80, Training Loss: 0.027373331785202026, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:19, Epoch: 55, Batch: 90, Training Loss: 0.0353702262043953, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:20, Epoch: 55, Batch: 100, Training Loss: 0.033736998215317726, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:20, Epoch: 55, Batch: 110, Training Loss: 0.02533999942243099, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:21, Epoch: 55, Batch: 120, Training Loss: 0.0426876574754715, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:22, Epoch: 55, Batch: 130, Training Loss: 0.04022102318704128, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:23, Epoch: 55, Batch: 140, Training Loss: 0.04895668961107731, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:23, Epoch: 55, Batch: 150, Training Loss: 0.04416362456977367, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:24, Epoch: 55, Batch: 160, Training Loss: 0.03608738854527473, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:25, Epoch: 55, Batch: 170, Training Loss: 0.030176547914743425, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:26, Epoch: 55, Batch: 180, Training Loss: 0.027644553780555726, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:26, Epoch: 55, Batch: 190, Training Loss: 0.041151262819767, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:27, Epoch: 55, Batch: 200, Training Loss: 0.04516546241939068, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:28, Epoch: 55, Batch: 210, Training Loss: 0.05373413674533367, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:29, Epoch: 55, Batch: 220, Training Loss: 0.04174082726240158, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:29, Epoch: 55, Batch: 230, Training Loss: 0.056317801028490065, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:30, Epoch: 55, Batch: 240, Training Loss: 0.03744753301143646, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:31, Epoch: 55, Batch: 250, Training Loss: 0.03284940682351589, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:32, Epoch: 55, Batch: 260, Training Loss: 0.03579077422618866, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:32, Epoch: 55, Batch: 270, Training Loss: 0.03379801064729691, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:33, Epoch: 55, Batch: 280, Training Loss: 0.02845381200313568, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:34, Epoch: 55, Batch: 290, Training Loss: 0.03159219659864902, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:35, Epoch: 55, Batch: 300, Training Loss: 0.02674221955239773, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:35, Epoch: 55, Batch: 310, Training Loss: 0.05180889703333378, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:36, Epoch: 55, Batch: 320, Training Loss: 0.03460273779928684, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:37, Epoch: 55, Batch: 330, Training Loss: 0.0472297765314579, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:38, Epoch: 55, Batch: 340, Training Loss: 0.0569541797041893, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:38, Epoch: 55, Batch: 350, Training Loss: 0.03419914394617081, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:39, Epoch: 55, Batch: 360, Training Loss: 0.029546480625867844, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:40, Epoch: 55, Batch: 370, Training Loss: 0.039040616154670714, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:41, Epoch: 55, Batch: 380, Training Loss: 0.05594126731157303, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:41, Epoch: 55, Batch: 390, Training Loss: 0.04122198559343815, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:42, Epoch: 55, Batch: 400, Training Loss: 0.04144134670495987, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:43, Epoch: 55, Batch: 410, Training Loss: 0.05875483490526676, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:44, Epoch: 55, Batch: 420, Training Loss: 0.041278062760829924, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:44, Epoch: 55, Batch: 430, Training Loss: 0.03879353068768978, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:45, Epoch: 55, Batch: 440, Training Loss: 0.030756616592407228, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:46, Epoch: 55, Batch: 450, Training Loss: 0.036548352986574176, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:47, Epoch: 55, Batch: 460, Training Loss: 0.02886747196316719, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:47, Epoch: 55, Batch: 470, Training Loss: 0.030443315207958222, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:48, Epoch: 55, Batch: 480, Training Loss: 0.02377655394375324, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:49, Epoch: 55, Batch: 490, Training Loss: 0.04397144317626953, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:50, Epoch: 55, Batch: 500, Training Loss: 0.03828749731183052, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:51, Epoch: 55, Batch: 510, Training Loss: 0.03574588224291801, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:51, Epoch: 55, Batch: 520, Training Loss: 0.03788606002926827, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:52, Epoch: 55, Batch: 530, Training Loss: 0.02512080781161785, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:53, Epoch: 55, Batch: 540, Training Loss: 0.05760102272033692, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:53, Epoch: 55, Batch: 550, Training Loss: 0.03948147147893906, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:54, Epoch: 55, Batch: 560, Training Loss: 0.0382419228553772, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:55, Epoch: 55, Batch: 570, Training Loss: 0.06032581962645054, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:56, Epoch: 55, Batch: 580, Training Loss: 0.04299259148538113, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:56, Epoch: 55, Batch: 590, Training Loss: 0.03517336919903755, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:57, Epoch: 55, Batch: 600, Training Loss: 0.04440491311252117, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:58, Epoch: 55, Batch: 610, Training Loss: 0.01963798515498638, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:59, Epoch: 55, Batch: 620, Training Loss: 0.06521118097007275, LR: 0.0010000000000000002
Time, 2019-01-01T23:11:59, Epoch: 55, Batch: 630, Training Loss: 0.0355014655739069, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:00, Epoch: 55, Batch: 640, Training Loss: 0.04153774008154869, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:01, Epoch: 55, Batch: 650, Training Loss: 0.044672229886054994, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:02, Epoch: 55, Batch: 660, Training Loss: 0.04321938268840313, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:02, Epoch: 55, Batch: 670, Training Loss: 0.038372182101011273, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:03, Epoch: 55, Batch: 680, Training Loss: 0.044570448249578475, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:04, Epoch: 55, Batch: 690, Training Loss: 0.04269438721239567, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:05, Epoch: 55, Batch: 700, Training Loss: 0.035938626527786253, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:05, Epoch: 55, Batch: 710, Training Loss: 0.03432178497314453, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:06, Epoch: 55, Batch: 720, Training Loss: 0.03396572060883045, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:07, Epoch: 55, Batch: 730, Training Loss: 0.04474397450685501, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:08, Epoch: 55, Batch: 740, Training Loss: 0.034749577194452284, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:08, Epoch: 55, Batch: 750, Training Loss: 0.05118535719811916, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:09, Epoch: 55, Batch: 760, Training Loss: 0.04713705778121948, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:10, Epoch: 55, Batch: 770, Training Loss: 0.03686745800077915, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:11, Epoch: 55, Batch: 780, Training Loss: 0.02435819134116173, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:12, Epoch: 55, Batch: 790, Training Loss: 0.033834639191627505, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:13, Epoch: 55, Batch: 800, Training Loss: 0.04005031995475292, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:13, Epoch: 55, Batch: 810, Training Loss: 0.04769835881888866, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:14, Epoch: 55, Batch: 820, Training Loss: 0.041781320422887805, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:15, Epoch: 55, Batch: 830, Training Loss: 0.04732208885252476, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:16, Epoch: 55, Batch: 840, Training Loss: 0.03276333697140217, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:16, Epoch: 55, Batch: 850, Training Loss: 0.03673158809542656, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:17, Epoch: 55, Batch: 860, Training Loss: 0.030572833865880965, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:18, Epoch: 55, Batch: 870, Training Loss: 0.03578502722084522, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:19, Epoch: 55, Batch: 880, Training Loss: 0.04129759073257446, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:19, Epoch: 55, Batch: 890, Training Loss: 0.046907464787364, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:20, Epoch: 55, Batch: 900, Training Loss: 0.0412419505417347, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:21, Epoch: 55, Batch: 910, Training Loss: 0.034579509496688844, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:22, Epoch: 55, Batch: 920, Training Loss: 0.034922124445438386, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:22, Epoch: 55, Batch: 930, Training Loss: 0.0691447403281927, LR: 0.0010000000000000002
Epoch: 55, Validation Top 1 acc: 98.63654327392578
Epoch: 55, Validation Top 5 acc: 100.0
Epoch: 55, Validation Set Loss: 0.04485394060611725
Start training epoch 56
Time, 2019-01-01T23:12:28, Epoch: 56, Batch: 10, Training Loss: 0.04939565733075142, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:29, Epoch: 56, Batch: 20, Training Loss: 0.025615255907177926, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:30, Epoch: 56, Batch: 30, Training Loss: 0.021961823850870133, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:31, Epoch: 56, Batch: 40, Training Loss: 0.03074116036295891, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:31, Epoch: 56, Batch: 50, Training Loss: 0.031761303171515466, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:32, Epoch: 56, Batch: 60, Training Loss: 0.03431606739759445, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:33, Epoch: 56, Batch: 70, Training Loss: 0.03388582095503807, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:34, Epoch: 56, Batch: 80, Training Loss: 0.0445490725338459, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:34, Epoch: 56, Batch: 90, Training Loss: 0.02814179062843323, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:35, Epoch: 56, Batch: 100, Training Loss: 0.05945836752653122, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:36, Epoch: 56, Batch: 110, Training Loss: 0.06109308488667011, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:37, Epoch: 56, Batch: 120, Training Loss: 0.0361250601708889, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:37, Epoch: 56, Batch: 130, Training Loss: 0.048398842290043834, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:38, Epoch: 56, Batch: 140, Training Loss: 0.029183844104409218, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:39, Epoch: 56, Batch: 150, Training Loss: 0.0473297469317913, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:40, Epoch: 56, Batch: 160, Training Loss: 0.027743440121412277, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:40, Epoch: 56, Batch: 170, Training Loss: 0.027213455736637117, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:41, Epoch: 56, Batch: 180, Training Loss: 0.02856137342751026, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:42, Epoch: 56, Batch: 190, Training Loss: 0.028549139574170112, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:43, Epoch: 56, Batch: 200, Training Loss: 0.04394679665565491, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:44, Epoch: 56, Batch: 210, Training Loss: 0.031897901371121404, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:44, Epoch: 56, Batch: 220, Training Loss: 0.05118783935904503, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:45, Epoch: 56, Batch: 230, Training Loss: 0.04374965317547321, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:46, Epoch: 56, Batch: 240, Training Loss: 0.05283653177320957, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:46, Epoch: 56, Batch: 250, Training Loss: 0.04853832200169563, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:47, Epoch: 56, Batch: 260, Training Loss: 0.024797987565398215, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:48, Epoch: 56, Batch: 270, Training Loss: 0.026694028824567794, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:49, Epoch: 56, Batch: 280, Training Loss: 0.044093938544392586, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:49, Epoch: 56, Batch: 290, Training Loss: 0.04267304614186287, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:50, Epoch: 56, Batch: 300, Training Loss: 0.037115618214011194, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:51, Epoch: 56, Batch: 310, Training Loss: 0.038976243883371356, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:52, Epoch: 56, Batch: 320, Training Loss: 0.04671275243163109, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:52, Epoch: 56, Batch: 330, Training Loss: 0.029116429388523102, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:53, Epoch: 56, Batch: 340, Training Loss: 0.039688920602202415, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:54, Epoch: 56, Batch: 350, Training Loss: 0.05547553449869156, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:55, Epoch: 56, Batch: 360, Training Loss: 0.03971615880727768, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:55, Epoch: 56, Batch: 370, Training Loss: 0.044911738857626914, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:56, Epoch: 56, Batch: 380, Training Loss: 0.04776507019996643, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:57, Epoch: 56, Batch: 390, Training Loss: 0.04511774145066738, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:58, Epoch: 56, Batch: 400, Training Loss: 0.028652360290288927, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:58, Epoch: 56, Batch: 410, Training Loss: 0.03716928623616696, LR: 0.0010000000000000002
Time, 2019-01-01T23:12:59, Epoch: 56, Batch: 420, Training Loss: 0.04272962659597397, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:00, Epoch: 56, Batch: 430, Training Loss: 0.03150847181677818, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:01, Epoch: 56, Batch: 440, Training Loss: 0.028272932022809984, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:01, Epoch: 56, Batch: 450, Training Loss: 0.030026306957006456, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:02, Epoch: 56, Batch: 460, Training Loss: 0.040877009928226474, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:03, Epoch: 56, Batch: 470, Training Loss: 0.04137340933084488, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:04, Epoch: 56, Batch: 480, Training Loss: 0.028700796142220496, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:04, Epoch: 56, Batch: 490, Training Loss: 0.03424185588955879, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:05, Epoch: 56, Batch: 500, Training Loss: 0.04431133978068828, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:06, Epoch: 56, Batch: 510, Training Loss: 0.05037213079631329, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:07, Epoch: 56, Batch: 520, Training Loss: 0.030374106764793397, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:07, Epoch: 56, Batch: 530, Training Loss: 0.05439518168568611, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:08, Epoch: 56, Batch: 540, Training Loss: 0.04722646586596966, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:09, Epoch: 56, Batch: 550, Training Loss: 0.03166466802358627, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:10, Epoch: 56, Batch: 560, Training Loss: 0.03554169647395611, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:10, Epoch: 56, Batch: 570, Training Loss: 0.021947120502591132, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:11, Epoch: 56, Batch: 580, Training Loss: 0.048875953629612924, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:12, Epoch: 56, Batch: 590, Training Loss: 0.037474165111780165, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:13, Epoch: 56, Batch: 600, Training Loss: 0.056017686426639554, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:13, Epoch: 56, Batch: 610, Training Loss: 0.040423667430877684, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:14, Epoch: 56, Batch: 620, Training Loss: 0.021631328761577605, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:15, Epoch: 56, Batch: 630, Training Loss: 0.06385384872555733, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:15, Epoch: 56, Batch: 640, Training Loss: 0.029498955607414244, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:16, Epoch: 56, Batch: 650, Training Loss: 0.02996167205274105, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:17, Epoch: 56, Batch: 660, Training Loss: 0.047525198012590406, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:18, Epoch: 56, Batch: 670, Training Loss: 0.04906214699149132, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:18, Epoch: 56, Batch: 680, Training Loss: 0.039337160810828206, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:19, Epoch: 56, Batch: 690, Training Loss: 0.03766679838299751, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:20, Epoch: 56, Batch: 700, Training Loss: 0.029834948480129242, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:21, Epoch: 56, Batch: 710, Training Loss: 0.04156339541077614, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:21, Epoch: 56, Batch: 720, Training Loss: 0.03540495857596397, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:22, Epoch: 56, Batch: 730, Training Loss: 0.0320944294333458, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:23, Epoch: 56, Batch: 740, Training Loss: 0.03478042483329773, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:24, Epoch: 56, Batch: 750, Training Loss: 0.03473484516143799, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:24, Epoch: 56, Batch: 760, Training Loss: 0.03522494956851006, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:25, Epoch: 56, Batch: 770, Training Loss: 0.027570002526044846, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:26, Epoch: 56, Batch: 780, Training Loss: 0.04594227783381939, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:27, Epoch: 56, Batch: 790, Training Loss: 0.0348424643278122, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:27, Epoch: 56, Batch: 800, Training Loss: 0.04130377769470215, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:28, Epoch: 56, Batch: 810, Training Loss: 0.043252869322896, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:29, Epoch: 56, Batch: 820, Training Loss: 0.03984426185488701, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:30, Epoch: 56, Batch: 830, Training Loss: 0.043335375189781186, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:30, Epoch: 56, Batch: 840, Training Loss: 0.05997395068407059, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:31, Epoch: 56, Batch: 850, Training Loss: 0.03746316507458687, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:32, Epoch: 56, Batch: 860, Training Loss: 0.0446986973285675, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:32, Epoch: 56, Batch: 870, Training Loss: 0.034170617908239366, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:33, Epoch: 56, Batch: 880, Training Loss: 0.03969222828745842, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:34, Epoch: 56, Batch: 890, Training Loss: 0.03345209695398808, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:35, Epoch: 56, Batch: 900, Training Loss: 0.03818640410900116, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:35, Epoch: 56, Batch: 910, Training Loss: 0.04584439843893051, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:36, Epoch: 56, Batch: 920, Training Loss: 0.04692862443625927, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:37, Epoch: 56, Batch: 930, Training Loss: 0.02969210669398308, LR: 0.0010000000000000002
Epoch: 56, Validation Top 1 acc: 98.61663818359375
Epoch: 56, Validation Top 5 acc: 100.0
Epoch: 56, Validation Set Loss: 0.04407326877117157
Start training epoch 57
Time, 2019-01-01T23:13:43, Epoch: 57, Batch: 10, Training Loss: 0.04699315838515759, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:44, Epoch: 57, Batch: 20, Training Loss: 0.042724745348095894, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:44, Epoch: 57, Batch: 30, Training Loss: 0.03167092651128769, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:45, Epoch: 57, Batch: 40, Training Loss: 0.03668107911944389, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:46, Epoch: 57, Batch: 50, Training Loss: 0.047968107461929324, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:46, Epoch: 57, Batch: 60, Training Loss: 0.032224034518003465, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:47, Epoch: 57, Batch: 70, Training Loss: 0.03861882574856281, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:48, Epoch: 57, Batch: 80, Training Loss: 0.03392884135246277, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:49, Epoch: 57, Batch: 90, Training Loss: 0.03719313219189644, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:49, Epoch: 57, Batch: 100, Training Loss: 0.05694983303546906, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:50, Epoch: 57, Batch: 110, Training Loss: 0.03233790472149849, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:51, Epoch: 57, Batch: 120, Training Loss: 0.048766689375042915, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:52, Epoch: 57, Batch: 130, Training Loss: 0.021800978109240532, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:52, Epoch: 57, Batch: 140, Training Loss: 0.04081774540245533, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:53, Epoch: 57, Batch: 150, Training Loss: 0.05007484406232834, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:54, Epoch: 57, Batch: 160, Training Loss: 0.04412609338760376, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:55, Epoch: 57, Batch: 170, Training Loss: 0.03941772468388081, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:55, Epoch: 57, Batch: 180, Training Loss: 0.028908895328640938, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:56, Epoch: 57, Batch: 190, Training Loss: 0.041582657396793364, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:57, Epoch: 57, Batch: 200, Training Loss: 0.05329195447266102, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:58, Epoch: 57, Batch: 210, Training Loss: 0.04496166780591011, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:58, Epoch: 57, Batch: 220, Training Loss: 0.0443426713347435, LR: 0.0010000000000000002
Time, 2019-01-01T23:13:59, Epoch: 57, Batch: 230, Training Loss: 0.03000015616416931, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:00, Epoch: 57, Batch: 240, Training Loss: 0.03895240090787411, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:01, Epoch: 57, Batch: 250, Training Loss: 0.030919347330927847, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:01, Epoch: 57, Batch: 260, Training Loss: 0.04787226766347885, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:02, Epoch: 57, Batch: 270, Training Loss: 0.04718801453709602, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:03, Epoch: 57, Batch: 280, Training Loss: 0.02996794581413269, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:04, Epoch: 57, Batch: 290, Training Loss: 0.03540231101214886, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:04, Epoch: 57, Batch: 300, Training Loss: 0.04520975276827812, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:05, Epoch: 57, Batch: 310, Training Loss: 0.057752443477511406, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:06, Epoch: 57, Batch: 320, Training Loss: 0.026185014098882676, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:07, Epoch: 57, Batch: 330, Training Loss: 0.025968839600682258, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:07, Epoch: 57, Batch: 340, Training Loss: 0.03460085056722164, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:08, Epoch: 57, Batch: 350, Training Loss: 0.03234677687287331, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:09, Epoch: 57, Batch: 360, Training Loss: 0.035874242708086965, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:10, Epoch: 57, Batch: 370, Training Loss: 0.025092131271958352, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:10, Epoch: 57, Batch: 380, Training Loss: 0.05406424887478352, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:11, Epoch: 57, Batch: 390, Training Loss: 0.03518247790634632, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:12, Epoch: 57, Batch: 400, Training Loss: 0.04597173631191254, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:12, Epoch: 57, Batch: 410, Training Loss: 0.0390766829252243, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:13, Epoch: 57, Batch: 420, Training Loss: 0.04421217031776905, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:14, Epoch: 57, Batch: 430, Training Loss: 0.027485976368188857, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:15, Epoch: 57, Batch: 440, Training Loss: 0.03742905333638191, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:15, Epoch: 57, Batch: 450, Training Loss: 0.041544918343424796, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:16, Epoch: 57, Batch: 460, Training Loss: 0.030047256499528885, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:17, Epoch: 57, Batch: 470, Training Loss: 0.04305012375116348, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:18, Epoch: 57, Batch: 480, Training Loss: 0.038494642823934555, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:18, Epoch: 57, Batch: 490, Training Loss: 0.026709359139204025, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:19, Epoch: 57, Batch: 500, Training Loss: 0.03842364735901356, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:20, Epoch: 57, Batch: 510, Training Loss: 0.03780913949012756, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:21, Epoch: 57, Batch: 520, Training Loss: 0.04150381423532963, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:21, Epoch: 57, Batch: 530, Training Loss: 0.039809242635965345, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:22, Epoch: 57, Batch: 540, Training Loss: 0.05503855384886265, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:23, Epoch: 57, Batch: 550, Training Loss: 0.03570308238267898, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:23, Epoch: 57, Batch: 560, Training Loss: 0.0357684813439846, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:24, Epoch: 57, Batch: 570, Training Loss: 0.036779668927192685, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:25, Epoch: 57, Batch: 580, Training Loss: 0.04481805898249149, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:26, Epoch: 57, Batch: 590, Training Loss: 0.04293067455291748, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:27, Epoch: 57, Batch: 600, Training Loss: 0.03574065715074539, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:27, Epoch: 57, Batch: 610, Training Loss: 0.03106776103377342, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:28, Epoch: 57, Batch: 620, Training Loss: 0.034339525178074835, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:29, Epoch: 57, Batch: 630, Training Loss: 0.036893976107239726, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:30, Epoch: 57, Batch: 640, Training Loss: 0.02385496310889721, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:31, Epoch: 57, Batch: 650, Training Loss: 0.034743092209100726, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:32, Epoch: 57, Batch: 660, Training Loss: 0.04146942943334579, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:32, Epoch: 57, Batch: 670, Training Loss: 0.06493411995470524, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:33, Epoch: 57, Batch: 680, Training Loss: 0.027438221499323845, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:34, Epoch: 57, Batch: 690, Training Loss: 0.041788771748542786, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:35, Epoch: 57, Batch: 700, Training Loss: 0.04532346911728382, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:35, Epoch: 57, Batch: 710, Training Loss: 0.026671337336301802, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:36, Epoch: 57, Batch: 720, Training Loss: 0.05636785700917244, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:37, Epoch: 57, Batch: 730, Training Loss: 0.03199464678764343, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:38, Epoch: 57, Batch: 740, Training Loss: 0.03002765737473965, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:38, Epoch: 57, Batch: 750, Training Loss: 0.04824114590883255, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:39, Epoch: 57, Batch: 760, Training Loss: 0.03945595398545265, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:40, Epoch: 57, Batch: 770, Training Loss: 0.03869404792785645, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:41, Epoch: 57, Batch: 780, Training Loss: 0.04118521809577942, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:41, Epoch: 57, Batch: 790, Training Loss: 0.027612417936325073, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:42, Epoch: 57, Batch: 800, Training Loss: 0.04308982715010643, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:43, Epoch: 57, Batch: 810, Training Loss: 0.04262238703668118, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:44, Epoch: 57, Batch: 820, Training Loss: 0.04160696938633919, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:45, Epoch: 57, Batch: 830, Training Loss: 0.04638667143881321, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:46, Epoch: 57, Batch: 840, Training Loss: 0.034368453174829484, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:46, Epoch: 57, Batch: 850, Training Loss: 0.06148958876729012, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:47, Epoch: 57, Batch: 860, Training Loss: 0.047500326111912725, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:48, Epoch: 57, Batch: 870, Training Loss: 0.035753509029746056, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:49, Epoch: 57, Batch: 880, Training Loss: 0.03861727304756642, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:49, Epoch: 57, Batch: 890, Training Loss: 0.026846982166171075, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:50, Epoch: 57, Batch: 900, Training Loss: 0.02782031111419201, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:51, Epoch: 57, Batch: 910, Training Loss: 0.045543931797146796, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:52, Epoch: 57, Batch: 920, Training Loss: 0.0459099531173706, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:52, Epoch: 57, Batch: 930, Training Loss: 0.04812802970409393, LR: 0.0010000000000000002
Epoch: 57, Validation Top 1 acc: 98.65644836425781
Epoch: 57, Validation Top 5 acc: 100.0
Epoch: 57, Validation Set Loss: 0.04356347396969795
Start training epoch 58
Time, 2019-01-01T23:14:59, Epoch: 58, Batch: 10, Training Loss: 0.049508900940418245, LR: 0.0010000000000000002
Time, 2019-01-01T23:14:59, Epoch: 58, Batch: 20, Training Loss: 0.037890273705124856, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:00, Epoch: 58, Batch: 30, Training Loss: 0.031157728284597397, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:01, Epoch: 58, Batch: 40, Training Loss: 0.05126509889960289, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:02, Epoch: 58, Batch: 50, Training Loss: 0.04583662934601307, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:02, Epoch: 58, Batch: 60, Training Loss: 0.01991291418671608, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:03, Epoch: 58, Batch: 70, Training Loss: 0.03570233844220638, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:04, Epoch: 58, Batch: 80, Training Loss: 0.030804282054305077, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:05, Epoch: 58, Batch: 90, Training Loss: 0.030565999448299408, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:05, Epoch: 58, Batch: 100, Training Loss: 0.03346872888505459, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:06, Epoch: 58, Batch: 110, Training Loss: 0.04257508106529713, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:07, Epoch: 58, Batch: 120, Training Loss: 0.024380333721637726, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:08, Epoch: 58, Batch: 130, Training Loss: 0.03205180391669273, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:08, Epoch: 58, Batch: 140, Training Loss: 0.040043449774384496, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:09, Epoch: 58, Batch: 150, Training Loss: 0.028637369722127916, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:10, Epoch: 58, Batch: 160, Training Loss: 0.030335602909326555, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:11, Epoch: 58, Batch: 170, Training Loss: 0.03793621398508549, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:11, Epoch: 58, Batch: 180, Training Loss: 0.03823722004890442, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:12, Epoch: 58, Batch: 190, Training Loss: 0.025292123109102248, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:13, Epoch: 58, Batch: 200, Training Loss: 0.03582144267857075, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:14, Epoch: 58, Batch: 210, Training Loss: 0.03594053685665131, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:15, Epoch: 58, Batch: 220, Training Loss: 0.04020603410899639, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:15, Epoch: 58, Batch: 230, Training Loss: 0.04767805784940719, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:16, Epoch: 58, Batch: 240, Training Loss: 0.029080092161893844, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:17, Epoch: 58, Batch: 250, Training Loss: 0.05355198346078396, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:18, Epoch: 58, Batch: 260, Training Loss: 0.04214942306280136, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:18, Epoch: 58, Batch: 270, Training Loss: 0.055621113255620004, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:19, Epoch: 58, Batch: 280, Training Loss: 0.03680838346481323, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:20, Epoch: 58, Batch: 290, Training Loss: 0.030861308425664903, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:21, Epoch: 58, Batch: 300, Training Loss: 0.049079921841621396, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:21, Epoch: 58, Batch: 310, Training Loss: 0.04634981714189053, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:22, Epoch: 58, Batch: 320, Training Loss: 0.046776434779167174, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:23, Epoch: 58, Batch: 330, Training Loss: 0.043190741539001466, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:24, Epoch: 58, Batch: 340, Training Loss: 0.039835396036505696, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:24, Epoch: 58, Batch: 350, Training Loss: 0.04016263596713543, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:25, Epoch: 58, Batch: 360, Training Loss: 0.04559741765260696, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:26, Epoch: 58, Batch: 370, Training Loss: 0.04792911075055599, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:27, Epoch: 58, Batch: 380, Training Loss: 0.04241425544023514, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:28, Epoch: 58, Batch: 390, Training Loss: 0.04245673976838589, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:28, Epoch: 58, Batch: 400, Training Loss: 0.05141445882618427, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:29, Epoch: 58, Batch: 410, Training Loss: 0.04657687097787857, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:30, Epoch: 58, Batch: 420, Training Loss: 0.03555379435420036, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:31, Epoch: 58, Batch: 430, Training Loss: 0.024137433618307114, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:31, Epoch: 58, Batch: 440, Training Loss: 0.050491560250520706, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:32, Epoch: 58, Batch: 450, Training Loss: 0.03576414883136749, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:33, Epoch: 58, Batch: 460, Training Loss: 0.04246384911239147, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:34, Epoch: 58, Batch: 470, Training Loss: 0.037095251679420474, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:34, Epoch: 58, Batch: 480, Training Loss: 0.03748965933918953, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:35, Epoch: 58, Batch: 490, Training Loss: 0.031404266506433486, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:36, Epoch: 58, Batch: 500, Training Loss: 0.028945380449295045, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:37, Epoch: 58, Batch: 510, Training Loss: 0.0353267639875412, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:38, Epoch: 58, Batch: 520, Training Loss: 0.05031910054385662, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:38, Epoch: 58, Batch: 530, Training Loss: 0.047684769704937936, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:39, Epoch: 58, Batch: 540, Training Loss: 0.05276801250874996, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:40, Epoch: 58, Batch: 550, Training Loss: 0.03490950837731362, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:41, Epoch: 58, Batch: 560, Training Loss: 0.032024243474006654, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:41, Epoch: 58, Batch: 570, Training Loss: 0.037454362213611606, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:42, Epoch: 58, Batch: 580, Training Loss: 0.03416198678314686, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:43, Epoch: 58, Batch: 590, Training Loss: 0.038048888370394704, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:44, Epoch: 58, Batch: 600, Training Loss: 0.036750064417719844, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:45, Epoch: 58, Batch: 610, Training Loss: 0.03482380844652653, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:45, Epoch: 58, Batch: 620, Training Loss: 0.04007676839828491, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:46, Epoch: 58, Batch: 630, Training Loss: 0.02457730658352375, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:47, Epoch: 58, Batch: 640, Training Loss: 0.037360084056854245, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:48, Epoch: 58, Batch: 650, Training Loss: 0.03636230044066906, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:48, Epoch: 58, Batch: 660, Training Loss: 0.031720428913831714, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:49, Epoch: 58, Batch: 670, Training Loss: 0.0540851641446352, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:50, Epoch: 58, Batch: 680, Training Loss: 0.04518328532576561, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:51, Epoch: 58, Batch: 690, Training Loss: 0.04613805711269379, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:51, Epoch: 58, Batch: 700, Training Loss: 0.03625976741313934, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:52, Epoch: 58, Batch: 710, Training Loss: 0.028973234072327613, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:53, Epoch: 58, Batch: 720, Training Loss: 0.031103039905428887, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:54, Epoch: 58, Batch: 730, Training Loss: 0.04238080903887749, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:54, Epoch: 58, Batch: 740, Training Loss: 0.039098001271486285, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:55, Epoch: 58, Batch: 750, Training Loss: 0.028917861357331275, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:56, Epoch: 58, Batch: 760, Training Loss: 0.023266125842928885, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:57, Epoch: 58, Batch: 770, Training Loss: 0.04353499412536621, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:58, Epoch: 58, Batch: 780, Training Loss: 0.03820752501487732, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:58, Epoch: 58, Batch: 790, Training Loss: 0.031670023873448375, LR: 0.0010000000000000002
Time, 2019-01-01T23:15:59, Epoch: 58, Batch: 800, Training Loss: 0.05839816741645336, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:00, Epoch: 58, Batch: 810, Training Loss: 0.035500528663396834, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:01, Epoch: 58, Batch: 820, Training Loss: 0.0403921227902174, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:01, Epoch: 58, Batch: 830, Training Loss: 0.06214279979467392, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:02, Epoch: 58, Batch: 840, Training Loss: 0.047641396895051, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:03, Epoch: 58, Batch: 850, Training Loss: 0.03275967054069042, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:04, Epoch: 58, Batch: 860, Training Loss: 0.03849908672273159, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:04, Epoch: 58, Batch: 870, Training Loss: 0.029849474132061005, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:05, Epoch: 58, Batch: 880, Training Loss: 0.048512843623757365, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:06, Epoch: 58, Batch: 890, Training Loss: 0.04299590177834034, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:07, Epoch: 58, Batch: 900, Training Loss: 0.029058301448822023, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:07, Epoch: 58, Batch: 910, Training Loss: 0.04863303005695343, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:08, Epoch: 58, Batch: 920, Training Loss: 0.036796603351831436, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:09, Epoch: 58, Batch: 930, Training Loss: 0.03510827869176865, LR: 0.0010000000000000002
Epoch: 58, Validation Top 1 acc: 98.6863021850586
Epoch: 58, Validation Top 5 acc: 100.0
Epoch: 58, Validation Set Loss: 0.04380566254258156
Start training epoch 59
Time, 2019-01-01T23:16:15, Epoch: 59, Batch: 10, Training Loss: 0.037483416870236395, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:16, Epoch: 59, Batch: 20, Training Loss: 0.03403923399746418, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:16, Epoch: 59, Batch: 30, Training Loss: 0.04328261055052281, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:17, Epoch: 59, Batch: 40, Training Loss: 0.03779927231371403, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:18, Epoch: 59, Batch: 50, Training Loss: 0.03126626536250114, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:19, Epoch: 59, Batch: 60, Training Loss: 0.06746255829930306, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:19, Epoch: 59, Batch: 70, Training Loss: 0.03106519505381584, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:20, Epoch: 59, Batch: 80, Training Loss: 0.0463158093392849, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:21, Epoch: 59, Batch: 90, Training Loss: 0.03332078866660595, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:22, Epoch: 59, Batch: 100, Training Loss: 0.04997768364846707, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:23, Epoch: 59, Batch: 110, Training Loss: 0.02710619233548641, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:23, Epoch: 59, Batch: 120, Training Loss: 0.036616331338882445, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:24, Epoch: 59, Batch: 130, Training Loss: 0.035831968486309054, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:25, Epoch: 59, Batch: 140, Training Loss: 0.032574615254998204, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:26, Epoch: 59, Batch: 150, Training Loss: 0.03579898700118065, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:26, Epoch: 59, Batch: 160, Training Loss: 0.041951137408614156, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:27, Epoch: 59, Batch: 170, Training Loss: 0.028649710491299628, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:28, Epoch: 59, Batch: 180, Training Loss: 0.04358536936342716, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:29, Epoch: 59, Batch: 190, Training Loss: 0.05051102563738823, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:29, Epoch: 59, Batch: 200, Training Loss: 0.025837136059999467, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:30, Epoch: 59, Batch: 210, Training Loss: 0.030126017704606056, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:31, Epoch: 59, Batch: 220, Training Loss: 0.04148848280310631, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:32, Epoch: 59, Batch: 230, Training Loss: 0.028305502980947493, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:32, Epoch: 59, Batch: 240, Training Loss: 0.03598228506743908, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:33, Epoch: 59, Batch: 250, Training Loss: 0.05389997437596321, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:34, Epoch: 59, Batch: 260, Training Loss: 0.03486899770796299, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:35, Epoch: 59, Batch: 270, Training Loss: 0.036148425191640854, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:36, Epoch: 59, Batch: 280, Training Loss: 0.038980850204825404, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:36, Epoch: 59, Batch: 290, Training Loss: 0.043754047527909276, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:37, Epoch: 59, Batch: 300, Training Loss: 0.03974994607269764, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:38, Epoch: 59, Batch: 310, Training Loss: 0.0412400059401989, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:39, Epoch: 59, Batch: 320, Training Loss: 0.05234706625342369, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:39, Epoch: 59, Batch: 330, Training Loss: 0.02844819873571396, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:40, Epoch: 59, Batch: 340, Training Loss: 0.04039456360042095, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:41, Epoch: 59, Batch: 350, Training Loss: 0.04306163936853409, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:42, Epoch: 59, Batch: 360, Training Loss: 0.03530375137925148, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:42, Epoch: 59, Batch: 370, Training Loss: 0.0480619128793478, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:43, Epoch: 59, Batch: 380, Training Loss: 0.03462675139307976, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:44, Epoch: 59, Batch: 390, Training Loss: 0.030964918062090875, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:45, Epoch: 59, Batch: 400, Training Loss: 0.052713063359260556, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:45, Epoch: 59, Batch: 410, Training Loss: 0.04506357870995999, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:46, Epoch: 59, Batch: 420, Training Loss: 0.03919945061206818, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:47, Epoch: 59, Batch: 430, Training Loss: 0.03641429729759693, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:48, Epoch: 59, Batch: 440, Training Loss: 0.03172848597168922, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:48, Epoch: 59, Batch: 450, Training Loss: 0.030604081600904463, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:49, Epoch: 59, Batch: 460, Training Loss: 0.032438788935542105, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:50, Epoch: 59, Batch: 470, Training Loss: 0.049438872188329694, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:51, Epoch: 59, Batch: 480, Training Loss: 0.03638253509998322, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:51, Epoch: 59, Batch: 490, Training Loss: 0.03447014763951302, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:52, Epoch: 59, Batch: 500, Training Loss: 0.03730188794434071, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:53, Epoch: 59, Batch: 510, Training Loss: 0.028349561244249345, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:54, Epoch: 59, Batch: 520, Training Loss: 0.047104747220873835, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:54, Epoch: 59, Batch: 530, Training Loss: 0.034356151893734935, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:55, Epoch: 59, Batch: 540, Training Loss: 0.03227781690657139, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:56, Epoch: 59, Batch: 550, Training Loss: 0.054122264683246615, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:57, Epoch: 59, Batch: 560, Training Loss: 0.030884219333529472, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:57, Epoch: 59, Batch: 570, Training Loss: 0.034146780893206594, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:58, Epoch: 59, Batch: 580, Training Loss: 0.04153505899012089, LR: 0.0010000000000000002
Time, 2019-01-01T23:16:59, Epoch: 59, Batch: 590, Training Loss: 0.044733671098947526, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:00, Epoch: 59, Batch: 600, Training Loss: 0.05085379742085934, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:00, Epoch: 59, Batch: 610, Training Loss: 0.0424053955823183, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:01, Epoch: 59, Batch: 620, Training Loss: 0.03626080304384231, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:02, Epoch: 59, Batch: 630, Training Loss: 0.03632234036922455, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:03, Epoch: 59, Batch: 640, Training Loss: 0.03187556490302086, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:03, Epoch: 59, Batch: 650, Training Loss: 0.04054309874773025, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:04, Epoch: 59, Batch: 660, Training Loss: 0.022636521235108377, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:05, Epoch: 59, Batch: 670, Training Loss: 0.0444475244730711, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:06, Epoch: 59, Batch: 680, Training Loss: 0.03737410381436348, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:06, Epoch: 59, Batch: 690, Training Loss: 0.034448928013443944, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:07, Epoch: 59, Batch: 700, Training Loss: 0.020003697276115416, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:08, Epoch: 59, Batch: 710, Training Loss: 0.04434555880725384, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:09, Epoch: 59, Batch: 720, Training Loss: 0.028065601363778114, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:10, Epoch: 59, Batch: 730, Training Loss: 0.033741455897688864, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:10, Epoch: 59, Batch: 740, Training Loss: 0.037149854376912116, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:11, Epoch: 59, Batch: 750, Training Loss: 0.038806309923529625, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:12, Epoch: 59, Batch: 760, Training Loss: 0.036793694645166394, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:13, Epoch: 59, Batch: 770, Training Loss: 0.046903236955404284, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:13, Epoch: 59, Batch: 780, Training Loss: 0.032649582251906395, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:14, Epoch: 59, Batch: 790, Training Loss: 0.034130224585533143, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:15, Epoch: 59, Batch: 800, Training Loss: 0.031835134327411654, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:16, Epoch: 59, Batch: 810, Training Loss: 0.025107429176568986, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:16, Epoch: 59, Batch: 820, Training Loss: 0.0357537642121315, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:17, Epoch: 59, Batch: 830, Training Loss: 0.04621506184339523, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:18, Epoch: 59, Batch: 840, Training Loss: 0.03325250931084156, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:19, Epoch: 59, Batch: 850, Training Loss: 0.06067805588245392, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:19, Epoch: 59, Batch: 860, Training Loss: 0.031037408858537674, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:20, Epoch: 59, Batch: 870, Training Loss: 0.04434206932783127, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:21, Epoch: 59, Batch: 880, Training Loss: 0.046119124814867976, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:22, Epoch: 59, Batch: 890, Training Loss: 0.02988033927977085, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:22, Epoch: 59, Batch: 900, Training Loss: 0.044807910174131396, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:23, Epoch: 59, Batch: 910, Training Loss: 0.04975448288023472, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:24, Epoch: 59, Batch: 920, Training Loss: 0.04797236137092113, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:25, Epoch: 59, Batch: 930, Training Loss: 0.03841696083545685, LR: 0.0010000000000000002
Epoch: 59, Validation Top 1 acc: 98.69625854492188
Epoch: 59, Validation Top 5 acc: 100.0
Epoch: 59, Validation Set Loss: 0.043523937463760376
Start training epoch 60
Time, 2019-01-01T23:17:31, Epoch: 60, Batch: 10, Training Loss: 0.03762767277657986, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:31, Epoch: 60, Batch: 20, Training Loss: 0.045340456813573835, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:32, Epoch: 60, Batch: 30, Training Loss: 0.03720384053885937, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:33, Epoch: 60, Batch: 40, Training Loss: 0.04093094244599342, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:34, Epoch: 60, Batch: 50, Training Loss: 0.045384453237056734, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:34, Epoch: 60, Batch: 60, Training Loss: 0.02528548538684845, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:35, Epoch: 60, Batch: 70, Training Loss: 0.0421183280646801, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:36, Epoch: 60, Batch: 80, Training Loss: 0.031182019039988518, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:37, Epoch: 60, Batch: 90, Training Loss: 0.045062530785799026, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:37, Epoch: 60, Batch: 100, Training Loss: 0.03772583194077015, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:38, Epoch: 60, Batch: 110, Training Loss: 0.04210603646934032, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:39, Epoch: 60, Batch: 120, Training Loss: 0.022210536152124406, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:40, Epoch: 60, Batch: 130, Training Loss: 0.02567899487912655, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:41, Epoch: 60, Batch: 140, Training Loss: 0.03323854096233845, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:41, Epoch: 60, Batch: 150, Training Loss: 0.04128266796469689, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:42, Epoch: 60, Batch: 160, Training Loss: 0.03742271289229393, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:43, Epoch: 60, Batch: 170, Training Loss: 0.036701001971960065, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:44, Epoch: 60, Batch: 180, Training Loss: 0.035427477955818173, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:44, Epoch: 60, Batch: 190, Training Loss: 0.05036656111478806, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:45, Epoch: 60, Batch: 200, Training Loss: 0.0363267183303833, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:46, Epoch: 60, Batch: 210, Training Loss: 0.03600134812295437, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:47, Epoch: 60, Batch: 220, Training Loss: 0.030325690656900404, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:47, Epoch: 60, Batch: 230, Training Loss: 0.052078993245959285, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:48, Epoch: 60, Batch: 240, Training Loss: 0.03874207399785519, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:49, Epoch: 60, Batch: 250, Training Loss: 0.054506053775548936, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:50, Epoch: 60, Batch: 260, Training Loss: 0.04181746952235699, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:51, Epoch: 60, Batch: 270, Training Loss: 0.05234986171126366, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:51, Epoch: 60, Batch: 280, Training Loss: 0.06196858510375023, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:52, Epoch: 60, Batch: 290, Training Loss: 0.05263819247484207, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:53, Epoch: 60, Batch: 300, Training Loss: 0.039165204763412474, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:54, Epoch: 60, Batch: 310, Training Loss: 0.05455632172524929, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:54, Epoch: 60, Batch: 320, Training Loss: 0.05089692436158657, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:55, Epoch: 60, Batch: 330, Training Loss: 0.03910552151501179, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:56, Epoch: 60, Batch: 340, Training Loss: 0.02999381460249424, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:57, Epoch: 60, Batch: 350, Training Loss: 0.048372963815927504, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:58, Epoch: 60, Batch: 360, Training Loss: 0.0432451993227005, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:58, Epoch: 60, Batch: 370, Training Loss: 0.035855609178543094, LR: 0.0010000000000000002
Time, 2019-01-01T23:17:59, Epoch: 60, Batch: 380, Training Loss: 0.03455770500004292, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:00, Epoch: 60, Batch: 390, Training Loss: 0.028379109501838685, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:01, Epoch: 60, Batch: 400, Training Loss: 0.03920198939740658, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:01, Epoch: 60, Batch: 410, Training Loss: 0.03482215479016304, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:02, Epoch: 60, Batch: 420, Training Loss: 0.04042035564780235, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:03, Epoch: 60, Batch: 430, Training Loss: 0.02859666831791401, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:04, Epoch: 60, Batch: 440, Training Loss: 0.02647909037768841, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:04, Epoch: 60, Batch: 450, Training Loss: 0.02992016524076462, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:05, Epoch: 60, Batch: 460, Training Loss: 0.02996487095952034, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:06, Epoch: 60, Batch: 470, Training Loss: 0.04141944870352745, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:07, Epoch: 60, Batch: 480, Training Loss: 0.043725477531552315, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:07, Epoch: 60, Batch: 490, Training Loss: 0.029395941644906998, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:08, Epoch: 60, Batch: 500, Training Loss: 0.03963342532515526, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:09, Epoch: 60, Batch: 510, Training Loss: 0.05186534151434898, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:10, Epoch: 60, Batch: 520, Training Loss: 0.03144202679395676, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:11, Epoch: 60, Batch: 530, Training Loss: 0.031940621137619016, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:11, Epoch: 60, Batch: 540, Training Loss: 0.03741556815803051, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:12, Epoch: 60, Batch: 550, Training Loss: 0.02492316849529743, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:13, Epoch: 60, Batch: 560, Training Loss: 0.04926732145249844, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:13, Epoch: 60, Batch: 570, Training Loss: 0.034162473306059836, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:14, Epoch: 60, Batch: 580, Training Loss: 0.03695488646626473, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:15, Epoch: 60, Batch: 590, Training Loss: 0.04496954046189785, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:16, Epoch: 60, Batch: 600, Training Loss: 0.04435396566987038, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:17, Epoch: 60, Batch: 610, Training Loss: 0.033397606760263446, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:17, Epoch: 60, Batch: 620, Training Loss: 0.047165288031101225, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:18, Epoch: 60, Batch: 630, Training Loss: 0.023776881396770477, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:19, Epoch: 60, Batch: 640, Training Loss: 0.03243682458996773, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:20, Epoch: 60, Batch: 650, Training Loss: 0.03115089759230614, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:20, Epoch: 60, Batch: 660, Training Loss: 0.027172399684786797, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:21, Epoch: 60, Batch: 670, Training Loss: 0.02694183699786663, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:22, Epoch: 60, Batch: 680, Training Loss: 0.04060659743845463, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:23, Epoch: 60, Batch: 690, Training Loss: 0.028241055458784102, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:24, Epoch: 60, Batch: 700, Training Loss: 0.04202467724680901, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:24, Epoch: 60, Batch: 710, Training Loss: 0.03995946310460567, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:25, Epoch: 60, Batch: 720, Training Loss: 0.04235993772745132, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:26, Epoch: 60, Batch: 730, Training Loss: 0.04228670634329319, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:27, Epoch: 60, Batch: 740, Training Loss: 0.028739259764552116, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:27, Epoch: 60, Batch: 750, Training Loss: 0.03161770664155483, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:28, Epoch: 60, Batch: 760, Training Loss: 0.0410246454179287, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:29, Epoch: 60, Batch: 770, Training Loss: 0.029411540180444718, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:30, Epoch: 60, Batch: 780, Training Loss: 0.037906209379434584, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:31, Epoch: 60, Batch: 790, Training Loss: 0.045318032056093215, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:31, Epoch: 60, Batch: 800, Training Loss: 0.04439055174589157, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:32, Epoch: 60, Batch: 810, Training Loss: 0.03748573511838913, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:33, Epoch: 60, Batch: 820, Training Loss: 0.051487159729003903, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:34, Epoch: 60, Batch: 830, Training Loss: 0.03195750638842583, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:34, Epoch: 60, Batch: 840, Training Loss: 0.045796331390738486, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:35, Epoch: 60, Batch: 850, Training Loss: 0.024992237612605096, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:36, Epoch: 60, Batch: 860, Training Loss: 0.02881847880780697, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:37, Epoch: 60, Batch: 870, Training Loss: 0.04685794860124588, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:38, Epoch: 60, Batch: 880, Training Loss: 0.04471385627985001, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:38, Epoch: 60, Batch: 890, Training Loss: 0.0329496581107378, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:39, Epoch: 60, Batch: 900, Training Loss: 0.03509244918823242, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:40, Epoch: 60, Batch: 910, Training Loss: 0.038372987881302834, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:41, Epoch: 60, Batch: 920, Training Loss: 0.03542595952749252, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:42, Epoch: 60, Batch: 930, Training Loss: 0.047072059288620946, LR: 0.0010000000000000002
Epoch: 60, Validation Top 1 acc: 98.67635345458984
Epoch: 60, Validation Top 5 acc: 100.0
Epoch: 60, Validation Set Loss: 0.044250622391700745
Start training epoch 61
Time, 2019-01-01T23:18:48, Epoch: 61, Batch: 10, Training Loss: 0.03763061426579952, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:49, Epoch: 61, Batch: 20, Training Loss: 0.0243243008852005, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:50, Epoch: 61, Batch: 30, Training Loss: 0.024958594888448715, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:50, Epoch: 61, Batch: 40, Training Loss: 0.04016581512987614, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:51, Epoch: 61, Batch: 50, Training Loss: 0.0325972318649292, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:52, Epoch: 61, Batch: 60, Training Loss: 0.02533152438700199, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:53, Epoch: 61, Batch: 70, Training Loss: 0.033619675785303116, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:54, Epoch: 61, Batch: 80, Training Loss: 0.03740306794643402, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:55, Epoch: 61, Batch: 90, Training Loss: 0.023733117803931236, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:55, Epoch: 61, Batch: 100, Training Loss: 0.04713292717933655, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:56, Epoch: 61, Batch: 110, Training Loss: 0.04967347756028175, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:57, Epoch: 61, Batch: 120, Training Loss: 0.06118959188461304, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:58, Epoch: 61, Batch: 130, Training Loss: 0.03604805022478104, LR: 0.0010000000000000002
Time, 2019-01-01T23:18:59, Epoch: 61, Batch: 140, Training Loss: 0.035647259280085566, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:00, Epoch: 61, Batch: 150, Training Loss: 0.039921092242002486, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:00, Epoch: 61, Batch: 160, Training Loss: 0.028690847009420394, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:01, Epoch: 61, Batch: 170, Training Loss: 0.03018971122801304, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:02, Epoch: 61, Batch: 180, Training Loss: 0.04415520653128624, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:03, Epoch: 61, Batch: 190, Training Loss: 0.04477300234138966, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:04, Epoch: 61, Batch: 200, Training Loss: 0.024961480498313905, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:04, Epoch: 61, Batch: 210, Training Loss: 0.0524211086332798, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:05, Epoch: 61, Batch: 220, Training Loss: 0.0362980030477047, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:06, Epoch: 61, Batch: 230, Training Loss: 0.04564489312469959, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:07, Epoch: 61, Batch: 240, Training Loss: 0.028998323529958726, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:08, Epoch: 61, Batch: 250, Training Loss: 0.051842699944972995, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:08, Epoch: 61, Batch: 260, Training Loss: 0.021767376363277434, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:09, Epoch: 61, Batch: 270, Training Loss: 0.04313517212867737, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:10, Epoch: 61, Batch: 280, Training Loss: 0.04731703922152519, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:11, Epoch: 61, Batch: 290, Training Loss: 0.024760926514863967, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:12, Epoch: 61, Batch: 300, Training Loss: 0.038447529822587964, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:13, Epoch: 61, Batch: 310, Training Loss: 0.024507172405719757, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:14, Epoch: 61, Batch: 320, Training Loss: 0.05174602679908276, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:15, Epoch: 61, Batch: 330, Training Loss: 0.03281182423233986, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:16, Epoch: 61, Batch: 340, Training Loss: 0.0395258966833353, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:17, Epoch: 61, Batch: 350, Training Loss: 0.0408388938754797, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:17, Epoch: 61, Batch: 360, Training Loss: 0.038857384398579595, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:18, Epoch: 61, Batch: 370, Training Loss: 0.04269094690680504, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:19, Epoch: 61, Batch: 380, Training Loss: 0.03151747174561024, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:20, Epoch: 61, Batch: 390, Training Loss: 0.036608441919088366, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:21, Epoch: 61, Batch: 400, Training Loss: 0.04891604892909527, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:22, Epoch: 61, Batch: 410, Training Loss: 0.028213326632976533, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:23, Epoch: 61, Batch: 420, Training Loss: 0.05706244483590126, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:24, Epoch: 61, Batch: 430, Training Loss: 0.030633098259568215, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:24, Epoch: 61, Batch: 440, Training Loss: 0.05641875192523003, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:25, Epoch: 61, Batch: 450, Training Loss: 0.03287711441516876, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:26, Epoch: 61, Batch: 460, Training Loss: 0.03728230521082878, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:27, Epoch: 61, Batch: 470, Training Loss: 0.026521481946110724, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:28, Epoch: 61, Batch: 480, Training Loss: 0.04454940147697926, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:28, Epoch: 61, Batch: 490, Training Loss: 0.05625857375562191, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:29, Epoch: 61, Batch: 500, Training Loss: 0.030270665511488916, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:30, Epoch: 61, Batch: 510, Training Loss: 0.027868513390421866, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:31, Epoch: 61, Batch: 520, Training Loss: 0.0356699775904417, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:31, Epoch: 61, Batch: 530, Training Loss: 0.06462458148598671, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:32, Epoch: 61, Batch: 540, Training Loss: 0.049992429465055464, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:33, Epoch: 61, Batch: 550, Training Loss: 0.030234968289732933, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:34, Epoch: 61, Batch: 560, Training Loss: 0.030657235905528067, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:35, Epoch: 61, Batch: 570, Training Loss: 0.04889629520475865, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:36, Epoch: 61, Batch: 580, Training Loss: 0.03264328241348267, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:36, Epoch: 61, Batch: 590, Training Loss: 0.046757510676980016, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:37, Epoch: 61, Batch: 600, Training Loss: 0.027875392884016036, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:38, Epoch: 61, Batch: 610, Training Loss: 0.02501964122056961, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:39, Epoch: 61, Batch: 620, Training Loss: 0.02806938625872135, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:40, Epoch: 61, Batch: 630, Training Loss: 0.042417466267943384, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:40, Epoch: 61, Batch: 640, Training Loss: 0.043331129103899, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:41, Epoch: 61, Batch: 650, Training Loss: 0.05661802925169468, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:42, Epoch: 61, Batch: 660, Training Loss: 0.04576036147773266, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:43, Epoch: 61, Batch: 670, Training Loss: 0.025930435955524446, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:44, Epoch: 61, Batch: 680, Training Loss: 0.055085495859384534, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:44, Epoch: 61, Batch: 690, Training Loss: 0.027891405299305914, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:45, Epoch: 61, Batch: 700, Training Loss: 0.031148651242256166, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:46, Epoch: 61, Batch: 710, Training Loss: 0.040385700017213824, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:47, Epoch: 61, Batch: 720, Training Loss: 0.06358508728444576, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:47, Epoch: 61, Batch: 730, Training Loss: 0.025402090698480605, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:48, Epoch: 61, Batch: 740, Training Loss: 0.03367079049348831, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:49, Epoch: 61, Batch: 750, Training Loss: 0.04676876664161682, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:50, Epoch: 61, Batch: 760, Training Loss: 0.031560782343149185, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:51, Epoch: 61, Batch: 770, Training Loss: 0.03338364697992802, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:51, Epoch: 61, Batch: 780, Training Loss: 0.04637448489665985, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:52, Epoch: 61, Batch: 790, Training Loss: 0.02984994985163212, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:53, Epoch: 61, Batch: 800, Training Loss: 0.03059503063559532, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:54, Epoch: 61, Batch: 810, Training Loss: 0.03967224806547165, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:54, Epoch: 61, Batch: 820, Training Loss: 0.035247742757201196, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:55, Epoch: 61, Batch: 830, Training Loss: 0.06529500409960746, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:56, Epoch: 61, Batch: 840, Training Loss: 0.030555351823568343, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:57, Epoch: 61, Batch: 850, Training Loss: 0.03695210888981819, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:58, Epoch: 61, Batch: 860, Training Loss: 0.03237241618335247, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:59, Epoch: 61, Batch: 870, Training Loss: 0.038812252879142764, LR: 0.0010000000000000002
Time, 2019-01-01T23:19:59, Epoch: 61, Batch: 880, Training Loss: 0.02594558633863926, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:00, Epoch: 61, Batch: 890, Training Loss: 0.04660430774092674, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:01, Epoch: 61, Batch: 900, Training Loss: 0.02643247991800308, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:02, Epoch: 61, Batch: 910, Training Loss: 0.03462362177670002, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:02, Epoch: 61, Batch: 920, Training Loss: 0.027794773876667022, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:03, Epoch: 61, Batch: 930, Training Loss: 0.05162249356508255, LR: 0.0010000000000000002
Epoch: 61, Validation Top 1 acc: 98.70620727539062
Epoch: 61, Validation Top 5 acc: 100.0
Epoch: 61, Validation Set Loss: 0.043095316737890244
Start training epoch 62
Time, 2019-01-01T23:20:09, Epoch: 62, Batch: 10, Training Loss: 0.04318883717060089, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:10, Epoch: 62, Batch: 20, Training Loss: 0.05870615169405937, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:11, Epoch: 62, Batch: 30, Training Loss: 0.04557557068765163, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:12, Epoch: 62, Batch: 40, Training Loss: 0.033787757903337476, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:13, Epoch: 62, Batch: 50, Training Loss: 0.029390534013509752, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:13, Epoch: 62, Batch: 60, Training Loss: 0.0385772917419672, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:14, Epoch: 62, Batch: 70, Training Loss: 0.03283661119639873, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:15, Epoch: 62, Batch: 80, Training Loss: 0.024754973500967024, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:16, Epoch: 62, Batch: 90, Training Loss: 0.04929783418774605, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:16, Epoch: 62, Batch: 100, Training Loss: 0.0401122085750103, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:17, Epoch: 62, Batch: 110, Training Loss: 0.0440529614686966, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:18, Epoch: 62, Batch: 120, Training Loss: 0.04296911172568798, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:19, Epoch: 62, Batch: 130, Training Loss: 0.049542739242315295, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:19, Epoch: 62, Batch: 140, Training Loss: 0.03060409314930439, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:20, Epoch: 62, Batch: 150, Training Loss: 0.029580670222640038, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:21, Epoch: 62, Batch: 160, Training Loss: 0.03248051926493645, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:22, Epoch: 62, Batch: 170, Training Loss: 0.039380431175231934, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:23, Epoch: 62, Batch: 180, Training Loss: 0.03866436704993248, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:23, Epoch: 62, Batch: 190, Training Loss: 0.03991356119513512, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:24, Epoch: 62, Batch: 200, Training Loss: 0.06404082886874676, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:25, Epoch: 62, Batch: 210, Training Loss: 0.03690180964767933, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:26, Epoch: 62, Batch: 220, Training Loss: 0.03615544997155666, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:27, Epoch: 62, Batch: 230, Training Loss: 0.051929447799921036, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:27, Epoch: 62, Batch: 240, Training Loss: 0.028987443819642068, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:28, Epoch: 62, Batch: 250, Training Loss: 0.03555084727704525, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:29, Epoch: 62, Batch: 260, Training Loss: 0.03636819235980511, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:30, Epoch: 62, Batch: 270, Training Loss: 0.03898915946483612, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:31, Epoch: 62, Batch: 280, Training Loss: 0.03360005095601082, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:31, Epoch: 62, Batch: 290, Training Loss: 0.022309104353189467, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:32, Epoch: 62, Batch: 300, Training Loss: 0.04788426794111729, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:33, Epoch: 62, Batch: 310, Training Loss: 0.04274215772747993, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:34, Epoch: 62, Batch: 320, Training Loss: 0.028654545545578003, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:35, Epoch: 62, Batch: 330, Training Loss: 0.03847301155328751, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:35, Epoch: 62, Batch: 340, Training Loss: 0.03139497078955174, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:36, Epoch: 62, Batch: 350, Training Loss: 0.039830608665943144, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:37, Epoch: 62, Batch: 360, Training Loss: 0.06078842245042324, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:38, Epoch: 62, Batch: 370, Training Loss: 0.05391770116984844, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:38, Epoch: 62, Batch: 380, Training Loss: 0.04237696416676044, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:39, Epoch: 62, Batch: 390, Training Loss: 0.0471039891242981, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:40, Epoch: 62, Batch: 400, Training Loss: 0.027046524733304978, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:41, Epoch: 62, Batch: 410, Training Loss: 0.04393915496766567, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:42, Epoch: 62, Batch: 420, Training Loss: 0.027504880726337434, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:42, Epoch: 62, Batch: 430, Training Loss: 0.05229699686169624, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:43, Epoch: 62, Batch: 440, Training Loss: 0.0448562428355217, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:44, Epoch: 62, Batch: 450, Training Loss: 0.01809002533555031, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:45, Epoch: 62, Batch: 460, Training Loss: 0.03161624893546104, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:45, Epoch: 62, Batch: 470, Training Loss: 0.0418386984616518, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:46, Epoch: 62, Batch: 480, Training Loss: 0.03761999495327473, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:47, Epoch: 62, Batch: 490, Training Loss: 0.03842920288443565, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:48, Epoch: 62, Batch: 500, Training Loss: 0.04395646043121815, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:49, Epoch: 62, Batch: 510, Training Loss: 0.04408760406076908, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:49, Epoch: 62, Batch: 520, Training Loss: 0.05291106514632702, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:50, Epoch: 62, Batch: 530, Training Loss: 0.028214123845100404, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:51, Epoch: 62, Batch: 540, Training Loss: 0.03890277594327927, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:52, Epoch: 62, Batch: 550, Training Loss: 0.033836730569601056, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:53, Epoch: 62, Batch: 560, Training Loss: 0.03392686918377876, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:53, Epoch: 62, Batch: 570, Training Loss: 0.04949476085603237, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:54, Epoch: 62, Batch: 580, Training Loss: 0.03601019866764545, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:55, Epoch: 62, Batch: 590, Training Loss: 0.029933567345142364, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:56, Epoch: 62, Batch: 600, Training Loss: 0.03221209608018398, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:57, Epoch: 62, Batch: 610, Training Loss: 0.05232947878539562, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:58, Epoch: 62, Batch: 620, Training Loss: 0.03336548060178757, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:58, Epoch: 62, Batch: 630, Training Loss: 0.04006944224238396, LR: 0.0010000000000000002
Time, 2019-01-01T23:20:59, Epoch: 62, Batch: 640, Training Loss: 0.026546645909547806, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:00, Epoch: 62, Batch: 650, Training Loss: 0.04123682156205177, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:01, Epoch: 62, Batch: 660, Training Loss: 0.03930729106068611, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:02, Epoch: 62, Batch: 670, Training Loss: 0.03611307367682457, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:02, Epoch: 62, Batch: 680, Training Loss: 0.028332096710801124, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:03, Epoch: 62, Batch: 690, Training Loss: 0.03538182191550732, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:04, Epoch: 62, Batch: 700, Training Loss: 0.034426867589354516, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:05, Epoch: 62, Batch: 710, Training Loss: 0.023446288704872132, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:06, Epoch: 62, Batch: 720, Training Loss: 0.05586362034082413, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:07, Epoch: 62, Batch: 730, Training Loss: 0.03951774500310421, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:07, Epoch: 62, Batch: 740, Training Loss: 0.04128157272934914, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:08, Epoch: 62, Batch: 750, Training Loss: 0.026819875836372374, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:09, Epoch: 62, Batch: 760, Training Loss: 0.022354695945978165, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:10, Epoch: 62, Batch: 770, Training Loss: 0.020924700051546098, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:10, Epoch: 62, Batch: 780, Training Loss: 0.032563627883791926, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:11, Epoch: 62, Batch: 790, Training Loss: 0.02977532781660557, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:12, Epoch: 62, Batch: 800, Training Loss: 0.03164794594049454, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:13, Epoch: 62, Batch: 810, Training Loss: 0.03001416102051735, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:14, Epoch: 62, Batch: 820, Training Loss: 0.037288005277514455, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:15, Epoch: 62, Batch: 830, Training Loss: 0.04254799298942089, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:15, Epoch: 62, Batch: 840, Training Loss: 0.026117603480815887, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:16, Epoch: 62, Batch: 850, Training Loss: 0.038279281556606294, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:17, Epoch: 62, Batch: 860, Training Loss: 0.036378328874707225, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:18, Epoch: 62, Batch: 870, Training Loss: 0.03421502150595188, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:18, Epoch: 62, Batch: 880, Training Loss: 0.033515436574816704, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:19, Epoch: 62, Batch: 890, Training Loss: 0.03732088133692742, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:20, Epoch: 62, Batch: 900, Training Loss: 0.03367026895284653, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:21, Epoch: 62, Batch: 910, Training Loss: 0.025339948385953902, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:22, Epoch: 62, Batch: 920, Training Loss: 0.05739617757499218, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:23, Epoch: 62, Batch: 930, Training Loss: 0.04086574949324131, LR: 0.0010000000000000002
Epoch: 62, Validation Top 1 acc: 98.7161636352539
Epoch: 62, Validation Top 5 acc: 100.0
Epoch: 62, Validation Set Loss: 0.04340122267603874
Start training epoch 63
Time, 2019-01-01T23:21:29, Epoch: 63, Batch: 10, Training Loss: 0.0410581111907959, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:30, Epoch: 63, Batch: 20, Training Loss: 0.03346054628491402, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:30, Epoch: 63, Batch: 30, Training Loss: 0.038628722354769704, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:31, Epoch: 63, Batch: 40, Training Loss: 0.02915533185005188, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:32, Epoch: 63, Batch: 50, Training Loss: 0.033374537527561185, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:33, Epoch: 63, Batch: 60, Training Loss: 0.033022158592939374, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:34, Epoch: 63, Batch: 70, Training Loss: 0.0343148585408926, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:34, Epoch: 63, Batch: 80, Training Loss: 0.05503726750612259, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:35, Epoch: 63, Batch: 90, Training Loss: 0.04618813320994377, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:36, Epoch: 63, Batch: 100, Training Loss: 0.04664374366402626, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:37, Epoch: 63, Batch: 110, Training Loss: 0.028929026424884798, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:37, Epoch: 63, Batch: 120, Training Loss: 0.029976135492324828, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:38, Epoch: 63, Batch: 130, Training Loss: 0.045471292734146115, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:39, Epoch: 63, Batch: 140, Training Loss: 0.03405357226729393, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:40, Epoch: 63, Batch: 150, Training Loss: 0.04056958518922329, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:41, Epoch: 63, Batch: 160, Training Loss: 0.03700190670788288, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:41, Epoch: 63, Batch: 170, Training Loss: 0.0405281774699688, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:42, Epoch: 63, Batch: 180, Training Loss: 0.03664964586496353, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:43, Epoch: 63, Batch: 190, Training Loss: 0.027838364616036414, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:44, Epoch: 63, Batch: 200, Training Loss: 0.03192145749926567, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:44, Epoch: 63, Batch: 210, Training Loss: 0.041974563896656034, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:45, Epoch: 63, Batch: 220, Training Loss: 0.03086600750684738, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:46, Epoch: 63, Batch: 230, Training Loss: 0.05097773969173432, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:47, Epoch: 63, Batch: 240, Training Loss: 0.03042420223355293, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:47, Epoch: 63, Batch: 250, Training Loss: 0.03539728559553623, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:48, Epoch: 63, Batch: 260, Training Loss: 0.03788447082042694, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:49, Epoch: 63, Batch: 270, Training Loss: 0.05805006995797157, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:50, Epoch: 63, Batch: 280, Training Loss: 0.03632514476776123, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:51, Epoch: 63, Batch: 290, Training Loss: 0.038408661261200905, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:51, Epoch: 63, Batch: 300, Training Loss: 0.036331598088145255, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:52, Epoch: 63, Batch: 310, Training Loss: 0.04260221049189568, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:53, Epoch: 63, Batch: 320, Training Loss: 0.04196579903364182, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:54, Epoch: 63, Batch: 330, Training Loss: 0.03503630757331848, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:54, Epoch: 63, Batch: 340, Training Loss: 0.02802741900086403, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:55, Epoch: 63, Batch: 350, Training Loss: 0.036558859422802924, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:56, Epoch: 63, Batch: 360, Training Loss: 0.031894703209400174, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:57, Epoch: 63, Batch: 370, Training Loss: 0.03101854957640171, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:58, Epoch: 63, Batch: 380, Training Loss: 0.0415272768586874, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:58, Epoch: 63, Batch: 390, Training Loss: 0.0450969111174345, LR: 0.0010000000000000002
Time, 2019-01-01T23:21:59, Epoch: 63, Batch: 400, Training Loss: 0.03895942494273186, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:00, Epoch: 63, Batch: 410, Training Loss: 0.05178687945008278, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:01, Epoch: 63, Batch: 420, Training Loss: 0.04112058952450752, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:02, Epoch: 63, Batch: 430, Training Loss: 0.0327878151088953, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:03, Epoch: 63, Batch: 440, Training Loss: 0.03494037427008152, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:03, Epoch: 63, Batch: 450, Training Loss: 0.046955230087041853, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:04, Epoch: 63, Batch: 460, Training Loss: 0.0412675142288208, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:05, Epoch: 63, Batch: 470, Training Loss: 0.03038346990942955, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:06, Epoch: 63, Batch: 480, Training Loss: 0.03020782731473446, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:07, Epoch: 63, Batch: 490, Training Loss: 0.056338944286108014, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:07, Epoch: 63, Batch: 500, Training Loss: 0.03456808105111122, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:08, Epoch: 63, Batch: 510, Training Loss: 0.04655296392738819, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:09, Epoch: 63, Batch: 520, Training Loss: 0.041476166993379596, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:10, Epoch: 63, Batch: 530, Training Loss: 0.03939492404460907, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:11, Epoch: 63, Batch: 540, Training Loss: 0.035934795066714285, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:11, Epoch: 63, Batch: 550, Training Loss: 0.030220664292573928, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:12, Epoch: 63, Batch: 560, Training Loss: 0.048097424954175946, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:13, Epoch: 63, Batch: 570, Training Loss: 0.034721901267766954, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:14, Epoch: 63, Batch: 580, Training Loss: 0.0290635421872139, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:15, Epoch: 63, Batch: 590, Training Loss: 0.042721742764115334, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:15, Epoch: 63, Batch: 600, Training Loss: 0.03264136090874672, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:16, Epoch: 63, Batch: 610, Training Loss: 0.03502272851765156, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:17, Epoch: 63, Batch: 620, Training Loss: 0.04005403257906437, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:18, Epoch: 63, Batch: 630, Training Loss: 0.05701583176851273, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:19, Epoch: 63, Batch: 640, Training Loss: 0.04731685295701027, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:20, Epoch: 63, Batch: 650, Training Loss: 0.02314702160656452, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:20, Epoch: 63, Batch: 660, Training Loss: 0.03382844515144825, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:21, Epoch: 63, Batch: 670, Training Loss: 0.023772380128502846, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:22, Epoch: 63, Batch: 680, Training Loss: 0.0446370605379343, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:23, Epoch: 63, Batch: 690, Training Loss: 0.04671417586505413, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:24, Epoch: 63, Batch: 700, Training Loss: 0.051648881658911704, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:25, Epoch: 63, Batch: 710, Training Loss: 0.028054872527718544, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:25, Epoch: 63, Batch: 720, Training Loss: 0.036465102806687355, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:26, Epoch: 63, Batch: 730, Training Loss: 0.03078525960445404, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:27, Epoch: 63, Batch: 740, Training Loss: 0.038862984627485275, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:28, Epoch: 63, Batch: 750, Training Loss: 0.03923148177564144, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:29, Epoch: 63, Batch: 760, Training Loss: 0.05333305895328522, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:30, Epoch: 63, Batch: 770, Training Loss: 0.04044022709131241, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:31, Epoch: 63, Batch: 780, Training Loss: 0.04274122081696987, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:32, Epoch: 63, Batch: 790, Training Loss: 0.037951691448688506, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:33, Epoch: 63, Batch: 800, Training Loss: 0.03604207411408424, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:34, Epoch: 63, Batch: 810, Training Loss: 0.030329808592796326, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:34, Epoch: 63, Batch: 820, Training Loss: 0.04342349767684937, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:35, Epoch: 63, Batch: 830, Training Loss: 0.049338510259985924, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:36, Epoch: 63, Batch: 840, Training Loss: 0.018892509490251543, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:37, Epoch: 63, Batch: 850, Training Loss: 0.032548677176237106, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:38, Epoch: 63, Batch: 860, Training Loss: 0.040452956408262256, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:38, Epoch: 63, Batch: 870, Training Loss: 0.027396483346819878, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:39, Epoch: 63, Batch: 880, Training Loss: 0.03165725320577621, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:40, Epoch: 63, Batch: 890, Training Loss: 0.02792477756738663, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:41, Epoch: 63, Batch: 900, Training Loss: 0.029868210852146148, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:42, Epoch: 63, Batch: 910, Training Loss: 0.035209009796380995, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:42, Epoch: 63, Batch: 920, Training Loss: 0.03646701164543629, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:43, Epoch: 63, Batch: 930, Training Loss: 0.03466472700238228, LR: 0.0010000000000000002
Epoch: 63, Validation Top 1 acc: 98.7161636352539
Epoch: 63, Validation Top 5 acc: 100.0
Epoch: 63, Validation Set Loss: 0.04355550929903984
Start training epoch 64
Time, 2019-01-01T23:22:49, Epoch: 64, Batch: 10, Training Loss: 0.026581020653247835, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:50, Epoch: 64, Batch: 20, Training Loss: 0.03235461451113224, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:51, Epoch: 64, Batch: 30, Training Loss: 0.03767042569816113, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:52, Epoch: 64, Batch: 40, Training Loss: 0.02563946694135666, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:52, Epoch: 64, Batch: 50, Training Loss: 0.04506191127002239, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:53, Epoch: 64, Batch: 60, Training Loss: 0.023010719567537308, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:54, Epoch: 64, Batch: 70, Training Loss: 0.024711351469159128, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:55, Epoch: 64, Batch: 80, Training Loss: 0.04707893505692482, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:55, Epoch: 64, Batch: 90, Training Loss: 0.03594730123877525, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:56, Epoch: 64, Batch: 100, Training Loss: 0.0342567179352045, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:57, Epoch: 64, Batch: 110, Training Loss: 0.03826947994530201, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:58, Epoch: 64, Batch: 120, Training Loss: 0.03647671565413475, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:58, Epoch: 64, Batch: 130, Training Loss: 0.03902112878859043, LR: 0.0010000000000000002
Time, 2019-01-01T23:22:59, Epoch: 64, Batch: 140, Training Loss: 0.047592127695679665, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:00, Epoch: 64, Batch: 150, Training Loss: 0.04260286428034306, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:01, Epoch: 64, Batch: 160, Training Loss: 0.05358441993594169, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:02, Epoch: 64, Batch: 170, Training Loss: 0.0528814360499382, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:02, Epoch: 64, Batch: 180, Training Loss: 0.034297018125653264, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:03, Epoch: 64, Batch: 190, Training Loss: 0.035435694828629495, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:04, Epoch: 64, Batch: 200, Training Loss: 0.031121670082211495, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:05, Epoch: 64, Batch: 210, Training Loss: 0.030070669203996658, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:05, Epoch: 64, Batch: 220, Training Loss: 0.03450163416564465, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:06, Epoch: 64, Batch: 230, Training Loss: 0.03497211523354053, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:07, Epoch: 64, Batch: 240, Training Loss: 0.031125741824507714, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:08, Epoch: 64, Batch: 250, Training Loss: 0.045681152120232583, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:08, Epoch: 64, Batch: 260, Training Loss: 0.05885097086429596, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:09, Epoch: 64, Batch: 270, Training Loss: 0.03658667430281639, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:10, Epoch: 64, Batch: 280, Training Loss: 0.03587433360517025, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:11, Epoch: 64, Batch: 290, Training Loss: 0.03114190772175789, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:12, Epoch: 64, Batch: 300, Training Loss: 0.04444851838052273, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:13, Epoch: 64, Batch: 310, Training Loss: 0.04920689463615417, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:13, Epoch: 64, Batch: 320, Training Loss: 0.04227069020271301, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:14, Epoch: 64, Batch: 330, Training Loss: 0.02366526797413826, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:15, Epoch: 64, Batch: 340, Training Loss: 0.035770117118954656, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:16, Epoch: 64, Batch: 350, Training Loss: 0.052627087384462354, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:17, Epoch: 64, Batch: 360, Training Loss: 0.0372966680675745, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:17, Epoch: 64, Batch: 370, Training Loss: 0.04154289215803146, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:18, Epoch: 64, Batch: 380, Training Loss: 0.03643525764346123, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:19, Epoch: 64, Batch: 390, Training Loss: 0.04058823250234127, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:20, Epoch: 64, Batch: 400, Training Loss: 0.03246406987309456, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:20, Epoch: 64, Batch: 410, Training Loss: 0.03544406183063984, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:21, Epoch: 64, Batch: 420, Training Loss: 0.024701519310474394, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:22, Epoch: 64, Batch: 430, Training Loss: 0.029937302321195604, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:23, Epoch: 64, Batch: 440, Training Loss: 0.03891303278505802, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:24, Epoch: 64, Batch: 450, Training Loss: 0.05589470863342285, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:24, Epoch: 64, Batch: 460, Training Loss: 0.029127372801303862, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:25, Epoch: 64, Batch: 470, Training Loss: 0.031170453876256943, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:26, Epoch: 64, Batch: 480, Training Loss: 0.026605632901191712, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:27, Epoch: 64, Batch: 490, Training Loss: 0.03342852666974068, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:28, Epoch: 64, Batch: 500, Training Loss: 0.03657226264476776, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:28, Epoch: 64, Batch: 510, Training Loss: 0.02661844715476036, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:29, Epoch: 64, Batch: 520, Training Loss: 0.041001103818416595, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:30, Epoch: 64, Batch: 530, Training Loss: 0.032385139167308806, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:31, Epoch: 64, Batch: 540, Training Loss: 0.08685041069984437, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:32, Epoch: 64, Batch: 550, Training Loss: 0.03618488498032093, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:32, Epoch: 64, Batch: 560, Training Loss: 0.028995658084750176, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:33, Epoch: 64, Batch: 570, Training Loss: 0.031120043992996217, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:34, Epoch: 64, Batch: 580, Training Loss: 0.04439848586916924, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:35, Epoch: 64, Batch: 590, Training Loss: 0.04667044878005981, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:35, Epoch: 64, Batch: 600, Training Loss: 0.056426165997982024, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:36, Epoch: 64, Batch: 610, Training Loss: 0.03479284569621086, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:37, Epoch: 64, Batch: 620, Training Loss: 0.032218882068991664, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:38, Epoch: 64, Batch: 630, Training Loss: 0.03370533622801304, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:38, Epoch: 64, Batch: 640, Training Loss: 0.030320558696985245, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:39, Epoch: 64, Batch: 650, Training Loss: 0.03483424559235573, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:40, Epoch: 64, Batch: 660, Training Loss: 0.030011502280831336, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:41, Epoch: 64, Batch: 670, Training Loss: 0.03637857474386692, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:41, Epoch: 64, Batch: 680, Training Loss: 0.035227684304118156, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:42, Epoch: 64, Batch: 690, Training Loss: 0.027672694623470308, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:43, Epoch: 64, Batch: 700, Training Loss: 0.03196927383542061, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:44, Epoch: 64, Batch: 710, Training Loss: 0.048969469591975213, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:44, Epoch: 64, Batch: 720, Training Loss: 0.04840310364961624, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:45, Epoch: 64, Batch: 730, Training Loss: 0.024056605994701385, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:46, Epoch: 64, Batch: 740, Training Loss: 0.0252131424844265, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:47, Epoch: 64, Batch: 750, Training Loss: 0.03410930670797825, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:48, Epoch: 64, Batch: 760, Training Loss: 0.03732364177703858, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:49, Epoch: 64, Batch: 770, Training Loss: 0.03321466706693173, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:50, Epoch: 64, Batch: 780, Training Loss: 0.037894172966480254, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:51, Epoch: 64, Batch: 790, Training Loss: 0.047069800272583964, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:52, Epoch: 64, Batch: 800, Training Loss: 0.04772995449602604, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:52, Epoch: 64, Batch: 810, Training Loss: 0.024810509011149406, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:53, Epoch: 64, Batch: 820, Training Loss: 0.05876849628984928, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:54, Epoch: 64, Batch: 830, Training Loss: 0.02449364699423313, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:55, Epoch: 64, Batch: 840, Training Loss: 0.04935026243329048, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:56, Epoch: 64, Batch: 850, Training Loss: 0.039202137291431426, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:57, Epoch: 64, Batch: 860, Training Loss: 0.0366355124861002, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:57, Epoch: 64, Batch: 870, Training Loss: 0.05542925670742989, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:58, Epoch: 64, Batch: 880, Training Loss: 0.03665168732404709, LR: 0.0010000000000000002
Time, 2019-01-01T23:23:59, Epoch: 64, Batch: 890, Training Loss: 0.030584760010242462, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:00, Epoch: 64, Batch: 900, Training Loss: 0.041981227695941925, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:00, Epoch: 64, Batch: 910, Training Loss: 0.03244871087372303, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:01, Epoch: 64, Batch: 920, Training Loss: 0.028766416385769843, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:02, Epoch: 64, Batch: 930, Training Loss: 0.03620223440229893, LR: 0.0010000000000000002
Epoch: 64, Validation Top 1 acc: 98.70620727539062
Epoch: 64, Validation Top 5 acc: 100.0
Epoch: 64, Validation Set Loss: 0.04359773173928261
Start training epoch 65
Time, 2019-01-01T23:24:08, Epoch: 65, Batch: 10, Training Loss: 0.03743112906813621, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:09, Epoch: 65, Batch: 20, Training Loss: 0.041070454940199855, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:10, Epoch: 65, Batch: 30, Training Loss: 0.03737420216202736, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:10, Epoch: 65, Batch: 40, Training Loss: 0.03864717222750187, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:11, Epoch: 65, Batch: 50, Training Loss: 0.03208240643143654, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:12, Epoch: 65, Batch: 60, Training Loss: 0.04512486830353737, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:13, Epoch: 65, Batch: 70, Training Loss: 0.037496519088745114, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:14, Epoch: 65, Batch: 80, Training Loss: 0.030680645257234573, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:14, Epoch: 65, Batch: 90, Training Loss: 0.04415070377290249, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:15, Epoch: 65, Batch: 100, Training Loss: 0.03878110684454441, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:16, Epoch: 65, Batch: 110, Training Loss: 0.05109774582087993, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:16, Epoch: 65, Batch: 120, Training Loss: 0.042778320610523224, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:17, Epoch: 65, Batch: 130, Training Loss: 0.03595142513513565, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:18, Epoch: 65, Batch: 140, Training Loss: 0.026427435502409936, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:19, Epoch: 65, Batch: 150, Training Loss: 0.03609105199575424, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:19, Epoch: 65, Batch: 160, Training Loss: 0.03446250408887863, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:20, Epoch: 65, Batch: 170, Training Loss: 0.03340318128466606, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:21, Epoch: 65, Batch: 180, Training Loss: 0.03969584219157696, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:22, Epoch: 65, Batch: 190, Training Loss: 0.03505528084933758, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:23, Epoch: 65, Batch: 200, Training Loss: 0.03790784552693367, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:23, Epoch: 65, Batch: 210, Training Loss: 0.03676433861255646, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:24, Epoch: 65, Batch: 220, Training Loss: 0.033893778920173645, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:25, Epoch: 65, Batch: 230, Training Loss: 0.03840696699917316, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:26, Epoch: 65, Batch: 240, Training Loss: 0.047072868794202805, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:26, Epoch: 65, Batch: 250, Training Loss: 0.030225086957216263, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:27, Epoch: 65, Batch: 260, Training Loss: 0.045543307438492775, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:28, Epoch: 65, Batch: 270, Training Loss: 0.03672444559633732, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:29, Epoch: 65, Batch: 280, Training Loss: 0.044924328848719594, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:30, Epoch: 65, Batch: 290, Training Loss: 0.03991067782044411, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:30, Epoch: 65, Batch: 300, Training Loss: 0.036255286261439326, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:31, Epoch: 65, Batch: 310, Training Loss: 0.028791331127285956, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:32, Epoch: 65, Batch: 320, Training Loss: 0.043159928917884824, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:33, Epoch: 65, Batch: 330, Training Loss: 0.04055936224758625, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:34, Epoch: 65, Batch: 340, Training Loss: 0.04311177171766758, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:34, Epoch: 65, Batch: 350, Training Loss: 0.032679253816604616, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:35, Epoch: 65, Batch: 360, Training Loss: 0.03555380254983902, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:36, Epoch: 65, Batch: 370, Training Loss: 0.036461140215396884, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:37, Epoch: 65, Batch: 380, Training Loss: 0.041868891939520834, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:38, Epoch: 65, Batch: 390, Training Loss: 0.03866248577833176, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:39, Epoch: 65, Batch: 400, Training Loss: 0.042092562094330786, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:40, Epoch: 65, Batch: 410, Training Loss: 0.02590070329606533, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:40, Epoch: 65, Batch: 420, Training Loss: 0.03797900080680847, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:41, Epoch: 65, Batch: 430, Training Loss: 0.031336541101336476, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:42, Epoch: 65, Batch: 440, Training Loss: 0.062006843462586406, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:43, Epoch: 65, Batch: 450, Training Loss: 0.038867687061429027, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:44, Epoch: 65, Batch: 460, Training Loss: 0.03441643603146076, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:45, Epoch: 65, Batch: 470, Training Loss: 0.03141503110527992, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:45, Epoch: 65, Batch: 480, Training Loss: 0.05577745884656906, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:46, Epoch: 65, Batch: 490, Training Loss: 0.0407328587025404, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:47, Epoch: 65, Batch: 500, Training Loss: 0.02638232260942459, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:48, Epoch: 65, Batch: 510, Training Loss: 0.034333391860127446, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:49, Epoch: 65, Batch: 520, Training Loss: 0.05289963260293007, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:49, Epoch: 65, Batch: 530, Training Loss: 0.050473304092884065, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:50, Epoch: 65, Batch: 540, Training Loss: 0.029888569563627242, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:51, Epoch: 65, Batch: 550, Training Loss: 0.024757716804742813, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:52, Epoch: 65, Batch: 560, Training Loss: 0.03835220001637936, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:52, Epoch: 65, Batch: 570, Training Loss: 0.04311305619776249, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:53, Epoch: 65, Batch: 580, Training Loss: 0.026308268308639526, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:54, Epoch: 65, Batch: 590, Training Loss: 0.03440976068377495, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:55, Epoch: 65, Batch: 600, Training Loss: 0.03351785689592361, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:56, Epoch: 65, Batch: 610, Training Loss: 0.04091606438159943, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:56, Epoch: 65, Batch: 620, Training Loss: 0.03108505718410015, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:57, Epoch: 65, Batch: 630, Training Loss: 0.037072925642132756, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:58, Epoch: 65, Batch: 640, Training Loss: 0.041447386145591736, LR: 0.0010000000000000002
Time, 2019-01-01T23:24:59, Epoch: 65, Batch: 650, Training Loss: 0.02675757147371769, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:00, Epoch: 65, Batch: 660, Training Loss: 0.025592882931232453, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:00, Epoch: 65, Batch: 670, Training Loss: 0.03285341672599316, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:01, Epoch: 65, Batch: 680, Training Loss: 0.036107465997338294, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:02, Epoch: 65, Batch: 690, Training Loss: 0.036989151313900945, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:03, Epoch: 65, Batch: 700, Training Loss: 0.07514444403350354, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:03, Epoch: 65, Batch: 710, Training Loss: 0.027072449773550035, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:04, Epoch: 65, Batch: 720, Training Loss: 0.027124442160129547, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:05, Epoch: 65, Batch: 730, Training Loss: 0.03214743882417679, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:06, Epoch: 65, Batch: 740, Training Loss: 0.03034911900758743, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:07, Epoch: 65, Batch: 750, Training Loss: 0.027336779981851578, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:08, Epoch: 65, Batch: 760, Training Loss: 0.028060422837734224, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:09, Epoch: 65, Batch: 770, Training Loss: 0.04810457155108452, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:10, Epoch: 65, Batch: 780, Training Loss: 0.04879692606627941, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:11, Epoch: 65, Batch: 790, Training Loss: 0.036960015445947646, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:11, Epoch: 65, Batch: 800, Training Loss: 0.041371709853410724, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:12, Epoch: 65, Batch: 810, Training Loss: 0.031209062784910202, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:13, Epoch: 65, Batch: 820, Training Loss: 0.04759982861578464, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:14, Epoch: 65, Batch: 830, Training Loss: 0.04781520664691925, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:14, Epoch: 65, Batch: 840, Training Loss: 0.023710003495216368, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:15, Epoch: 65, Batch: 850, Training Loss: 0.05296661704778671, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:16, Epoch: 65, Batch: 860, Training Loss: 0.06265868656337262, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:17, Epoch: 65, Batch: 870, Training Loss: 0.03255695775151253, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:17, Epoch: 65, Batch: 880, Training Loss: 0.047952978312969206, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:18, Epoch: 65, Batch: 890, Training Loss: 0.044526466727256776, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:19, Epoch: 65, Batch: 900, Training Loss: 0.03072810024023056, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:20, Epoch: 65, Batch: 910, Training Loss: 0.036619657278060914, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:21, Epoch: 65, Batch: 920, Training Loss: 0.03544425442814827, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:21, Epoch: 65, Batch: 930, Training Loss: 0.03325948491692543, LR: 0.0010000000000000002
Epoch: 65, Validation Top 1 acc: 98.62659454345703
Epoch: 65, Validation Top 5 acc: 100.0
Epoch: 65, Validation Set Loss: 0.04434750974178314
Start training epoch 66
Time, 2019-01-01T23:25:27, Epoch: 66, Batch: 10, Training Loss: 0.019081269204616547, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:28, Epoch: 66, Batch: 20, Training Loss: 0.03141271024942398, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:29, Epoch: 66, Batch: 30, Training Loss: 0.02026074454188347, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:30, Epoch: 66, Batch: 40, Training Loss: 0.03243546336889267, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:30, Epoch: 66, Batch: 50, Training Loss: 0.02860340066254139, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:31, Epoch: 66, Batch: 60, Training Loss: 0.03137575313448906, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:32, Epoch: 66, Batch: 70, Training Loss: 0.03697199448943138, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:33, Epoch: 66, Batch: 80, Training Loss: 0.0634493250399828, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:33, Epoch: 66, Batch: 90, Training Loss: 0.04641474634408951, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:34, Epoch: 66, Batch: 100, Training Loss: 0.021161862835288047, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:35, Epoch: 66, Batch: 110, Training Loss: 0.04431113749742508, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:36, Epoch: 66, Batch: 120, Training Loss: 0.038514212146401404, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:36, Epoch: 66, Batch: 130, Training Loss: 0.03353112116456032, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:37, Epoch: 66, Batch: 140, Training Loss: 0.0671079196035862, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:38, Epoch: 66, Batch: 150, Training Loss: 0.044632092118263245, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:39, Epoch: 66, Batch: 160, Training Loss: 0.03841136544942856, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:40, Epoch: 66, Batch: 170, Training Loss: 0.040453153848648074, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:40, Epoch: 66, Batch: 180, Training Loss: 0.03601434007287026, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:41, Epoch: 66, Batch: 190, Training Loss: 0.03934532180428505, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:42, Epoch: 66, Batch: 200, Training Loss: 0.028112829476594926, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:43, Epoch: 66, Batch: 210, Training Loss: 0.03694092854857445, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:43, Epoch: 66, Batch: 220, Training Loss: 0.0350037083029747, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:44, Epoch: 66, Batch: 230, Training Loss: 0.032967012748122214, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:45, Epoch: 66, Batch: 240, Training Loss: 0.06400411576032639, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:46, Epoch: 66, Batch: 250, Training Loss: 0.027539659291505814, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:47, Epoch: 66, Batch: 260, Training Loss: 0.04008793085813522, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:47, Epoch: 66, Batch: 270, Training Loss: 0.029751184582710265, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:48, Epoch: 66, Batch: 280, Training Loss: 0.04434150867164135, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:49, Epoch: 66, Batch: 290, Training Loss: 0.043372514098882674, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:50, Epoch: 66, Batch: 300, Training Loss: 0.043042254820466044, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:50, Epoch: 66, Batch: 310, Training Loss: 0.03943566605448723, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:51, Epoch: 66, Batch: 320, Training Loss: 0.03661377988755703, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:52, Epoch: 66, Batch: 330, Training Loss: 0.03356590643525124, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:53, Epoch: 66, Batch: 340, Training Loss: 0.04990992285311222, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:54, Epoch: 66, Batch: 350, Training Loss: 0.028713155537843704, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:54, Epoch: 66, Batch: 360, Training Loss: 0.04113132804632187, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:55, Epoch: 66, Batch: 370, Training Loss: 0.042938463389873505, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:56, Epoch: 66, Batch: 380, Training Loss: 0.057544853538274765, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:57, Epoch: 66, Batch: 390, Training Loss: 0.05358425378799438, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:57, Epoch: 66, Batch: 400, Training Loss: 0.03245849162340164, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:58, Epoch: 66, Batch: 410, Training Loss: 0.045598436146974564, LR: 0.0010000000000000002
Time, 2019-01-01T23:25:59, Epoch: 66, Batch: 420, Training Loss: 0.028659462556242944, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:00, Epoch: 66, Batch: 430, Training Loss: 0.039415837824344636, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:00, Epoch: 66, Batch: 440, Training Loss: 0.0411075234413147, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:01, Epoch: 66, Batch: 450, Training Loss: 0.03480375334620476, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:02, Epoch: 66, Batch: 460, Training Loss: 0.04781577289104462, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:03, Epoch: 66, Batch: 470, Training Loss: 0.03051101863384247, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:04, Epoch: 66, Batch: 480, Training Loss: 0.05027307569980621, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:04, Epoch: 66, Batch: 490, Training Loss: 0.048413516581058504, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:05, Epoch: 66, Batch: 500, Training Loss: 0.02618543803691864, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:06, Epoch: 66, Batch: 510, Training Loss: 0.02780596986413002, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:07, Epoch: 66, Batch: 520, Training Loss: 0.03079923428595066, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:07, Epoch: 66, Batch: 530, Training Loss: 0.053511235490441324, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:08, Epoch: 66, Batch: 540, Training Loss: 0.03081100806593895, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:09, Epoch: 66, Batch: 550, Training Loss: 0.04203593507409096, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:10, Epoch: 66, Batch: 560, Training Loss: 0.04829048179090023, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:11, Epoch: 66, Batch: 570, Training Loss: 0.03876950852572918, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:11, Epoch: 66, Batch: 580, Training Loss: 0.034391074627637866, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:12, Epoch: 66, Batch: 590, Training Loss: 0.03571067750453949, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:13, Epoch: 66, Batch: 600, Training Loss: 0.024113674834370612, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:13, Epoch: 66, Batch: 610, Training Loss: 0.04594895169138909, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:14, Epoch: 66, Batch: 620, Training Loss: 0.0345977496355772, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:15, Epoch: 66, Batch: 630, Training Loss: 0.03165597505867481, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:16, Epoch: 66, Batch: 640, Training Loss: 0.04640008397400379, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:17, Epoch: 66, Batch: 650, Training Loss: 0.02969457134604454, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:17, Epoch: 66, Batch: 660, Training Loss: 0.02197052389383316, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:18, Epoch: 66, Batch: 670, Training Loss: 0.03115527182817459, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:19, Epoch: 66, Batch: 680, Training Loss: 0.045676184073090556, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:20, Epoch: 66, Batch: 690, Training Loss: 0.03551828861236572, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:20, Epoch: 66, Batch: 700, Training Loss: 0.057288844138383865, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:21, Epoch: 66, Batch: 710, Training Loss: 0.027739458531141282, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:22, Epoch: 66, Batch: 720, Training Loss: 0.02366679459810257, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:23, Epoch: 66, Batch: 730, Training Loss: 0.030798644945025443, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:23, Epoch: 66, Batch: 740, Training Loss: 0.041134410351514814, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:24, Epoch: 66, Batch: 750, Training Loss: 0.047184427827596666, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:25, Epoch: 66, Batch: 760, Training Loss: 0.036484868824481965, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:26, Epoch: 66, Batch: 770, Training Loss: 0.03910829350352287, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:27, Epoch: 66, Batch: 780, Training Loss: 0.02956208810210228, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:27, Epoch: 66, Batch: 790, Training Loss: 0.04055110514163971, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:28, Epoch: 66, Batch: 800, Training Loss: 0.036309870332479476, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:29, Epoch: 66, Batch: 810, Training Loss: 0.030522314459085466, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:30, Epoch: 66, Batch: 820, Training Loss: 0.029711010307073592, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:30, Epoch: 66, Batch: 830, Training Loss: 0.056502573937177655, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:31, Epoch: 66, Batch: 840, Training Loss: 0.020917029678821565, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:32, Epoch: 66, Batch: 850, Training Loss: 0.02551705650985241, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:33, Epoch: 66, Batch: 860, Training Loss: 0.06500107273459435, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:33, Epoch: 66, Batch: 870, Training Loss: 0.04220939576625824, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:34, Epoch: 66, Batch: 880, Training Loss: 0.04514773674309254, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:35, Epoch: 66, Batch: 890, Training Loss: 0.02893727719783783, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:36, Epoch: 66, Batch: 900, Training Loss: 0.03076331727206707, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:37, Epoch: 66, Batch: 910, Training Loss: 0.04745824448764324, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:37, Epoch: 66, Batch: 920, Training Loss: 0.04065941274166107, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:38, Epoch: 66, Batch: 930, Training Loss: 0.04287815168499946, LR: 0.0010000000000000002
Epoch: 66, Validation Top 1 acc: 98.6664047241211
Epoch: 66, Validation Top 5 acc: 100.0
Epoch: 66, Validation Set Loss: 0.04384535923600197
Start training epoch 67
Time, 2019-01-01T23:26:44, Epoch: 67, Batch: 10, Training Loss: 0.03790803700685501, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:45, Epoch: 67, Batch: 20, Training Loss: 0.02472882196307182, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:46, Epoch: 67, Batch: 30, Training Loss: 0.03720961846411228, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:46, Epoch: 67, Batch: 40, Training Loss: 0.04657192528247833, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:47, Epoch: 67, Batch: 50, Training Loss: 0.03594065569341183, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:48, Epoch: 67, Batch: 60, Training Loss: 0.041626110300421716, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:49, Epoch: 67, Batch: 70, Training Loss: 0.050524041429162024, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:49, Epoch: 67, Batch: 80, Training Loss: 0.05180127657949925, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:50, Epoch: 67, Batch: 90, Training Loss: 0.04199131615459919, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:51, Epoch: 67, Batch: 100, Training Loss: 0.024930282682180404, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:52, Epoch: 67, Batch: 110, Training Loss: 0.034357575699687004, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:52, Epoch: 67, Batch: 120, Training Loss: 0.029694121330976486, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:53, Epoch: 67, Batch: 130, Training Loss: 0.03512413911521435, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:54, Epoch: 67, Batch: 140, Training Loss: 0.030615633726119994, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:55, Epoch: 67, Batch: 150, Training Loss: 0.04282084479928017, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:55, Epoch: 67, Batch: 160, Training Loss: 0.03557376861572266, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:56, Epoch: 67, Batch: 170, Training Loss: 0.02533881366252899, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:57, Epoch: 67, Batch: 180, Training Loss: 0.041569529101252556, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:58, Epoch: 67, Batch: 190, Training Loss: 0.03146384432911873, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:58, Epoch: 67, Batch: 200, Training Loss: 0.05443141460418701, LR: 0.0010000000000000002
Time, 2019-01-01T23:26:59, Epoch: 67, Batch: 210, Training Loss: 0.03705548532307148, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:00, Epoch: 67, Batch: 220, Training Loss: 0.030164673924446106, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:01, Epoch: 67, Batch: 230, Training Loss: 0.02439885251224041, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:01, Epoch: 67, Batch: 240, Training Loss: 0.03201199285686016, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:02, Epoch: 67, Batch: 250, Training Loss: 0.046951962262392045, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:03, Epoch: 67, Batch: 260, Training Loss: 0.040130522847175595, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:04, Epoch: 67, Batch: 270, Training Loss: 0.03357820883393288, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:04, Epoch: 67, Batch: 280, Training Loss: 0.03766482248902321, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:05, Epoch: 67, Batch: 290, Training Loss: 0.05691509433090687, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:06, Epoch: 67, Batch: 300, Training Loss: 0.04101279005408287, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:07, Epoch: 67, Batch: 310, Training Loss: 0.033604813367128374, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:07, Epoch: 67, Batch: 320, Training Loss: 0.04540443867444992, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:08, Epoch: 67, Batch: 330, Training Loss: 0.03381314948201179, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:09, Epoch: 67, Batch: 340, Training Loss: 0.04445845223963261, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:10, Epoch: 67, Batch: 350, Training Loss: 0.03664243593811989, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:10, Epoch: 67, Batch: 360, Training Loss: 0.034106668457388876, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:11, Epoch: 67, Batch: 370, Training Loss: 0.0304425660520792, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:12, Epoch: 67, Batch: 380, Training Loss: 0.040327700600028035, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:13, Epoch: 67, Batch: 390, Training Loss: 0.022710420191287994, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:13, Epoch: 67, Batch: 400, Training Loss: 0.03144192360341549, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:14, Epoch: 67, Batch: 410, Training Loss: 0.036745521053671834, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:15, Epoch: 67, Batch: 420, Training Loss: 0.029311523586511613, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:16, Epoch: 67, Batch: 430, Training Loss: 0.028302046284079552, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:16, Epoch: 67, Batch: 440, Training Loss: 0.04487435296177864, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:17, Epoch: 67, Batch: 450, Training Loss: 0.039968733116984366, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:18, Epoch: 67, Batch: 460, Training Loss: 0.03203216642141342, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:19, Epoch: 67, Batch: 470, Training Loss: 0.03645417056977749, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:19, Epoch: 67, Batch: 480, Training Loss: 0.03613329119980335, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:20, Epoch: 67, Batch: 490, Training Loss: 0.02818130925297737, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:21, Epoch: 67, Batch: 500, Training Loss: 0.0292423814535141, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:22, Epoch: 67, Batch: 510, Training Loss: 0.031291219592094424, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:22, Epoch: 67, Batch: 520, Training Loss: 0.030301421880722046, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:23, Epoch: 67, Batch: 530, Training Loss: 0.03893412128090858, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:24, Epoch: 67, Batch: 540, Training Loss: 0.04224817454814911, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:25, Epoch: 67, Batch: 550, Training Loss: 0.03292807266116142, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:25, Epoch: 67, Batch: 560, Training Loss: 0.0258827306330204, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:26, Epoch: 67, Batch: 570, Training Loss: 0.044107209891080856, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:27, Epoch: 67, Batch: 580, Training Loss: 0.04859670363366604, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:28, Epoch: 67, Batch: 590, Training Loss: 0.022824662178754805, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:28, Epoch: 67, Batch: 600, Training Loss: 0.03100556917488575, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:29, Epoch: 67, Batch: 610, Training Loss: 0.047370852902531624, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:30, Epoch: 67, Batch: 620, Training Loss: 0.04740059934556484, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:31, Epoch: 67, Batch: 630, Training Loss: 0.02863321378827095, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:31, Epoch: 67, Batch: 640, Training Loss: 0.051762934774160385, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:32, Epoch: 67, Batch: 650, Training Loss: 0.03400432877242565, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:33, Epoch: 67, Batch: 660, Training Loss: 0.048208809643983844, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:34, Epoch: 67, Batch: 670, Training Loss: 0.05922327116131783, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:34, Epoch: 67, Batch: 680, Training Loss: 0.04237731769680977, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:35, Epoch: 67, Batch: 690, Training Loss: 0.021536438167095183, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:36, Epoch: 67, Batch: 700, Training Loss: 0.04168539233505726, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:37, Epoch: 67, Batch: 710, Training Loss: 0.0356598000973463, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:37, Epoch: 67, Batch: 720, Training Loss: 0.02804260328412056, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:38, Epoch: 67, Batch: 730, Training Loss: 0.03144195303320885, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:39, Epoch: 67, Batch: 740, Training Loss: 0.04981196410953999, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:40, Epoch: 67, Batch: 750, Training Loss: 0.045097161829471585, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:41, Epoch: 67, Batch: 760, Training Loss: 0.03323481194674969, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:41, Epoch: 67, Batch: 770, Training Loss: 0.038546708226203916, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:42, Epoch: 67, Batch: 780, Training Loss: 0.041878646984696385, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:43, Epoch: 67, Batch: 790, Training Loss: 0.02749788500368595, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:44, Epoch: 67, Batch: 800, Training Loss: 0.05241841189563275, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:45, Epoch: 67, Batch: 810, Training Loss: 0.040150071308016774, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:45, Epoch: 67, Batch: 820, Training Loss: 0.04254118502140045, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:46, Epoch: 67, Batch: 830, Training Loss: 0.043164006993174554, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:47, Epoch: 67, Batch: 840, Training Loss: 0.04543041177093983, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:48, Epoch: 67, Batch: 850, Training Loss: 0.02959129437804222, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:48, Epoch: 67, Batch: 860, Training Loss: 0.03527346886694431, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:49, Epoch: 67, Batch: 870, Training Loss: 0.041286058723926544, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:50, Epoch: 67, Batch: 880, Training Loss: 0.04941602982580662, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:51, Epoch: 67, Batch: 890, Training Loss: 0.031080441921949385, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:51, Epoch: 67, Batch: 900, Training Loss: 0.05305990502238274, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:52, Epoch: 67, Batch: 910, Training Loss: 0.040464942902326585, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:53, Epoch: 67, Batch: 920, Training Loss: 0.047712096571922304, LR: 0.0010000000000000002
Time, 2019-01-01T23:27:54, Epoch: 67, Batch: 930, Training Loss: 0.030735157430171967, LR: 0.0010000000000000002
Epoch: 67, Validation Top 1 acc: 98.6664047241211
Epoch: 67, Validation Top 5 acc: 100.0
Epoch: 67, Validation Set Loss: 0.04380982741713524
Start training epoch 68
Time, 2019-01-01T23:28:00, Epoch: 68, Batch: 10, Training Loss: 0.03806401416659355, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:00, Epoch: 68, Batch: 20, Training Loss: 0.0468911238014698, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:01, Epoch: 68, Batch: 30, Training Loss: 0.03959001488983631, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:02, Epoch: 68, Batch: 40, Training Loss: 0.02443646900355816, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:02, Epoch: 68, Batch: 50, Training Loss: 0.04034114181995392, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:03, Epoch: 68, Batch: 60, Training Loss: 0.025473356992006303, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:04, Epoch: 68, Batch: 70, Training Loss: 0.037841060757637025, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:05, Epoch: 68, Batch: 80, Training Loss: 0.040106941014528275, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:05, Epoch: 68, Batch: 90, Training Loss: 0.0475287776440382, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:06, Epoch: 68, Batch: 100, Training Loss: 0.05178804658353329, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:07, Epoch: 68, Batch: 110, Training Loss: 0.035678263381123544, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:08, Epoch: 68, Batch: 120, Training Loss: 0.0351457592099905, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:09, Epoch: 68, Batch: 130, Training Loss: 0.03681478649377823, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:09, Epoch: 68, Batch: 140, Training Loss: 0.04685378186404705, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:10, Epoch: 68, Batch: 150, Training Loss: 0.03274182938039303, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:11, Epoch: 68, Batch: 160, Training Loss: 0.038573041930794714, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:12, Epoch: 68, Batch: 170, Training Loss: 0.03355411216616631, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:12, Epoch: 68, Batch: 180, Training Loss: 0.038613615185022356, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:13, Epoch: 68, Batch: 190, Training Loss: 0.047647484391927716, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:14, Epoch: 68, Batch: 200, Training Loss: 0.05904202088713646, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:15, Epoch: 68, Batch: 210, Training Loss: 0.02391984760761261, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:15, Epoch: 68, Batch: 220, Training Loss: 0.03592954985797405, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:16, Epoch: 68, Batch: 230, Training Loss: 0.05377847775816917, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:17, Epoch: 68, Batch: 240, Training Loss: 0.0403818566352129, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:18, Epoch: 68, Batch: 250, Training Loss: 0.03885014951229095, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:18, Epoch: 68, Batch: 260, Training Loss: 0.027679461240768432, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:19, Epoch: 68, Batch: 270, Training Loss: 0.037697214633226395, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:20, Epoch: 68, Batch: 280, Training Loss: 0.05046835169196129, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:21, Epoch: 68, Batch: 290, Training Loss: 0.04028787724673748, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:21, Epoch: 68, Batch: 300, Training Loss: 0.046551136672496794, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:22, Epoch: 68, Batch: 310, Training Loss: 0.028863730654120446, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:23, Epoch: 68, Batch: 320, Training Loss: 0.03167090862989426, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:24, Epoch: 68, Batch: 330, Training Loss: 0.0506822008639574, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:24, Epoch: 68, Batch: 340, Training Loss: 0.04837311282753944, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:25, Epoch: 68, Batch: 350, Training Loss: 0.03416127525269985, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:26, Epoch: 68, Batch: 360, Training Loss: 0.02570261061191559, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:27, Epoch: 68, Batch: 370, Training Loss: 0.0425914641469717, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:27, Epoch: 68, Batch: 380, Training Loss: 0.03091149888932705, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:28, Epoch: 68, Batch: 390, Training Loss: 0.03934229575097561, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:29, Epoch: 68, Batch: 400, Training Loss: 0.03167691789567471, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:30, Epoch: 68, Batch: 410, Training Loss: 0.032167380303144456, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:30, Epoch: 68, Batch: 420, Training Loss: 0.02942074425518513, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:31, Epoch: 68, Batch: 430, Training Loss: 0.03542391695082188, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:32, Epoch: 68, Batch: 440, Training Loss: 0.031795600429177284, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:33, Epoch: 68, Batch: 450, Training Loss: 0.04022262319922447, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:33, Epoch: 68, Batch: 460, Training Loss: 0.036530449986457825, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:34, Epoch: 68, Batch: 470, Training Loss: 0.034750687330961226, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:35, Epoch: 68, Batch: 480, Training Loss: 0.03503879308700562, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:36, Epoch: 68, Batch: 490, Training Loss: 0.03480130806565285, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:36, Epoch: 68, Batch: 500, Training Loss: 0.022530308365821837, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:37, Epoch: 68, Batch: 510, Training Loss: 0.04310655929148197, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:38, Epoch: 68, Batch: 520, Training Loss: 0.030891220644116403, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:39, Epoch: 68, Batch: 530, Training Loss: 0.035190146416425705, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:40, Epoch: 68, Batch: 540, Training Loss: 0.044234832748770714, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:40, Epoch: 68, Batch: 550, Training Loss: 0.03417342975735664, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:41, Epoch: 68, Batch: 560, Training Loss: 0.03247858360409737, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:42, Epoch: 68, Batch: 570, Training Loss: 0.041885624453425405, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:43, Epoch: 68, Batch: 580, Training Loss: 0.02946830838918686, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:43, Epoch: 68, Batch: 590, Training Loss: 0.03962194360792637, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:44, Epoch: 68, Batch: 600, Training Loss: 0.028339389339089395, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:45, Epoch: 68, Batch: 610, Training Loss: 0.04467585645616055, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:45, Epoch: 68, Batch: 620, Training Loss: 0.03714018650352955, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:46, Epoch: 68, Batch: 630, Training Loss: 0.05666959956288338, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:47, Epoch: 68, Batch: 640, Training Loss: 0.018242891877889633, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:48, Epoch: 68, Batch: 650, Training Loss: 0.03882039748132229, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:48, Epoch: 68, Batch: 660, Training Loss: 0.04626018963754177, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:49, Epoch: 68, Batch: 670, Training Loss: 0.029318057745695115, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:50, Epoch: 68, Batch: 680, Training Loss: 0.04419938176870346, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:51, Epoch: 68, Batch: 690, Training Loss: 0.04109814018011093, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:51, Epoch: 68, Batch: 700, Training Loss: 0.052797036990523336, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:52, Epoch: 68, Batch: 710, Training Loss: 0.047352536395192144, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:53, Epoch: 68, Batch: 720, Training Loss: 0.06352462023496627, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:54, Epoch: 68, Batch: 730, Training Loss: 0.02189456596970558, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:54, Epoch: 68, Batch: 740, Training Loss: 0.02622997872531414, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:55, Epoch: 68, Batch: 750, Training Loss: 0.03269801139831543, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:56, Epoch: 68, Batch: 760, Training Loss: 0.03488485515117645, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:57, Epoch: 68, Batch: 770, Training Loss: 0.04818199574947357, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:57, Epoch: 68, Batch: 780, Training Loss: 0.05018629729747772, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:58, Epoch: 68, Batch: 790, Training Loss: 0.03426043391227722, LR: 0.0010000000000000002
Time, 2019-01-01T23:28:59, Epoch: 68, Batch: 800, Training Loss: 0.035060815513134, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:00, Epoch: 68, Batch: 810, Training Loss: 0.04358447790145874, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:00, Epoch: 68, Batch: 820, Training Loss: 0.026763810589909554, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:01, Epoch: 68, Batch: 830, Training Loss: 0.019733915477991103, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:02, Epoch: 68, Batch: 840, Training Loss: 0.04408055916428566, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:03, Epoch: 68, Batch: 850, Training Loss: 0.03726363033056259, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:04, Epoch: 68, Batch: 860, Training Loss: 0.027918343245983124, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:04, Epoch: 68, Batch: 870, Training Loss: 0.02898005545139313, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:05, Epoch: 68, Batch: 880, Training Loss: 0.043059753999114034, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:06, Epoch: 68, Batch: 890, Training Loss: 0.03545642569661141, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:07, Epoch: 68, Batch: 900, Training Loss: 0.029672013223171236, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:07, Epoch: 68, Batch: 910, Training Loss: 0.034110959619283676, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:08, Epoch: 68, Batch: 920, Training Loss: 0.027849673852324487, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:09, Epoch: 68, Batch: 930, Training Loss: 0.03547002673149109, LR: 0.0010000000000000002
Epoch: 68, Validation Top 1 acc: 98.65644836425781
Epoch: 68, Validation Top 5 acc: 100.0
Epoch: 68, Validation Set Loss: 0.043207403272390366
Start training epoch 69
Time, 2019-01-01T23:29:15, Epoch: 69, Batch: 10, Training Loss: 0.029603297263383864, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:16, Epoch: 69, Batch: 20, Training Loss: 0.034164369106292725, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:17, Epoch: 69, Batch: 30, Training Loss: 0.04051989056169987, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:17, Epoch: 69, Batch: 40, Training Loss: 0.06129683256149292, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:18, Epoch: 69, Batch: 50, Training Loss: 0.03477486483752727, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:19, Epoch: 69, Batch: 60, Training Loss: 0.04418580681085586, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:20, Epoch: 69, Batch: 70, Training Loss: 0.02672642730176449, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:20, Epoch: 69, Batch: 80, Training Loss: 0.04187020547688007, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:21, Epoch: 69, Batch: 90, Training Loss: 0.031223658472299576, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:22, Epoch: 69, Batch: 100, Training Loss: 0.023963500186800955, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:23, Epoch: 69, Batch: 110, Training Loss: 0.042696700990200044, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:23, Epoch: 69, Batch: 120, Training Loss: 0.04494181089103222, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:24, Epoch: 69, Batch: 130, Training Loss: 0.057880578190088273, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:25, Epoch: 69, Batch: 140, Training Loss: 0.03996083438396454, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:26, Epoch: 69, Batch: 150, Training Loss: 0.02303648442029953, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:27, Epoch: 69, Batch: 160, Training Loss: 0.025861746817827224, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:27, Epoch: 69, Batch: 170, Training Loss: 0.04472539946436882, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:28, Epoch: 69, Batch: 180, Training Loss: 0.041747010126709935, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:29, Epoch: 69, Batch: 190, Training Loss: 0.04068558216094971, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:30, Epoch: 69, Batch: 200, Training Loss: 0.05015277191996574, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:30, Epoch: 69, Batch: 210, Training Loss: 0.029468061029911043, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:31, Epoch: 69, Batch: 220, Training Loss: 0.04185150936245918, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:32, Epoch: 69, Batch: 230, Training Loss: 0.047216493636369705, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:33, Epoch: 69, Batch: 240, Training Loss: 0.045763881877064705, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:33, Epoch: 69, Batch: 250, Training Loss: 0.03604904823005199, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:34, Epoch: 69, Batch: 260, Training Loss: 0.030841387435793878, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:35, Epoch: 69, Batch: 270, Training Loss: 0.028600747510790826, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:36, Epoch: 69, Batch: 280, Training Loss: 0.05550370290875435, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:36, Epoch: 69, Batch: 290, Training Loss: 0.04900126904249191, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:37, Epoch: 69, Batch: 300, Training Loss: 0.019994277507066727, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:38, Epoch: 69, Batch: 310, Training Loss: 0.043421697616577146, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:39, Epoch: 69, Batch: 320, Training Loss: 0.02878388427197933, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:39, Epoch: 69, Batch: 330, Training Loss: 0.04555513896048069, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:40, Epoch: 69, Batch: 340, Training Loss: 0.03777189180254936, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:41, Epoch: 69, Batch: 350, Training Loss: 0.046209280379116535, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:42, Epoch: 69, Batch: 360, Training Loss: 0.038231120258569715, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:43, Epoch: 69, Batch: 370, Training Loss: 0.02629633992910385, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:43, Epoch: 69, Batch: 380, Training Loss: 0.030933956429362298, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:44, Epoch: 69, Batch: 390, Training Loss: 0.035176905989646914, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:45, Epoch: 69, Batch: 400, Training Loss: 0.03161385878920555, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:46, Epoch: 69, Batch: 410, Training Loss: 0.06113474816083908, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:46, Epoch: 69, Batch: 420, Training Loss: 0.03682217672467232, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:47, Epoch: 69, Batch: 430, Training Loss: 0.031584407389163974, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:48, Epoch: 69, Batch: 440, Training Loss: 0.04148479886353016, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:49, Epoch: 69, Batch: 450, Training Loss: 0.053765587508678436, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:49, Epoch: 69, Batch: 460, Training Loss: 0.022659434378147124, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:50, Epoch: 69, Batch: 470, Training Loss: 0.03082336634397507, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:51, Epoch: 69, Batch: 480, Training Loss: 0.027355626598000525, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:52, Epoch: 69, Batch: 490, Training Loss: 0.03416914828121662, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:52, Epoch: 69, Batch: 500, Training Loss: 0.03095794580876827, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:53, Epoch: 69, Batch: 510, Training Loss: 0.04259902089834213, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:54, Epoch: 69, Batch: 520, Training Loss: 0.034061307087540627, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:55, Epoch: 69, Batch: 530, Training Loss: 0.04035200923681259, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:55, Epoch: 69, Batch: 540, Training Loss: 0.03926383592188358, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:56, Epoch: 69, Batch: 550, Training Loss: 0.028434377908706666, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:57, Epoch: 69, Batch: 560, Training Loss: 0.026307680830359458, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:58, Epoch: 69, Batch: 570, Training Loss: 0.03107408285140991, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:58, Epoch: 69, Batch: 580, Training Loss: 0.029838325828313826, LR: 0.0010000000000000002
Time, 2019-01-01T23:29:59, Epoch: 69, Batch: 590, Training Loss: 0.040100154280662534, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:00, Epoch: 69, Batch: 600, Training Loss: 0.03341163583099842, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:01, Epoch: 69, Batch: 610, Training Loss: 0.04586377553641796, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:01, Epoch: 69, Batch: 620, Training Loss: 0.029626622796058655, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:02, Epoch: 69, Batch: 630, Training Loss: 0.04055239744484425, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:03, Epoch: 69, Batch: 640, Training Loss: 0.03305617868900299, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:04, Epoch: 69, Batch: 650, Training Loss: 0.029069183766841887, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:04, Epoch: 69, Batch: 660, Training Loss: 0.03402055278420448, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:05, Epoch: 69, Batch: 670, Training Loss: 0.03208332695066929, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:06, Epoch: 69, Batch: 680, Training Loss: 0.04315633922815323, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:07, Epoch: 69, Batch: 690, Training Loss: 0.02502153366804123, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:07, Epoch: 69, Batch: 700, Training Loss: 0.022665125504136085, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:08, Epoch: 69, Batch: 710, Training Loss: 0.024944835901260377, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:09, Epoch: 69, Batch: 720, Training Loss: 0.03558694198727608, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:10, Epoch: 69, Batch: 730, Training Loss: 0.06834086552262306, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:10, Epoch: 69, Batch: 740, Training Loss: 0.02307688891887665, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:11, Epoch: 69, Batch: 750, Training Loss: 0.03654387816786766, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:12, Epoch: 69, Batch: 760, Training Loss: 0.04576603136956692, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:13, Epoch: 69, Batch: 770, Training Loss: 0.038394837826490405, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:13, Epoch: 69, Batch: 780, Training Loss: 0.02865857109427452, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:14, Epoch: 69, Batch: 790, Training Loss: 0.044723141565918924, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:15, Epoch: 69, Batch: 800, Training Loss: 0.03721129335463047, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:16, Epoch: 69, Batch: 810, Training Loss: 0.040705376863479616, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:16, Epoch: 69, Batch: 820, Training Loss: 0.05378638207912445, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:17, Epoch: 69, Batch: 830, Training Loss: 0.038673537969589236, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:18, Epoch: 69, Batch: 840, Training Loss: 0.034801080077886584, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:19, Epoch: 69, Batch: 850, Training Loss: 0.03561904728412628, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:19, Epoch: 69, Batch: 860, Training Loss: 0.026690560951828958, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:20, Epoch: 69, Batch: 870, Training Loss: 0.03382517024874687, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:21, Epoch: 69, Batch: 880, Training Loss: 0.038395604491233824, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:21, Epoch: 69, Batch: 890, Training Loss: 0.04170001186430454, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:22, Epoch: 69, Batch: 900, Training Loss: 0.049490426853299144, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:23, Epoch: 69, Batch: 910, Training Loss: 0.04998358823359013, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:24, Epoch: 69, Batch: 920, Training Loss: 0.04729895628988743, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:24, Epoch: 69, Batch: 930, Training Loss: 0.03702008500695229, LR: 0.0010000000000000002
Epoch: 69, Validation Top 1 acc: 98.6664047241211
Epoch: 69, Validation Top 5 acc: 100.0
Epoch: 69, Validation Set Loss: 0.04382575675845146
Start training epoch 70
Time, 2019-01-01T23:30:31, Epoch: 70, Batch: 10, Training Loss: 0.047628581523895264, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:31, Epoch: 70, Batch: 20, Training Loss: 0.04206834509968758, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:32, Epoch: 70, Batch: 30, Training Loss: 0.03563414067029953, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:33, Epoch: 70, Batch: 40, Training Loss: 0.04064838327467442, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:34, Epoch: 70, Batch: 50, Training Loss: 0.04863144382834435, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:34, Epoch: 70, Batch: 60, Training Loss: 0.034468800947070125, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:35, Epoch: 70, Batch: 70, Training Loss: 0.044715945795178416, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:36, Epoch: 70, Batch: 80, Training Loss: 0.03337320052087307, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:37, Epoch: 70, Batch: 90, Training Loss: 0.03737684339284897, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:37, Epoch: 70, Batch: 100, Training Loss: 0.03556262105703354, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:38, Epoch: 70, Batch: 110, Training Loss: 0.047096339613199235, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:39, Epoch: 70, Batch: 120, Training Loss: 0.03508044071495533, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:40, Epoch: 70, Batch: 130, Training Loss: 0.03288911059498787, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:40, Epoch: 70, Batch: 140, Training Loss: 0.04091683924198151, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:41, Epoch: 70, Batch: 150, Training Loss: 0.03152990452945233, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:42, Epoch: 70, Batch: 160, Training Loss: 0.03902109302580357, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:43, Epoch: 70, Batch: 170, Training Loss: 0.03131853863596916, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:44, Epoch: 70, Batch: 180, Training Loss: 0.041532478854060174, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:44, Epoch: 70, Batch: 190, Training Loss: 0.02742124944925308, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:45, Epoch: 70, Batch: 200, Training Loss: 0.027088455855846405, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:46, Epoch: 70, Batch: 210, Training Loss: 0.022237394377589224, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:47, Epoch: 70, Batch: 220, Training Loss: 0.02353776842355728, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:48, Epoch: 70, Batch: 230, Training Loss: 0.036662197485566136, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:48, Epoch: 70, Batch: 240, Training Loss: 0.031244812533259392, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:49, Epoch: 70, Batch: 250, Training Loss: 0.034702488034963605, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:50, Epoch: 70, Batch: 260, Training Loss: 0.04136204198002815, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:51, Epoch: 70, Batch: 270, Training Loss: 0.055043953284621236, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:52, Epoch: 70, Batch: 280, Training Loss: 0.04492996521294117, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:53, Epoch: 70, Batch: 290, Training Loss: 0.03152824603021145, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:53, Epoch: 70, Batch: 300, Training Loss: 0.031875261664390565, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:54, Epoch: 70, Batch: 310, Training Loss: 0.03058537468314171, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:55, Epoch: 70, Batch: 320, Training Loss: 0.041224397346377374, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:56, Epoch: 70, Batch: 330, Training Loss: 0.04012050405144692, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:56, Epoch: 70, Batch: 340, Training Loss: 0.036169103160500524, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:57, Epoch: 70, Batch: 350, Training Loss: 0.043225274235010144, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:58, Epoch: 70, Batch: 360, Training Loss: 0.0258883036673069, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:59, Epoch: 70, Batch: 370, Training Loss: 0.03852045089006424, LR: 0.0010000000000000002
Time, 2019-01-01T23:30:59, Epoch: 70, Batch: 380, Training Loss: 0.04774295762181282, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:00, Epoch: 70, Batch: 390, Training Loss: 0.05158774480223656, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:01, Epoch: 70, Batch: 400, Training Loss: 0.02526341415941715, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:02, Epoch: 70, Batch: 410, Training Loss: 0.035601042583584784, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:03, Epoch: 70, Batch: 420, Training Loss: 0.02227115407586098, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:03, Epoch: 70, Batch: 430, Training Loss: 0.03833724446594715, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:04, Epoch: 70, Batch: 440, Training Loss: 0.05104169920086861, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:05, Epoch: 70, Batch: 450, Training Loss: 0.05058308988809586, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:06, Epoch: 70, Batch: 460, Training Loss: 0.03455857485532761, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:06, Epoch: 70, Batch: 470, Training Loss: 0.03972622901201248, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:07, Epoch: 70, Batch: 480, Training Loss: 0.033146350830793384, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:08, Epoch: 70, Batch: 490, Training Loss: 0.04512316882610321, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:09, Epoch: 70, Batch: 500, Training Loss: 0.03301474303007126, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:10, Epoch: 70, Batch: 510, Training Loss: 0.04655771479010582, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:10, Epoch: 70, Batch: 520, Training Loss: 0.04038843773305416, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:11, Epoch: 70, Batch: 530, Training Loss: 0.03542386554181576, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:12, Epoch: 70, Batch: 540, Training Loss: 0.026405603066086768, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:13, Epoch: 70, Batch: 550, Training Loss: 0.037476411834359166, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:13, Epoch: 70, Batch: 560, Training Loss: 0.039826327562332155, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:14, Epoch: 70, Batch: 570, Training Loss: 0.03975014351308346, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:15, Epoch: 70, Batch: 580, Training Loss: 0.043870233744382856, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:16, Epoch: 70, Batch: 590, Training Loss: 0.03743497803807259, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:17, Epoch: 70, Batch: 600, Training Loss: 0.025479510053992273, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:17, Epoch: 70, Batch: 610, Training Loss: 0.03917608857154846, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:18, Epoch: 70, Batch: 620, Training Loss: 0.03053724840283394, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:19, Epoch: 70, Batch: 630, Training Loss: 0.048925692588090895, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:20, Epoch: 70, Batch: 640, Training Loss: 0.03308401927351952, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:20, Epoch: 70, Batch: 650, Training Loss: 0.02803341969847679, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:21, Epoch: 70, Batch: 660, Training Loss: 0.034088675305247305, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:22, Epoch: 70, Batch: 670, Training Loss: 0.029494630917906762, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:23, Epoch: 70, Batch: 680, Training Loss: 0.048746335133910176, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:23, Epoch: 70, Batch: 690, Training Loss: 0.04113793335855007, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:24, Epoch: 70, Batch: 700, Training Loss: 0.02600649558007717, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:25, Epoch: 70, Batch: 710, Training Loss: 0.02431141138076782, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:26, Epoch: 70, Batch: 720, Training Loss: 0.05551839619874954, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:26, Epoch: 70, Batch: 730, Training Loss: 0.03049813359975815, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:27, Epoch: 70, Batch: 740, Training Loss: 0.029602651298046113, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:28, Epoch: 70, Batch: 750, Training Loss: 0.03350429981946945, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:29, Epoch: 70, Batch: 760, Training Loss: 0.04717311672866344, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:29, Epoch: 70, Batch: 770, Training Loss: 0.028585219755768776, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:30, Epoch: 70, Batch: 780, Training Loss: 0.022546327486634255, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:31, Epoch: 70, Batch: 790, Training Loss: 0.03327182270586491, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:32, Epoch: 70, Batch: 800, Training Loss: 0.02292143478989601, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:32, Epoch: 70, Batch: 810, Training Loss: 0.05393438637256622, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:33, Epoch: 70, Batch: 820, Training Loss: 0.04308127872645855, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:34, Epoch: 70, Batch: 830, Training Loss: 0.0440589040517807, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:35, Epoch: 70, Batch: 840, Training Loss: 0.029634558036923407, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:36, Epoch: 70, Batch: 850, Training Loss: 0.04607051126658916, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:36, Epoch: 70, Batch: 860, Training Loss: 0.04028024524450302, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:37, Epoch: 70, Batch: 870, Training Loss: 0.028351015225052834, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:38, Epoch: 70, Batch: 880, Training Loss: 0.04461472928524017, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:39, Epoch: 70, Batch: 890, Training Loss: 0.03809482678771019, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:39, Epoch: 70, Batch: 900, Training Loss: 0.04729157574474811, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:40, Epoch: 70, Batch: 910, Training Loss: 0.04614680893719196, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:41, Epoch: 70, Batch: 920, Training Loss: 0.04502124041318893, LR: 0.0010000000000000002
Time, 2019-01-01T23:31:42, Epoch: 70, Batch: 930, Training Loss: 0.0385839119553566, LR: 0.0010000000000000002
Epoch: 70, Validation Top 1 acc: 98.72611236572266
Epoch: 70, Validation Top 5 acc: 100.0
Epoch: 70, Validation Set Loss: 0.04394444450736046