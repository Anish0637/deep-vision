----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 6, 28, 28]             156
         AvgPool2d-2            [-1, 6, 14, 14]               0
           Sigmoid-3            [-1, 6, 14, 14]               0
            Conv2d-4           [-1, 16, 10, 10]           2,416
         AvgPool2d-5             [-1, 16, 5, 5]               0
           Sigmoid-6             [-1, 16, 5, 5]               0
            Linear-7                  [-1, 120]          48,120
              Tanh-8                  [-1, 120]               0
            Linear-9                   [-1, 84]          10,164
             Tanh-10                   [-1, 84]               0
           Linear-11                   [-1, 10]             850
================================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.08
Params size (MB): 0.24
Estimated Total Size (MB): 0.31
----------------------------------------------------------------
Epoch: 0, Validation Top 1 acc: 9.481609344482422
Epoch: 0, Validation Top 5 acc: 51.161048889160156
Epoch: 0, Validation Set Loss: 2.307643175125122
Start training epoch 1
Time, 2019-01-01T16:45:09, Epoch: 1, Batch: 10, Training Loss: 2.3175955057144164, LR: 0.1
Time, 2019-01-01T16:45:10, Epoch: 1, Batch: 20, Training Loss: 2.330446243286133, LR: 0.1
Time, 2019-01-01T16:45:11, Epoch: 1, Batch: 30, Training Loss: 2.297659158706665, LR: 0.1
Time, 2019-01-01T16:45:11, Epoch: 1, Batch: 40, Training Loss: 2.2430073261260985, LR: 0.1
Time, 2019-01-01T16:45:12, Epoch: 1, Batch: 50, Training Loss: 1.9928738236427308, LR: 0.1
Time, 2019-01-01T16:45:13, Epoch: 1, Batch: 60, Training Loss: 1.5222064971923828, LR: 0.1
Time, 2019-01-01T16:45:14, Epoch: 1, Batch: 70, Training Loss: 1.1453745484352111, LR: 0.1
Time, 2019-01-01T16:45:15, Epoch: 1, Batch: 80, Training Loss: 0.9880090594291687, LR: 0.1
Time, 2019-01-01T16:45:16, Epoch: 1, Batch: 90, Training Loss: 0.9504330456256866, LR: 0.1
Time, 2019-01-01T16:45:16, Epoch: 1, Batch: 100, Training Loss: 0.7520317226648331, LR: 0.1
Time, 2019-01-01T16:45:17, Epoch: 1, Batch: 110, Training Loss: 0.6956277310848236, LR: 0.1
Time, 2019-01-01T16:45:18, Epoch: 1, Batch: 120, Training Loss: 0.5762327015399933, LR: 0.1
Time, 2019-01-01T16:45:19, Epoch: 1, Batch: 130, Training Loss: 0.5810738325119018, LR: 0.1
Time, 2019-01-01T16:45:20, Epoch: 1, Batch: 140, Training Loss: 0.4386929631233215, LR: 0.1
Time, 2019-01-01T16:45:21, Epoch: 1, Batch: 150, Training Loss: 0.5067431658506394, LR: 0.1
Time, 2019-01-01T16:45:22, Epoch: 1, Batch: 160, Training Loss: 0.40368465334177017, LR: 0.1
Time, 2019-01-01T16:45:22, Epoch: 1, Batch: 170, Training Loss: 0.43313329219818114, LR: 0.1
Time, 2019-01-01T16:45:23, Epoch: 1, Batch: 180, Training Loss: 0.416933873295784, LR: 0.1
Time, 2019-01-01T16:45:24, Epoch: 1, Batch: 190, Training Loss: 0.4109126299619675, LR: 0.1
Time, 2019-01-01T16:45:25, Epoch: 1, Batch: 200, Training Loss: 0.46081546097993853, LR: 0.1
Time, 2019-01-01T16:45:26, Epoch: 1, Batch: 210, Training Loss: 0.4080255597829819, LR: 0.1
Time, 2019-01-01T16:45:27, Epoch: 1, Batch: 220, Training Loss: 0.3849604994058609, LR: 0.1
Time, 2019-01-01T16:45:27, Epoch: 1, Batch: 230, Training Loss: 0.24969317615032197, LR: 0.1
Time, 2019-01-01T16:45:28, Epoch: 1, Batch: 240, Training Loss: 0.30744009613990786, LR: 0.1
Time, 2019-01-01T16:45:29, Epoch: 1, Batch: 250, Training Loss: 0.33572611808776853, LR: 0.1
Time, 2019-01-01T16:45:30, Epoch: 1, Batch: 260, Training Loss: 0.3461411088705063, LR: 0.1
Time, 2019-01-01T16:45:31, Epoch: 1, Batch: 270, Training Loss: 0.2912804365158081, LR: 0.1
Time, 2019-01-01T16:45:32, Epoch: 1, Batch: 280, Training Loss: 0.28749445378780364, LR: 0.1
Time, 2019-01-01T16:45:33, Epoch: 1, Batch: 290, Training Loss: 0.2784235030412674, LR: 0.1
Time, 2019-01-01T16:45:34, Epoch: 1, Batch: 300, Training Loss: 0.22035212218761444, LR: 0.1
Time, 2019-01-01T16:45:35, Epoch: 1, Batch: 310, Training Loss: 0.3411887839436531, LR: 0.1
Time, 2019-01-01T16:45:36, Epoch: 1, Batch: 320, Training Loss: 0.2544438034296036, LR: 0.1
Time, 2019-01-01T16:45:37, Epoch: 1, Batch: 330, Training Loss: 0.21964525505900384, LR: 0.1
Time, 2019-01-01T16:45:38, Epoch: 1, Batch: 340, Training Loss: 0.243173748254776, LR: 0.1
Time, 2019-01-01T16:45:39, Epoch: 1, Batch: 350, Training Loss: 0.20117718502879142, LR: 0.1
Time, 2019-01-01T16:45:40, Epoch: 1, Batch: 360, Training Loss: 0.23905159831047057, LR: 0.1
Time, 2019-01-01T16:45:40, Epoch: 1, Batch: 370, Training Loss: 0.2186166413128376, LR: 0.1
Time, 2019-01-01T16:45:41, Epoch: 1, Batch: 380, Training Loss: 0.20409067273139953, LR: 0.1
Time, 2019-01-01T16:45:42, Epoch: 1, Batch: 390, Training Loss: 0.2111286662518978, LR: 0.1
Time, 2019-01-01T16:45:43, Epoch: 1, Batch: 400, Training Loss: 0.25287812650203706, LR: 0.1
Time, 2019-01-01T16:45:44, Epoch: 1, Batch: 410, Training Loss: 0.22254203110933304, LR: 0.1
Time, 2019-01-01T16:45:45, Epoch: 1, Batch: 420, Training Loss: 0.21676063612103463, LR: 0.1
Time, 2019-01-01T16:45:46, Epoch: 1, Batch: 430, Training Loss: 0.16792871281504632, LR: 0.1
Time, 2019-01-01T16:45:47, Epoch: 1, Batch: 440, Training Loss: 0.16779102459549905, LR: 0.1
Time, 2019-01-01T16:45:48, Epoch: 1, Batch: 450, Training Loss: 0.21826755180954932, LR: 0.1
Time, 2019-01-01T16:45:49, Epoch: 1, Batch: 460, Training Loss: 0.2565105654299259, LR: 0.1
Time, 2019-01-01T16:45:50, Epoch: 1, Batch: 470, Training Loss: 0.22529503032565118, LR: 0.1
Time, 2019-01-01T16:45:51, Epoch: 1, Batch: 480, Training Loss: 0.17851774990558625, LR: 0.1
Time, 2019-01-01T16:45:52, Epoch: 1, Batch: 490, Training Loss: 0.2286805361509323, LR: 0.1
Time, 2019-01-01T16:45:53, Epoch: 1, Batch: 500, Training Loss: 0.19559786319732667, LR: 0.1
Time, 2019-01-01T16:45:54, Epoch: 1, Batch: 510, Training Loss: 0.19595499336719513, LR: 0.1
Time, 2019-01-01T16:45:54, Epoch: 1, Batch: 520, Training Loss: 0.19494074545800685, LR: 0.1
Time, 2019-01-01T16:45:55, Epoch: 1, Batch: 530, Training Loss: 0.22495675161480905, LR: 0.1
Time, 2019-01-01T16:45:56, Epoch: 1, Batch: 540, Training Loss: 0.2115890182554722, LR: 0.1
Time, 2019-01-01T16:45:57, Epoch: 1, Batch: 550, Training Loss: 0.19666177406907082, LR: 0.1
Time, 2019-01-01T16:45:58, Epoch: 1, Batch: 560, Training Loss: 0.2018684573471546, LR: 0.1
Time, 2019-01-01T16:45:59, Epoch: 1, Batch: 570, Training Loss: 0.24356483668088913, LR: 0.1
Time, 2019-01-01T16:46:00, Epoch: 1, Batch: 580, Training Loss: 0.1978568732738495, LR: 0.1
Time, 2019-01-01T16:46:01, Epoch: 1, Batch: 590, Training Loss: 0.2179168239235878, LR: 0.1
Time, 2019-01-01T16:46:01, Epoch: 1, Batch: 600, Training Loss: 0.2030552625656128, LR: 0.1
Time, 2019-01-01T16:46:02, Epoch: 1, Batch: 610, Training Loss: 0.19609909877181053, LR: 0.1
Time, 2019-01-01T16:46:03, Epoch: 1, Batch: 620, Training Loss: 0.18037215769290924, LR: 0.1
Time, 2019-01-01T16:46:04, Epoch: 1, Batch: 630, Training Loss: 0.17588537633419038, LR: 0.1
Time, 2019-01-01T16:46:05, Epoch: 1, Batch: 640, Training Loss: 0.17469497099518777, LR: 0.1
Time, 2019-01-01T16:46:06, Epoch: 1, Batch: 650, Training Loss: 0.17770544216036796, LR: 0.1
Time, 2019-01-01T16:46:07, Epoch: 1, Batch: 660, Training Loss: 0.13293821662664412, LR: 0.1
Time, 2019-01-01T16:46:08, Epoch: 1, Batch: 670, Training Loss: 0.17694369405508042, LR: 0.1
Time, 2019-01-01T16:46:08, Epoch: 1, Batch: 680, Training Loss: 0.2597116746008396, LR: 0.1
Time, 2019-01-01T16:46:09, Epoch: 1, Batch: 690, Training Loss: 0.15212613195180893, LR: 0.1
Time, 2019-01-01T16:46:10, Epoch: 1, Batch: 700, Training Loss: 0.15305257439613343, LR: 0.1
Time, 2019-01-01T16:46:11, Epoch: 1, Batch: 710, Training Loss: 0.19110808074474334, LR: 0.1
Time, 2019-01-01T16:46:12, Epoch: 1, Batch: 720, Training Loss: 0.1595878578722477, LR: 0.1
Time, 2019-01-01T16:46:13, Epoch: 1, Batch: 730, Training Loss: 0.18698139786720275, LR: 0.1
Time, 2019-01-01T16:46:14, Epoch: 1, Batch: 740, Training Loss: 0.19541677907109262, LR: 0.1
Time, 2019-01-01T16:46:15, Epoch: 1, Batch: 750, Training Loss: 0.1728965863585472, LR: 0.1
Time, 2019-01-01T16:46:16, Epoch: 1, Batch: 760, Training Loss: 0.24646558985114098, LR: 0.1
Time, 2019-01-01T16:46:16, Epoch: 1, Batch: 770, Training Loss: 0.2453540548682213, LR: 0.1
Time, 2019-01-01T16:46:17, Epoch: 1, Batch: 780, Training Loss: 0.2917212963104248, LR: 0.1
Time, 2019-01-01T16:46:18, Epoch: 1, Batch: 790, Training Loss: 0.3868522852659225, LR: 0.1
Time, 2019-01-01T16:46:19, Epoch: 1, Batch: 800, Training Loss: 0.24236961156129838, LR: 0.1
Time, 2019-01-01T16:46:20, Epoch: 1, Batch: 810, Training Loss: 0.18170551881194114, LR: 0.1
Time, 2019-01-01T16:46:21, Epoch: 1, Batch: 820, Training Loss: 0.21307679116725922, LR: 0.1
Time, 2019-01-01T16:46:22, Epoch: 1, Batch: 830, Training Loss: 0.211616587638855, LR: 0.1
Time, 2019-01-01T16:46:23, Epoch: 1, Batch: 840, Training Loss: 0.2278993621468544, LR: 0.1
Time, 2019-01-01T16:46:23, Epoch: 1, Batch: 850, Training Loss: 0.25119150131940843, LR: 0.1
Time, 2019-01-01T16:46:24, Epoch: 1, Batch: 860, Training Loss: 0.2379303067922592, LR: 0.1
Time, 2019-01-01T16:46:25, Epoch: 1, Batch: 870, Training Loss: 0.22840625941753387, LR: 0.1
Time, 2019-01-01T16:46:26, Epoch: 1, Batch: 880, Training Loss: 0.29592910781502724, LR: 0.1
Time, 2019-01-01T16:46:27, Epoch: 1, Batch: 890, Training Loss: 0.20075104832649232, LR: 0.1
Time, 2019-01-01T16:46:27, Epoch: 1, Batch: 900, Training Loss: 0.1658834010362625, LR: 0.1
Time, 2019-01-01T16:46:28, Epoch: 1, Batch: 910, Training Loss: 0.15383599251508712, LR: 0.1
Time, 2019-01-01T16:46:29, Epoch: 1, Batch: 920, Training Loss: 0.1541222631931305, LR: 0.1
Time, 2019-01-01T16:46:30, Epoch: 1, Batch: 930, Training Loss: 0.19457844346761705, LR: 0.1
Epoch: 1, Validation Top 1 acc: 93.77165222167969
Epoch: 1, Validation Top 5 acc: 99.86507415771484
Epoch: 1, Validation Set Loss: 0.19308753311634064
Start training epoch 2
Time, 2019-01-01T16:47:02, Epoch: 2, Batch: 10, Training Loss: 0.26160033494234086, LR: 0.1
Time, 2019-01-01T16:47:03, Epoch: 2, Batch: 20, Training Loss: 0.22232121601700783, LR: 0.1
Time, 2019-01-01T16:47:04, Epoch: 2, Batch: 30, Training Loss: 0.17911418676376342, LR: 0.1
Time, 2019-01-01T16:47:05, Epoch: 2, Batch: 40, Training Loss: 0.23643916547298433, LR: 0.1
Time, 2019-01-01T16:47:06, Epoch: 2, Batch: 50, Training Loss: 0.18957068249583245, LR: 0.1
Time, 2019-01-01T16:47:06, Epoch: 2, Batch: 60, Training Loss: 0.18098835721611978, LR: 0.1
Time, 2019-01-01T16:47:07, Epoch: 2, Batch: 70, Training Loss: 0.23181211948394775, LR: 0.1
Time, 2019-01-01T16:47:08, Epoch: 2, Batch: 80, Training Loss: 0.24557488337159156, LR: 0.1
Time, 2019-01-01T16:47:09, Epoch: 2, Batch: 90, Training Loss: 0.2113843709230423, LR: 0.1
Time, 2019-01-01T16:47:10, Epoch: 2, Batch: 100, Training Loss: 0.2591519519686699, LR: 0.1
Time, 2019-01-01T16:47:10, Epoch: 2, Batch: 110, Training Loss: 0.24103495180606843, LR: 0.1
Time, 2019-01-01T16:47:11, Epoch: 2, Batch: 120, Training Loss: 0.21815044283866883, LR: 0.1
Time, 2019-01-01T16:47:12, Epoch: 2, Batch: 130, Training Loss: 0.15439919456839563, LR: 0.1
Time, 2019-01-01T16:47:13, Epoch: 2, Batch: 140, Training Loss: 0.1437273532152176, LR: 0.1
Time, 2019-01-01T16:47:14, Epoch: 2, Batch: 150, Training Loss: 0.2000788927078247, LR: 0.1
Time, 2019-01-01T16:47:14, Epoch: 2, Batch: 160, Training Loss: 0.19037656262516975, LR: 0.1
Time, 2019-01-01T16:47:15, Epoch: 2, Batch: 170, Training Loss: 0.16530225798487663, LR: 0.1
Time, 2019-01-01T16:47:16, Epoch: 2, Batch: 180, Training Loss: 0.14185018613934516, LR: 0.1
Time, 2019-01-01T16:47:17, Epoch: 2, Batch: 190, Training Loss: 0.14645355343818664, LR: 0.1
Time, 2019-01-01T16:47:18, Epoch: 2, Batch: 200, Training Loss: 0.20402675569057466, LR: 0.1
Time, 2019-01-01T16:47:18, Epoch: 2, Batch: 210, Training Loss: 0.1946896143257618, LR: 0.1
Time, 2019-01-01T16:47:19, Epoch: 2, Batch: 220, Training Loss: 0.16685959324240685, LR: 0.1
Time, 2019-01-01T16:47:20, Epoch: 2, Batch: 230, Training Loss: 0.14692349657416343, LR: 0.1
Time, 2019-01-01T16:47:21, Epoch: 2, Batch: 240, Training Loss: 0.13454411551356316, LR: 0.1
Time, 2019-01-01T16:47:21, Epoch: 2, Batch: 250, Training Loss: 0.20033985152840614, LR: 0.1
Time, 2019-01-01T16:47:22, Epoch: 2, Batch: 260, Training Loss: 0.17756800428032876, LR: 0.1
Time, 2019-01-01T16:47:23, Epoch: 2, Batch: 270, Training Loss: 0.1519073650240898, LR: 0.1
Time, 2019-01-01T16:47:24, Epoch: 2, Batch: 280, Training Loss: 0.1833498403429985, LR: 0.1
Time, 2019-01-01T16:47:24, Epoch: 2, Batch: 290, Training Loss: 0.23009704202413558, LR: 0.1
Time, 2019-01-01T16:47:25, Epoch: 2, Batch: 300, Training Loss: 0.17355881705880166, LR: 0.1
Time, 2019-01-01T16:47:26, Epoch: 2, Batch: 310, Training Loss: 0.15103007927536966, LR: 0.1
Time, 2019-01-01T16:47:27, Epoch: 2, Batch: 320, Training Loss: 0.11225203350186348, LR: 0.1
Time, 2019-01-01T16:47:28, Epoch: 2, Batch: 330, Training Loss: 0.186710237711668, LR: 0.1
Time, 2019-01-01T16:47:28, Epoch: 2, Batch: 340, Training Loss: 0.18246547505259514, LR: 0.1
Time, 2019-01-01T16:47:29, Epoch: 2, Batch: 350, Training Loss: 0.12112028039991855, LR: 0.1
Time, 2019-01-01T16:47:30, Epoch: 2, Batch: 360, Training Loss: 0.15350864976644515, LR: 0.1
Time, 2019-01-01T16:47:31, Epoch: 2, Batch: 370, Training Loss: 0.17655366733670236, LR: 0.1
Time, 2019-01-01T16:47:32, Epoch: 2, Batch: 380, Training Loss: 0.1604113556444645, LR: 0.1
Time, 2019-01-01T16:47:32, Epoch: 2, Batch: 390, Training Loss: 0.090561718121171, LR: 0.1
Time, 2019-01-01T16:47:33, Epoch: 2, Batch: 400, Training Loss: 0.1388050690293312, LR: 0.1
Time, 2019-01-01T16:47:34, Epoch: 2, Batch: 410, Training Loss: 0.13578008785843848, LR: 0.1
Time, 2019-01-01T16:47:35, Epoch: 2, Batch: 420, Training Loss: 0.2121187888085842, LR: 0.1
Time, 2019-01-01T16:47:35, Epoch: 2, Batch: 430, Training Loss: 0.20656518265604973, LR: 0.1
Time, 2019-01-01T16:47:36, Epoch: 2, Batch: 440, Training Loss: 0.13317862823605536, LR: 0.1
Time, 2019-01-01T16:47:37, Epoch: 2, Batch: 450, Training Loss: 0.13994214981794356, LR: 0.1
Time, 2019-01-01T16:47:38, Epoch: 2, Batch: 460, Training Loss: 0.12276349812746049, LR: 0.1
Time, 2019-01-01T16:47:38, Epoch: 2, Batch: 470, Training Loss: 0.15271969586610795, LR: 0.1
Time, 2019-01-01T16:47:39, Epoch: 2, Batch: 480, Training Loss: 0.12769499197602271, LR: 0.1
Time, 2019-01-01T16:47:40, Epoch: 2, Batch: 490, Training Loss: 0.13631611615419387, LR: 0.1
Time, 2019-01-01T16:47:41, Epoch: 2, Batch: 500, Training Loss: 0.13396735712885857, LR: 0.1
Time, 2019-01-01T16:47:41, Epoch: 2, Batch: 510, Training Loss: 0.11751628816127777, LR: 0.1
Time, 2019-01-01T16:47:42, Epoch: 2, Batch: 520, Training Loss: 0.20353008732199668, LR: 0.1
Time, 2019-01-01T16:47:43, Epoch: 2, Batch: 530, Training Loss: 0.16621251441538334, LR: 0.1
Time, 2019-01-01T16:47:44, Epoch: 2, Batch: 540, Training Loss: 0.20122976899147033, LR: 0.1
Time, 2019-01-01T16:47:44, Epoch: 2, Batch: 550, Training Loss: 0.1727969691157341, LR: 0.1
Time, 2019-01-01T16:47:45, Epoch: 2, Batch: 560, Training Loss: 0.1238553561270237, LR: 0.1
Time, 2019-01-01T16:47:46, Epoch: 2, Batch: 570, Training Loss: 0.16356786265969275, LR: 0.1
Time, 2019-01-01T16:47:47, Epoch: 2, Batch: 580, Training Loss: 0.1578782670199871, LR: 0.1
Time, 2019-01-01T16:47:48, Epoch: 2, Batch: 590, Training Loss: 0.1563906379044056, LR: 0.1
Time, 2019-01-01T16:47:48, Epoch: 2, Batch: 600, Training Loss: 0.12759778797626495, LR: 0.1
Time, 2019-01-01T16:47:49, Epoch: 2, Batch: 610, Training Loss: 0.1320456214249134, LR: 0.1
Time, 2019-01-01T16:47:50, Epoch: 2, Batch: 620, Training Loss: 0.14097250923514365, LR: 0.1
Time, 2019-01-01T16:47:51, Epoch: 2, Batch: 630, Training Loss: 0.11048968732357026, LR: 0.1
Time, 2019-01-01T16:47:52, Epoch: 2, Batch: 640, Training Loss: 0.17897868528962135, LR: 0.1
Time, 2019-01-01T16:47:53, Epoch: 2, Batch: 650, Training Loss: 0.13309550583362578, LR: 0.1
Time, 2019-01-01T16:47:53, Epoch: 2, Batch: 660, Training Loss: 0.19942767694592475, LR: 0.1
Time, 2019-01-01T16:47:54, Epoch: 2, Batch: 670, Training Loss: 0.13313009440898896, LR: 0.1
Time, 2019-01-01T16:47:55, Epoch: 2, Batch: 680, Training Loss: 0.1554377056658268, LR: 0.1
Time, 2019-01-01T16:47:56, Epoch: 2, Batch: 690, Training Loss: 0.169500894844532, LR: 0.1
Time, 2019-01-01T16:47:57, Epoch: 2, Batch: 700, Training Loss: 0.1479126490652561, LR: 0.1
Time, 2019-01-01T16:47:57, Epoch: 2, Batch: 710, Training Loss: 0.19665208235383033, LR: 0.1
Time, 2019-01-01T16:47:58, Epoch: 2, Batch: 720, Training Loss: 0.2119704455137253, LR: 0.1
Time, 2019-01-01T16:47:59, Epoch: 2, Batch: 730, Training Loss: 0.16279964670538902, LR: 0.1
Time, 2019-01-01T16:48:00, Epoch: 2, Batch: 740, Training Loss: 0.17326128259301185, LR: 0.1
Time, 2019-01-01T16:48:01, Epoch: 2, Batch: 750, Training Loss: 0.15876487120985985, LR: 0.1
Time, 2019-01-01T16:48:01, Epoch: 2, Batch: 760, Training Loss: 0.15266187116503716, LR: 0.1
Time, 2019-01-01T16:48:02, Epoch: 2, Batch: 770, Training Loss: 0.1503314658999443, LR: 0.1
Time, 2019-01-01T16:48:03, Epoch: 2, Batch: 780, Training Loss: 0.1396687164902687, LR: 0.1
Time, 2019-01-01T16:48:04, Epoch: 2, Batch: 790, Training Loss: 0.12379945330321789, LR: 0.1
Time, 2019-01-01T16:48:05, Epoch: 2, Batch: 800, Training Loss: 0.13649980947375298, LR: 0.1
Time, 2019-01-01T16:48:05, Epoch: 2, Batch: 810, Training Loss: 0.1342327743768692, LR: 0.1
Time, 2019-01-01T16:48:06, Epoch: 2, Batch: 820, Training Loss: 0.1222052350640297, LR: 0.1
Time, 2019-01-01T16:48:07, Epoch: 2, Batch: 830, Training Loss: 0.1531279157847166, LR: 0.1
Time, 2019-01-01T16:48:08, Epoch: 2, Batch: 840, Training Loss: 0.08912898153066635, LR: 0.1
Time, 2019-01-01T16:48:09, Epoch: 2, Batch: 850, Training Loss: 0.14384248219430446, LR: 0.1
Time, 2019-01-01T16:48:09, Epoch: 2, Batch: 860, Training Loss: 0.2145795226097107, LR: 0.1
Time, 2019-01-01T16:48:10, Epoch: 2, Batch: 870, Training Loss: 0.21239024698734282, LR: 0.1
Time, 2019-01-01T16:48:11, Epoch: 2, Batch: 880, Training Loss: 0.15620494708418847, LR: 0.1
Time, 2019-01-01T16:48:12, Epoch: 2, Batch: 890, Training Loss: 0.14497088752686976, LR: 0.1
Time, 2019-01-01T16:48:13, Epoch: 2, Batch: 900, Training Loss: 0.13570268750190734, LR: 0.1
Time, 2019-01-01T16:48:13, Epoch: 2, Batch: 910, Training Loss: 0.16308048516511917, LR: 0.1
Time, 2019-01-01T16:48:14, Epoch: 2, Batch: 920, Training Loss: 0.15166100412607192, LR: 0.1
Time, 2019-01-01T16:48:15, Epoch: 2, Batch: 930, Training Loss: 0.1321489132940769, LR: 0.1
Epoch: 2, Validation Top 1 acc: 95.01265716552734
Epoch: 2, Validation Top 5 acc: 99.89006042480469
Epoch: 2, Validation Set Loss: 0.15640370547771454
Start training epoch 3
Time, 2019-01-01T16:48:45, Epoch: 3, Batch: 10, Training Loss: 0.20828648135066033, LR: 0.1
Time, 2019-01-01T16:48:46, Epoch: 3, Batch: 20, Training Loss: 0.20370730832219125, LR: 0.1
Time, 2019-01-01T16:48:47, Epoch: 3, Batch: 30, Training Loss: 0.1579689212143421, LR: 0.1
Time, 2019-01-01T16:48:48, Epoch: 3, Batch: 40, Training Loss: 0.1761560395359993, LR: 0.1
Time, 2019-01-01T16:48:48, Epoch: 3, Batch: 50, Training Loss: 0.1405237138271332, LR: 0.1
Time, 2019-01-01T16:48:49, Epoch: 3, Batch: 60, Training Loss: 0.17811191827058792, LR: 0.1
Time, 2019-01-01T16:48:50, Epoch: 3, Batch: 70, Training Loss: 0.12878917902708054, LR: 0.1
Time, 2019-01-01T16:48:51, Epoch: 3, Batch: 80, Training Loss: 0.15247613415122033, LR: 0.1
Time, 2019-01-01T16:48:52, Epoch: 3, Batch: 90, Training Loss: 0.16216660514473916, LR: 0.1
Time, 2019-01-01T16:48:53, Epoch: 3, Batch: 100, Training Loss: 0.1631619930267334, LR: 0.1
Time, 2019-01-01T16:48:53, Epoch: 3, Batch: 110, Training Loss: 0.1671746104955673, LR: 0.1
Time, 2019-01-01T16:48:54, Epoch: 3, Batch: 120, Training Loss: 0.13800640776753426, LR: 0.1
Time, 2019-01-01T16:48:55, Epoch: 3, Batch: 130, Training Loss: 0.13578494563698768, LR: 0.1
Time, 2019-01-01T16:48:56, Epoch: 3, Batch: 140, Training Loss: 0.15392450094223023, LR: 0.1
Time, 2019-01-01T16:48:57, Epoch: 3, Batch: 150, Training Loss: 0.11582645811140538, LR: 0.1
Time, 2019-01-01T16:48:58, Epoch: 3, Batch: 160, Training Loss: 0.18258109614253043, LR: 0.1
Time, 2019-01-01T16:48:59, Epoch: 3, Batch: 170, Training Loss: 0.16583432182669638, LR: 0.1
Time, 2019-01-01T16:48:59, Epoch: 3, Batch: 180, Training Loss: 0.13842326551675796, LR: 0.1
Time, 2019-01-01T16:49:00, Epoch: 3, Batch: 190, Training Loss: 0.1346202116459608, LR: 0.1
Time, 2019-01-01T16:49:01, Epoch: 3, Batch: 200, Training Loss: 0.14973998367786406, LR: 0.1
Time, 2019-01-01T16:49:02, Epoch: 3, Batch: 210, Training Loss: 0.13672710321843623, LR: 0.1
Time, 2019-01-01T16:49:03, Epoch: 3, Batch: 220, Training Loss: 0.2274048626422882, LR: 0.1
Time, 2019-01-01T16:49:04, Epoch: 3, Batch: 230, Training Loss: 0.16997964158654214, LR: 0.1
Time, 2019-01-01T16:49:04, Epoch: 3, Batch: 240, Training Loss: 0.14916143864393233, LR: 0.1
Time, 2019-01-01T16:49:05, Epoch: 3, Batch: 250, Training Loss: 0.15562210977077484, LR: 0.1
Time, 2019-01-01T16:49:06, Epoch: 3, Batch: 260, Training Loss: 0.17236111164093018, LR: 0.1
Time, 2019-01-01T16:49:07, Epoch: 3, Batch: 270, Training Loss: 0.2108665570616722, LR: 0.1
Time, 2019-01-01T16:49:08, Epoch: 3, Batch: 280, Training Loss: 0.17777035161852836, LR: 0.1
Time, 2019-01-01T16:49:09, Epoch: 3, Batch: 290, Training Loss: 0.20190515071153642, LR: 0.1
Time, 2019-01-01T16:49:10, Epoch: 3, Batch: 300, Training Loss: 0.2063346989452839, LR: 0.1
Time, 2019-01-01T16:49:10, Epoch: 3, Batch: 310, Training Loss: 0.2318684436380863, LR: 0.1
Time, 2019-01-01T16:49:11, Epoch: 3, Batch: 320, Training Loss: 0.17847955636680127, LR: 0.1
Time, 2019-01-01T16:49:12, Epoch: 3, Batch: 330, Training Loss: 0.1838819034397602, LR: 0.1
Time, 2019-01-01T16:49:13, Epoch: 3, Batch: 340, Training Loss: 0.1743338018655777, LR: 0.1
Time, 2019-01-01T16:49:14, Epoch: 3, Batch: 350, Training Loss: 0.15019211992621423, LR: 0.1
Time, 2019-01-01T16:49:15, Epoch: 3, Batch: 360, Training Loss: 0.1337505653500557, LR: 0.1
Time, 2019-01-01T16:49:16, Epoch: 3, Batch: 370, Training Loss: 0.16433405354619027, LR: 0.1
Time, 2019-01-01T16:49:17, Epoch: 3, Batch: 380, Training Loss: 0.16811862364411354, LR: 0.1
Time, 2019-01-01T16:49:18, Epoch: 3, Batch: 390, Training Loss: 0.176951165497303, LR: 0.1
Time, 2019-01-01T16:49:18, Epoch: 3, Batch: 400, Training Loss: 0.17024777010083197, LR: 0.1
Time, 2019-01-01T16:49:19, Epoch: 3, Batch: 410, Training Loss: 0.17287961542606353, LR: 0.1
Time, 2019-01-01T16:49:20, Epoch: 3, Batch: 420, Training Loss: 0.17987748458981515, LR: 0.1
Time, 2019-01-01T16:49:21, Epoch: 3, Batch: 430, Training Loss: 0.14202453717589378, LR: 0.1
Time, 2019-01-01T16:49:22, Epoch: 3, Batch: 440, Training Loss: 0.16440874934196473, LR: 0.1
Time, 2019-01-01T16:49:23, Epoch: 3, Batch: 450, Training Loss: 0.1875844981521368, LR: 0.1
Time, 2019-01-01T16:49:24, Epoch: 3, Batch: 460, Training Loss: 0.21244437098503113, LR: 0.1
Time, 2019-01-01T16:49:25, Epoch: 3, Batch: 470, Training Loss: 0.18211910761892797, LR: 0.1
Time, 2019-01-01T16:49:26, Epoch: 3, Batch: 480, Training Loss: 0.15092434287071227, LR: 0.1
Time, 2019-01-01T16:49:26, Epoch: 3, Batch: 490, Training Loss: 0.19796034768223764, LR: 0.1
Time, 2019-01-01T16:49:27, Epoch: 3, Batch: 500, Training Loss: 0.1422511085867882, LR: 0.1
Time, 2019-01-01T16:49:28, Epoch: 3, Batch: 510, Training Loss: 0.11251184195280076, LR: 0.1
Time, 2019-01-01T16:49:29, Epoch: 3, Batch: 520, Training Loss: 0.13984308913350105, LR: 0.1
Time, 2019-01-01T16:49:30, Epoch: 3, Batch: 530, Training Loss: 0.14166471287608146, LR: 0.1
Time, 2019-01-01T16:49:31, Epoch: 3, Batch: 540, Training Loss: 0.17724821865558624, LR: 0.1
Time, 2019-01-01T16:49:32, Epoch: 3, Batch: 550, Training Loss: 0.10984221175312996, LR: 0.1
Time, 2019-01-01T16:49:33, Epoch: 3, Batch: 560, Training Loss: 0.13839063346385955, LR: 0.1
Time, 2019-01-01T16:49:33, Epoch: 3, Batch: 570, Training Loss: 0.22480005472898484, LR: 0.1
Time, 2019-01-01T16:49:34, Epoch: 3, Batch: 580, Training Loss: 0.14337093606591225, LR: 0.1
Time, 2019-01-01T16:49:35, Epoch: 3, Batch: 590, Training Loss: 0.10659336149692536, LR: 0.1
Time, 2019-01-01T16:49:36, Epoch: 3, Batch: 600, Training Loss: 0.11404149904847145, LR: 0.1
Time, 2019-01-01T16:49:37, Epoch: 3, Batch: 610, Training Loss: 0.128006449341774, LR: 0.1
Time, 2019-01-01T16:49:38, Epoch: 3, Batch: 620, Training Loss: 0.12806915640830993, LR: 0.1
Time, 2019-01-01T16:49:38, Epoch: 3, Batch: 630, Training Loss: 0.12577180415391923, LR: 0.1
Time, 2019-01-01T16:49:39, Epoch: 3, Batch: 640, Training Loss: 0.14610513150691987, LR: 0.1
Time, 2019-01-01T16:49:40, Epoch: 3, Batch: 650, Training Loss: 0.12066417559981346, LR: 0.1
Time, 2019-01-01T16:49:41, Epoch: 3, Batch: 660, Training Loss: 0.15020575672388076, LR: 0.1
Time, 2019-01-01T16:49:42, Epoch: 3, Batch: 670, Training Loss: 0.17234594672918319, LR: 0.1
Time, 2019-01-01T16:49:43, Epoch: 3, Batch: 680, Training Loss: 0.12139529958367348, LR: 0.1
Time, 2019-01-01T16:49:44, Epoch: 3, Batch: 690, Training Loss: 0.16670996993780135, LR: 0.1
Time, 2019-01-01T16:49:44, Epoch: 3, Batch: 700, Training Loss: 0.1247402235865593, LR: 0.1
Time, 2019-01-01T16:49:45, Epoch: 3, Batch: 710, Training Loss: 0.16150506734848022, LR: 0.1
Time, 2019-01-01T16:49:46, Epoch: 3, Batch: 720, Training Loss: 0.1681552417576313, LR: 0.1
Time, 2019-01-01T16:49:47, Epoch: 3, Batch: 730, Training Loss: 0.1290487267076969, LR: 0.1
Time, 2019-01-01T16:49:48, Epoch: 3, Batch: 740, Training Loss: 0.191330187022686, LR: 0.1
Time, 2019-01-01T16:49:49, Epoch: 3, Batch: 750, Training Loss: 0.17501742616295815, LR: 0.1
Time, 2019-01-01T16:49:50, Epoch: 3, Batch: 760, Training Loss: 0.1924750655889511, LR: 0.1
Time, 2019-01-01T16:49:50, Epoch: 3, Batch: 770, Training Loss: 0.10253216922283173, LR: 0.1
Time, 2019-01-01T16:49:51, Epoch: 3, Batch: 780, Training Loss: 0.13917489498853683, LR: 0.1
Time, 2019-01-01T16:49:52, Epoch: 3, Batch: 790, Training Loss: 0.09447867423295975, LR: 0.1
Time, 2019-01-01T16:49:53, Epoch: 3, Batch: 800, Training Loss: 0.09988444969058037, LR: 0.1
Time, 2019-01-01T16:49:54, Epoch: 3, Batch: 810, Training Loss: 0.12210034057497979, LR: 0.1
Time, 2019-01-01T16:49:55, Epoch: 3, Batch: 820, Training Loss: 0.13010587468743323, LR: 0.1
Time, 2019-01-01T16:49:55, Epoch: 3, Batch: 830, Training Loss: 0.14866227991878986, LR: 0.1
Time, 2019-01-01T16:49:56, Epoch: 3, Batch: 840, Training Loss: 0.10972772464156151, LR: 0.1
Time, 2019-01-01T16:49:57, Epoch: 3, Batch: 850, Training Loss: 0.15879136100411415, LR: 0.1
Time, 2019-01-01T16:49:58, Epoch: 3, Batch: 860, Training Loss: 0.10821006223559379, LR: 0.1
Time, 2019-01-01T16:49:59, Epoch: 3, Batch: 870, Training Loss: 0.11229484900832176, LR: 0.1
Time, 2019-01-01T16:50:00, Epoch: 3, Batch: 880, Training Loss: 0.18411750495433807, LR: 0.1
Time, 2019-01-01T16:50:01, Epoch: 3, Batch: 890, Training Loss: 0.13727505169808865, LR: 0.1
Time, 2019-01-01T16:50:01, Epoch: 3, Batch: 900, Training Loss: 0.11994653046131135, LR: 0.1
Time, 2019-01-01T16:50:02, Epoch: 3, Batch: 910, Training Loss: 0.14821742102503777, LR: 0.1
Time, 2019-01-01T16:50:03, Epoch: 3, Batch: 920, Training Loss: 0.14818502366542816, LR: 0.1
Time, 2019-01-01T16:50:04, Epoch: 3, Batch: 930, Training Loss: 0.11311434134840966, LR: 0.1
Epoch: 3, Validation Top 1 acc: 97.11154174804688
Epoch: 3, Validation Top 5 acc: 99.94503021240234
Epoch: 3, Validation Set Loss: 0.09272436797618866
Start training epoch 4
Time, 2019-01-01T16:50:38, Epoch: 4, Batch: 10, Training Loss: 0.08745167404413223, LR: 0.1
Time, 2019-01-01T16:50:38, Epoch: 4, Batch: 20, Training Loss: 0.09340266957879066, LR: 0.1
Time, 2019-01-01T16:50:39, Epoch: 4, Batch: 30, Training Loss: 0.12030473500490188, LR: 0.1
Time, 2019-01-01T16:50:40, Epoch: 4, Batch: 40, Training Loss: 0.09736040979623795, LR: 0.1
Time, 2019-01-01T16:50:41, Epoch: 4, Batch: 50, Training Loss: 0.10454631000757217, LR: 0.1
Time, 2019-01-01T16:50:42, Epoch: 4, Batch: 60, Training Loss: 0.08715673610568046, LR: 0.1
Time, 2019-01-01T16:50:43, Epoch: 4, Batch: 70, Training Loss: 0.09528106227517127, LR: 0.1
Time, 2019-01-01T16:50:44, Epoch: 4, Batch: 80, Training Loss: 0.10326782613992691, LR: 0.1
Time, 2019-01-01T16:50:44, Epoch: 4, Batch: 90, Training Loss: 0.12367982119321823, LR: 0.1
Time, 2019-01-01T16:50:45, Epoch: 4, Batch: 100, Training Loss: 0.1213259980082512, LR: 0.1
Time, 2019-01-01T16:50:46, Epoch: 4, Batch: 110, Training Loss: 0.14821989759802817, LR: 0.1
Time, 2019-01-01T16:50:47, Epoch: 4, Batch: 120, Training Loss: 0.1519414886832237, LR: 0.1
Time, 2019-01-01T16:50:48, Epoch: 4, Batch: 130, Training Loss: 0.13948042020201684, LR: 0.1
Time, 2019-01-01T16:50:49, Epoch: 4, Batch: 140, Training Loss: 0.14695737436413764, LR: 0.1
Time, 2019-01-01T16:50:49, Epoch: 4, Batch: 150, Training Loss: 0.18022796958684922, LR: 0.1
Time, 2019-01-01T16:50:50, Epoch: 4, Batch: 160, Training Loss: 0.21343708783388138, LR: 0.1
Time, 2019-01-01T16:50:51, Epoch: 4, Batch: 170, Training Loss: 0.14372079968452453, LR: 0.1
Time, 2019-01-01T16:50:52, Epoch: 4, Batch: 180, Training Loss: 0.14092555940151213, LR: 0.1
Time, 2019-01-01T16:50:53, Epoch: 4, Batch: 190, Training Loss: 0.11018873006105423, LR: 0.1
Time, 2019-01-01T16:50:54, Epoch: 4, Batch: 200, Training Loss: 0.14815266579389572, LR: 0.1
Time, 2019-01-01T16:50:55, Epoch: 4, Batch: 210, Training Loss: 0.12150306813418865, LR: 0.1
Time, 2019-01-01T16:50:55, Epoch: 4, Batch: 220, Training Loss: 0.1476823553442955, LR: 0.1
Time, 2019-01-01T16:50:56, Epoch: 4, Batch: 230, Training Loss: 0.1932102471590042, LR: 0.1
Time, 2019-01-01T16:50:57, Epoch: 4, Batch: 240, Training Loss: 0.20716924518346785, LR: 0.1
Time, 2019-01-01T16:50:58, Epoch: 4, Batch: 250, Training Loss: 0.16938968524336814, LR: 0.1
Time, 2019-01-01T16:50:59, Epoch: 4, Batch: 260, Training Loss: 0.1746302478015423, LR: 0.1
Time, 2019-01-01T16:51:00, Epoch: 4, Batch: 270, Training Loss: 0.17346660494804383, LR: 0.1
Time, 2019-01-01T16:51:00, Epoch: 4, Batch: 280, Training Loss: 0.13178018257021903, LR: 0.1
Time, 2019-01-01T16:51:01, Epoch: 4, Batch: 290, Training Loss: 0.13170281536877154, LR: 0.1
Time, 2019-01-01T16:51:02, Epoch: 4, Batch: 300, Training Loss: 0.08948157206177712, LR: 0.1
Time, 2019-01-01T16:51:03, Epoch: 4, Batch: 310, Training Loss: 0.15713683441281318, LR: 0.1
Time, 2019-01-01T16:51:04, Epoch: 4, Batch: 320, Training Loss: 0.09904718026518822, LR: 0.1
Time, 2019-01-01T16:51:05, Epoch: 4, Batch: 330, Training Loss: 0.10172623693943024, LR: 0.1
Time, 2019-01-01T16:51:05, Epoch: 4, Batch: 340, Training Loss: 0.10391053333878517, LR: 0.1
Time, 2019-01-01T16:51:06, Epoch: 4, Batch: 350, Training Loss: 0.1046073704957962, LR: 0.1
Time, 2019-01-01T16:51:07, Epoch: 4, Batch: 360, Training Loss: 0.08700780719518661, LR: 0.1
Time, 2019-01-01T16:51:08, Epoch: 4, Batch: 370, Training Loss: 0.1211497277021408, LR: 0.1
Time, 2019-01-01T16:51:09, Epoch: 4, Batch: 380, Training Loss: 0.15630951076745986, LR: 0.1
Time, 2019-01-01T16:51:10, Epoch: 4, Batch: 390, Training Loss: 0.11638909727334976, LR: 0.1
Time, 2019-01-01T16:51:11, Epoch: 4, Batch: 400, Training Loss: 0.08554294630885124, LR: 0.1
Time, 2019-01-01T16:51:11, Epoch: 4, Batch: 410, Training Loss: 0.09866421520709992, LR: 0.1
Time, 2019-01-01T16:51:12, Epoch: 4, Batch: 420, Training Loss: 0.10090007670223713, LR: 0.1
Time, 2019-01-01T16:51:13, Epoch: 4, Batch: 430, Training Loss: 0.12299726009368897, LR: 0.1
Time, 2019-01-01T16:51:14, Epoch: 4, Batch: 440, Training Loss: 0.11652466990053653, LR: 0.1
Time, 2019-01-01T16:51:15, Epoch: 4, Batch: 450, Training Loss: 0.07193409353494644, LR: 0.1
Time, 2019-01-01T16:51:16, Epoch: 4, Batch: 460, Training Loss: 0.10152467414736747, LR: 0.1
Time, 2019-01-01T16:51:17, Epoch: 4, Batch: 470, Training Loss: 0.11260492168366909, LR: 0.1
Time, 2019-01-01T16:51:18, Epoch: 4, Batch: 480, Training Loss: 0.1307351879775524, LR: 0.1
Time, 2019-01-01T16:51:18, Epoch: 4, Batch: 490, Training Loss: 0.11734596788883209, LR: 0.1
Time, 2019-01-01T16:51:19, Epoch: 4, Batch: 500, Training Loss: 0.12839176878333092, LR: 0.1
Time, 2019-01-01T16:51:20, Epoch: 4, Batch: 510, Training Loss: 0.10248823873698712, LR: 0.1
Time, 2019-01-01T16:51:21, Epoch: 4, Batch: 520, Training Loss: 0.08922957777976989, LR: 0.1
Time, 2019-01-01T16:51:22, Epoch: 4, Batch: 530, Training Loss: 0.14244709387421609, LR: 0.1
Time, 2019-01-01T16:51:23, Epoch: 4, Batch: 540, Training Loss: 0.14510632902383805, LR: 0.1
Time, 2019-01-01T16:51:24, Epoch: 4, Batch: 550, Training Loss: 0.10649030357599258, LR: 0.1
Time, 2019-01-01T16:51:25, Epoch: 4, Batch: 560, Training Loss: 0.11985883042216301, LR: 0.1
Time, 2019-01-01T16:51:25, Epoch: 4, Batch: 570, Training Loss: 0.1503111481666565, LR: 0.1
Time, 2019-01-01T16:51:26, Epoch: 4, Batch: 580, Training Loss: 0.116208004206419, LR: 0.1
Time, 2019-01-01T16:51:27, Epoch: 4, Batch: 590, Training Loss: 0.13769532218575478, LR: 0.1
Time, 2019-01-01T16:51:28, Epoch: 4, Batch: 600, Training Loss: 0.14728937447071075, LR: 0.1
Time, 2019-01-01T16:51:29, Epoch: 4, Batch: 610, Training Loss: 0.10878039821982384, LR: 0.1
Time, 2019-01-01T16:51:30, Epoch: 4, Batch: 620, Training Loss: 0.14234318360686302, LR: 0.1
Time, 2019-01-01T16:51:31, Epoch: 4, Batch: 630, Training Loss: 0.11410498134791851, LR: 0.1
Time, 2019-01-01T16:51:32, Epoch: 4, Batch: 640, Training Loss: 0.10658555589616299, LR: 0.1
Time, 2019-01-01T16:51:33, Epoch: 4, Batch: 650, Training Loss: 0.11905325055122376, LR: 0.1
Time, 2019-01-01T16:51:34, Epoch: 4, Batch: 660, Training Loss: 0.11492816582322121, LR: 0.1
Time, 2019-01-01T16:51:35, Epoch: 4, Batch: 670, Training Loss: 0.10599976852536201, LR: 0.1
Time, 2019-01-01T16:51:35, Epoch: 4, Batch: 680, Training Loss: 0.15784987695515157, LR: 0.1
Time, 2019-01-01T16:51:36, Epoch: 4, Batch: 690, Training Loss: 0.10419546365737915, LR: 0.1
Time, 2019-01-01T16:51:37, Epoch: 4, Batch: 700, Training Loss: 0.14530014246702194, LR: 0.1
Time, 2019-01-01T16:51:38, Epoch: 4, Batch: 710, Training Loss: 0.1257170967757702, LR: 0.1
Time, 2019-01-01T16:51:39, Epoch: 4, Batch: 720, Training Loss: 0.11400949805974961, LR: 0.1
Time, 2019-01-01T16:51:40, Epoch: 4, Batch: 730, Training Loss: 0.08839993998408317, LR: 0.1
Time, 2019-01-01T16:51:41, Epoch: 4, Batch: 740, Training Loss: 0.10211208090186119, LR: 0.1
Time, 2019-01-01T16:51:42, Epoch: 4, Batch: 750, Training Loss: 0.10378049947321415, LR: 0.1
Time, 2019-01-01T16:51:43, Epoch: 4, Batch: 760, Training Loss: 0.1345399409532547, LR: 0.1
Time, 2019-01-01T16:51:44, Epoch: 4, Batch: 770, Training Loss: 0.09391863346099853, LR: 0.1
Time, 2019-01-01T16:51:44, Epoch: 4, Batch: 780, Training Loss: 0.1378484345972538, LR: 0.1
Time, 2019-01-01T16:51:45, Epoch: 4, Batch: 790, Training Loss: 0.1448747731745243, LR: 0.1
Time, 2019-01-01T16:51:46, Epoch: 4, Batch: 800, Training Loss: 0.11338598877191544, LR: 0.1
Time, 2019-01-01T16:51:47, Epoch: 4, Batch: 810, Training Loss: 0.1639118015766144, LR: 0.1
Time, 2019-01-01T16:51:48, Epoch: 4, Batch: 820, Training Loss: 0.13944359123706818, LR: 0.1
Time, 2019-01-01T16:51:49, Epoch: 4, Batch: 830, Training Loss: 0.16455632224678993, LR: 0.1
Time, 2019-01-01T16:51:50, Epoch: 4, Batch: 840, Training Loss: 0.1333581067621708, LR: 0.1
Time, 2019-01-01T16:51:51, Epoch: 4, Batch: 850, Training Loss: 0.15094895362854005, LR: 0.1
Time, 2019-01-01T16:51:52, Epoch: 4, Batch: 860, Training Loss: 0.1328795187175274, LR: 0.1
Time, 2019-01-01T16:51:53, Epoch: 4, Batch: 870, Training Loss: 0.15153706446290016, LR: 0.1
Time, 2019-01-01T16:51:53, Epoch: 4, Batch: 880, Training Loss: 0.15530639812350272, LR: 0.1
Time, 2019-01-01T16:51:54, Epoch: 4, Batch: 890, Training Loss: 0.27874419912695886, LR: 0.1
Time, 2019-01-01T16:51:55, Epoch: 4, Batch: 900, Training Loss: 0.19793271645903587, LR: 0.1
Time, 2019-01-01T16:51:56, Epoch: 4, Batch: 910, Training Loss: 0.20030673891305922, LR: 0.1
Time, 2019-01-01T16:51:57, Epoch: 4, Batch: 920, Training Loss: 0.13207071423530578, LR: 0.1
Time, 2019-01-01T16:51:58, Epoch: 4, Batch: 930, Training Loss: 0.22054631859064103, LR: 0.1
Epoch: 4, Validation Top 1 acc: 94.26972198486328
Epoch: 4, Validation Top 5 acc: 99.88506317138672
Epoch: 4, Validation Set Loss: 0.1746184080839157
Start training epoch 5
Time, 2019-01-01T16:52:28, Epoch: 5, Batch: 10, Training Loss: 0.17624519243836403, LR: 0.1
Time, 2019-01-01T16:52:29, Epoch: 5, Batch: 20, Training Loss: 0.21970597580075263, LR: 0.1
Time, 2019-01-01T16:52:30, Epoch: 5, Batch: 30, Training Loss: 0.19420664831995965, LR: 0.1
Time, 2019-01-01T16:52:31, Epoch: 5, Batch: 40, Training Loss: 0.17622753456234933, LR: 0.1
Time, 2019-01-01T16:52:32, Epoch: 5, Batch: 50, Training Loss: 0.1607833869755268, LR: 0.1
Time, 2019-01-01T16:52:33, Epoch: 5, Batch: 60, Training Loss: 0.10730992183089257, LR: 0.1
Time, 2019-01-01T16:52:34, Epoch: 5, Batch: 70, Training Loss: 0.12346297726035119, LR: 0.1
Time, 2019-01-01T16:52:35, Epoch: 5, Batch: 80, Training Loss: 0.14528781101107596, LR: 0.1
Time, 2019-01-01T16:52:35, Epoch: 5, Batch: 90, Training Loss: 0.18893597647547722, LR: 0.1
Time, 2019-01-01T16:52:37, Epoch: 5, Batch: 100, Training Loss: 0.12561794444918634, LR: 0.1
Time, 2019-01-01T16:52:37, Epoch: 5, Batch: 110, Training Loss: 0.1527326837182045, LR: 0.1
Time, 2019-01-01T16:52:38, Epoch: 5, Batch: 120, Training Loss: 0.18777335435152054, LR: 0.1
Time, 2019-01-01T16:52:39, Epoch: 5, Batch: 130, Training Loss: 0.14182185456156732, LR: 0.1
Time, 2019-01-01T16:52:40, Epoch: 5, Batch: 140, Training Loss: 0.12386524081230163, LR: 0.1
Time, 2019-01-01T16:52:41, Epoch: 5, Batch: 150, Training Loss: 0.13951102793216705, LR: 0.1
Time, 2019-01-01T16:52:42, Epoch: 5, Batch: 160, Training Loss: 0.09705773666501046, LR: 0.1
Time, 2019-01-01T16:52:42, Epoch: 5, Batch: 170, Training Loss: 0.1528610274195671, LR: 0.1
Time, 2019-01-01T16:52:43, Epoch: 5, Batch: 180, Training Loss: 0.1407641850411892, LR: 0.1
Time, 2019-01-01T16:52:44, Epoch: 5, Batch: 190, Training Loss: 0.19564143642783166, LR: 0.1
Time, 2019-01-01T16:52:45, Epoch: 5, Batch: 200, Training Loss: 0.17895738705992698, LR: 0.1
Time, 2019-01-01T16:52:46, Epoch: 5, Batch: 210, Training Loss: 0.1477114237844944, LR: 0.1
Time, 2019-01-01T16:52:46, Epoch: 5, Batch: 220, Training Loss: 0.14746445417404175, LR: 0.1
Time, 2019-01-01T16:52:47, Epoch: 5, Batch: 230, Training Loss: 0.13264547511935235, LR: 0.1
Time, 2019-01-01T16:52:48, Epoch: 5, Batch: 240, Training Loss: 0.1804704524576664, LR: 0.1
Time, 2019-01-01T16:52:49, Epoch: 5, Batch: 250, Training Loss: 0.1711145482957363, LR: 0.1
Time, 2019-01-01T16:52:50, Epoch: 5, Batch: 260, Training Loss: 0.12107729464769364, LR: 0.1
Time, 2019-01-01T16:52:51, Epoch: 5, Batch: 270, Training Loss: 0.12310097292065621, LR: 0.1
Time, 2019-01-01T16:52:51, Epoch: 5, Batch: 280, Training Loss: 0.12182750552892685, LR: 0.1
Time, 2019-01-01T16:52:52, Epoch: 5, Batch: 290, Training Loss: 0.1342889964580536, LR: 0.1
Time, 2019-01-01T16:52:53, Epoch: 5, Batch: 300, Training Loss: 0.12877196222543716, LR: 0.1
Time, 2019-01-01T16:52:54, Epoch: 5, Batch: 310, Training Loss: 0.15362375266849995, LR: 0.1
Time, 2019-01-01T16:52:55, Epoch: 5, Batch: 320, Training Loss: 0.15890525579452514, LR: 0.1
Time, 2019-01-01T16:52:56, Epoch: 5, Batch: 330, Training Loss: 0.14709070920944214, LR: 0.1
Time, 2019-01-01T16:52:56, Epoch: 5, Batch: 340, Training Loss: 0.11179743185639382, LR: 0.1
Time, 2019-01-01T16:52:57, Epoch: 5, Batch: 350, Training Loss: 0.15754074826836587, LR: 0.1
Time, 2019-01-01T16:52:58, Epoch: 5, Batch: 360, Training Loss: 0.1657208301126957, LR: 0.1
Time, 2019-01-01T16:52:59, Epoch: 5, Batch: 370, Training Loss: 0.14877144917845725, LR: 0.1
Time, 2019-01-01T16:53:00, Epoch: 5, Batch: 380, Training Loss: 0.16552953496575357, LR: 0.1
Time, 2019-01-01T16:53:01, Epoch: 5, Batch: 390, Training Loss: 0.14247289001941682, LR: 0.1
Time, 2019-01-01T16:53:01, Epoch: 5, Batch: 400, Training Loss: 0.13866958394646645, LR: 0.1
Time, 2019-01-01T16:53:02, Epoch: 5, Batch: 410, Training Loss: 0.17532508820295334, LR: 0.1
Time, 2019-01-01T16:53:03, Epoch: 5, Batch: 420, Training Loss: 0.13826703801751136, LR: 0.1
Time, 2019-01-01T16:53:04, Epoch: 5, Batch: 430, Training Loss: 0.13124013915657998, LR: 0.1
Time, 2019-01-01T16:53:05, Epoch: 5, Batch: 440, Training Loss: 0.1453738771378994, LR: 0.1
Time, 2019-01-01T16:53:05, Epoch: 5, Batch: 450, Training Loss: 0.10897560454905034, LR: 0.1
Time, 2019-01-01T16:53:06, Epoch: 5, Batch: 460, Training Loss: 0.1381081908941269, LR: 0.1
Time, 2019-01-01T16:53:07, Epoch: 5, Batch: 470, Training Loss: 0.1401945374906063, LR: 0.1
Time, 2019-01-01T16:53:08, Epoch: 5, Batch: 480, Training Loss: 0.0973447546362877, LR: 0.1
Time, 2019-01-01T16:53:09, Epoch: 5, Batch: 490, Training Loss: 0.1425440117716789, LR: 0.1
Time, 2019-01-01T16:53:10, Epoch: 5, Batch: 500, Training Loss: 0.14826721996068953, LR: 0.1
Time, 2019-01-01T16:53:10, Epoch: 5, Batch: 510, Training Loss: 0.08421333804726601, LR: 0.1
Time, 2019-01-01T16:53:11, Epoch: 5, Batch: 520, Training Loss: 0.15692529156804086, LR: 0.1
Time, 2019-01-01T16:53:12, Epoch: 5, Batch: 530, Training Loss: 0.1292466014623642, LR: 0.1
Time, 2019-01-01T16:53:13, Epoch: 5, Batch: 540, Training Loss: 0.1466530442237854, LR: 0.1
Time, 2019-01-01T16:53:14, Epoch: 5, Batch: 550, Training Loss: 0.1482055403292179, LR: 0.1
Time, 2019-01-01T16:53:15, Epoch: 5, Batch: 560, Training Loss: 0.13750008270144462, LR: 0.1
Time, 2019-01-01T16:53:16, Epoch: 5, Batch: 570, Training Loss: 0.145969083532691, LR: 0.1
Time, 2019-01-01T16:53:16, Epoch: 5, Batch: 580, Training Loss: 0.1318665288388729, LR: 0.1
Time, 2019-01-01T16:53:17, Epoch: 5, Batch: 590, Training Loss: 0.08910584822297096, LR: 0.1
Time, 2019-01-01T16:53:18, Epoch: 5, Batch: 600, Training Loss: 0.12028443589806556, LR: 0.1
Time, 2019-01-01T16:53:19, Epoch: 5, Batch: 610, Training Loss: 0.1368451587855816, LR: 0.1
Time, 2019-01-01T16:53:20, Epoch: 5, Batch: 620, Training Loss: 0.1400579709559679, LR: 0.1
Time, 2019-01-01T16:53:21, Epoch: 5, Batch: 630, Training Loss: 0.09194612056016922, LR: 0.1
Time, 2019-01-01T16:53:21, Epoch: 5, Batch: 640, Training Loss: 0.12822197750210762, LR: 0.1
Time, 2019-01-01T16:53:22, Epoch: 5, Batch: 650, Training Loss: 0.11220639161765575, LR: 0.1
Time, 2019-01-01T16:53:23, Epoch: 5, Batch: 660, Training Loss: 0.09797330275177955, LR: 0.1
Time, 2019-01-01T16:53:24, Epoch: 5, Batch: 670, Training Loss: 0.1044793326407671, LR: 0.1
Time, 2019-01-01T16:53:25, Epoch: 5, Batch: 680, Training Loss: 0.12289584316313266, LR: 0.1
Time, 2019-01-01T16:53:26, Epoch: 5, Batch: 690, Training Loss: 0.1360230751335621, LR: 0.1
Time, 2019-01-01T16:53:27, Epoch: 5, Batch: 700, Training Loss: 0.11092314682900906, LR: 0.1
Time, 2019-01-01T16:53:27, Epoch: 5, Batch: 710, Training Loss: 0.10718391388654709, LR: 0.1
Time, 2019-01-01T16:53:28, Epoch: 5, Batch: 720, Training Loss: 0.11970235407352448, LR: 0.1
Time, 2019-01-01T16:53:29, Epoch: 5, Batch: 730, Training Loss: 0.13853597566485404, LR: 0.1
Time, 2019-01-01T16:53:30, Epoch: 5, Batch: 740, Training Loss: 0.13976473808288575, LR: 0.1
Time, 2019-01-01T16:53:31, Epoch: 5, Batch: 750, Training Loss: 0.11556797809898853, LR: 0.1
Time, 2019-01-01T16:53:32, Epoch: 5, Batch: 760, Training Loss: 0.10449182912707329, LR: 0.1
Time, 2019-01-01T16:53:33, Epoch: 5, Batch: 770, Training Loss: 0.11467943191528321, LR: 0.1
Time, 2019-01-01T16:53:33, Epoch: 5, Batch: 780, Training Loss: 0.09721394181251526, LR: 0.1
Time, 2019-01-01T16:53:34, Epoch: 5, Batch: 790, Training Loss: 0.12828412167727948, LR: 0.1
Time, 2019-01-01T16:53:35, Epoch: 5, Batch: 800, Training Loss: 0.10820739977061748, LR: 0.1
Time, 2019-01-01T16:53:36, Epoch: 5, Batch: 810, Training Loss: 0.13833087012171746, LR: 0.1
Time, 2019-01-01T16:53:37, Epoch: 5, Batch: 820, Training Loss: 0.10038941577076912, LR: 0.1
Time, 2019-01-01T16:53:38, Epoch: 5, Batch: 830, Training Loss: 0.13292995430529117, LR: 0.1
Time, 2019-01-01T16:53:39, Epoch: 5, Batch: 840, Training Loss: 0.11256927624344826, LR: 0.1
Time, 2019-01-01T16:53:40, Epoch: 5, Batch: 850, Training Loss: 0.10367680117487907, LR: 0.1
Time, 2019-01-01T16:53:41, Epoch: 5, Batch: 860, Training Loss: 0.12633880898356437, LR: 0.1
Time, 2019-01-01T16:53:41, Epoch: 5, Batch: 870, Training Loss: 0.16981073543429376, LR: 0.1
Time, 2019-01-01T16:53:42, Epoch: 5, Batch: 880, Training Loss: 0.10008296370506287, LR: 0.1
Time, 2019-01-01T16:53:43, Epoch: 5, Batch: 890, Training Loss: 0.09947389140725135, LR: 0.1
Time, 2019-01-01T16:53:44, Epoch: 5, Batch: 900, Training Loss: 0.16627727560698985, LR: 0.1
Time, 2019-01-01T16:53:45, Epoch: 5, Batch: 910, Training Loss: 0.11696415096521377, LR: 0.1
Time, 2019-01-01T16:53:46, Epoch: 5, Batch: 920, Training Loss: 0.10618202276527881, LR: 0.1
Time, 2019-01-01T16:53:47, Epoch: 5, Batch: 930, Training Loss: 0.13864952102303504, LR: 0.1
Epoch: 5, Validation Top 1 acc: 96.74173736572266
Epoch: 5, Validation Top 5 acc: 99.9333724975586
Epoch: 5, Validation Set Loss: 0.10654330998659134
Start training epoch 6
Time, 2019-01-01T16:54:22, Epoch: 6, Batch: 10, Training Loss: 0.10392566844820976, LR: 0.1
Time, 2019-01-01T16:54:23, Epoch: 6, Batch: 20, Training Loss: 0.13577168732881545, LR: 0.1
Time, 2019-01-01T16:54:23, Epoch: 6, Batch: 30, Training Loss: 0.1485052891075611, LR: 0.1
Time, 2019-01-01T16:54:24, Epoch: 6, Batch: 40, Training Loss: 0.11372515670955181, LR: 0.1
Time, 2019-01-01T16:54:25, Epoch: 6, Batch: 50, Training Loss: 0.1045625627040863, LR: 0.1
Time, 2019-01-01T16:54:26, Epoch: 6, Batch: 60, Training Loss: 0.10585464499890804, LR: 0.1
Time, 2019-01-01T16:54:27, Epoch: 6, Batch: 70, Training Loss: 0.1344603881239891, LR: 0.1
Time, 2019-01-01T16:54:28, Epoch: 6, Batch: 80, Training Loss: 0.15312357097864152, LR: 0.1
Time, 2019-01-01T16:54:29, Epoch: 6, Batch: 90, Training Loss: 0.1843389891088009, LR: 0.1
Time, 2019-01-01T16:54:29, Epoch: 6, Batch: 100, Training Loss: 0.12992132008075713, LR: 0.1
Time, 2019-01-01T16:54:30, Epoch: 6, Batch: 110, Training Loss: 0.13528383374214173, LR: 0.1
Time, 2019-01-01T16:54:31, Epoch: 6, Batch: 120, Training Loss: 0.13929276540875435, LR: 0.1
Time, 2019-01-01T16:54:32, Epoch: 6, Batch: 130, Training Loss: 0.14854044169187547, LR: 0.1
Time, 2019-01-01T16:54:33, Epoch: 6, Batch: 140, Training Loss: 0.1110928513109684, LR: 0.1
Time, 2019-01-01T16:54:34, Epoch: 6, Batch: 150, Training Loss: 0.1008661575615406, LR: 0.1
Time, 2019-01-01T16:54:35, Epoch: 6, Batch: 160, Training Loss: 0.10204889737069607, LR: 0.1
Time, 2019-01-01T16:54:35, Epoch: 6, Batch: 170, Training Loss: 0.164935652166605, LR: 0.1
Time, 2019-01-01T16:54:36, Epoch: 6, Batch: 180, Training Loss: 0.1333149064332247, LR: 0.1
Time, 2019-01-01T16:54:37, Epoch: 6, Batch: 190, Training Loss: 0.15596138015389444, LR: 0.1
Time, 2019-01-01T16:54:38, Epoch: 6, Batch: 200, Training Loss: 0.17795601300895214, LR: 0.1
Time, 2019-01-01T16:54:38, Epoch: 6, Batch: 210, Training Loss: 0.15714767947793007, LR: 0.1
Time, 2019-01-01T16:54:39, Epoch: 6, Batch: 220, Training Loss: 0.16542629897594452, LR: 0.1
Time, 2019-01-01T16:54:40, Epoch: 6, Batch: 230, Training Loss: 0.13997348323464393, LR: 0.1
Time, 2019-01-01T16:54:41, Epoch: 6, Batch: 240, Training Loss: 0.14790578186511993, LR: 0.1
Time, 2019-01-01T16:54:42, Epoch: 6, Batch: 250, Training Loss: 0.12909318655729293, LR: 0.1
Time, 2019-01-01T16:54:42, Epoch: 6, Batch: 260, Training Loss: 0.12236282378435134, LR: 0.1
Time, 2019-01-01T16:54:43, Epoch: 6, Batch: 270, Training Loss: 0.16379309296607972, LR: 0.1
Time, 2019-01-01T16:54:44, Epoch: 6, Batch: 280, Training Loss: 0.12838634997606277, LR: 0.1
Time, 2019-01-01T16:54:45, Epoch: 6, Batch: 290, Training Loss: 0.11428550556302071, LR: 0.1
Time, 2019-01-01T16:54:46, Epoch: 6, Batch: 300, Training Loss: 0.19131785184144973, LR: 0.1
Time, 2019-01-01T16:54:47, Epoch: 6, Batch: 310, Training Loss: 0.26879121363162994, LR: 0.1
Time, 2019-01-01T16:54:47, Epoch: 6, Batch: 320, Training Loss: 0.30924793481826784, LR: 0.1
Time, 2019-01-01T16:54:48, Epoch: 6, Batch: 330, Training Loss: 0.3618312641978264, LR: 0.1
Time, 2019-01-01T16:54:49, Epoch: 6, Batch: 340, Training Loss: 0.30094764530658724, LR: 0.1
Time, 2019-01-01T16:54:50, Epoch: 6, Batch: 350, Training Loss: 0.184236066788435, LR: 0.1
Time, 2019-01-01T16:54:50, Epoch: 6, Batch: 360, Training Loss: 0.1803140588104725, LR: 0.1
Time, 2019-01-01T16:54:51, Epoch: 6, Batch: 370, Training Loss: 0.1829467423260212, LR: 0.1
Time, 2019-01-01T16:54:52, Epoch: 6, Batch: 380, Training Loss: 0.2236762225627899, LR: 0.1
Time, 2019-01-01T16:54:53, Epoch: 6, Batch: 390, Training Loss: 0.20757055282592773, LR: 0.1
Time, 2019-01-01T16:54:54, Epoch: 6, Batch: 400, Training Loss: 0.16642272993922233, LR: 0.1
Time, 2019-01-01T16:54:55, Epoch: 6, Batch: 410, Training Loss: 0.22302139475941657, LR: 0.1
Time, 2019-01-01T16:54:56, Epoch: 6, Batch: 420, Training Loss: 0.22436313405632974, LR: 0.1
Time, 2019-01-01T16:54:56, Epoch: 6, Batch: 430, Training Loss: 0.19657748937606812, LR: 0.1
Time, 2019-01-01T16:54:57, Epoch: 6, Batch: 440, Training Loss: 0.24454860985279084, LR: 0.1
Time, 2019-01-01T16:54:58, Epoch: 6, Batch: 450, Training Loss: 0.24454436004161834, LR: 0.1
Time, 2019-01-01T16:54:59, Epoch: 6, Batch: 460, Training Loss: 0.20419458150863648, LR: 0.1
Time, 2019-01-01T16:55:00, Epoch: 6, Batch: 470, Training Loss: 0.17070566713809968, LR: 0.1
Time, 2019-01-01T16:55:01, Epoch: 6, Batch: 480, Training Loss: 0.19828357100486754, LR: 0.1
Time, 2019-01-01T16:55:01, Epoch: 6, Batch: 490, Training Loss: 0.22620193883776665, LR: 0.1
Time, 2019-01-01T16:55:02, Epoch: 6, Batch: 500, Training Loss: 0.15658776015043258, LR: 0.1
Time, 2019-01-01T16:55:03, Epoch: 6, Batch: 510, Training Loss: 0.20665232688188553, LR: 0.1
Time, 2019-01-01T16:55:04, Epoch: 6, Batch: 520, Training Loss: 0.2325294718146324, LR: 0.1
Time, 2019-01-01T16:55:05, Epoch: 6, Batch: 530, Training Loss: 0.22438110336661338, LR: 0.1
Time, 2019-01-01T16:55:06, Epoch: 6, Batch: 540, Training Loss: 0.2002939149737358, LR: 0.1
Time, 2019-01-01T16:55:07, Epoch: 6, Batch: 550, Training Loss: 0.15186618492007256, LR: 0.1
Time, 2019-01-01T16:55:07, Epoch: 6, Batch: 560, Training Loss: 0.12861944288015364, LR: 0.1
Time, 2019-01-01T16:55:08, Epoch: 6, Batch: 570, Training Loss: 0.14899181947112083, LR: 0.1
Time, 2019-01-01T16:55:09, Epoch: 6, Batch: 580, Training Loss: 0.12366070747375488, LR: 0.1
Time, 2019-01-01T16:55:10, Epoch: 6, Batch: 590, Training Loss: 0.13185584209859372, LR: 0.1
Time, 2019-01-01T16:55:11, Epoch: 6, Batch: 600, Training Loss: 0.15005183592438698, LR: 0.1
Time, 2019-01-01T16:55:12, Epoch: 6, Batch: 610, Training Loss: 0.11702663153409958, LR: 0.1
Time, 2019-01-01T16:55:13, Epoch: 6, Batch: 620, Training Loss: 0.13630904108285904, LR: 0.1
Time, 2019-01-01T16:55:13, Epoch: 6, Batch: 630, Training Loss: 0.18008398860692978, LR: 0.1
Time, 2019-01-01T16:55:14, Epoch: 6, Batch: 640, Training Loss: 0.12998193129897118, LR: 0.1
Time, 2019-01-01T16:55:15, Epoch: 6, Batch: 650, Training Loss: 0.11501251757144929, LR: 0.1
Time, 2019-01-01T16:55:16, Epoch: 6, Batch: 660, Training Loss: 0.09963203445076943, LR: 0.1
Time, 2019-01-01T16:55:17, Epoch: 6, Batch: 670, Training Loss: 0.11173120439052582, LR: 0.1
Time, 2019-01-01T16:55:18, Epoch: 6, Batch: 680, Training Loss: 0.15427921786904336, LR: 0.1
Time, 2019-01-01T16:55:18, Epoch: 6, Batch: 690, Training Loss: 0.13632091507315636, LR: 0.1
Time, 2019-01-01T16:55:19, Epoch: 6, Batch: 700, Training Loss: 0.12783256769180298, LR: 0.1
Time, 2019-01-01T16:55:20, Epoch: 6, Batch: 710, Training Loss: 0.12895735651254653, LR: 0.1
Time, 2019-01-01T16:55:21, Epoch: 6, Batch: 720, Training Loss: 0.1240353349596262, LR: 0.1
Time, 2019-01-01T16:55:22, Epoch: 6, Batch: 730, Training Loss: 0.10163486450910568, LR: 0.1
Time, 2019-01-01T16:55:23, Epoch: 6, Batch: 740, Training Loss: 0.10208879262208939, LR: 0.1
Time, 2019-01-01T16:55:23, Epoch: 6, Batch: 750, Training Loss: 0.10027114935219288, LR: 0.1
Time, 2019-01-01T16:55:24, Epoch: 6, Batch: 760, Training Loss: 0.12542784586548805, LR: 0.1
Time, 2019-01-01T16:55:25, Epoch: 6, Batch: 770, Training Loss: 0.11894347220659256, LR: 0.1
Time, 2019-01-01T16:55:26, Epoch: 6, Batch: 780, Training Loss: 0.10390234962105752, LR: 0.1
Time, 2019-01-01T16:55:27, Epoch: 6, Batch: 790, Training Loss: 0.0921556793153286, LR: 0.1
Time, 2019-01-01T16:55:27, Epoch: 6, Batch: 800, Training Loss: 0.1477811984717846, LR: 0.1
Time, 2019-01-01T16:55:28, Epoch: 6, Batch: 810, Training Loss: 0.15386359840631486, LR: 0.1
Time, 2019-01-01T16:55:29, Epoch: 6, Batch: 820, Training Loss: 0.13582685738801956, LR: 0.1
Time, 2019-01-01T16:55:30, Epoch: 6, Batch: 830, Training Loss: 0.10535960644483566, LR: 0.1
Time, 2019-01-01T16:55:31, Epoch: 6, Batch: 840, Training Loss: 0.10743094012141227, LR: 0.1
Time, 2019-01-01T16:55:32, Epoch: 6, Batch: 850, Training Loss: 0.07342888973653316, LR: 0.1
Time, 2019-01-01T16:55:32, Epoch: 6, Batch: 860, Training Loss: 0.1353646218776703, LR: 0.1
Time, 2019-01-01T16:55:33, Epoch: 6, Batch: 870, Training Loss: 0.11913976669311524, LR: 0.1
Time, 2019-01-01T16:55:34, Epoch: 6, Batch: 880, Training Loss: 0.13807084523141383, LR: 0.1
Time, 2019-01-01T16:55:35, Epoch: 6, Batch: 890, Training Loss: 0.1046733520925045, LR: 0.1
Time, 2019-01-01T16:55:36, Epoch: 6, Batch: 900, Training Loss: 0.12014978900551795, LR: 0.1
Time, 2019-01-01T16:55:37, Epoch: 6, Batch: 910, Training Loss: 0.15412590950727462, LR: 0.1
Time, 2019-01-01T16:55:37, Epoch: 6, Batch: 920, Training Loss: 0.15923042297363282, LR: 0.1
Time, 2019-01-01T16:55:38, Epoch: 6, Batch: 930, Training Loss: 0.10437693744897843, LR: 0.1
Epoch: 6, Validation Top 1 acc: 96.74340057373047
Epoch: 6, Validation Top 5 acc: 99.9333724975586
Epoch: 6, Validation Set Loss: 0.10626622289419174
Start training epoch 7
Time, 2019-01-01T16:56:11, Epoch: 7, Batch: 10, Training Loss: 0.10001880303025246, LR: 0.1
Time, 2019-01-01T16:56:12, Epoch: 7, Batch: 20, Training Loss: 0.09476430416107177, LR: 0.1
Time, 2019-01-01T16:56:12, Epoch: 7, Batch: 30, Training Loss: 0.15477578118443489, LR: 0.1
Time, 2019-01-01T16:56:13, Epoch: 7, Batch: 40, Training Loss: 0.12930210009217263, LR: 0.1
Time, 2019-01-01T16:56:14, Epoch: 7, Batch: 50, Training Loss: 0.12368137240409852, LR: 0.1
Time, 2019-01-01T16:56:15, Epoch: 7, Batch: 60, Training Loss: 0.13680294156074524, LR: 0.1
Time, 2019-01-01T16:56:16, Epoch: 7, Batch: 70, Training Loss: 0.10466780699789524, LR: 0.1
Time, 2019-01-01T16:56:17, Epoch: 7, Batch: 80, Training Loss: 0.12203478217124938, LR: 0.1
Time, 2019-01-01T16:56:17, Epoch: 7, Batch: 90, Training Loss: 0.1305108606815338, LR: 0.1
Time, 2019-01-01T16:56:18, Epoch: 7, Batch: 100, Training Loss: 0.14952378273010253, LR: 0.1
Time, 2019-01-01T16:56:19, Epoch: 7, Batch: 110, Training Loss: 0.09251056984066963, LR: 0.1
Time, 2019-01-01T16:56:20, Epoch: 7, Batch: 120, Training Loss: 0.12765566259622574, LR: 0.1
Time, 2019-01-01T16:56:21, Epoch: 7, Batch: 130, Training Loss: 0.15682838670909405, LR: 0.1
Time, 2019-01-01T16:56:22, Epoch: 7, Batch: 140, Training Loss: 0.14984867870807647, LR: 0.1
Time, 2019-01-01T16:56:23, Epoch: 7, Batch: 150, Training Loss: 0.14407784454524517, LR: 0.1
Time, 2019-01-01T16:56:23, Epoch: 7, Batch: 160, Training Loss: 0.13516133800148963, LR: 0.1
Time, 2019-01-01T16:56:24, Epoch: 7, Batch: 170, Training Loss: 0.15645211562514305, LR: 0.1
Time, 2019-01-01T16:56:25, Epoch: 7, Batch: 180, Training Loss: 0.15510187074542045, LR: 0.1
Time, 2019-01-01T16:56:26, Epoch: 7, Batch: 190, Training Loss: 0.13870289996266366, LR: 0.1
Time, 2019-01-01T16:56:27, Epoch: 7, Batch: 200, Training Loss: 0.11120043620467186, LR: 0.1
Time, 2019-01-01T16:56:28, Epoch: 7, Batch: 210, Training Loss: 0.1294216476380825, LR: 0.1
Time, 2019-01-01T16:56:29, Epoch: 7, Batch: 220, Training Loss: 0.12626967579126358, LR: 0.1
Time, 2019-01-01T16:56:29, Epoch: 7, Batch: 230, Training Loss: 0.11956305727362633, LR: 0.1
Time, 2019-01-01T16:56:30, Epoch: 7, Batch: 240, Training Loss: 0.13599011898040772, LR: 0.1
Time, 2019-01-01T16:56:31, Epoch: 7, Batch: 250, Training Loss: 0.10453800410032273, LR: 0.1
Time, 2019-01-01T16:56:32, Epoch: 7, Batch: 260, Training Loss: 0.20822488144040108, LR: 0.1
Time, 2019-01-01T16:56:33, Epoch: 7, Batch: 270, Training Loss: 0.15851412564516068, LR: 0.1
Time, 2019-01-01T16:56:34, Epoch: 7, Batch: 280, Training Loss: 0.18293620198965072, LR: 0.1
Time, 2019-01-01T16:56:34, Epoch: 7, Batch: 290, Training Loss: 0.14426222443580627, LR: 0.1
Time, 2019-01-01T16:56:35, Epoch: 7, Batch: 300, Training Loss: 0.10409419313073158, LR: 0.1
Time, 2019-01-01T16:56:36, Epoch: 7, Batch: 310, Training Loss: 0.13986381590366365, LR: 0.1
Time, 2019-01-01T16:56:37, Epoch: 7, Batch: 320, Training Loss: 0.17652913853526114, LR: 0.1
Time, 2019-01-01T16:56:38, Epoch: 7, Batch: 330, Training Loss: 0.08298306129872798, LR: 0.1
Time, 2019-01-01T16:56:39, Epoch: 7, Batch: 340, Training Loss: 0.16266416907310485, LR: 0.1
Time, 2019-01-01T16:56:39, Epoch: 7, Batch: 350, Training Loss: 0.14605515524744989, LR: 0.1
Time, 2019-01-01T16:56:40, Epoch: 7, Batch: 360, Training Loss: 0.11591514199972153, LR: 0.1
Time, 2019-01-01T16:56:41, Epoch: 7, Batch: 370, Training Loss: 0.09662817642092705, LR: 0.1
Time, 2019-01-01T16:56:42, Epoch: 7, Batch: 380, Training Loss: 0.0948324091732502, LR: 0.1
Time, 2019-01-01T16:56:43, Epoch: 7, Batch: 390, Training Loss: 0.1425565578043461, LR: 0.1
Time, 2019-01-01T16:56:44, Epoch: 7, Batch: 400, Training Loss: 0.15510174185037612, LR: 0.1
Time, 2019-01-01T16:56:45, Epoch: 7, Batch: 410, Training Loss: 0.1244666650891304, LR: 0.1
Time, 2019-01-01T16:56:45, Epoch: 7, Batch: 420, Training Loss: 0.1525905691087246, LR: 0.1
Time, 2019-01-01T16:56:46, Epoch: 7, Batch: 430, Training Loss: 0.14579291567206382, LR: 0.1
Time, 2019-01-01T16:56:47, Epoch: 7, Batch: 440, Training Loss: 0.12041786909103394, LR: 0.1
Time, 2019-01-01T16:56:48, Epoch: 7, Batch: 450, Training Loss: 0.08920679986476898, LR: 0.1
Time, 2019-01-01T16:56:49, Epoch: 7, Batch: 460, Training Loss: 0.12518001943826676, LR: 0.1
Time, 2019-01-01T16:56:50, Epoch: 7, Batch: 470, Training Loss: 0.08712844997644424, LR: 0.1
Time, 2019-01-01T16:56:51, Epoch: 7, Batch: 480, Training Loss: 0.13535693287849426, LR: 0.1
Time, 2019-01-01T16:56:51, Epoch: 7, Batch: 490, Training Loss: 0.12521228492259978, LR: 0.1
Time, 2019-01-01T16:56:52, Epoch: 7, Batch: 500, Training Loss: 0.08901468887925149, LR: 0.1
Time, 2019-01-01T16:56:53, Epoch: 7, Batch: 510, Training Loss: 0.10450556278228759, LR: 0.1
Time, 2019-01-01T16:56:54, Epoch: 7, Batch: 520, Training Loss: 0.1118876650929451, LR: 0.1
Time, 2019-01-01T16:56:55, Epoch: 7, Batch: 530, Training Loss: 0.1059051662683487, LR: 0.1
Time, 2019-01-01T16:56:56, Epoch: 7, Batch: 540, Training Loss: 0.1014240574091673, LR: 0.1
Time, 2019-01-01T16:56:57, Epoch: 7, Batch: 550, Training Loss: 0.14293193370103835, LR: 0.1
Time, 2019-01-01T16:56:58, Epoch: 7, Batch: 560, Training Loss: 0.07524247393012047, LR: 0.1
Time, 2019-01-01T16:56:58, Epoch: 7, Batch: 570, Training Loss: 0.11060866937041283, LR: 0.1
Time, 2019-01-01T16:56:59, Epoch: 7, Batch: 580, Training Loss: 0.07496469840407372, LR: 0.1
Time, 2019-01-01T16:57:00, Epoch: 7, Batch: 590, Training Loss: 0.1182062342762947, LR: 0.1
Time, 2019-01-01T16:57:01, Epoch: 7, Batch: 600, Training Loss: 0.10694830119609833, LR: 0.1
Time, 2019-01-01T16:57:02, Epoch: 7, Batch: 610, Training Loss: 0.0900334820151329, LR: 0.1
Time, 2019-01-01T16:57:03, Epoch: 7, Batch: 620, Training Loss: 0.10147280171513558, LR: 0.1
Time, 2019-01-01T16:57:04, Epoch: 7, Batch: 630, Training Loss: 0.1369634062051773, LR: 0.1
Time, 2019-01-01T16:57:04, Epoch: 7, Batch: 640, Training Loss: 0.08568028435111046, LR: 0.1
Time, 2019-01-01T16:57:05, Epoch: 7, Batch: 650, Training Loss: 0.0956938374787569, LR: 0.1
Time, 2019-01-01T16:57:06, Epoch: 7, Batch: 660, Training Loss: 0.11894069984555244, LR: 0.1
Time, 2019-01-01T16:57:07, Epoch: 7, Batch: 670, Training Loss: 0.12215735763311386, LR: 0.1
Time, 2019-01-01T16:57:08, Epoch: 7, Batch: 680, Training Loss: 0.10562844574451447, LR: 0.1
Time, 2019-01-01T16:57:09, Epoch: 7, Batch: 690, Training Loss: 0.11218461282551288, LR: 0.1
Time, 2019-01-01T16:57:10, Epoch: 7, Batch: 700, Training Loss: 0.1256743509322405, LR: 0.1
Time, 2019-01-01T16:57:10, Epoch: 7, Batch: 710, Training Loss: 0.13653870522975922, LR: 0.1
Time, 2019-01-01T16:57:11, Epoch: 7, Batch: 720, Training Loss: 0.13080914840102195, LR: 0.1
Time, 2019-01-01T16:57:12, Epoch: 7, Batch: 730, Training Loss: 0.13570297360420228, LR: 0.1
Time, 2019-01-01T16:57:13, Epoch: 7, Batch: 740, Training Loss: 0.14112480506300926, LR: 0.1
Time, 2019-01-01T16:57:14, Epoch: 7, Batch: 750, Training Loss: 0.10853158608078957, LR: 0.1
Time, 2019-01-01T16:57:15, Epoch: 7, Batch: 760, Training Loss: 0.11881383806467057, LR: 0.1
Time, 2019-01-01T16:57:15, Epoch: 7, Batch: 770, Training Loss: 0.17496275901794434, LR: 0.1
Time, 2019-01-01T16:57:16, Epoch: 7, Batch: 780, Training Loss: 0.1251403011381626, LR: 0.1
Time, 2019-01-01T16:57:17, Epoch: 7, Batch: 790, Training Loss: 0.12703224495053292, LR: 0.1
Time, 2019-01-01T16:57:18, Epoch: 7, Batch: 800, Training Loss: 0.104519584774971, LR: 0.1
Time, 2019-01-01T16:57:19, Epoch: 7, Batch: 810, Training Loss: 0.10200824588537216, LR: 0.1
Time, 2019-01-01T16:57:20, Epoch: 7, Batch: 820, Training Loss: 0.1171944186091423, LR: 0.1
Time, 2019-01-01T16:57:20, Epoch: 7, Batch: 830, Training Loss: 0.13121050968766212, LR: 0.1
Time, 2019-01-01T16:57:21, Epoch: 7, Batch: 840, Training Loss: 0.11169513091444969, LR: 0.1
Time, 2019-01-01T16:57:22, Epoch: 7, Batch: 850, Training Loss: 0.1129533238708973, LR: 0.1
Time, 2019-01-01T16:57:23, Epoch: 7, Batch: 860, Training Loss: 0.12057243697345257, LR: 0.1
Time, 2019-01-01T16:57:24, Epoch: 7, Batch: 870, Training Loss: 0.16152387708425522, LR: 0.1
Time, 2019-01-01T16:57:25, Epoch: 7, Batch: 880, Training Loss: 0.13332879468798636, LR: 0.1
Time, 2019-01-01T16:57:25, Epoch: 7, Batch: 890, Training Loss: 0.20809573158621789, LR: 0.1
Time, 2019-01-01T16:57:26, Epoch: 7, Batch: 900, Training Loss: 0.14711880385875703, LR: 0.1
Time, 2019-01-01T16:57:27, Epoch: 7, Batch: 910, Training Loss: 0.13816440030932425, LR: 0.1
Time, 2019-01-01T16:57:28, Epoch: 7, Batch: 920, Training Loss: 0.12624537721276283, LR: 0.1
Time, 2019-01-01T16:57:29, Epoch: 7, Batch: 930, Training Loss: 0.15877206921577453, LR: 0.1
Epoch: 7, Validation Top 1 acc: 96.2686538696289
Epoch: 7, Validation Top 5 acc: 99.92837524414062
Epoch: 7, Validation Set Loss: 0.12508438527584076
Start training epoch 8
Time, 2019-01-01T16:58:00, Epoch: 8, Batch: 10, Training Loss: 0.15794450491666795, LR: 0.1
Time, 2019-01-01T16:58:01, Epoch: 8, Batch: 20, Training Loss: 0.1729901149868965, LR: 0.1
Time, 2019-01-01T16:58:01, Epoch: 8, Batch: 30, Training Loss: 0.1850902706384659, LR: 0.1
Time, 2019-01-01T16:58:02, Epoch: 8, Batch: 40, Training Loss: 0.13481592014431953, LR: 0.1
Time, 2019-01-01T16:58:03, Epoch: 8, Batch: 50, Training Loss: 0.15549098886549473, LR: 0.1
Time, 2019-01-01T16:58:04, Epoch: 8, Batch: 60, Training Loss: 0.18129546716809272, LR: 0.1
Time, 2019-01-01T16:58:05, Epoch: 8, Batch: 70, Training Loss: 0.20591969639062882, LR: 0.1
Time, 2019-01-01T16:58:05, Epoch: 8, Batch: 80, Training Loss: 0.17436527088284492, LR: 0.1
Time, 2019-01-01T16:58:06, Epoch: 8, Batch: 90, Training Loss: 0.16944080144166945, LR: 0.1
Time, 2019-01-01T16:58:07, Epoch: 8, Batch: 100, Training Loss: 0.12497042417526245, LR: 0.1
Time, 2019-01-01T16:58:08, Epoch: 8, Batch: 110, Training Loss: 0.15333101227879525, LR: 0.1
Time, 2019-01-01T16:58:09, Epoch: 8, Batch: 120, Training Loss: 0.16696840822696685, LR: 0.1
Time, 2019-01-01T16:58:09, Epoch: 8, Batch: 130, Training Loss: 0.14863463044166564, LR: 0.1
Time, 2019-01-01T16:58:10, Epoch: 8, Batch: 140, Training Loss: 0.15390306562185288, LR: 0.1
Time, 2019-01-01T16:58:11, Epoch: 8, Batch: 150, Training Loss: 0.16358914375305175, LR: 0.1
Time, 2019-01-01T16:58:12, Epoch: 8, Batch: 160, Training Loss: 0.17682893872261046, LR: 0.1
Time, 2019-01-01T16:58:12, Epoch: 8, Batch: 170, Training Loss: 0.15858641304075718, LR: 0.1
Time, 2019-01-01T16:58:13, Epoch: 8, Batch: 180, Training Loss: 0.16924186125397683, LR: 0.1
Time, 2019-01-01T16:58:14, Epoch: 8, Batch: 190, Training Loss: 0.149903317540884, LR: 0.1
Time, 2019-01-01T16:58:15, Epoch: 8, Batch: 200, Training Loss: 0.1995752602815628, LR: 0.1
Time, 2019-01-01T16:58:15, Epoch: 8, Batch: 210, Training Loss: 0.21074846461415292, LR: 0.1
Time, 2019-01-01T16:58:16, Epoch: 8, Batch: 220, Training Loss: 0.15802735909819604, LR: 0.1
Time, 2019-01-01T16:58:17, Epoch: 8, Batch: 230, Training Loss: 0.2275325194001198, LR: 0.1
Time, 2019-01-01T16:58:18, Epoch: 8, Batch: 240, Training Loss: 0.1810681689530611, LR: 0.1
Time, 2019-01-01T16:58:19, Epoch: 8, Batch: 250, Training Loss: 0.25886655002832415, LR: 0.1
Time, 2019-01-01T16:58:19, Epoch: 8, Batch: 260, Training Loss: 0.24561928436160088, LR: 0.1
Time, 2019-01-01T16:58:20, Epoch: 8, Batch: 270, Training Loss: 0.24593817070126534, LR: 0.1
Time, 2019-01-01T16:58:21, Epoch: 8, Batch: 280, Training Loss: 0.18784447461366655, LR: 0.1
Time, 2019-01-01T16:58:22, Epoch: 8, Batch: 290, Training Loss: 0.22641290724277496, LR: 0.1
Time, 2019-01-01T16:58:23, Epoch: 8, Batch: 300, Training Loss: 0.17991067692637444, LR: 0.1
Time, 2019-01-01T16:58:23, Epoch: 8, Batch: 310, Training Loss: 0.16362915188074112, LR: 0.1
Time, 2019-01-01T16:58:24, Epoch: 8, Batch: 320, Training Loss: 0.14206668585538865, LR: 0.1
Time, 2019-01-01T16:58:25, Epoch: 8, Batch: 330, Training Loss: 0.14849568754434586, LR: 0.1
Time, 2019-01-01T16:58:26, Epoch: 8, Batch: 340, Training Loss: 0.12702704966068268, LR: 0.1
Time, 2019-01-01T16:58:27, Epoch: 8, Batch: 350, Training Loss: 0.22158344089984894, LR: 0.1
Time, 2019-01-01T16:58:27, Epoch: 8, Batch: 360, Training Loss: 0.1926970049738884, LR: 0.1
Time, 2019-01-01T16:58:28, Epoch: 8, Batch: 370, Training Loss: 0.18177743405103683, LR: 0.1
Time, 2019-01-01T16:58:29, Epoch: 8, Batch: 380, Training Loss: 0.1623331733047962, LR: 0.1
Time, 2019-01-01T16:58:30, Epoch: 8, Batch: 390, Training Loss: 0.14343216866254807, LR: 0.1
Time, 2019-01-01T16:58:31, Epoch: 8, Batch: 400, Training Loss: 0.15179574713110924, LR: 0.1
Time, 2019-01-01T16:58:32, Epoch: 8, Batch: 410, Training Loss: 0.20591286122798919, LR: 0.1
Time, 2019-01-01T16:58:32, Epoch: 8, Batch: 420, Training Loss: 0.21595654860138894, LR: 0.1
Time, 2019-01-01T16:58:33, Epoch: 8, Batch: 430, Training Loss: 0.187075437605381, LR: 0.1
Time, 2019-01-01T16:58:34, Epoch: 8, Batch: 440, Training Loss: 0.19337129965424538, LR: 0.1
Time, 2019-01-01T16:58:35, Epoch: 8, Batch: 450, Training Loss: 0.27818777710199355, LR: 0.1
Time, 2019-01-01T16:58:36, Epoch: 8, Batch: 460, Training Loss: 0.21879607141017915, LR: 0.1
Time, 2019-01-01T16:58:37, Epoch: 8, Batch: 470, Training Loss: 0.22449740916490554, LR: 0.1
Time, 2019-01-01T16:58:38, Epoch: 8, Batch: 480, Training Loss: 0.1719623848795891, LR: 0.1
Time, 2019-01-01T16:58:38, Epoch: 8, Batch: 490, Training Loss: 0.18522878885269164, LR: 0.1
Time, 2019-01-01T16:58:39, Epoch: 8, Batch: 500, Training Loss: 0.16363098323345185, LR: 0.1
Time, 2019-01-01T16:58:40, Epoch: 8, Batch: 510, Training Loss: 0.15456582233309746, LR: 0.1
Time, 2019-01-01T16:58:41, Epoch: 8, Batch: 520, Training Loss: 0.1209975928068161, LR: 0.1
Time, 2019-01-01T16:58:42, Epoch: 8, Batch: 530, Training Loss: 0.1418917328119278, LR: 0.1
Time, 2019-01-01T16:58:43, Epoch: 8, Batch: 540, Training Loss: 0.14080155678093434, LR: 0.1
Time, 2019-01-01T16:58:44, Epoch: 8, Batch: 550, Training Loss: 0.14571084976196289, LR: 0.1
Time, 2019-01-01T16:58:44, Epoch: 8, Batch: 560, Training Loss: 0.12963858619332314, LR: 0.1
Time, 2019-01-01T16:58:45, Epoch: 8, Batch: 570, Training Loss: 0.12129215598106384, LR: 0.1
Time, 2019-01-01T16:58:46, Epoch: 8, Batch: 580, Training Loss: 0.13389580883085728, LR: 0.1
Time, 2019-01-01T16:58:47, Epoch: 8, Batch: 590, Training Loss: 0.20306466817855834, LR: 0.1
Time, 2019-01-01T16:58:48, Epoch: 8, Batch: 600, Training Loss: 0.2046084150671959, LR: 0.1
Time, 2019-01-01T16:58:49, Epoch: 8, Batch: 610, Training Loss: 0.20469193831086158, LR: 0.1
Time, 2019-01-01T16:58:50, Epoch: 8, Batch: 620, Training Loss: 0.20423345267772675, LR: 0.1
Time, 2019-01-01T16:58:50, Epoch: 8, Batch: 630, Training Loss: 0.15672819316387177, LR: 0.1
Time, 2019-01-01T16:58:51, Epoch: 8, Batch: 640, Training Loss: 0.13846629448235034, LR: 0.1
Time, 2019-01-01T16:58:52, Epoch: 8, Batch: 650, Training Loss: 0.16754481866955756, LR: 0.1
Time, 2019-01-01T16:58:53, Epoch: 8, Batch: 660, Training Loss: 0.14676275178790094, LR: 0.1
Time, 2019-01-01T16:58:54, Epoch: 8, Batch: 670, Training Loss: 0.13011980652809144, LR: 0.1
Time, 2019-01-01T16:58:55, Epoch: 8, Batch: 680, Training Loss: 0.1297863930463791, LR: 0.1
Time, 2019-01-01T16:58:55, Epoch: 8, Batch: 690, Training Loss: 0.1425444159656763, LR: 0.1
Time, 2019-01-01T16:58:56, Epoch: 8, Batch: 700, Training Loss: 0.16033730730414392, LR: 0.1
Time, 2019-01-01T16:58:57, Epoch: 8, Batch: 710, Training Loss: 0.13938721641898155, LR: 0.1
Time, 2019-01-01T16:58:58, Epoch: 8, Batch: 720, Training Loss: 0.13089619465172292, LR: 0.1
Time, 2019-01-01T16:58:59, Epoch: 8, Batch: 730, Training Loss: 0.13340915739536285, LR: 0.1
Time, 2019-01-01T16:59:00, Epoch: 8, Batch: 740, Training Loss: 0.15045549124479293, LR: 0.1
Time, 2019-01-01T16:59:01, Epoch: 8, Batch: 750, Training Loss: 0.10405912920832634, LR: 0.1
Time, 2019-01-01T16:59:01, Epoch: 8, Batch: 760, Training Loss: 0.11338606029748917, LR: 0.1
Time, 2019-01-01T16:59:02, Epoch: 8, Batch: 770, Training Loss: 0.117920982837677, LR: 0.1
Time, 2019-01-01T16:59:03, Epoch: 8, Batch: 780, Training Loss: 0.12978661023080348, LR: 0.1
Time, 2019-01-01T16:59:04, Epoch: 8, Batch: 790, Training Loss: 0.12602345421910285, LR: 0.1
Time, 2019-01-01T16:59:05, Epoch: 8, Batch: 800, Training Loss: 0.1425742454826832, LR: 0.1
Time, 2019-01-01T16:59:06, Epoch: 8, Batch: 810, Training Loss: 0.19058429077267647, LR: 0.1
Time, 2019-01-01T16:59:06, Epoch: 8, Batch: 820, Training Loss: 0.1519392676651478, LR: 0.1
Time, 2019-01-01T16:59:07, Epoch: 8, Batch: 830, Training Loss: 0.09770533964037895, LR: 0.1
Time, 2019-01-01T16:59:08, Epoch: 8, Batch: 840, Training Loss: 0.09670802131295204, LR: 0.1
Time, 2019-01-01T16:59:09, Epoch: 8, Batch: 850, Training Loss: 0.12356684356927872, LR: 0.1
Time, 2019-01-01T16:59:10, Epoch: 8, Batch: 860, Training Loss: 0.09054263345897198, LR: 0.1
Time, 2019-01-01T16:59:11, Epoch: 8, Batch: 870, Training Loss: 0.1417039878666401, LR: 0.1
Time, 2019-01-01T16:59:11, Epoch: 8, Batch: 880, Training Loss: 0.1284366048872471, LR: 0.1
Time, 2019-01-01T16:59:12, Epoch: 8, Batch: 890, Training Loss: 0.1272203229367733, LR: 0.1
Time, 2019-01-01T16:59:13, Epoch: 8, Batch: 900, Training Loss: 0.1296737253665924, LR: 0.1
Time, 2019-01-01T16:59:14, Epoch: 8, Batch: 910, Training Loss: 0.11712780147790909, LR: 0.1
Time, 2019-01-01T16:59:15, Epoch: 8, Batch: 920, Training Loss: 0.09721747040748596, LR: 0.1
Time, 2019-01-01T16:59:16, Epoch: 8, Batch: 930, Training Loss: 0.06612730771303177, LR: 0.1
Epoch: 8, Validation Top 1 acc: 96.38359832763672
Epoch: 8, Validation Top 5 acc: 99.94003295898438
Epoch: 8, Validation Set Loss: 0.1118936762213707
Start training epoch 9
Time, 2019-01-01T16:59:45, Epoch: 9, Batch: 10, Training Loss: 0.14294632971286775, LR: 0.1
Time, 2019-01-01T16:59:46, Epoch: 9, Batch: 20, Training Loss: 0.14280943050980568, LR: 0.1
Time, 2019-01-01T16:59:47, Epoch: 9, Batch: 30, Training Loss: 0.1463607780635357, LR: 0.1
Time, 2019-01-01T16:59:48, Epoch: 9, Batch: 40, Training Loss: 0.11701797544956208, LR: 0.1
Time, 2019-01-01T16:59:48, Epoch: 9, Batch: 50, Training Loss: 0.10228823274374008, LR: 0.1
Time, 2019-01-01T16:59:49, Epoch: 9, Batch: 60, Training Loss: 0.10218948051333428, LR: 0.1
Time, 2019-01-01T16:59:50, Epoch: 9, Batch: 70, Training Loss: 0.1044289018958807, LR: 0.1
Time, 2019-01-01T16:59:51, Epoch: 9, Batch: 80, Training Loss: 0.09976591467857361, LR: 0.1
Time, 2019-01-01T16:59:52, Epoch: 9, Batch: 90, Training Loss: 0.1211792565882206, LR: 0.1
Time, 2019-01-01T16:59:53, Epoch: 9, Batch: 100, Training Loss: 0.09481155499815941, LR: 0.1
Time, 2019-01-01T16:59:53, Epoch: 9, Batch: 110, Training Loss: 0.1303316682577133, LR: 0.1
Time, 2019-01-01T16:59:54, Epoch: 9, Batch: 120, Training Loss: 0.1180614709854126, LR: 0.1
Time, 2019-01-01T16:59:55, Epoch: 9, Batch: 130, Training Loss: 0.11300008706748485, LR: 0.1
Time, 2019-01-01T16:59:56, Epoch: 9, Batch: 140, Training Loss: 0.08198351301252842, LR: 0.1
Time, 2019-01-01T16:59:57, Epoch: 9, Batch: 150, Training Loss: 0.11523645222187043, LR: 0.1
Time, 2019-01-01T16:59:57, Epoch: 9, Batch: 160, Training Loss: 0.20781974270939826, LR: 0.1
Time, 2019-01-01T16:59:58, Epoch: 9, Batch: 170, Training Loss: 0.14669239446520804, LR: 0.1
Time, 2019-01-01T16:59:59, Epoch: 9, Batch: 180, Training Loss: 0.1591848537325859, LR: 0.1
Time, 2019-01-01T17:00:00, Epoch: 9, Batch: 190, Training Loss: 0.13853088468313218, LR: 0.1
Time, 2019-01-01T17:00:00, Epoch: 9, Batch: 200, Training Loss: 0.09160970672965049, LR: 0.1
Time, 2019-01-01T17:00:01, Epoch: 9, Batch: 210, Training Loss: 0.08997743055224419, LR: 0.1
Time, 2019-01-01T17:00:02, Epoch: 9, Batch: 220, Training Loss: 0.1217399463057518, LR: 0.1
Time, 2019-01-01T17:00:03, Epoch: 9, Batch: 230, Training Loss: 0.11741574369370937, LR: 0.1
Time, 2019-01-01T17:00:03, Epoch: 9, Batch: 240, Training Loss: 0.09013100899755955, LR: 0.1
Time, 2019-01-01T17:00:04, Epoch: 9, Batch: 250, Training Loss: 0.1602082796394825, LR: 0.1
Time, 2019-01-01T17:00:05, Epoch: 9, Batch: 260, Training Loss: 0.10600875243544579, LR: 0.1
Time, 2019-01-01T17:00:06, Epoch: 9, Batch: 270, Training Loss: 0.11790020018815994, LR: 0.1
Time, 2019-01-01T17:00:07, Epoch: 9, Batch: 280, Training Loss: 0.12430617660284042, LR: 0.1
Time, 2019-01-01T17:00:07, Epoch: 9, Batch: 290, Training Loss: 0.11168262884020805, LR: 0.1
Time, 2019-01-01T17:00:08, Epoch: 9, Batch: 300, Training Loss: 0.1534041702747345, LR: 0.1
Time, 2019-01-01T17:00:09, Epoch: 9, Batch: 310, Training Loss: 0.14565442502498627, LR: 0.1
Time, 2019-01-01T17:00:10, Epoch: 9, Batch: 320, Training Loss: 0.09279543831944466, LR: 0.1
Time, 2019-01-01T17:00:10, Epoch: 9, Batch: 330, Training Loss: 0.09368067197501659, LR: 0.1
Time, 2019-01-01T17:00:11, Epoch: 9, Batch: 340, Training Loss: 0.0987930480390787, LR: 0.1
Time, 2019-01-01T17:00:12, Epoch: 9, Batch: 350, Training Loss: 0.09790441170334815, LR: 0.1
Time, 2019-01-01T17:00:13, Epoch: 9, Batch: 360, Training Loss: 0.1030820831656456, LR: 0.1
Time, 2019-01-01T17:00:14, Epoch: 9, Batch: 370, Training Loss: 0.11923202723264695, LR: 0.1
Time, 2019-01-01T17:00:14, Epoch: 9, Batch: 380, Training Loss: 0.09754967242479325, LR: 0.1
Time, 2019-01-01T17:00:15, Epoch: 9, Batch: 390, Training Loss: 0.15267131775617598, LR: 0.1
Time, 2019-01-01T17:00:16, Epoch: 9, Batch: 400, Training Loss: 0.12015249356627464, LR: 0.1
Time, 2019-01-01T17:00:17, Epoch: 9, Batch: 410, Training Loss: 0.10142121352255344, LR: 0.1
Time, 2019-01-01T17:00:17, Epoch: 9, Batch: 420, Training Loss: 0.11334276795387269, LR: 0.1
Time, 2019-01-01T17:00:18, Epoch: 9, Batch: 430, Training Loss: 0.15126461982727052, LR: 0.1
Time, 2019-01-01T17:00:19, Epoch: 9, Batch: 440, Training Loss: 0.10119866877794266, LR: 0.1
Time, 2019-01-01T17:00:20, Epoch: 9, Batch: 450, Training Loss: 0.08735786229372025, LR: 0.1
Time, 2019-01-01T17:00:21, Epoch: 9, Batch: 460, Training Loss: 0.08318520784378051, LR: 0.1
Time, 2019-01-01T17:00:21, Epoch: 9, Batch: 470, Training Loss: 0.09242875203490257, LR: 0.1
Time, 2019-01-01T17:00:22, Epoch: 9, Batch: 480, Training Loss: 0.10634826198220253, LR: 0.1
Time, 2019-01-01T17:00:23, Epoch: 9, Batch: 490, Training Loss: 0.13244256228208542, LR: 0.1
Time, 2019-01-01T17:00:24, Epoch: 9, Batch: 500, Training Loss: 0.14142738059163093, LR: 0.1
Time, 2019-01-01T17:00:25, Epoch: 9, Batch: 510, Training Loss: 0.12096614018082619, LR: 0.1
Time, 2019-01-01T17:00:26, Epoch: 9, Batch: 520, Training Loss: 0.1367305938154459, LR: 0.1
Time, 2019-01-01T17:00:26, Epoch: 9, Batch: 530, Training Loss: 0.14947071745991708, LR: 0.1
Time, 2019-01-01T17:00:27, Epoch: 9, Batch: 540, Training Loss: 0.12049677297472954, LR: 0.1
Time, 2019-01-01T17:00:28, Epoch: 9, Batch: 550, Training Loss: 0.18709490373730658, LR: 0.1
Time, 2019-01-01T17:00:29, Epoch: 9, Batch: 560, Training Loss: 0.1570763684809208, LR: 0.1
Time, 2019-01-01T17:00:30, Epoch: 9, Batch: 570, Training Loss: 0.15062069781124593, LR: 0.1
Time, 2019-01-01T17:00:30, Epoch: 9, Batch: 580, Training Loss: 0.10181838050484657, LR: 0.1
Time, 2019-01-01T17:00:31, Epoch: 9, Batch: 590, Training Loss: 0.13195646405220032, LR: 0.1
Time, 2019-01-01T17:00:32, Epoch: 9, Batch: 600, Training Loss: 0.1438529908657074, LR: 0.1
Time, 2019-01-01T17:00:33, Epoch: 9, Batch: 610, Training Loss: 0.12855962067842483, LR: 0.1
Time, 2019-01-01T17:00:33, Epoch: 9, Batch: 620, Training Loss: 0.20286617130041124, LR: 0.1
Time, 2019-01-01T17:00:34, Epoch: 9, Batch: 630, Training Loss: 0.14611456617712976, LR: 0.1
Time, 2019-01-01T17:00:35, Epoch: 9, Batch: 640, Training Loss: 0.23610494956374167, LR: 0.1
Time, 2019-01-01T17:00:35, Epoch: 9, Batch: 650, Training Loss: 0.35331398993730545, LR: 0.1
Time, 2019-01-01T17:00:36, Epoch: 9, Batch: 660, Training Loss: 0.23270490765571594, LR: 0.1
Time, 2019-01-01T17:00:37, Epoch: 9, Batch: 670, Training Loss: 0.1808440826833248, LR: 0.1
Time, 2019-01-01T17:00:37, Epoch: 9, Batch: 680, Training Loss: 0.21389933079481124, LR: 0.1
Time, 2019-01-01T17:00:38, Epoch: 9, Batch: 690, Training Loss: 0.25148657858371737, LR: 0.1
Time, 2019-01-01T17:00:39, Epoch: 9, Batch: 700, Training Loss: 0.1820294179022312, LR: 0.1
Time, 2019-01-01T17:00:40, Epoch: 9, Batch: 710, Training Loss: 0.1732122518122196, LR: 0.1
Time, 2019-01-01T17:00:40, Epoch: 9, Batch: 720, Training Loss: 0.1433322586119175, LR: 0.1
Time, 2019-01-01T17:00:41, Epoch: 9, Batch: 730, Training Loss: 0.17428581044077873, LR: 0.1
Time, 2019-01-01T17:00:42, Epoch: 9, Batch: 740, Training Loss: 0.16008486300706865, LR: 0.1
Time, 2019-01-01T17:00:43, Epoch: 9, Batch: 750, Training Loss: 0.17679412215948104, LR: 0.1
Time, 2019-01-01T17:00:43, Epoch: 9, Batch: 760, Training Loss: 0.17050396278500557, LR: 0.1
Time, 2019-01-01T17:00:44, Epoch: 9, Batch: 770, Training Loss: 0.15320351123809814, LR: 0.1
Time, 2019-01-01T17:00:45, Epoch: 9, Batch: 780, Training Loss: 0.15131064429879187, LR: 0.1
Time, 2019-01-01T17:00:45, Epoch: 9, Batch: 790, Training Loss: 0.15044859498739244, LR: 0.1
Time, 2019-01-01T17:00:46, Epoch: 9, Batch: 800, Training Loss: 0.208548591285944, LR: 0.1
Time, 2019-01-01T17:00:47, Epoch: 9, Batch: 810, Training Loss: 0.15850879549980162, LR: 0.1
Time, 2019-01-01T17:00:48, Epoch: 9, Batch: 820, Training Loss: 0.139984280616045, LR: 0.1
Time, 2019-01-01T17:00:49, Epoch: 9, Batch: 830, Training Loss: 0.14235313534736632, LR: 0.1
Time, 2019-01-01T17:00:49, Epoch: 9, Batch: 840, Training Loss: 0.13016302958130838, LR: 0.1
Time, 2019-01-01T17:00:50, Epoch: 9, Batch: 850, Training Loss: 0.1361745186150074, LR: 0.1
Time, 2019-01-01T17:00:51, Epoch: 9, Batch: 860, Training Loss: 0.11389752924442291, LR: 0.1
Time, 2019-01-01T17:00:51, Epoch: 9, Batch: 870, Training Loss: 0.18708440512418748, LR: 0.1
Time, 2019-01-01T17:00:52, Epoch: 9, Batch: 880, Training Loss: 0.16653048768639564, LR: 0.1
Time, 2019-01-01T17:00:53, Epoch: 9, Batch: 890, Training Loss: 0.1447918653488159, LR: 0.1
Time, 2019-01-01T17:00:54, Epoch: 9, Batch: 900, Training Loss: 0.1842642046511173, LR: 0.1
Time, 2019-01-01T17:00:54, Epoch: 9, Batch: 910, Training Loss: 0.1973571076989174, LR: 0.1
Time, 2019-01-01T17:00:55, Epoch: 9, Batch: 920, Training Loss: 0.1521675854921341, LR: 0.1
Time, 2019-01-01T17:00:56, Epoch: 9, Batch: 930, Training Loss: 0.17564901299774646, LR: 0.1
Epoch: 9, Validation Top 1 acc: 95.86553955078125
Epoch: 9, Validation Top 5 acc: 99.89505767822266
Epoch: 9, Validation Set Loss: 0.13257530331611633
Start training epoch 10
Time, 2019-01-01T17:01:22, Epoch: 10, Batch: 10, Training Loss: 0.11132859513163566, LR: 0.010000000000000002
Time, 2019-01-01T17:01:23, Epoch: 10, Batch: 20, Training Loss: 0.11797518283128738, LR: 0.010000000000000002
Time, 2019-01-01T17:01:24, Epoch: 10, Batch: 30, Training Loss: 0.08774779997766018, LR: 0.010000000000000002
Time, 2019-01-01T17:01:24, Epoch: 10, Batch: 40, Training Loss: 0.13356612771749496, LR: 0.010000000000000002
Time, 2019-01-01T17:01:25, Epoch: 10, Batch: 50, Training Loss: 0.0884157694876194, LR: 0.010000000000000002
Time, 2019-01-01T17:01:26, Epoch: 10, Batch: 60, Training Loss: 0.10251603350043297, LR: 0.010000000000000002
Time, 2019-01-01T17:01:27, Epoch: 10, Batch: 70, Training Loss: 0.10849522836506367, LR: 0.010000000000000002
Time, 2019-01-01T17:01:27, Epoch: 10, Batch: 80, Training Loss: 0.09963628575205803, LR: 0.010000000000000002
Time, 2019-01-01T17:01:28, Epoch: 10, Batch: 90, Training Loss: 0.09226238690316677, LR: 0.010000000000000002
Time, 2019-01-01T17:01:29, Epoch: 10, Batch: 100, Training Loss: 0.11708133816719055, LR: 0.010000000000000002
Time, 2019-01-01T17:01:30, Epoch: 10, Batch: 110, Training Loss: 0.10819635838270188, LR: 0.010000000000000002
Time, 2019-01-01T17:01:30, Epoch: 10, Batch: 120, Training Loss: 0.08059471547603607, LR: 0.010000000000000002
Time, 2019-01-01T17:01:31, Epoch: 10, Batch: 130, Training Loss: 0.10891642272472382, LR: 0.010000000000000002
Time, 2019-01-01T17:01:32, Epoch: 10, Batch: 140, Training Loss: 0.08065035305917263, LR: 0.010000000000000002
Time, 2019-01-01T17:01:32, Epoch: 10, Batch: 150, Training Loss: 0.08478015586733818, LR: 0.010000000000000002
Time, 2019-01-01T17:01:33, Epoch: 10, Batch: 160, Training Loss: 0.07922290302813054, LR: 0.010000000000000002
Time, 2019-01-01T17:01:34, Epoch: 10, Batch: 170, Training Loss: 0.09815453141927719, LR: 0.010000000000000002
Time, 2019-01-01T17:01:35, Epoch: 10, Batch: 180, Training Loss: 0.10967049486935139, LR: 0.010000000000000002
Time, 2019-01-01T17:01:35, Epoch: 10, Batch: 190, Training Loss: 0.06617394536733627, LR: 0.010000000000000002
Time, 2019-01-01T17:01:36, Epoch: 10, Batch: 200, Training Loss: 0.09105859957635402, LR: 0.010000000000000002
Time, 2019-01-01T17:01:37, Epoch: 10, Batch: 210, Training Loss: 0.12228154614567757, LR: 0.010000000000000002
Time, 2019-01-01T17:01:37, Epoch: 10, Batch: 220, Training Loss: 0.08271219916641712, LR: 0.010000000000000002
Time, 2019-01-01T17:01:38, Epoch: 10, Batch: 230, Training Loss: 0.10572587847709655, LR: 0.010000000000000002
Time, 2019-01-01T17:01:39, Epoch: 10, Batch: 240, Training Loss: 0.11363002695143223, LR: 0.010000000000000002
Time, 2019-01-01T17:01:40, Epoch: 10, Batch: 250, Training Loss: 0.0893507320433855, LR: 0.010000000000000002
Time, 2019-01-01T17:01:40, Epoch: 10, Batch: 260, Training Loss: 0.06228385642170906, LR: 0.010000000000000002
Time, 2019-01-01T17:01:41, Epoch: 10, Batch: 270, Training Loss: 0.08705358020961285, LR: 0.010000000000000002
Time, 2019-01-01T17:01:42, Epoch: 10, Batch: 280, Training Loss: 0.08227359987795353, LR: 0.010000000000000002
Time, 2019-01-01T17:01:43, Epoch: 10, Batch: 290, Training Loss: 0.10130021795630455, LR: 0.010000000000000002
Time, 2019-01-01T17:01:43, Epoch: 10, Batch: 300, Training Loss: 0.09276729933917523, LR: 0.010000000000000002
Time, 2019-01-01T17:01:44, Epoch: 10, Batch: 310, Training Loss: 0.094459617882967, LR: 0.010000000000000002
Time, 2019-01-01T17:01:45, Epoch: 10, Batch: 320, Training Loss: 0.0778701949864626, LR: 0.010000000000000002
Time, 2019-01-01T17:01:46, Epoch: 10, Batch: 330, Training Loss: 0.08174382783472538, LR: 0.010000000000000002
Time, 2019-01-01T17:01:46, Epoch: 10, Batch: 340, Training Loss: 0.05722863338887692, LR: 0.010000000000000002
Time, 2019-01-01T17:01:47, Epoch: 10, Batch: 350, Training Loss: 0.10576958023011684, LR: 0.010000000000000002
Time, 2019-01-01T17:01:48, Epoch: 10, Batch: 360, Training Loss: 0.07248910740017891, LR: 0.010000000000000002
Time, 2019-01-01T17:01:48, Epoch: 10, Batch: 370, Training Loss: 0.09039318561553955, LR: 0.010000000000000002
Time, 2019-01-01T17:01:49, Epoch: 10, Batch: 380, Training Loss: 0.09116301238536835, LR: 0.010000000000000002
Time, 2019-01-01T17:01:50, Epoch: 10, Batch: 390, Training Loss: 0.06988433562219143, LR: 0.010000000000000002
Time, 2019-01-01T17:01:51, Epoch: 10, Batch: 400, Training Loss: 0.09133533835411071, LR: 0.010000000000000002
Time, 2019-01-01T17:01:51, Epoch: 10, Batch: 410, Training Loss: 0.06660285964608192, LR: 0.010000000000000002
Time, 2019-01-01T17:01:52, Epoch: 10, Batch: 420, Training Loss: 0.06335315741598606, LR: 0.010000000000000002
Time, 2019-01-01T17:01:53, Epoch: 10, Batch: 430, Training Loss: 0.062486378103494646, LR: 0.010000000000000002
Time, 2019-01-01T17:01:54, Epoch: 10, Batch: 440, Training Loss: 0.04725540652871132, LR: 0.010000000000000002
Time, 2019-01-01T17:01:54, Epoch: 10, Batch: 450, Training Loss: 0.1042105309665203, LR: 0.010000000000000002
Time, 2019-01-01T17:01:55, Epoch: 10, Batch: 460, Training Loss: 0.07256877794861794, LR: 0.010000000000000002
Time, 2019-01-01T17:01:56, Epoch: 10, Batch: 470, Training Loss: 0.08799414858222007, LR: 0.010000000000000002
Time, 2019-01-01T17:01:57, Epoch: 10, Batch: 480, Training Loss: 0.0826602466404438, LR: 0.010000000000000002
Time, 2019-01-01T17:01:57, Epoch: 10, Batch: 490, Training Loss: 0.08434399403631687, LR: 0.010000000000000002
Time, 2019-01-01T17:01:58, Epoch: 10, Batch: 500, Training Loss: 0.0683711651712656, LR: 0.010000000000000002
Time, 2019-01-01T17:01:59, Epoch: 10, Batch: 510, Training Loss: 0.09249306470155716, LR: 0.010000000000000002
Time, 2019-01-01T17:01:59, Epoch: 10, Batch: 520, Training Loss: 0.050377064198255536, LR: 0.010000000000000002
Time, 2019-01-01T17:02:00, Epoch: 10, Batch: 530, Training Loss: 0.0908847615122795, LR: 0.010000000000000002
Time, 2019-01-01T17:02:01, Epoch: 10, Batch: 540, Training Loss: 0.08944940157234668, LR: 0.010000000000000002
Time, 2019-01-01T17:02:02, Epoch: 10, Batch: 550, Training Loss: 0.09144655242562294, LR: 0.010000000000000002
Time, 2019-01-01T17:02:02, Epoch: 10, Batch: 560, Training Loss: 0.08940708935260773, LR: 0.010000000000000002
Time, 2019-01-01T17:02:03, Epoch: 10, Batch: 570, Training Loss: 0.07479475885629654, LR: 0.010000000000000002
Time, 2019-01-01T17:02:04, Epoch: 10, Batch: 580, Training Loss: 0.08569392301142216, LR: 0.010000000000000002
Time, 2019-01-01T17:02:04, Epoch: 10, Batch: 590, Training Loss: 0.09229668639600278, LR: 0.010000000000000002
Time, 2019-01-01T17:02:05, Epoch: 10, Batch: 600, Training Loss: 0.07195885479450226, LR: 0.010000000000000002
Time, 2019-01-01T17:02:06, Epoch: 10, Batch: 610, Training Loss: 0.0776625458151102, LR: 0.010000000000000002
Time, 2019-01-01T17:02:07, Epoch: 10, Batch: 620, Training Loss: 0.07462146990001202, LR: 0.010000000000000002
Time, 2019-01-01T17:02:07, Epoch: 10, Batch: 630, Training Loss: 0.05923502780497074, LR: 0.010000000000000002
Time, 2019-01-01T17:02:08, Epoch: 10, Batch: 640, Training Loss: 0.09417129829525947, LR: 0.010000000000000002
Time, 2019-01-01T17:02:09, Epoch: 10, Batch: 650, Training Loss: 0.06829581037163734, LR: 0.010000000000000002
Time, 2019-01-01T17:02:10, Epoch: 10, Batch: 660, Training Loss: 0.10847472287714481, LR: 0.010000000000000002
Time, 2019-01-01T17:02:10, Epoch: 10, Batch: 670, Training Loss: 0.08186114653944969, LR: 0.010000000000000002
Time, 2019-01-01T17:02:11, Epoch: 10, Batch: 680, Training Loss: 0.06476237364113331, LR: 0.010000000000000002
Time, 2019-01-01T17:02:12, Epoch: 10, Batch: 690, Training Loss: 0.08699151873588562, LR: 0.010000000000000002
Time, 2019-01-01T17:02:12, Epoch: 10, Batch: 700, Training Loss: 0.09566445127129555, LR: 0.010000000000000002
Time, 2019-01-01T17:02:13, Epoch: 10, Batch: 710, Training Loss: 0.061670270189642905, LR: 0.010000000000000002
Time, 2019-01-01T17:02:14, Epoch: 10, Batch: 720, Training Loss: 0.0894999723881483, LR: 0.010000000000000002
Time, 2019-01-01T17:02:15, Epoch: 10, Batch: 730, Training Loss: 0.09191544353961945, LR: 0.010000000000000002
Time, 2019-01-01T17:02:15, Epoch: 10, Batch: 740, Training Loss: 0.09649323225021363, LR: 0.010000000000000002
Time, 2019-01-01T17:02:16, Epoch: 10, Batch: 750, Training Loss: 0.08531744554638862, LR: 0.010000000000000002
Time, 2019-01-01T17:02:17, Epoch: 10, Batch: 760, Training Loss: 0.0842096708714962, LR: 0.010000000000000002
Time, 2019-01-01T17:02:18, Epoch: 10, Batch: 770, Training Loss: 0.09465101137757301, LR: 0.010000000000000002
Time, 2019-01-01T17:02:18, Epoch: 10, Batch: 780, Training Loss: 0.04598029777407646, LR: 0.010000000000000002
Time, 2019-01-01T17:02:19, Epoch: 10, Batch: 790, Training Loss: 0.0799270361661911, LR: 0.010000000000000002
Time, 2019-01-01T17:02:20, Epoch: 10, Batch: 800, Training Loss: 0.07347019612789155, LR: 0.010000000000000002
Time, 2019-01-01T17:02:20, Epoch: 10, Batch: 810, Training Loss: 0.1000716459006071, LR: 0.010000000000000002
Time, 2019-01-01T17:02:21, Epoch: 10, Batch: 820, Training Loss: 0.07632924281060696, LR: 0.010000000000000002
Time, 2019-01-01T17:02:22, Epoch: 10, Batch: 830, Training Loss: 0.08255460187792778, LR: 0.010000000000000002
Time, 2019-01-01T17:02:23, Epoch: 10, Batch: 840, Training Loss: 0.08114655911922455, LR: 0.010000000000000002
Time, 2019-01-01T17:02:23, Epoch: 10, Batch: 850, Training Loss: 0.1067722350358963, LR: 0.010000000000000002
Time, 2019-01-01T17:02:24, Epoch: 10, Batch: 860, Training Loss: 0.08561578840017318, LR: 0.010000000000000002
Time, 2019-01-01T17:02:25, Epoch: 10, Batch: 870, Training Loss: 0.08553249686956406, LR: 0.010000000000000002
Time, 2019-01-01T17:02:26, Epoch: 10, Batch: 880, Training Loss: 0.06226238161325455, LR: 0.010000000000000002
Time, 2019-01-01T17:02:26, Epoch: 10, Batch: 890, Training Loss: 0.07377747148275375, LR: 0.010000000000000002
Time, 2019-01-01T17:02:27, Epoch: 10, Batch: 900, Training Loss: 0.09531552158296108, LR: 0.010000000000000002
Time, 2019-01-01T17:02:28, Epoch: 10, Batch: 910, Training Loss: 0.08945644311606885, LR: 0.010000000000000002
Time, 2019-01-01T17:02:29, Epoch: 10, Batch: 920, Training Loss: 0.06278692670166493, LR: 0.010000000000000002
Time, 2019-01-01T17:02:29, Epoch: 10, Batch: 930, Training Loss: 0.09963755458593368, LR: 0.010000000000000002
Epoch: 10, Validation Top 1 acc: 97.62960052490234
Epoch: 10, Validation Top 5 acc: 99.9633560180664
Epoch: 10, Validation Set Loss: 0.07598181813955307
Start training epoch 11
Time, 2019-01-01T17:02:55, Epoch: 11, Batch: 10, Training Loss: 0.08770539835095406, LR: 0.010000000000000002
Time, 2019-01-01T17:02:56, Epoch: 11, Batch: 20, Training Loss: 0.09706883728504181, LR: 0.010000000000000002
Time, 2019-01-01T17:02:56, Epoch: 11, Batch: 30, Training Loss: 0.04841054081916809, LR: 0.010000000000000002
Time, 2019-01-01T17:02:57, Epoch: 11, Batch: 40, Training Loss: 0.08312090374529361, LR: 0.010000000000000002
Time, 2019-01-01T17:02:58, Epoch: 11, Batch: 50, Training Loss: 0.06354246884584427, LR: 0.010000000000000002
Time, 2019-01-01T17:02:59, Epoch: 11, Batch: 60, Training Loss: 0.09069819711148738, LR: 0.010000000000000002
Time, 2019-01-01T17:02:59, Epoch: 11, Batch: 70, Training Loss: 0.050194915011525156, LR: 0.010000000000000002
Time, 2019-01-01T17:03:00, Epoch: 11, Batch: 80, Training Loss: 0.09979416355490685, LR: 0.010000000000000002
Time, 2019-01-01T17:03:01, Epoch: 11, Batch: 90, Training Loss: 0.08917728662490845, LR: 0.010000000000000002
Time, 2019-01-01T17:03:01, Epoch: 11, Batch: 100, Training Loss: 0.08040382079780102, LR: 0.010000000000000002
Time, 2019-01-01T17:03:02, Epoch: 11, Batch: 110, Training Loss: 0.06019473671913147, LR: 0.010000000000000002
Time, 2019-01-01T17:03:03, Epoch: 11, Batch: 120, Training Loss: 0.09488362297415734, LR: 0.010000000000000002
Time, 2019-01-01T17:03:04, Epoch: 11, Batch: 130, Training Loss: 0.09729367718100548, LR: 0.010000000000000002
Time, 2019-01-01T17:03:04, Epoch: 11, Batch: 140, Training Loss: 0.0526489220559597, LR: 0.010000000000000002
Time, 2019-01-01T17:03:05, Epoch: 11, Batch: 150, Training Loss: 0.0920521542429924, LR: 0.010000000000000002
Time, 2019-01-01T17:03:06, Epoch: 11, Batch: 160, Training Loss: 0.07082285061478615, LR: 0.010000000000000002
Time, 2019-01-01T17:03:07, Epoch: 11, Batch: 170, Training Loss: 0.08292010240256786, LR: 0.010000000000000002
Time, 2019-01-01T17:03:07, Epoch: 11, Batch: 180, Training Loss: 0.09047116115689277, LR: 0.010000000000000002
Time, 2019-01-01T17:03:08, Epoch: 11, Batch: 190, Training Loss: 0.059086985513567926, LR: 0.010000000000000002
Time, 2019-01-01T17:03:09, Epoch: 11, Batch: 200, Training Loss: 0.0877380020916462, LR: 0.010000000000000002
Time, 2019-01-01T17:03:09, Epoch: 11, Batch: 210, Training Loss: 0.07277161329984665, LR: 0.010000000000000002
Time, 2019-01-01T17:03:10, Epoch: 11, Batch: 220, Training Loss: 0.09550861716270446, LR: 0.010000000000000002
Time, 2019-01-01T17:03:11, Epoch: 11, Batch: 230, Training Loss: 0.07276837639510632, LR: 0.010000000000000002
Time, 2019-01-01T17:03:12, Epoch: 11, Batch: 240, Training Loss: 0.0728759728372097, LR: 0.010000000000000002
Time, 2019-01-01T17:03:12, Epoch: 11, Batch: 250, Training Loss: 0.08125268518924714, LR: 0.010000000000000002
Time, 2019-01-01T17:03:13, Epoch: 11, Batch: 260, Training Loss: 0.07000091634690761, LR: 0.010000000000000002
Time, 2019-01-01T17:03:14, Epoch: 11, Batch: 270, Training Loss: 0.08355117812752724, LR: 0.010000000000000002
Time, 2019-01-01T17:03:15, Epoch: 11, Batch: 280, Training Loss: 0.0872037686407566, LR: 0.010000000000000002
Time, 2019-01-01T17:03:15, Epoch: 11, Batch: 290, Training Loss: 0.08136072307825089, LR: 0.010000000000000002
Time, 2019-01-01T17:03:16, Epoch: 11, Batch: 300, Training Loss: 0.06872188299894333, LR: 0.010000000000000002
Time, 2019-01-01T17:03:17, Epoch: 11, Batch: 310, Training Loss: 0.09516094028949737, LR: 0.010000000000000002
Time, 2019-01-01T17:03:18, Epoch: 11, Batch: 320, Training Loss: 0.08730801567435265, LR: 0.010000000000000002
Time, 2019-01-01T17:03:18, Epoch: 11, Batch: 330, Training Loss: 0.08621512316167354, LR: 0.010000000000000002
Time, 2019-01-01T17:03:19, Epoch: 11, Batch: 340, Training Loss: 0.07720858380198478, LR: 0.010000000000000002
Time, 2019-01-01T17:03:20, Epoch: 11, Batch: 350, Training Loss: 0.06575711332261562, LR: 0.010000000000000002
Time, 2019-01-01T17:03:20, Epoch: 11, Batch: 360, Training Loss: 0.08929231651127338, LR: 0.010000000000000002
Time, 2019-01-01T17:03:21, Epoch: 11, Batch: 370, Training Loss: 0.08075124472379684, LR: 0.010000000000000002
Time, 2019-01-01T17:03:22, Epoch: 11, Batch: 380, Training Loss: 0.07240425646305085, LR: 0.010000000000000002
Time, 2019-01-01T17:03:23, Epoch: 11, Batch: 390, Training Loss: 0.08479675948619843, LR: 0.010000000000000002
Time, 2019-01-01T17:03:23, Epoch: 11, Batch: 400, Training Loss: 0.06444555781781673, LR: 0.010000000000000002
Time, 2019-01-01T17:03:24, Epoch: 11, Batch: 410, Training Loss: 0.07126503027975559, LR: 0.010000000000000002
Time, 2019-01-01T17:03:25, Epoch: 11, Batch: 420, Training Loss: 0.07197703085839749, LR: 0.010000000000000002
Time, 2019-01-01T17:03:26, Epoch: 11, Batch: 430, Training Loss: 0.09224464856088162, LR: 0.010000000000000002
Time, 2019-01-01T17:03:26, Epoch: 11, Batch: 440, Training Loss: 0.07367442324757575, LR: 0.010000000000000002
Time, 2019-01-01T17:03:27, Epoch: 11, Batch: 450, Training Loss: 0.12481309697031975, LR: 0.010000000000000002
Time, 2019-01-01T17:03:28, Epoch: 11, Batch: 460, Training Loss: 0.11450104489922523, LR: 0.010000000000000002
Time, 2019-01-01T17:03:29, Epoch: 11, Batch: 470, Training Loss: 0.08075438849627972, LR: 0.010000000000000002
Time, 2019-01-01T17:03:29, Epoch: 11, Batch: 480, Training Loss: 0.06701629161834717, LR: 0.010000000000000002
Time, 2019-01-01T17:03:30, Epoch: 11, Batch: 490, Training Loss: 0.0698059305548668, LR: 0.010000000000000002
Time, 2019-01-01T17:03:31, Epoch: 11, Batch: 500, Training Loss: 0.0771881427615881, LR: 0.010000000000000002
Time, 2019-01-01T17:03:32, Epoch: 11, Batch: 510, Training Loss: 0.08242086544632912, LR: 0.010000000000000002
Time, 2019-01-01T17:03:32, Epoch: 11, Batch: 520, Training Loss: 0.058542109280824664, LR: 0.010000000000000002
Time, 2019-01-01T17:03:33, Epoch: 11, Batch: 530, Training Loss: 0.061766299232840535, LR: 0.010000000000000002
Time, 2019-01-01T17:03:34, Epoch: 11, Batch: 540, Training Loss: 0.06609030663967133, LR: 0.010000000000000002
Time, 2019-01-01T17:03:34, Epoch: 11, Batch: 550, Training Loss: 0.0800500363111496, LR: 0.010000000000000002
Time, 2019-01-01T17:03:35, Epoch: 11, Batch: 560, Training Loss: 0.0942158292979002, LR: 0.010000000000000002
Time, 2019-01-01T17:03:36, Epoch: 11, Batch: 570, Training Loss: 0.08704308941960334, LR: 0.010000000000000002
Time, 2019-01-01T17:03:37, Epoch: 11, Batch: 580, Training Loss: 0.06817216016352176, LR: 0.010000000000000002
Time, 2019-01-01T17:03:37, Epoch: 11, Batch: 590, Training Loss: 0.058171593397855756, LR: 0.010000000000000002
Time, 2019-01-01T17:03:38, Epoch: 11, Batch: 600, Training Loss: 0.07102695368230343, LR: 0.010000000000000002
Time, 2019-01-01T17:03:39, Epoch: 11, Batch: 610, Training Loss: 0.05504855513572693, LR: 0.010000000000000002
Time, 2019-01-01T17:03:40, Epoch: 11, Batch: 620, Training Loss: 0.07028631195425987, LR: 0.010000000000000002
Time, 2019-01-01T17:03:40, Epoch: 11, Batch: 630, Training Loss: 0.04254552237689495, LR: 0.010000000000000002
Time, 2019-01-01T17:03:41, Epoch: 11, Batch: 640, Training Loss: 0.0635998196899891, LR: 0.010000000000000002
Time, 2019-01-01T17:03:42, Epoch: 11, Batch: 650, Training Loss: 0.08199656903743743, LR: 0.010000000000000002
Time, 2019-01-01T17:03:43, Epoch: 11, Batch: 660, Training Loss: 0.07827503383159637, LR: 0.010000000000000002
Time, 2019-01-01T17:03:43, Epoch: 11, Batch: 670, Training Loss: 0.07362139895558358, LR: 0.010000000000000002
Time, 2019-01-01T17:03:44, Epoch: 11, Batch: 680, Training Loss: 0.058562330156564715, LR: 0.010000000000000002
Time, 2019-01-01T17:03:45, Epoch: 11, Batch: 690, Training Loss: 0.08896900713443756, LR: 0.010000000000000002
Time, 2019-01-01T17:03:45, Epoch: 11, Batch: 700, Training Loss: 0.06817940287292004, LR: 0.010000000000000002
Time, 2019-01-01T17:03:46, Epoch: 11, Batch: 710, Training Loss: 0.04810745120048523, LR: 0.010000000000000002
Time, 2019-01-01T17:03:47, Epoch: 11, Batch: 720, Training Loss: 0.07338817678391933, LR: 0.010000000000000002
Time, 2019-01-01T17:03:48, Epoch: 11, Batch: 730, Training Loss: 0.07745248675346375, LR: 0.010000000000000002
Time, 2019-01-01T17:03:48, Epoch: 11, Batch: 740, Training Loss: 0.07088262811303139, LR: 0.010000000000000002
Time, 2019-01-01T17:03:49, Epoch: 11, Batch: 750, Training Loss: 0.07042683586478234, LR: 0.010000000000000002
Time, 2019-01-01T17:03:50, Epoch: 11, Batch: 760, Training Loss: 0.04933223910629749, LR: 0.010000000000000002
Time, 2019-01-01T17:03:51, Epoch: 11, Batch: 770, Training Loss: 0.07348055839538574, LR: 0.010000000000000002
Time, 2019-01-01T17:03:51, Epoch: 11, Batch: 780, Training Loss: 0.09614035338163376, LR: 0.010000000000000002
Time, 2019-01-01T17:03:52, Epoch: 11, Batch: 790, Training Loss: 0.08742988631129264, LR: 0.010000000000000002
Time, 2019-01-01T17:03:53, Epoch: 11, Batch: 800, Training Loss: 0.06360087431967258, LR: 0.010000000000000002
Time, 2019-01-01T17:03:54, Epoch: 11, Batch: 810, Training Loss: 0.060647119954228404, LR: 0.010000000000000002
Time, 2019-01-01T17:03:54, Epoch: 11, Batch: 820, Training Loss: 0.07220315970480443, LR: 0.010000000000000002
Time, 2019-01-01T17:03:55, Epoch: 11, Batch: 830, Training Loss: 0.06517323032021523, LR: 0.010000000000000002
Time, 2019-01-01T17:03:56, Epoch: 11, Batch: 840, Training Loss: 0.08659012466669083, LR: 0.010000000000000002
Time, 2019-01-01T17:03:56, Epoch: 11, Batch: 850, Training Loss: 0.048360076546669004, LR: 0.010000000000000002
Time, 2019-01-01T17:03:57, Epoch: 11, Batch: 860, Training Loss: 0.06061447709798813, LR: 0.010000000000000002
Time, 2019-01-01T17:03:58, Epoch: 11, Batch: 870, Training Loss: 0.07588846981525421, LR: 0.010000000000000002
Time, 2019-01-01T17:03:59, Epoch: 11, Batch: 880, Training Loss: 0.0902597676962614, LR: 0.010000000000000002
Time, 2019-01-01T17:03:59, Epoch: 11, Batch: 890, Training Loss: 0.08011446297168731, LR: 0.010000000000000002
Time, 2019-01-01T17:04:00, Epoch: 11, Batch: 900, Training Loss: 0.05658980011940003, LR: 0.010000000000000002
Time, 2019-01-01T17:04:01, Epoch: 11, Batch: 910, Training Loss: 0.04882232248783112, LR: 0.010000000000000002
Time, 2019-01-01T17:04:02, Epoch: 11, Batch: 920, Training Loss: 0.07058573625981808, LR: 0.010000000000000002
Time, 2019-01-01T17:04:02, Epoch: 11, Batch: 930, Training Loss: 0.07680258229374885, LR: 0.010000000000000002
Epoch: 11, Validation Top 1 acc: 97.76618957519531
Epoch: 11, Validation Top 5 acc: 99.9633560180664
Epoch: 11, Validation Set Loss: 0.07182585448026657
Start training epoch 12
Time, 2019-01-01T17:04:28, Epoch: 12, Batch: 10, Training Loss: 0.06082008332014084, LR: 0.010000000000000002
Time, 2019-01-01T17:04:29, Epoch: 12, Batch: 20, Training Loss: 0.07570808231830597, LR: 0.010000000000000002
Time, 2019-01-01T17:04:29, Epoch: 12, Batch: 30, Training Loss: 0.08415385149419308, LR: 0.010000000000000002
Time, 2019-01-01T17:04:30, Epoch: 12, Batch: 40, Training Loss: 0.0904944509267807, LR: 0.010000000000000002
Time, 2019-01-01T17:04:31, Epoch: 12, Batch: 50, Training Loss: 0.05334967523813248, LR: 0.010000000000000002
Time, 2019-01-01T17:04:32, Epoch: 12, Batch: 60, Training Loss: 0.0870464488863945, LR: 0.010000000000000002
Time, 2019-01-01T17:04:32, Epoch: 12, Batch: 70, Training Loss: 0.05786913223564625, LR: 0.010000000000000002
Time, 2019-01-01T17:04:33, Epoch: 12, Batch: 80, Training Loss: 0.09249827526509762, LR: 0.010000000000000002
Time, 2019-01-01T17:04:34, Epoch: 12, Batch: 90, Training Loss: 0.06409017592668534, LR: 0.010000000000000002
Time, 2019-01-01T17:04:34, Epoch: 12, Batch: 100, Training Loss: 0.078553606569767, LR: 0.010000000000000002
Time, 2019-01-01T17:04:35, Epoch: 12, Batch: 110, Training Loss: 0.052247405797243116, LR: 0.010000000000000002
Time, 2019-01-01T17:04:36, Epoch: 12, Batch: 120, Training Loss: 0.07650863640010357, LR: 0.010000000000000002
Time, 2019-01-01T17:04:37, Epoch: 12, Batch: 130, Training Loss: 0.05674612894654274, LR: 0.010000000000000002
Time, 2019-01-01T17:04:37, Epoch: 12, Batch: 140, Training Loss: 0.08363224267959594, LR: 0.010000000000000002
Time, 2019-01-01T17:04:38, Epoch: 12, Batch: 150, Training Loss: 0.05912584997713566, LR: 0.010000000000000002
Time, 2019-01-01T17:04:39, Epoch: 12, Batch: 160, Training Loss: 0.06114375665783882, LR: 0.010000000000000002
Time, 2019-01-01T17:04:40, Epoch: 12, Batch: 170, Training Loss: 0.09607659056782722, LR: 0.010000000000000002
Time, 2019-01-01T17:04:40, Epoch: 12, Batch: 180, Training Loss: 0.08926660157740116, LR: 0.010000000000000002
Time, 2019-01-01T17:04:41, Epoch: 12, Batch: 190, Training Loss: 0.0665399719029665, LR: 0.010000000000000002
Time, 2019-01-01T17:04:42, Epoch: 12, Batch: 200, Training Loss: 0.06839507408440113, LR: 0.010000000000000002
Time, 2019-01-01T17:04:43, Epoch: 12, Batch: 210, Training Loss: 0.06219830550253391, LR: 0.010000000000000002
Time, 2019-01-01T17:04:43, Epoch: 12, Batch: 220, Training Loss: 0.07619850859045982, LR: 0.010000000000000002
Time, 2019-01-01T17:04:44, Epoch: 12, Batch: 230, Training Loss: 0.07320088669657707, LR: 0.010000000000000002
Time, 2019-01-01T17:04:45, Epoch: 12, Batch: 240, Training Loss: 0.061772221326828004, LR: 0.010000000000000002
Time, 2019-01-01T17:04:45, Epoch: 12, Batch: 250, Training Loss: 0.09034179896116257, LR: 0.010000000000000002
Time, 2019-01-01T17:04:46, Epoch: 12, Batch: 260, Training Loss: 0.09575348272919655, LR: 0.010000000000000002
Time, 2019-01-01T17:04:47, Epoch: 12, Batch: 270, Training Loss: 0.05617008656263352, LR: 0.010000000000000002
Time, 2019-01-01T17:04:48, Epoch: 12, Batch: 280, Training Loss: 0.07119588367640972, LR: 0.010000000000000002
Time, 2019-01-01T17:04:48, Epoch: 12, Batch: 290, Training Loss: 0.08622762337327003, LR: 0.010000000000000002
Time, 2019-01-01T17:04:49, Epoch: 12, Batch: 300, Training Loss: 0.06916157156229019, LR: 0.010000000000000002
Time, 2019-01-01T17:04:50, Epoch: 12, Batch: 310, Training Loss: 0.06630851700901985, LR: 0.010000000000000002
Time, 2019-01-01T17:04:50, Epoch: 12, Batch: 320, Training Loss: 0.09645128175616265, LR: 0.010000000000000002
Time, 2019-01-01T17:04:51, Epoch: 12, Batch: 330, Training Loss: 0.07889477461576462, LR: 0.010000000000000002
Time, 2019-01-01T17:04:52, Epoch: 12, Batch: 340, Training Loss: 0.09292260333895683, LR: 0.010000000000000002
Time, 2019-01-01T17:04:53, Epoch: 12, Batch: 350, Training Loss: 0.06647891774773598, LR: 0.010000000000000002
Time, 2019-01-01T17:04:53, Epoch: 12, Batch: 360, Training Loss: 0.060678184404969214, LR: 0.010000000000000002
Time, 2019-01-01T17:04:54, Epoch: 12, Batch: 370, Training Loss: 0.08345526903867721, LR: 0.010000000000000002
Time, 2019-01-01T17:04:55, Epoch: 12, Batch: 380, Training Loss: 0.07655952051281929, LR: 0.010000000000000002
Time, 2019-01-01T17:04:56, Epoch: 12, Batch: 390, Training Loss: 0.08460948169231415, LR: 0.010000000000000002
Time, 2019-01-01T17:04:56, Epoch: 12, Batch: 400, Training Loss: 0.05218535587191582, LR: 0.010000000000000002
Time, 2019-01-01T17:04:57, Epoch: 12, Batch: 410, Training Loss: 0.06391321308910847, LR: 0.010000000000000002
Time, 2019-01-01T17:04:58, Epoch: 12, Batch: 420, Training Loss: 0.047188044711947444, LR: 0.010000000000000002
Time, 2019-01-01T17:04:59, Epoch: 12, Batch: 430, Training Loss: 0.09484962224960328, LR: 0.010000000000000002
Time, 2019-01-01T17:04:59, Epoch: 12, Batch: 440, Training Loss: 0.10000205039978027, LR: 0.010000000000000002
Time, 2019-01-01T17:05:00, Epoch: 12, Batch: 450, Training Loss: 0.06979180797934532, LR: 0.010000000000000002
Time, 2019-01-01T17:05:01, Epoch: 12, Batch: 460, Training Loss: 0.0786537542939186, LR: 0.010000000000000002
Time, 2019-01-01T17:05:02, Epoch: 12, Batch: 470, Training Loss: 0.0790163230150938, LR: 0.010000000000000002
Time, 2019-01-01T17:05:02, Epoch: 12, Batch: 480, Training Loss: 0.06906992644071579, LR: 0.010000000000000002
Time, 2019-01-01T17:05:03, Epoch: 12, Batch: 490, Training Loss: 0.07592717669904232, LR: 0.010000000000000002
Time, 2019-01-01T17:05:04, Epoch: 12, Batch: 500, Training Loss: 0.08787178918719292, LR: 0.010000000000000002
Time, 2019-01-01T17:05:05, Epoch: 12, Batch: 510, Training Loss: 0.08049725219607354, LR: 0.010000000000000002
Time, 2019-01-01T17:05:05, Epoch: 12, Batch: 520, Training Loss: 0.077882144972682, LR: 0.010000000000000002
Time, 2019-01-01T17:05:06, Epoch: 12, Batch: 530, Training Loss: 0.07496260330080987, LR: 0.010000000000000002
Time, 2019-01-01T17:05:07, Epoch: 12, Batch: 540, Training Loss: 0.1053623341023922, LR: 0.010000000000000002
Time, 2019-01-01T17:05:08, Epoch: 12, Batch: 550, Training Loss: 0.10436129048466683, LR: 0.010000000000000002
Time, 2019-01-01T17:05:08, Epoch: 12, Batch: 560, Training Loss: 0.07301605865359306, LR: 0.010000000000000002
Time, 2019-01-01T17:05:09, Epoch: 12, Batch: 570, Training Loss: 0.07237029150128364, LR: 0.010000000000000002
Time, 2019-01-01T17:05:10, Epoch: 12, Batch: 580, Training Loss: 0.05268046781420708, LR: 0.010000000000000002
Time, 2019-01-01T17:05:11, Epoch: 12, Batch: 590, Training Loss: 0.06478411331772804, LR: 0.010000000000000002
Time, 2019-01-01T17:05:11, Epoch: 12, Batch: 600, Training Loss: 0.07865197435021401, LR: 0.010000000000000002
Time, 2019-01-01T17:05:12, Epoch: 12, Batch: 610, Training Loss: 0.055864444375038146, LR: 0.010000000000000002
Time, 2019-01-01T17:05:13, Epoch: 12, Batch: 620, Training Loss: 0.08527091518044472, LR: 0.010000000000000002
Time, 2019-01-01T17:05:14, Epoch: 12, Batch: 630, Training Loss: 0.0819884978234768, LR: 0.010000000000000002
Time, 2019-01-01T17:05:14, Epoch: 12, Batch: 640, Training Loss: 0.05650405511260033, LR: 0.010000000000000002
Time, 2019-01-01T17:05:15, Epoch: 12, Batch: 650, Training Loss: 0.09424658641219139, LR: 0.010000000000000002
Time, 2019-01-01T17:05:16, Epoch: 12, Batch: 660, Training Loss: 0.0637673333287239, LR: 0.010000000000000002
Time, 2019-01-01T17:05:17, Epoch: 12, Batch: 670, Training Loss: 0.08540211915969849, LR: 0.010000000000000002
Time, 2019-01-01T17:05:18, Epoch: 12, Batch: 680, Training Loss: 0.0697240810841322, LR: 0.010000000000000002
Time, 2019-01-01T17:05:18, Epoch: 12, Batch: 690, Training Loss: 0.08778222352266311, LR: 0.010000000000000002
Time, 2019-01-01T17:05:19, Epoch: 12, Batch: 700, Training Loss: 0.05482383482158184, LR: 0.010000000000000002
Time, 2019-01-01T17:05:20, Epoch: 12, Batch: 710, Training Loss: 0.04740462526679039, LR: 0.010000000000000002
Time, 2019-01-01T17:05:21, Epoch: 12, Batch: 720, Training Loss: 0.08325882069766521, LR: 0.010000000000000002
Time, 2019-01-01T17:05:21, Epoch: 12, Batch: 730, Training Loss: 0.09578765444457531, LR: 0.010000000000000002
Time, 2019-01-01T17:05:22, Epoch: 12, Batch: 740, Training Loss: 0.07778493389487266, LR: 0.010000000000000002
Time, 2019-01-01T17:05:23, Epoch: 12, Batch: 750, Training Loss: 0.07611365616321564, LR: 0.010000000000000002
Time, 2019-01-01T17:05:24, Epoch: 12, Batch: 760, Training Loss: 0.09953358843922615, LR: 0.010000000000000002
Time, 2019-01-01T17:05:25, Epoch: 12, Batch: 770, Training Loss: 0.06873578280210495, LR: 0.010000000000000002
Time, 2019-01-01T17:05:26, Epoch: 12, Batch: 780, Training Loss: 0.08441346883773804, LR: 0.010000000000000002
Time, 2019-01-01T17:05:26, Epoch: 12, Batch: 790, Training Loss: 0.07250966988503933, LR: 0.010000000000000002
Time, 2019-01-01T17:05:27, Epoch: 12, Batch: 800, Training Loss: 0.06499749720096588, LR: 0.010000000000000002
Time, 2019-01-01T17:05:28, Epoch: 12, Batch: 810, Training Loss: 0.0609368946403265, LR: 0.010000000000000002
Time, 2019-01-01T17:05:29, Epoch: 12, Batch: 820, Training Loss: 0.06507412977516651, LR: 0.010000000000000002
Time, 2019-01-01T17:05:29, Epoch: 12, Batch: 830, Training Loss: 0.05977671965956688, LR: 0.010000000000000002
Time, 2019-01-01T17:05:30, Epoch: 12, Batch: 840, Training Loss: 0.07910586893558502, LR: 0.010000000000000002
Time, 2019-01-01T17:05:31, Epoch: 12, Batch: 850, Training Loss: 0.10399797260761261, LR: 0.010000000000000002
Time, 2019-01-01T17:05:32, Epoch: 12, Batch: 860, Training Loss: 0.08728033527731896, LR: 0.010000000000000002
Time, 2019-01-01T17:05:32, Epoch: 12, Batch: 870, Training Loss: 0.10919370353221894, LR: 0.010000000000000002
Time, 2019-01-01T17:05:33, Epoch: 12, Batch: 880, Training Loss: 0.07617967277765274, LR: 0.010000000000000002
Time, 2019-01-01T17:05:34, Epoch: 12, Batch: 890, Training Loss: 0.1042157880961895, LR: 0.010000000000000002
Time, 2019-01-01T17:05:35, Epoch: 12, Batch: 900, Training Loss: 0.09687362834811211, LR: 0.010000000000000002
Time, 2019-01-01T17:05:35, Epoch: 12, Batch: 910, Training Loss: 0.05799878872931004, LR: 0.010000000000000002
Time, 2019-01-01T17:05:36, Epoch: 12, Batch: 920, Training Loss: 0.11497394442558288, LR: 0.010000000000000002
Time, 2019-01-01T17:05:37, Epoch: 12, Batch: 930, Training Loss: 0.06810421720147133, LR: 0.010000000000000002
Epoch: 12, Validation Top 1 acc: 97.78451538085938
Epoch: 12, Validation Top 5 acc: 99.9716796875
Epoch: 12, Validation Set Loss: 0.07266184687614441
Start training epoch 13
Time, 2019-01-01T17:06:04, Epoch: 13, Batch: 10, Training Loss: 0.07347019277513027, LR: 0.010000000000000002
Time, 2019-01-01T17:06:04, Epoch: 13, Batch: 20, Training Loss: 0.10687586367130279, LR: 0.010000000000000002
Time, 2019-01-01T17:06:05, Epoch: 13, Batch: 30, Training Loss: 0.06351036243140698, LR: 0.010000000000000002
Time, 2019-01-01T17:06:06, Epoch: 13, Batch: 40, Training Loss: 0.10960753858089448, LR: 0.010000000000000002
Time, 2019-01-01T17:06:07, Epoch: 13, Batch: 50, Training Loss: 0.08472337797284127, LR: 0.010000000000000002
Time, 2019-01-01T17:06:07, Epoch: 13, Batch: 60, Training Loss: 0.05554084926843643, LR: 0.010000000000000002
Time, 2019-01-01T17:06:08, Epoch: 13, Batch: 70, Training Loss: 0.07221251092851162, LR: 0.010000000000000002
Time, 2019-01-01T17:06:09, Epoch: 13, Batch: 80, Training Loss: 0.07816153317689896, LR: 0.010000000000000002
Time, 2019-01-01T17:06:10, Epoch: 13, Batch: 90, Training Loss: 0.07222633734345436, LR: 0.010000000000000002
Time, 2019-01-01T17:06:10, Epoch: 13, Batch: 100, Training Loss: 0.08956449180841446, LR: 0.010000000000000002
Time, 2019-01-01T17:06:11, Epoch: 13, Batch: 110, Training Loss: 0.07259795404970645, LR: 0.010000000000000002
Time, 2019-01-01T17:06:12, Epoch: 13, Batch: 120, Training Loss: 0.08850357756018638, LR: 0.010000000000000002
Time, 2019-01-01T17:06:13, Epoch: 13, Batch: 130, Training Loss: 0.06508846171200275, LR: 0.010000000000000002
Time, 2019-01-01T17:06:13, Epoch: 13, Batch: 140, Training Loss: 0.08531649447977543, LR: 0.010000000000000002
Time, 2019-01-01T17:06:14, Epoch: 13, Batch: 150, Training Loss: 0.0752226747572422, LR: 0.010000000000000002
Time, 2019-01-01T17:06:15, Epoch: 13, Batch: 160, Training Loss: 0.08619122691452503, LR: 0.010000000000000002
Time, 2019-01-01T17:06:16, Epoch: 13, Batch: 170, Training Loss: 0.10491951256990432, LR: 0.010000000000000002
Time, 2019-01-01T17:06:16, Epoch: 13, Batch: 180, Training Loss: 0.06935155019164085, LR: 0.010000000000000002
Time, 2019-01-01T17:06:17, Epoch: 13, Batch: 190, Training Loss: 0.045248907059431076, LR: 0.010000000000000002
Time, 2019-01-01T17:06:18, Epoch: 13, Batch: 200, Training Loss: 0.05967235937714577, LR: 0.010000000000000002
Time, 2019-01-01T17:06:19, Epoch: 13, Batch: 210, Training Loss: 0.10789447948336602, LR: 0.010000000000000002
Time, 2019-01-01T17:06:19, Epoch: 13, Batch: 220, Training Loss: 0.06593418717384339, LR: 0.010000000000000002
Time, 2019-01-01T17:06:20, Epoch: 13, Batch: 230, Training Loss: 0.08317309021949768, LR: 0.010000000000000002
Time, 2019-01-01T17:06:21, Epoch: 13, Batch: 240, Training Loss: 0.08218561746180057, LR: 0.010000000000000002
Time, 2019-01-01T17:06:21, Epoch: 13, Batch: 250, Training Loss: 0.04749964140355587, LR: 0.010000000000000002
Time, 2019-01-01T17:06:22, Epoch: 13, Batch: 260, Training Loss: 0.07149589210748672, LR: 0.010000000000000002
Time, 2019-01-01T17:06:23, Epoch: 13, Batch: 270, Training Loss: 0.06284872479736806, LR: 0.010000000000000002
Time, 2019-01-01T17:06:24, Epoch: 13, Batch: 280, Training Loss: 0.13642439022660255, LR: 0.010000000000000002
Time, 2019-01-01T17:06:24, Epoch: 13, Batch: 290, Training Loss: 0.05251845046877861, LR: 0.010000000000000002
Time, 2019-01-01T17:06:25, Epoch: 13, Batch: 300, Training Loss: 0.07481633760035038, LR: 0.010000000000000002
Time, 2019-01-01T17:06:26, Epoch: 13, Batch: 310, Training Loss: 0.053764750435948375, LR: 0.010000000000000002
Time, 2019-01-01T17:06:27, Epoch: 13, Batch: 320, Training Loss: 0.04548302888870239, LR: 0.010000000000000002
Time, 2019-01-01T17:06:27, Epoch: 13, Batch: 330, Training Loss: 0.09267169013619422, LR: 0.010000000000000002
Time, 2019-01-01T17:06:28, Epoch: 13, Batch: 340, Training Loss: 0.10402473360300064, LR: 0.010000000000000002
Time, 2019-01-01T17:06:29, Epoch: 13, Batch: 350, Training Loss: 0.09021898023784161, LR: 0.010000000000000002
Time, 2019-01-01T17:06:30, Epoch: 13, Batch: 360, Training Loss: 0.07219484746456147, LR: 0.010000000000000002
Time, 2019-01-01T17:06:30, Epoch: 13, Batch: 370, Training Loss: 0.06984067559242249, LR: 0.010000000000000002
Time, 2019-01-01T17:06:31, Epoch: 13, Batch: 380, Training Loss: 0.053304724395275116, LR: 0.010000000000000002
Time, 2019-01-01T17:06:32, Epoch: 13, Batch: 390, Training Loss: 0.08758789598941803, LR: 0.010000000000000002
Time, 2019-01-01T17:06:32, Epoch: 13, Batch: 400, Training Loss: 0.05613429509103298, LR: 0.010000000000000002
Time, 2019-01-01T17:06:33, Epoch: 13, Batch: 410, Training Loss: 0.05634738728404045, LR: 0.010000000000000002
Time, 2019-01-01T17:06:34, Epoch: 13, Batch: 420, Training Loss: 0.08631783872842788, LR: 0.010000000000000002
Time, 2019-01-01T17:06:35, Epoch: 13, Batch: 430, Training Loss: 0.0705506831407547, LR: 0.010000000000000002
Time, 2019-01-01T17:06:35, Epoch: 13, Batch: 440, Training Loss: 0.06509518288075924, LR: 0.010000000000000002
Time, 2019-01-01T17:06:36, Epoch: 13, Batch: 450, Training Loss: 0.06487985104322433, LR: 0.010000000000000002
Time, 2019-01-01T17:06:37, Epoch: 13, Batch: 460, Training Loss: 0.05769093930721283, LR: 0.010000000000000002
Time, 2019-01-01T17:06:38, Epoch: 13, Batch: 470, Training Loss: 0.05510767363011837, LR: 0.010000000000000002
Time, 2019-01-01T17:06:38, Epoch: 13, Batch: 480, Training Loss: 0.05315387360751629, LR: 0.010000000000000002
Time, 2019-01-01T17:06:39, Epoch: 13, Batch: 490, Training Loss: 0.04464161843061447, LR: 0.010000000000000002
Time, 2019-01-01T17:06:40, Epoch: 13, Batch: 500, Training Loss: 0.08033451363444329, LR: 0.010000000000000002
Time, 2019-01-01T17:06:41, Epoch: 13, Batch: 510, Training Loss: 0.06850238293409347, LR: 0.010000000000000002
Time, 2019-01-01T17:06:41, Epoch: 13, Batch: 520, Training Loss: 0.05574707314372063, LR: 0.010000000000000002
Time, 2019-01-01T17:06:42, Epoch: 13, Batch: 530, Training Loss: 0.048246562480926514, LR: 0.010000000000000002
Time, 2019-01-01T17:06:43, Epoch: 13, Batch: 540, Training Loss: 0.05704369954764843, LR: 0.010000000000000002
Time, 2019-01-01T17:06:44, Epoch: 13, Batch: 550, Training Loss: 0.0802477739751339, LR: 0.010000000000000002
Time, 2019-01-01T17:06:44, Epoch: 13, Batch: 560, Training Loss: 0.07292067930102349, LR: 0.010000000000000002
Time, 2019-01-01T17:06:45, Epoch: 13, Batch: 570, Training Loss: 0.05616683289408684, LR: 0.010000000000000002
Time, 2019-01-01T17:06:46, Epoch: 13, Batch: 580, Training Loss: 0.07373236231505871, LR: 0.010000000000000002
Time, 2019-01-01T17:06:47, Epoch: 13, Batch: 590, Training Loss: 0.0506760835647583, LR: 0.010000000000000002
Time, 2019-01-01T17:06:47, Epoch: 13, Batch: 600, Training Loss: 0.0761252798140049, LR: 0.010000000000000002
Time, 2019-01-01T17:06:48, Epoch: 13, Batch: 610, Training Loss: 0.07576197572052479, LR: 0.010000000000000002
Time, 2019-01-01T17:06:49, Epoch: 13, Batch: 620, Training Loss: 0.07811053916811943, LR: 0.010000000000000002
Time, 2019-01-01T17:06:50, Epoch: 13, Batch: 630, Training Loss: 0.04815607890486717, LR: 0.010000000000000002
Time, 2019-01-01T17:06:50, Epoch: 13, Batch: 640, Training Loss: 0.06569348871707917, LR: 0.010000000000000002
Time, 2019-01-01T17:06:51, Epoch: 13, Batch: 650, Training Loss: 0.08605775013566017, LR: 0.010000000000000002
Time, 2019-01-01T17:06:52, Epoch: 13, Batch: 660, Training Loss: 0.08369020335376262, LR: 0.010000000000000002
Time, 2019-01-01T17:06:53, Epoch: 13, Batch: 670, Training Loss: 0.06653580479323865, LR: 0.010000000000000002
Time, 2019-01-01T17:06:53, Epoch: 13, Batch: 680, Training Loss: 0.06395832821726799, LR: 0.010000000000000002
Time, 2019-01-01T17:06:54, Epoch: 13, Batch: 690, Training Loss: 0.062207947671413424, LR: 0.010000000000000002
Time, 2019-01-01T17:06:55, Epoch: 13, Batch: 700, Training Loss: 0.07163549736142158, LR: 0.010000000000000002
Time, 2019-01-01T17:06:56, Epoch: 13, Batch: 710, Training Loss: 0.06380213499069214, LR: 0.010000000000000002
Time, 2019-01-01T17:06:56, Epoch: 13, Batch: 720, Training Loss: 0.0677385251969099, LR: 0.010000000000000002
Time, 2019-01-01T17:06:57, Epoch: 13, Batch: 730, Training Loss: 0.07895997278392315, LR: 0.010000000000000002
Time, 2019-01-01T17:06:58, Epoch: 13, Batch: 740, Training Loss: 0.05513583235442639, LR: 0.010000000000000002
Time, 2019-01-01T17:06:59, Epoch: 13, Batch: 750, Training Loss: 0.10023291036486626, LR: 0.010000000000000002
Time, 2019-01-01T17:06:59, Epoch: 13, Batch: 760, Training Loss: 0.0857808280736208, LR: 0.010000000000000002
Time, 2019-01-01T17:07:00, Epoch: 13, Batch: 770, Training Loss: 0.06009349077939987, LR: 0.010000000000000002
Time, 2019-01-01T17:07:01, Epoch: 13, Batch: 780, Training Loss: 0.07513742856681346, LR: 0.010000000000000002
Time, 2019-01-01T17:07:01, Epoch: 13, Batch: 790, Training Loss: 0.07699104025959969, LR: 0.010000000000000002
Time, 2019-01-01T17:07:02, Epoch: 13, Batch: 800, Training Loss: 0.07711594887077808, LR: 0.010000000000000002
Time, 2019-01-01T17:07:03, Epoch: 13, Batch: 810, Training Loss: 0.0801109466701746, LR: 0.010000000000000002
Time, 2019-01-01T17:07:04, Epoch: 13, Batch: 820, Training Loss: 0.07436003163456917, LR: 0.010000000000000002
Time, 2019-01-01T17:07:04, Epoch: 13, Batch: 830, Training Loss: 0.06788143962621689, LR: 0.010000000000000002
Time, 2019-01-01T17:07:05, Epoch: 13, Batch: 840, Training Loss: 0.0666485596448183, LR: 0.010000000000000002
Time, 2019-01-01T17:07:06, Epoch: 13, Batch: 850, Training Loss: 0.04005690738558769, LR: 0.010000000000000002
Time, 2019-01-01T17:07:07, Epoch: 13, Batch: 860, Training Loss: 0.05313263386487961, LR: 0.010000000000000002
Time, 2019-01-01T17:07:07, Epoch: 13, Batch: 870, Training Loss: 0.05597534440457821, LR: 0.010000000000000002
Time, 2019-01-01T17:07:08, Epoch: 13, Batch: 880, Training Loss: 0.07904492653906345, LR: 0.010000000000000002
Time, 2019-01-01T17:07:09, Epoch: 13, Batch: 890, Training Loss: 0.05888357385993004, LR: 0.010000000000000002
Time, 2019-01-01T17:07:10, Epoch: 13, Batch: 900, Training Loss: 0.07312913089990616, LR: 0.010000000000000002
Time, 2019-01-01T17:07:10, Epoch: 13, Batch: 910, Training Loss: 0.06665899232029915, LR: 0.010000000000000002
Time, 2019-01-01T17:07:11, Epoch: 13, Batch: 920, Training Loss: 0.06610017195343972, LR: 0.010000000000000002
Time, 2019-01-01T17:07:12, Epoch: 13, Batch: 930, Training Loss: 0.06608488522469998, LR: 0.010000000000000002
Epoch: 13, Validation Top 1 acc: 98.03937530517578
Epoch: 13, Validation Top 5 acc: 99.96668243408203
Epoch: 13, Validation Set Loss: 0.06454531848430634
Start training epoch 14
Time, 2019-01-01T17:07:38, Epoch: 14, Batch: 10, Training Loss: 0.08012247011065483, LR: 0.010000000000000002
Time, 2019-01-01T17:07:39, Epoch: 14, Batch: 20, Training Loss: 0.08440546095371246, LR: 0.010000000000000002
Time, 2019-01-01T17:07:39, Epoch: 14, Batch: 30, Training Loss: 0.06828808300197124, LR: 0.010000000000000002
Time, 2019-01-01T17:07:40, Epoch: 14, Batch: 40, Training Loss: 0.06358222402632237, LR: 0.010000000000000002
Time, 2019-01-01T17:07:41, Epoch: 14, Batch: 50, Training Loss: 0.05989662297070027, LR: 0.010000000000000002
Time, 2019-01-01T17:07:42, Epoch: 14, Batch: 60, Training Loss: 0.061383187770843506, LR: 0.010000000000000002
Time, 2019-01-01T17:07:42, Epoch: 14, Batch: 70, Training Loss: 0.06783497631549835, LR: 0.010000000000000002
Time, 2019-01-01T17:07:43, Epoch: 14, Batch: 80, Training Loss: 0.055995237082242966, LR: 0.010000000000000002
Time, 2019-01-01T17:07:44, Epoch: 14, Batch: 90, Training Loss: 0.0852359127253294, LR: 0.010000000000000002
Time, 2019-01-01T17:07:45, Epoch: 14, Batch: 100, Training Loss: 0.09315881952643394, LR: 0.010000000000000002
Time, 2019-01-01T17:07:45, Epoch: 14, Batch: 110, Training Loss: 0.07010123580694198, LR: 0.010000000000000002
Time, 2019-01-01T17:07:46, Epoch: 14, Batch: 120, Training Loss: 0.05340303629636765, LR: 0.010000000000000002
Time, 2019-01-01T17:07:47, Epoch: 14, Batch: 130, Training Loss: 0.06480797976255417, LR: 0.010000000000000002
Time, 2019-01-01T17:07:48, Epoch: 14, Batch: 140, Training Loss: 0.06904737092554569, LR: 0.010000000000000002
Time, 2019-01-01T17:07:48, Epoch: 14, Batch: 150, Training Loss: 0.0728397510945797, LR: 0.010000000000000002
Time, 2019-01-01T17:07:49, Epoch: 14, Batch: 160, Training Loss: 0.05491488724946976, LR: 0.010000000000000002
Time, 2019-01-01T17:07:50, Epoch: 14, Batch: 170, Training Loss: 0.08179450184106826, LR: 0.010000000000000002
Time, 2019-01-01T17:07:51, Epoch: 14, Batch: 180, Training Loss: 0.06223518364131451, LR: 0.010000000000000002
Time, 2019-01-01T17:07:51, Epoch: 14, Batch: 190, Training Loss: 0.07029125615954399, LR: 0.010000000000000002
Time, 2019-01-01T17:07:52, Epoch: 14, Batch: 200, Training Loss: 0.0729323249310255, LR: 0.010000000000000002
Time, 2019-01-01T17:07:53, Epoch: 14, Batch: 210, Training Loss: 0.0424030777066946, LR: 0.010000000000000002
Time, 2019-01-01T17:07:54, Epoch: 14, Batch: 220, Training Loss: 0.07043773680925369, LR: 0.010000000000000002
Time, 2019-01-01T17:07:54, Epoch: 14, Batch: 230, Training Loss: 0.07277147248387336, LR: 0.010000000000000002
Time, 2019-01-01T17:07:55, Epoch: 14, Batch: 240, Training Loss: 0.06036509796977043, LR: 0.010000000000000002
Time, 2019-01-01T17:07:56, Epoch: 14, Batch: 250, Training Loss: 0.06230865232646465, LR: 0.010000000000000002
Time, 2019-01-01T17:07:56, Epoch: 14, Batch: 260, Training Loss: 0.058069797977805135, LR: 0.010000000000000002
Time, 2019-01-01T17:07:57, Epoch: 14, Batch: 270, Training Loss: 0.08326253667473793, LR: 0.010000000000000002
Time, 2019-01-01T17:07:58, Epoch: 14, Batch: 280, Training Loss: 0.0624581977725029, LR: 0.010000000000000002
Time, 2019-01-01T17:07:59, Epoch: 14, Batch: 290, Training Loss: 0.0636975608766079, LR: 0.010000000000000002
Time, 2019-01-01T17:07:59, Epoch: 14, Batch: 300, Training Loss: 0.06637531518936157, LR: 0.010000000000000002
Time, 2019-01-01T17:08:00, Epoch: 14, Batch: 310, Training Loss: 0.046307622268795964, LR: 0.010000000000000002
Time, 2019-01-01T17:08:01, Epoch: 14, Batch: 320, Training Loss: 0.07835834249854087, LR: 0.010000000000000002
Time, 2019-01-01T17:08:02, Epoch: 14, Batch: 330, Training Loss: 0.04236460588872433, LR: 0.010000000000000002
Time, 2019-01-01T17:08:02, Epoch: 14, Batch: 340, Training Loss: 0.062251991033554076, LR: 0.010000000000000002
Time, 2019-01-01T17:08:03, Epoch: 14, Batch: 350, Training Loss: 0.06577030718326568, LR: 0.010000000000000002
Time, 2019-01-01T17:08:04, Epoch: 14, Batch: 360, Training Loss: 0.08978797048330307, LR: 0.010000000000000002
Time, 2019-01-01T17:08:05, Epoch: 14, Batch: 370, Training Loss: 0.06618795916438103, LR: 0.010000000000000002
Time, 2019-01-01T17:08:05, Epoch: 14, Batch: 380, Training Loss: 0.06529308408498764, LR: 0.010000000000000002
Time, 2019-01-01T17:08:06, Epoch: 14, Batch: 390, Training Loss: 0.053115914762020114, LR: 0.010000000000000002
Time, 2019-01-01T17:08:07, Epoch: 14, Batch: 400, Training Loss: 0.06808950603008271, LR: 0.010000000000000002
Time, 2019-01-01T17:08:08, Epoch: 14, Batch: 410, Training Loss: 0.07010969594120979, LR: 0.010000000000000002
Time, 2019-01-01T17:08:08, Epoch: 14, Batch: 420, Training Loss: 0.07780629843473434, LR: 0.010000000000000002
Time, 2019-01-01T17:08:09, Epoch: 14, Batch: 430, Training Loss: 0.06001981385052204, LR: 0.010000000000000002
Time, 2019-01-01T17:08:10, Epoch: 14, Batch: 440, Training Loss: 0.06368640586733817, LR: 0.010000000000000002
Time, 2019-01-01T17:08:10, Epoch: 14, Batch: 450, Training Loss: 0.08108386248350144, LR: 0.010000000000000002
Time, 2019-01-01T17:08:11, Epoch: 14, Batch: 460, Training Loss: 0.07785012796521187, LR: 0.010000000000000002
Time, 2019-01-01T17:08:12, Epoch: 14, Batch: 470, Training Loss: 0.051263543963432315, LR: 0.010000000000000002
Time, 2019-01-01T17:08:13, Epoch: 14, Batch: 480, Training Loss: 0.05591743141412735, LR: 0.010000000000000002
Time, 2019-01-01T17:08:13, Epoch: 14, Batch: 490, Training Loss: 0.0575908325612545, LR: 0.010000000000000002
Time, 2019-01-01T17:08:14, Epoch: 14, Batch: 500, Training Loss: 0.05379813648760319, LR: 0.010000000000000002
Time, 2019-01-01T17:08:15, Epoch: 14, Batch: 510, Training Loss: 0.06639336422085762, LR: 0.010000000000000002
Time, 2019-01-01T17:08:16, Epoch: 14, Batch: 520, Training Loss: 0.07884727753698825, LR: 0.010000000000000002
Time, 2019-01-01T17:08:16, Epoch: 14, Batch: 530, Training Loss: 0.05336984321475029, LR: 0.010000000000000002
Time, 2019-01-01T17:08:17, Epoch: 14, Batch: 540, Training Loss: 0.11126359924674034, LR: 0.010000000000000002
Time, 2019-01-01T17:08:18, Epoch: 14, Batch: 550, Training Loss: 0.0695436529815197, LR: 0.010000000000000002
Time, 2019-01-01T17:08:19, Epoch: 14, Batch: 560, Training Loss: 0.08036663010716438, LR: 0.010000000000000002
Time, 2019-01-01T17:08:19, Epoch: 14, Batch: 570, Training Loss: 0.07195233330130577, LR: 0.010000000000000002
Time, 2019-01-01T17:08:20, Epoch: 14, Batch: 580, Training Loss: 0.07042158916592597, LR: 0.010000000000000002
Time, 2019-01-01T17:08:21, Epoch: 14, Batch: 590, Training Loss: 0.08057625144720078, LR: 0.010000000000000002
Time, 2019-01-01T17:08:21, Epoch: 14, Batch: 600, Training Loss: 0.07693197540938854, LR: 0.010000000000000002
Time, 2019-01-01T17:08:22, Epoch: 14, Batch: 610, Training Loss: 0.047121232375502586, LR: 0.010000000000000002
Time, 2019-01-01T17:08:23, Epoch: 14, Batch: 620, Training Loss: 0.060382019728422165, LR: 0.010000000000000002
Time, 2019-01-01T17:08:24, Epoch: 14, Batch: 630, Training Loss: 0.06464858837425709, LR: 0.010000000000000002
Time, 2019-01-01T17:08:24, Epoch: 14, Batch: 640, Training Loss: 0.07939349412918091, LR: 0.010000000000000002
Time, 2019-01-01T17:08:25, Epoch: 14, Batch: 650, Training Loss: 0.07765156477689743, LR: 0.010000000000000002
Time, 2019-01-01T17:08:26, Epoch: 14, Batch: 660, Training Loss: 0.06740062348544598, LR: 0.010000000000000002
Time, 2019-01-01T17:08:27, Epoch: 14, Batch: 670, Training Loss: 0.07419361844658852, LR: 0.010000000000000002
Time, 2019-01-01T17:08:27, Epoch: 14, Batch: 680, Training Loss: 0.1001258485019207, LR: 0.010000000000000002
Time, 2019-01-01T17:08:28, Epoch: 14, Batch: 690, Training Loss: 0.07369261905550957, LR: 0.010000000000000002
Time, 2019-01-01T17:08:29, Epoch: 14, Batch: 700, Training Loss: 0.08091119900345803, LR: 0.010000000000000002
Time, 2019-01-01T17:08:30, Epoch: 14, Batch: 710, Training Loss: 0.07804775200784206, LR: 0.010000000000000002
Time, 2019-01-01T17:08:30, Epoch: 14, Batch: 720, Training Loss: 0.044983351975679396, LR: 0.010000000000000002
Time, 2019-01-01T17:08:31, Epoch: 14, Batch: 730, Training Loss: 0.06778287738561631, LR: 0.010000000000000002
Time, 2019-01-01T17:08:32, Epoch: 14, Batch: 740, Training Loss: 0.05484540164470673, LR: 0.010000000000000002
Time, 2019-01-01T17:08:33, Epoch: 14, Batch: 750, Training Loss: 0.07237212844192982, LR: 0.010000000000000002
Time, 2019-01-01T17:08:33, Epoch: 14, Batch: 760, Training Loss: 0.06203417591750622, LR: 0.010000000000000002
Time, 2019-01-01T17:08:34, Epoch: 14, Batch: 770, Training Loss: 0.058194400370121004, LR: 0.010000000000000002
Time, 2019-01-01T17:08:35, Epoch: 14, Batch: 780, Training Loss: 0.06715835072100163, LR: 0.010000000000000002
Time, 2019-01-01T17:08:35, Epoch: 14, Batch: 790, Training Loss: 0.057809940353035924, LR: 0.010000000000000002
Time, 2019-01-01T17:08:36, Epoch: 14, Batch: 800, Training Loss: 0.06366842612624168, LR: 0.010000000000000002
Time, 2019-01-01T17:08:37, Epoch: 14, Batch: 810, Training Loss: 0.05332852862775326, LR: 0.010000000000000002
Time, 2019-01-01T17:08:38, Epoch: 14, Batch: 820, Training Loss: 0.08933272324502468, LR: 0.010000000000000002
Time, 2019-01-01T17:08:38, Epoch: 14, Batch: 830, Training Loss: 0.06219529993832111, LR: 0.010000000000000002
Time, 2019-01-01T17:08:39, Epoch: 14, Batch: 840, Training Loss: 0.07003482803702354, LR: 0.010000000000000002
Time, 2019-01-01T17:08:40, Epoch: 14, Batch: 850, Training Loss: 0.05255272462964058, LR: 0.010000000000000002
Time, 2019-01-01T17:08:41, Epoch: 14, Batch: 860, Training Loss: 0.07421362176537513, LR: 0.010000000000000002
Time, 2019-01-01T17:08:41, Epoch: 14, Batch: 870, Training Loss: 0.06657808795571327, LR: 0.010000000000000002
Time, 2019-01-01T17:08:42, Epoch: 14, Batch: 880, Training Loss: 0.09323114566504956, LR: 0.010000000000000002
Time, 2019-01-01T17:08:43, Epoch: 14, Batch: 890, Training Loss: 0.07365235462784767, LR: 0.010000000000000002
Time, 2019-01-01T17:08:44, Epoch: 14, Batch: 900, Training Loss: 0.04570074081420898, LR: 0.010000000000000002
Time, 2019-01-01T17:08:44, Epoch: 14, Batch: 910, Training Loss: 0.05064025074243546, LR: 0.010000000000000002
Time, 2019-01-01T17:08:45, Epoch: 14, Batch: 920, Training Loss: 0.07337280474603176, LR: 0.010000000000000002
Time, 2019-01-01T17:08:46, Epoch: 14, Batch: 930, Training Loss: 0.07323403358459472, LR: 0.010000000000000002
Epoch: 14, Validation Top 1 acc: 97.94442749023438
Epoch: 14, Validation Top 5 acc: 99.96835327148438
Epoch: 14, Validation Set Loss: 0.06786786019802094
Start training epoch 15
Time, 2019-01-01T17:09:12, Epoch: 15, Batch: 10, Training Loss: 0.04867914840579033, LR: 0.010000000000000002
Time, 2019-01-01T17:09:13, Epoch: 15, Batch: 20, Training Loss: 0.05061609148979187, LR: 0.010000000000000002
Time, 2019-01-01T17:09:14, Epoch: 15, Batch: 30, Training Loss: 0.059449725225567816, LR: 0.010000000000000002
Time, 2019-01-01T17:09:15, Epoch: 15, Batch: 40, Training Loss: 0.0838445171713829, LR: 0.010000000000000002
Time, 2019-01-01T17:09:15, Epoch: 15, Batch: 50, Training Loss: 0.08068983629345894, LR: 0.010000000000000002
Time, 2019-01-01T17:09:16, Epoch: 15, Batch: 60, Training Loss: 0.06900030560791492, LR: 0.010000000000000002
Time, 2019-01-01T17:09:17, Epoch: 15, Batch: 70, Training Loss: 0.0658020794391632, LR: 0.010000000000000002
Time, 2019-01-01T17:09:17, Epoch: 15, Batch: 80, Training Loss: 0.06039729975163936, LR: 0.010000000000000002
Time, 2019-01-01T17:09:18, Epoch: 15, Batch: 90, Training Loss: 0.056122060492634775, LR: 0.010000000000000002
Time, 2019-01-01T17:09:19, Epoch: 15, Batch: 100, Training Loss: 0.05484281368553638, LR: 0.010000000000000002
Time, 2019-01-01T17:09:20, Epoch: 15, Batch: 110, Training Loss: 0.07012477591633796, LR: 0.010000000000000002
Time, 2019-01-01T17:09:20, Epoch: 15, Batch: 120, Training Loss: 0.07033331878483295, LR: 0.010000000000000002
Time, 2019-01-01T17:09:21, Epoch: 15, Batch: 130, Training Loss: 0.0608659952878952, LR: 0.010000000000000002
Time, 2019-01-01T17:09:22, Epoch: 15, Batch: 140, Training Loss: 0.0670653335750103, LR: 0.010000000000000002
Time, 2019-01-01T17:09:23, Epoch: 15, Batch: 150, Training Loss: 0.05543668493628502, LR: 0.010000000000000002
Time, 2019-01-01T17:09:23, Epoch: 15, Batch: 160, Training Loss: 0.06483833938837051, LR: 0.010000000000000002
Time, 2019-01-01T17:09:24, Epoch: 15, Batch: 170, Training Loss: 0.06572317257523537, LR: 0.010000000000000002
Time, 2019-01-01T17:09:25, Epoch: 15, Batch: 180, Training Loss: 0.05523058846592903, LR: 0.010000000000000002
Time, 2019-01-01T17:09:25, Epoch: 15, Batch: 190, Training Loss: 0.08391037732362747, LR: 0.010000000000000002
Time, 2019-01-01T17:09:26, Epoch: 15, Batch: 200, Training Loss: 0.08673378527164459, LR: 0.010000000000000002
Time, 2019-01-01T17:09:27, Epoch: 15, Batch: 210, Training Loss: 0.04976372756063938, LR: 0.010000000000000002
Time, 2019-01-01T17:09:28, Epoch: 15, Batch: 220, Training Loss: 0.05534825474023819, LR: 0.010000000000000002
Time, 2019-01-01T17:09:28, Epoch: 15, Batch: 230, Training Loss: 0.08677255511283874, LR: 0.010000000000000002
Time, 2019-01-01T17:09:29, Epoch: 15, Batch: 240, Training Loss: 0.05075377598404884, LR: 0.010000000000000002
Time, 2019-01-01T17:09:30, Epoch: 15, Batch: 250, Training Loss: 0.05213218592107296, LR: 0.010000000000000002
Time, 2019-01-01T17:09:31, Epoch: 15, Batch: 260, Training Loss: 0.06952409148216247, LR: 0.010000000000000002
Time, 2019-01-01T17:09:31, Epoch: 15, Batch: 270, Training Loss: 0.0720161534845829, LR: 0.010000000000000002
Time, 2019-01-01T17:09:32, Epoch: 15, Batch: 280, Training Loss: 0.07789947614073753, LR: 0.010000000000000002
Time, 2019-01-01T17:09:33, Epoch: 15, Batch: 290, Training Loss: 0.06000918820500374, LR: 0.010000000000000002
Time, 2019-01-01T17:09:34, Epoch: 15, Batch: 300, Training Loss: 0.05123537294566631, LR: 0.010000000000000002
Time, 2019-01-01T17:09:34, Epoch: 15, Batch: 310, Training Loss: 0.09879291839897633, LR: 0.010000000000000002
Time, 2019-01-01T17:09:35, Epoch: 15, Batch: 320, Training Loss: 0.07878157086670398, LR: 0.010000000000000002
Time, 2019-01-01T17:09:36, Epoch: 15, Batch: 330, Training Loss: 0.059556324407458304, LR: 0.010000000000000002
Time, 2019-01-01T17:09:36, Epoch: 15, Batch: 340, Training Loss: 0.0753312200307846, LR: 0.010000000000000002
Time, 2019-01-01T17:09:37, Epoch: 15, Batch: 350, Training Loss: 0.06283684186637402, LR: 0.010000000000000002
Time, 2019-01-01T17:09:38, Epoch: 15, Batch: 360, Training Loss: 0.07635757811367512, LR: 0.010000000000000002
Time, 2019-01-01T17:09:39, Epoch: 15, Batch: 370, Training Loss: 0.08309874907135964, LR: 0.010000000000000002
Time, 2019-01-01T17:09:39, Epoch: 15, Batch: 380, Training Loss: 0.06444069482386112, LR: 0.010000000000000002
Time, 2019-01-01T17:09:40, Epoch: 15, Batch: 390, Training Loss: 0.060080951824784276, LR: 0.010000000000000002
Time, 2019-01-01T17:09:41, Epoch: 15, Batch: 400, Training Loss: 0.07806395031511784, LR: 0.010000000000000002
Time, 2019-01-01T17:09:42, Epoch: 15, Batch: 410, Training Loss: 0.06953702755272388, LR: 0.010000000000000002
Time, 2019-01-01T17:09:42, Epoch: 15, Batch: 420, Training Loss: 0.03402283601462841, LR: 0.010000000000000002
Time, 2019-01-01T17:09:43, Epoch: 15, Batch: 430, Training Loss: 0.07280776426196098, LR: 0.010000000000000002
Time, 2019-01-01T17:09:44, Epoch: 15, Batch: 440, Training Loss: 0.07618863061070442, LR: 0.010000000000000002
Time, 2019-01-01T17:09:45, Epoch: 15, Batch: 450, Training Loss: 0.059126338362693785, LR: 0.010000000000000002
Time, 2019-01-01T17:09:45, Epoch: 15, Batch: 460, Training Loss: 0.07513870038092137, LR: 0.010000000000000002
Time, 2019-01-01T17:09:46, Epoch: 15, Batch: 470, Training Loss: 0.07705276608467101, LR: 0.010000000000000002
Time, 2019-01-01T17:09:47, Epoch: 15, Batch: 480, Training Loss: 0.09030285738408565, LR: 0.010000000000000002
Time, 2019-01-01T17:09:48, Epoch: 15, Batch: 490, Training Loss: 0.05700535885989666, LR: 0.010000000000000002
Time, 2019-01-01T17:09:49, Epoch: 15, Batch: 500, Training Loss: 0.07861257903277874, LR: 0.010000000000000002
Time, 2019-01-01T17:09:49, Epoch: 15, Batch: 510, Training Loss: 0.09253019094467163, LR: 0.010000000000000002
Time, 2019-01-01T17:09:50, Epoch: 15, Batch: 520, Training Loss: 0.06754333674907684, LR: 0.010000000000000002
Time, 2019-01-01T17:09:51, Epoch: 15, Batch: 530, Training Loss: 0.0668688178062439, LR: 0.010000000000000002
Time, 2019-01-01T17:09:52, Epoch: 15, Batch: 540, Training Loss: 0.0553124375641346, LR: 0.010000000000000002
Time, 2019-01-01T17:09:52, Epoch: 15, Batch: 550, Training Loss: 0.08073655776679516, LR: 0.010000000000000002
Time, 2019-01-01T17:09:53, Epoch: 15, Batch: 560, Training Loss: 0.05485835671424866, LR: 0.010000000000000002
Time, 2019-01-01T17:09:54, Epoch: 15, Batch: 570, Training Loss: 0.05752904787659645, LR: 0.010000000000000002
Time, 2019-01-01T17:09:55, Epoch: 15, Batch: 580, Training Loss: 0.08843418359756469, LR: 0.010000000000000002
Time, 2019-01-01T17:09:55, Epoch: 15, Batch: 590, Training Loss: 0.06584390178322792, LR: 0.010000000000000002
Time, 2019-01-01T17:09:56, Epoch: 15, Batch: 600, Training Loss: 0.05248270854353905, LR: 0.010000000000000002
Time, 2019-01-01T17:09:57, Epoch: 15, Batch: 610, Training Loss: 0.05482156313955784, LR: 0.010000000000000002
Time, 2019-01-01T17:09:57, Epoch: 15, Batch: 620, Training Loss: 0.06947075724601745, LR: 0.010000000000000002
Time, 2019-01-01T17:09:58, Epoch: 15, Batch: 630, Training Loss: 0.05924630090594292, LR: 0.010000000000000002
Time, 2019-01-01T17:09:59, Epoch: 15, Batch: 640, Training Loss: 0.07454791851341724, LR: 0.010000000000000002
Time, 2019-01-01T17:10:00, Epoch: 15, Batch: 650, Training Loss: 0.0617772065103054, LR: 0.010000000000000002
Time, 2019-01-01T17:10:00, Epoch: 15, Batch: 660, Training Loss: 0.06159529872238636, LR: 0.010000000000000002
Time, 2019-01-01T17:10:01, Epoch: 15, Batch: 670, Training Loss: 0.044082356989383696, LR: 0.010000000000000002
Time, 2019-01-01T17:10:02, Epoch: 15, Batch: 680, Training Loss: 0.05400733351707458, LR: 0.010000000000000002
Time, 2019-01-01T17:10:03, Epoch: 15, Batch: 690, Training Loss: 0.05980961509048939, LR: 0.010000000000000002
Time, 2019-01-01T17:10:03, Epoch: 15, Batch: 700, Training Loss: 0.0549003254622221, LR: 0.010000000000000002
Time, 2019-01-01T17:10:04, Epoch: 15, Batch: 710, Training Loss: 0.06723589450120926, LR: 0.010000000000000002
Time, 2019-01-01T17:10:05, Epoch: 15, Batch: 720, Training Loss: 0.07011754512786865, LR: 0.010000000000000002
Time, 2019-01-01T17:10:06, Epoch: 15, Batch: 730, Training Loss: 0.054524002596735954, LR: 0.010000000000000002
Time, 2019-01-01T17:10:06, Epoch: 15, Batch: 740, Training Loss: 0.08690616637468337, LR: 0.010000000000000002
Time, 2019-01-01T17:10:07, Epoch: 15, Batch: 750, Training Loss: 0.06793909519910812, LR: 0.010000000000000002
Time, 2019-01-01T17:10:08, Epoch: 15, Batch: 760, Training Loss: 0.060959001630544664, LR: 0.010000000000000002
Time, 2019-01-01T17:10:09, Epoch: 15, Batch: 770, Training Loss: 0.03835342116653919, LR: 0.010000000000000002
Time, 2019-01-01T17:10:09, Epoch: 15, Batch: 780, Training Loss: 0.08281825818121433, LR: 0.010000000000000002
Time, 2019-01-01T17:10:10, Epoch: 15, Batch: 790, Training Loss: 0.06728127673268318, LR: 0.010000000000000002
Time, 2019-01-01T17:10:11, Epoch: 15, Batch: 800, Training Loss: 0.06945769041776657, LR: 0.010000000000000002
Time, 2019-01-01T17:10:12, Epoch: 15, Batch: 810, Training Loss: 0.07200447916984558, LR: 0.010000000000000002
Time, 2019-01-01T17:10:12, Epoch: 15, Batch: 820, Training Loss: 0.07002859264612198, LR: 0.010000000000000002
Time, 2019-01-01T17:10:13, Epoch: 15, Batch: 830, Training Loss: 0.05443641468882561, LR: 0.010000000000000002
Time, 2019-01-01T17:10:14, Epoch: 15, Batch: 840, Training Loss: 0.0931974209845066, LR: 0.010000000000000002
Time, 2019-01-01T17:10:14, Epoch: 15, Batch: 850, Training Loss: 0.0642901562154293, LR: 0.010000000000000002
Time, 2019-01-01T17:10:15, Epoch: 15, Batch: 860, Training Loss: 0.05508308559656143, LR: 0.010000000000000002
Time, 2019-01-01T17:10:16, Epoch: 15, Batch: 870, Training Loss: 0.0578306820243597, LR: 0.010000000000000002
Time, 2019-01-01T17:10:17, Epoch: 15, Batch: 880, Training Loss: 0.0493579488247633, LR: 0.010000000000000002
Time, 2019-01-01T17:10:17, Epoch: 15, Batch: 890, Training Loss: 0.06048011593520641, LR: 0.010000000000000002
Time, 2019-01-01T17:10:18, Epoch: 15, Batch: 900, Training Loss: 0.05364269651472568, LR: 0.010000000000000002
Time, 2019-01-01T17:10:19, Epoch: 15, Batch: 910, Training Loss: 0.07124178819358348, LR: 0.010000000000000002
Time, 2019-01-01T17:10:20, Epoch: 15, Batch: 920, Training Loss: 0.07421262226998807, LR: 0.010000000000000002
Time, 2019-01-01T17:10:20, Epoch: 15, Batch: 930, Training Loss: 0.07910695746541023, LR: 0.010000000000000002
Epoch: 15, Validation Top 1 acc: 98.0743637084961
Epoch: 15, Validation Top 5 acc: 99.97335052490234
Epoch: 15, Validation Set Loss: 0.0652434378862381
Start training epoch 16
Time, 2019-01-01T17:10:47, Epoch: 16, Batch: 10, Training Loss: 0.04906792752444744, LR: 0.010000000000000002
Time, 2019-01-01T17:10:47, Epoch: 16, Batch: 20, Training Loss: 0.0851250559091568, LR: 0.010000000000000002
Time, 2019-01-01T17:10:48, Epoch: 16, Batch: 30, Training Loss: 0.06150152459740639, LR: 0.010000000000000002
Time, 2019-01-01T17:10:49, Epoch: 16, Batch: 40, Training Loss: 0.046127143502235415, LR: 0.010000000000000002
Time, 2019-01-01T17:10:50, Epoch: 16, Batch: 50, Training Loss: 0.07146993055939674, LR: 0.010000000000000002
Time, 2019-01-01T17:10:50, Epoch: 16, Batch: 60, Training Loss: 0.05428013913333416, LR: 0.010000000000000002
Time, 2019-01-01T17:10:51, Epoch: 16, Batch: 70, Training Loss: 0.06238795593380928, LR: 0.010000000000000002
Time, 2019-01-01T17:10:52, Epoch: 16, Batch: 80, Training Loss: 0.05961519479751587, LR: 0.010000000000000002
Time, 2019-01-01T17:10:53, Epoch: 16, Batch: 90, Training Loss: 0.048585683852434156, LR: 0.010000000000000002
Time, 2019-01-01T17:10:53, Epoch: 16, Batch: 100, Training Loss: 0.047552131861448285, LR: 0.010000000000000002
Time, 2019-01-01T17:10:54, Epoch: 16, Batch: 110, Training Loss: 0.060144396498799324, LR: 0.010000000000000002
Time, 2019-01-01T17:10:55, Epoch: 16, Batch: 120, Training Loss: 0.04072602577507496, LR: 0.010000000000000002
Time, 2019-01-01T17:10:55, Epoch: 16, Batch: 130, Training Loss: 0.044884282350540164, LR: 0.010000000000000002
Time, 2019-01-01T17:10:56, Epoch: 16, Batch: 140, Training Loss: 0.08035744279623032, LR: 0.010000000000000002
Time, 2019-01-01T17:10:57, Epoch: 16, Batch: 150, Training Loss: 0.05519901886582375, LR: 0.010000000000000002
Time, 2019-01-01T17:10:58, Epoch: 16, Batch: 160, Training Loss: 0.07083738557994365, LR: 0.010000000000000002
Time, 2019-01-01T17:10:59, Epoch: 16, Batch: 170, Training Loss: 0.06706877574324607, LR: 0.010000000000000002
Time, 2019-01-01T17:10:59, Epoch: 16, Batch: 180, Training Loss: 0.06311900913715363, LR: 0.010000000000000002
Time, 2019-01-01T17:11:00, Epoch: 16, Batch: 190, Training Loss: 0.08101416639983654, LR: 0.010000000000000002
Time, 2019-01-01T17:11:01, Epoch: 16, Batch: 200, Training Loss: 0.07352130077779293, LR: 0.010000000000000002
Time, 2019-01-01T17:11:01, Epoch: 16, Batch: 210, Training Loss: 0.06851737089455127, LR: 0.010000000000000002
Time, 2019-01-01T17:11:02, Epoch: 16, Batch: 220, Training Loss: 0.07422241009771824, LR: 0.010000000000000002
Time, 2019-01-01T17:11:03, Epoch: 16, Batch: 230, Training Loss: 0.07309482842683793, LR: 0.010000000000000002
Time, 2019-01-01T17:11:04, Epoch: 16, Batch: 240, Training Loss: 0.0613978423178196, LR: 0.010000000000000002
Time, 2019-01-01T17:11:04, Epoch: 16, Batch: 250, Training Loss: 0.0893737304955721, LR: 0.010000000000000002
Time, 2019-01-01T17:11:05, Epoch: 16, Batch: 260, Training Loss: 0.06554037816822529, LR: 0.010000000000000002
Time, 2019-01-01T17:11:06, Epoch: 16, Batch: 270, Training Loss: 0.08877211511135101, LR: 0.010000000000000002
Time, 2019-01-01T17:11:07, Epoch: 16, Batch: 280, Training Loss: 0.07384414859116077, LR: 0.010000000000000002
Time, 2019-01-01T17:11:07, Epoch: 16, Batch: 290, Training Loss: 0.05280798599123955, LR: 0.010000000000000002
Time, 2019-01-01T17:11:08, Epoch: 16, Batch: 300, Training Loss: 0.07883771136403084, LR: 0.010000000000000002
Time, 2019-01-01T17:11:09, Epoch: 16, Batch: 310, Training Loss: 0.056160871684551236, LR: 0.010000000000000002
Time, 2019-01-01T17:11:10, Epoch: 16, Batch: 320, Training Loss: 0.06109151318669319, LR: 0.010000000000000002
Time, 2019-01-01T17:11:10, Epoch: 16, Batch: 330, Training Loss: 0.07315344139933586, LR: 0.010000000000000002
Time, 2019-01-01T17:11:11, Epoch: 16, Batch: 340, Training Loss: 0.07664341777563095, LR: 0.010000000000000002
Time, 2019-01-01T17:11:12, Epoch: 16, Batch: 350, Training Loss: 0.08430787548422813, LR: 0.010000000000000002
Time, 2019-01-01T17:11:13, Epoch: 16, Batch: 360, Training Loss: 0.07755450010299683, LR: 0.010000000000000002
Time, 2019-01-01T17:11:13, Epoch: 16, Batch: 370, Training Loss: 0.09134950153529645, LR: 0.010000000000000002
Time, 2019-01-01T17:11:14, Epoch: 16, Batch: 380, Training Loss: 0.049101270362734796, LR: 0.010000000000000002
Time, 2019-01-01T17:11:15, Epoch: 16, Batch: 390, Training Loss: 0.07798242568969727, LR: 0.010000000000000002
Time, 2019-01-01T17:11:16, Epoch: 16, Batch: 400, Training Loss: 0.08350861184298992, LR: 0.010000000000000002
Time, 2019-01-01T17:11:16, Epoch: 16, Batch: 410, Training Loss: 0.09267972856760025, LR: 0.010000000000000002
Time, 2019-01-01T17:11:17, Epoch: 16, Batch: 420, Training Loss: 0.06960507482290268, LR: 0.010000000000000002
Time, 2019-01-01T17:11:18, Epoch: 16, Batch: 430, Training Loss: 0.10256141759455203, LR: 0.010000000000000002
Time, 2019-01-01T17:11:19, Epoch: 16, Batch: 440, Training Loss: 0.07953372858464718, LR: 0.010000000000000002
Time, 2019-01-01T17:11:19, Epoch: 16, Batch: 450, Training Loss: 0.04866596348583698, LR: 0.010000000000000002
Time, 2019-01-01T17:11:20, Epoch: 16, Batch: 460, Training Loss: 0.08718832321465016, LR: 0.010000000000000002
Time, 2019-01-01T17:11:21, Epoch: 16, Batch: 470, Training Loss: 0.06691331751644611, LR: 0.010000000000000002
Time, 2019-01-01T17:11:22, Epoch: 16, Batch: 480, Training Loss: 0.049796370416879655, LR: 0.010000000000000002
Time, 2019-01-01T17:11:22, Epoch: 16, Batch: 490, Training Loss: 0.05655505582690239, LR: 0.010000000000000002
Time, 2019-01-01T17:11:23, Epoch: 16, Batch: 500, Training Loss: 0.08040169663727284, LR: 0.010000000000000002
Time, 2019-01-01T17:11:24, Epoch: 16, Batch: 510, Training Loss: 0.060351860523223874, LR: 0.010000000000000002
Time, 2019-01-01T17:11:25, Epoch: 16, Batch: 520, Training Loss: 0.08622558414936066, LR: 0.010000000000000002
Time, 2019-01-01T17:11:25, Epoch: 16, Batch: 530, Training Loss: 0.04156740419566631, LR: 0.010000000000000002
Time, 2019-01-01T17:11:26, Epoch: 16, Batch: 540, Training Loss: 0.06213798895478249, LR: 0.010000000000000002
Time, 2019-01-01T17:11:27, Epoch: 16, Batch: 550, Training Loss: 0.06387440897524357, LR: 0.010000000000000002
Time, 2019-01-01T17:11:28, Epoch: 16, Batch: 560, Training Loss: 0.039259379357099534, LR: 0.010000000000000002
Time, 2019-01-01T17:11:28, Epoch: 16, Batch: 570, Training Loss: 0.07297318503260612, LR: 0.010000000000000002
Time, 2019-01-01T17:11:29, Epoch: 16, Batch: 580, Training Loss: 0.06524165347218513, LR: 0.010000000000000002
Time, 2019-01-01T17:11:30, Epoch: 16, Batch: 590, Training Loss: 0.089851014316082, LR: 0.010000000000000002
Time, 2019-01-01T17:11:31, Epoch: 16, Batch: 600, Training Loss: 0.07736796401441097, LR: 0.010000000000000002
Time, 2019-01-01T17:11:31, Epoch: 16, Batch: 610, Training Loss: 0.05579122342169285, LR: 0.010000000000000002
Time, 2019-01-01T17:11:32, Epoch: 16, Batch: 620, Training Loss: 0.06353873386979103, LR: 0.010000000000000002
Time, 2019-01-01T17:11:33, Epoch: 16, Batch: 630, Training Loss: 0.07953377142548561, LR: 0.010000000000000002
Time, 2019-01-01T17:11:33, Epoch: 16, Batch: 640, Training Loss: 0.06221116408705711, LR: 0.010000000000000002
Time, 2019-01-01T17:11:34, Epoch: 16, Batch: 650, Training Loss: 0.05142764411866665, LR: 0.010000000000000002
Time, 2019-01-01T17:11:35, Epoch: 16, Batch: 660, Training Loss: 0.05237184762954712, LR: 0.010000000000000002
Time, 2019-01-01T17:11:36, Epoch: 16, Batch: 670, Training Loss: 0.06378575712442398, LR: 0.010000000000000002
Time, 2019-01-01T17:11:36, Epoch: 16, Batch: 680, Training Loss: 0.0634254276752472, LR: 0.010000000000000002
Time, 2019-01-01T17:11:37, Epoch: 16, Batch: 690, Training Loss: 0.0678542409092188, LR: 0.010000000000000002
Time, 2019-01-01T17:11:38, Epoch: 16, Batch: 700, Training Loss: 0.050170135125517844, LR: 0.010000000000000002
Time, 2019-01-01T17:11:39, Epoch: 16, Batch: 710, Training Loss: 0.07448691166937352, LR: 0.010000000000000002
Time, 2019-01-01T17:11:39, Epoch: 16, Batch: 720, Training Loss: 0.09403829723596573, LR: 0.010000000000000002
Time, 2019-01-01T17:11:40, Epoch: 16, Batch: 730, Training Loss: 0.06565027385950088, LR: 0.010000000000000002
Time, 2019-01-01T17:11:41, Epoch: 16, Batch: 740, Training Loss: 0.05147644393146038, LR: 0.010000000000000002
Time, 2019-01-01T17:11:42, Epoch: 16, Batch: 750, Training Loss: 0.04912672452628612, LR: 0.010000000000000002
Time, 2019-01-01T17:11:42, Epoch: 16, Batch: 760, Training Loss: 0.0793933779001236, LR: 0.010000000000000002
Time, 2019-01-01T17:11:43, Epoch: 16, Batch: 770, Training Loss: 0.05391760505735874, LR: 0.010000000000000002
Time, 2019-01-01T17:11:44, Epoch: 16, Batch: 780, Training Loss: 0.07080391868948936, LR: 0.010000000000000002
Time, 2019-01-01T17:11:45, Epoch: 16, Batch: 790, Training Loss: 0.06181096769869328, LR: 0.010000000000000002
Time, 2019-01-01T17:11:45, Epoch: 16, Batch: 800, Training Loss: 0.0834434486925602, LR: 0.010000000000000002
Time, 2019-01-01T17:11:46, Epoch: 16, Batch: 810, Training Loss: 0.0452638067305088, LR: 0.010000000000000002
Time, 2019-01-01T17:11:47, Epoch: 16, Batch: 820, Training Loss: 0.07667694613337517, LR: 0.010000000000000002
Time, 2019-01-01T17:11:47, Epoch: 16, Batch: 830, Training Loss: 0.042420363426208495, LR: 0.010000000000000002
Time, 2019-01-01T17:11:48, Epoch: 16, Batch: 840, Training Loss: 0.07141173295676709, LR: 0.010000000000000002
Time, 2019-01-01T17:11:49, Epoch: 16, Batch: 850, Training Loss: 0.06656790748238564, LR: 0.010000000000000002
Time, 2019-01-01T17:11:50, Epoch: 16, Batch: 860, Training Loss: 0.0554032564163208, LR: 0.010000000000000002
Time, 2019-01-01T17:11:50, Epoch: 16, Batch: 870, Training Loss: 0.06466138772666455, LR: 0.010000000000000002
Time, 2019-01-01T17:11:51, Epoch: 16, Batch: 880, Training Loss: 0.06112179905176163, LR: 0.010000000000000002
Time, 2019-01-01T17:11:52, Epoch: 16, Batch: 890, Training Loss: 0.08024109080433846, LR: 0.010000000000000002
Time, 2019-01-01T17:11:53, Epoch: 16, Batch: 900, Training Loss: 0.059189687669277194, LR: 0.010000000000000002
Time, 2019-01-01T17:11:53, Epoch: 16, Batch: 910, Training Loss: 0.07437717840075493, LR: 0.010000000000000002
Time, 2019-01-01T17:11:54, Epoch: 16, Batch: 920, Training Loss: 0.05665921308100223, LR: 0.010000000000000002
Time, 2019-01-01T17:11:55, Epoch: 16, Batch: 930, Training Loss: 0.05530503652989864, LR: 0.010000000000000002
Epoch: 16, Validation Top 1 acc: 98.15264892578125
Epoch: 16, Validation Top 5 acc: 99.97667694091797
Epoch: 16, Validation Set Loss: 0.06272183358669281
Start training epoch 17
Time, 2019-01-01T17:12:22, Epoch: 17, Batch: 10, Training Loss: 0.059618080407381056, LR: 0.010000000000000002
Time, 2019-01-01T17:12:23, Epoch: 17, Batch: 20, Training Loss: 0.06158471405506134, LR: 0.010000000000000002
Time, 2019-01-01T17:12:24, Epoch: 17, Batch: 30, Training Loss: 0.07586637437343598, LR: 0.010000000000000002
Time, 2019-01-01T17:12:24, Epoch: 17, Batch: 40, Training Loss: 0.06939285136759281, LR: 0.010000000000000002
Time, 2019-01-01T17:12:25, Epoch: 17, Batch: 50, Training Loss: 0.08279899507761002, LR: 0.010000000000000002
Time, 2019-01-01T17:12:26, Epoch: 17, Batch: 60, Training Loss: 0.07394757494330406, LR: 0.010000000000000002
Time, 2019-01-01T17:12:27, Epoch: 17, Batch: 70, Training Loss: 0.07689196653664113, LR: 0.010000000000000002
Time, 2019-01-01T17:12:27, Epoch: 17, Batch: 80, Training Loss: 0.04711267054080963, LR: 0.010000000000000002
Time, 2019-01-01T17:12:28, Epoch: 17, Batch: 90, Training Loss: 0.07442609444260598, LR: 0.010000000000000002
Time, 2019-01-01T17:12:29, Epoch: 17, Batch: 100, Training Loss: 0.08981793820858001, LR: 0.010000000000000002
Time, 2019-01-01T17:12:29, Epoch: 17, Batch: 110, Training Loss: 0.06032052002847195, LR: 0.010000000000000002
Time, 2019-01-01T17:12:30, Epoch: 17, Batch: 120, Training Loss: 0.07684976346790791, LR: 0.010000000000000002
Time, 2019-01-01T17:12:31, Epoch: 17, Batch: 130, Training Loss: 0.0572487898170948, LR: 0.010000000000000002
Time, 2019-01-01T17:12:32, Epoch: 17, Batch: 140, Training Loss: 0.0802068717777729, LR: 0.010000000000000002
Time, 2019-01-01T17:12:32, Epoch: 17, Batch: 150, Training Loss: 0.06670288257300853, LR: 0.010000000000000002
Time, 2019-01-01T17:12:33, Epoch: 17, Batch: 160, Training Loss: 0.07420145720243454, LR: 0.010000000000000002
Time, 2019-01-01T17:12:34, Epoch: 17, Batch: 170, Training Loss: 0.07449268698692321, LR: 0.010000000000000002
Time, 2019-01-01T17:12:34, Epoch: 17, Batch: 180, Training Loss: 0.0519287034869194, LR: 0.010000000000000002
Time, 2019-01-01T17:12:35, Epoch: 17, Batch: 190, Training Loss: 0.05483600459992886, LR: 0.010000000000000002
Time, 2019-01-01T17:12:36, Epoch: 17, Batch: 200, Training Loss: 0.08444436639547348, LR: 0.010000000000000002
Time, 2019-01-01T17:12:37, Epoch: 17, Batch: 210, Training Loss: 0.08219672776758671, LR: 0.010000000000000002
Time, 2019-01-01T17:12:37, Epoch: 17, Batch: 220, Training Loss: 0.07409901395440102, LR: 0.010000000000000002
Time, 2019-01-01T17:12:38, Epoch: 17, Batch: 230, Training Loss: 0.06508537493646145, LR: 0.010000000000000002
Time, 2019-01-01T17:12:39, Epoch: 17, Batch: 240, Training Loss: 0.06255744621157647, LR: 0.010000000000000002
Time, 2019-01-01T17:12:40, Epoch: 17, Batch: 250, Training Loss: 0.04556742943823337, LR: 0.010000000000000002
Time, 2019-01-01T17:12:40, Epoch: 17, Batch: 260, Training Loss: 0.05540277995169163, LR: 0.010000000000000002
Time, 2019-01-01T17:12:41, Epoch: 17, Batch: 270, Training Loss: 0.07936861999332905, LR: 0.010000000000000002
Time, 2019-01-01T17:12:42, Epoch: 17, Batch: 280, Training Loss: 0.07298004403710365, LR: 0.010000000000000002
Time, 2019-01-01T17:12:42, Epoch: 17, Batch: 290, Training Loss: 0.06657262705266476, LR: 0.010000000000000002
Time, 2019-01-01T17:12:43, Epoch: 17, Batch: 300, Training Loss: 0.06033598817884922, LR: 0.010000000000000002
Time, 2019-01-01T17:12:44, Epoch: 17, Batch: 310, Training Loss: 0.07519232034683228, LR: 0.010000000000000002
Time, 2019-01-01T17:12:45, Epoch: 17, Batch: 320, Training Loss: 0.07330270484089851, LR: 0.010000000000000002
Time, 2019-01-01T17:12:45, Epoch: 17, Batch: 330, Training Loss: 0.07056052535772324, LR: 0.010000000000000002
Time, 2019-01-01T17:12:46, Epoch: 17, Batch: 340, Training Loss: 0.07911133728921413, LR: 0.010000000000000002
Time, 2019-01-01T17:12:47, Epoch: 17, Batch: 350, Training Loss: 0.07134521491825581, LR: 0.010000000000000002
Time, 2019-01-01T17:12:48, Epoch: 17, Batch: 360, Training Loss: 0.08993688970804214, LR: 0.010000000000000002
Time, 2019-01-01T17:12:48, Epoch: 17, Batch: 370, Training Loss: 0.08771142587065697, LR: 0.010000000000000002
Time, 2019-01-01T17:12:49, Epoch: 17, Batch: 380, Training Loss: 0.0780457891523838, LR: 0.010000000000000002
Time, 2019-01-01T17:12:50, Epoch: 17, Batch: 390, Training Loss: 0.05638835653662681, LR: 0.010000000000000002
Time, 2019-01-01T17:12:50, Epoch: 17, Batch: 400, Training Loss: 0.07767731547355652, LR: 0.010000000000000002
Time, 2019-01-01T17:12:51, Epoch: 17, Batch: 410, Training Loss: 0.06386532448232174, LR: 0.010000000000000002
Time, 2019-01-01T17:12:52, Epoch: 17, Batch: 420, Training Loss: 0.060127324610948565, LR: 0.010000000000000002
Time, 2019-01-01T17:12:53, Epoch: 17, Batch: 430, Training Loss: 0.07447474226355552, LR: 0.010000000000000002
Time, 2019-01-01T17:12:53, Epoch: 17, Batch: 440, Training Loss: 0.06142786853015423, LR: 0.010000000000000002
Time, 2019-01-01T17:12:54, Epoch: 17, Batch: 450, Training Loss: 0.06298846825957298, LR: 0.010000000000000002
Time, 2019-01-01T17:12:55, Epoch: 17, Batch: 460, Training Loss: 0.07343749180436135, LR: 0.010000000000000002
Time, 2019-01-01T17:12:55, Epoch: 17, Batch: 470, Training Loss: 0.052318287640810014, LR: 0.010000000000000002
Time, 2019-01-01T17:12:56, Epoch: 17, Batch: 480, Training Loss: 0.06213411130011082, LR: 0.010000000000000002
Time, 2019-01-01T17:12:57, Epoch: 17, Batch: 490, Training Loss: 0.07739575430750847, LR: 0.010000000000000002
Time, 2019-01-01T17:12:58, Epoch: 17, Batch: 500, Training Loss: 0.04433835484087467, LR: 0.010000000000000002
Time, 2019-01-01T17:12:58, Epoch: 17, Batch: 510, Training Loss: 0.04757840633392334, LR: 0.010000000000000002
Time, 2019-01-01T17:12:59, Epoch: 17, Batch: 520, Training Loss: 0.08111576214432717, LR: 0.010000000000000002
Time, 2019-01-01T17:13:00, Epoch: 17, Batch: 530, Training Loss: 0.08780112341046334, LR: 0.010000000000000002
Time, 2019-01-01T17:13:01, Epoch: 17, Batch: 540, Training Loss: 0.06577179953455925, LR: 0.010000000000000002
Time, 2019-01-01T17:13:01, Epoch: 17, Batch: 550, Training Loss: 0.06068819276988506, LR: 0.010000000000000002
Time, 2019-01-01T17:13:02, Epoch: 17, Batch: 560, Training Loss: 0.0939333163201809, LR: 0.010000000000000002
Time, 2019-01-01T17:13:03, Epoch: 17, Batch: 570, Training Loss: 0.08045226596295833, LR: 0.010000000000000002
Time, 2019-01-01T17:13:03, Epoch: 17, Batch: 580, Training Loss: 0.057962429523468015, LR: 0.010000000000000002
Time, 2019-01-01T17:13:04, Epoch: 17, Batch: 590, Training Loss: 0.06774116531014443, LR: 0.010000000000000002
Time, 2019-01-01T17:13:05, Epoch: 17, Batch: 600, Training Loss: 0.08361554183065892, LR: 0.010000000000000002
Time, 2019-01-01T17:13:06, Epoch: 17, Batch: 610, Training Loss: 0.07403088957071305, LR: 0.010000000000000002
Time, 2019-01-01T17:13:06, Epoch: 17, Batch: 620, Training Loss: 0.06058879196643829, LR: 0.010000000000000002
Time, 2019-01-01T17:13:07, Epoch: 17, Batch: 630, Training Loss: 0.05343501940369606, LR: 0.010000000000000002
Time, 2019-01-01T17:13:08, Epoch: 17, Batch: 640, Training Loss: 0.06077764369547367, LR: 0.010000000000000002
Time, 2019-01-01T17:13:08, Epoch: 17, Batch: 650, Training Loss: 0.07670918554067611, LR: 0.010000000000000002
Time, 2019-01-01T17:13:09, Epoch: 17, Batch: 660, Training Loss: 0.08419575840234757, LR: 0.010000000000000002
Time, 2019-01-01T17:13:10, Epoch: 17, Batch: 670, Training Loss: 0.06501550637185574, LR: 0.010000000000000002
Time, 2019-01-01T17:13:11, Epoch: 17, Batch: 680, Training Loss: 0.060588857531547545, LR: 0.010000000000000002
Time, 2019-01-01T17:13:11, Epoch: 17, Batch: 690, Training Loss: 0.0580962136387825, LR: 0.010000000000000002
Time, 2019-01-01T17:13:12, Epoch: 17, Batch: 700, Training Loss: 0.062253895401954654, LR: 0.010000000000000002
Time, 2019-01-01T17:13:13, Epoch: 17, Batch: 710, Training Loss: 0.07951730974018574, LR: 0.010000000000000002
Time, 2019-01-01T17:13:14, Epoch: 17, Batch: 720, Training Loss: 0.09137803018093109, LR: 0.010000000000000002
Time, 2019-01-01T17:13:14, Epoch: 17, Batch: 730, Training Loss: 0.06355398371815682, LR: 0.010000000000000002
Time, 2019-01-01T17:13:15, Epoch: 17, Batch: 740, Training Loss: 0.05823644623160362, LR: 0.010000000000000002
Time, 2019-01-01T17:13:16, Epoch: 17, Batch: 750, Training Loss: 0.07881366983056068, LR: 0.010000000000000002
Time, 2019-01-01T17:13:16, Epoch: 17, Batch: 760, Training Loss: 0.07025254219770431, LR: 0.010000000000000002
Time, 2019-01-01T17:13:17, Epoch: 17, Batch: 770, Training Loss: 0.058569930493831635, LR: 0.010000000000000002
Time, 2019-01-01T17:13:18, Epoch: 17, Batch: 780, Training Loss: 0.06919259876012802, LR: 0.010000000000000002
Time, 2019-01-01T17:13:19, Epoch: 17, Batch: 790, Training Loss: 0.05789165869355202, LR: 0.010000000000000002
Time, 2019-01-01T17:13:19, Epoch: 17, Batch: 800, Training Loss: 0.08698898255825042, LR: 0.010000000000000002
Time, 2019-01-01T17:13:20, Epoch: 17, Batch: 810, Training Loss: 0.04763309136033058, LR: 0.010000000000000002
Time, 2019-01-01T17:13:21, Epoch: 17, Batch: 820, Training Loss: 0.04653703793883324, LR: 0.010000000000000002
Time, 2019-01-01T17:13:21, Epoch: 17, Batch: 830, Training Loss: 0.06022662185132503, LR: 0.010000000000000002
Time, 2019-01-01T17:13:22, Epoch: 17, Batch: 840, Training Loss: 0.07179397940635682, LR: 0.010000000000000002
Time, 2019-01-01T17:13:23, Epoch: 17, Batch: 850, Training Loss: 0.08890461176633835, LR: 0.010000000000000002
Time, 2019-01-01T17:13:24, Epoch: 17, Batch: 860, Training Loss: 0.07391111887991428, LR: 0.010000000000000002
Time, 2019-01-01T17:13:24, Epoch: 17, Batch: 870, Training Loss: 0.07370524629950523, LR: 0.010000000000000002
Time, 2019-01-01T17:13:25, Epoch: 17, Batch: 880, Training Loss: 0.07570264600217343, LR: 0.010000000000000002
Time, 2019-01-01T17:13:26, Epoch: 17, Batch: 890, Training Loss: 0.0675934337079525, LR: 0.010000000000000002
Time, 2019-01-01T17:13:26, Epoch: 17, Batch: 900, Training Loss: 0.0832149986177683, LR: 0.010000000000000002
Time, 2019-01-01T17:13:27, Epoch: 17, Batch: 910, Training Loss: 0.04625104479491711, LR: 0.010000000000000002
Time, 2019-01-01T17:13:28, Epoch: 17, Batch: 920, Training Loss: 0.06862426549196243, LR: 0.010000000000000002
Time, 2019-01-01T17:13:29, Epoch: 17, Batch: 930, Training Loss: 0.07239341735839844, LR: 0.010000000000000002
Epoch: 17, Validation Top 1 acc: 97.92777252197266
Epoch: 17, Validation Top 5 acc: 99.97501373291016
Epoch: 17, Validation Set Loss: 0.0689900815486908
Start training epoch 18
Time, 2019-01-01T17:13:54, Epoch: 18, Batch: 10, Training Loss: 0.06296887025237083, LR: 0.010000000000000002
Time, 2019-01-01T17:13:55, Epoch: 18, Batch: 20, Training Loss: 0.050832153484225276, LR: 0.010000000000000002
Time, 2019-01-01T17:13:56, Epoch: 18, Batch: 30, Training Loss: 0.05119604617357254, LR: 0.010000000000000002
Time, 2019-01-01T17:13:56, Epoch: 18, Batch: 40, Training Loss: 0.06588685438036919, LR: 0.010000000000000002
Time, 2019-01-01T17:13:57, Epoch: 18, Batch: 50, Training Loss: 0.07344299964606762, LR: 0.010000000000000002
Time, 2019-01-01T17:13:58, Epoch: 18, Batch: 60, Training Loss: 0.06421269252896308, LR: 0.010000000000000002
Time, 2019-01-01T17:13:59, Epoch: 18, Batch: 70, Training Loss: 0.04849722310900688, LR: 0.010000000000000002
Time, 2019-01-01T17:13:59, Epoch: 18, Batch: 80, Training Loss: 0.06962310969829559, LR: 0.010000000000000002
Time, 2019-01-01T17:14:00, Epoch: 18, Batch: 90, Training Loss: 0.08280448131263256, LR: 0.010000000000000002
Time, 2019-01-01T17:14:01, Epoch: 18, Batch: 100, Training Loss: 0.06723977439105511, LR: 0.010000000000000002
Time, 2019-01-01T17:14:01, Epoch: 18, Batch: 110, Training Loss: 0.05235818959772587, LR: 0.010000000000000002
Time, 2019-01-01T17:14:02, Epoch: 18, Batch: 120, Training Loss: 0.048440859839320186, LR: 0.010000000000000002
Time, 2019-01-01T17:14:03, Epoch: 18, Batch: 130, Training Loss: 0.08036933168768882, LR: 0.010000000000000002
Time, 2019-01-01T17:14:04, Epoch: 18, Batch: 140, Training Loss: 0.09665148742496968, LR: 0.010000000000000002
Time, 2019-01-01T17:14:04, Epoch: 18, Batch: 150, Training Loss: 0.07781598940491677, LR: 0.010000000000000002
Time, 2019-01-01T17:14:05, Epoch: 18, Batch: 160, Training Loss: 0.0755682960152626, LR: 0.010000000000000002
Time, 2019-01-01T17:14:06, Epoch: 18, Batch: 170, Training Loss: 0.06644417867064475, LR: 0.010000000000000002
Time, 2019-01-01T17:14:07, Epoch: 18, Batch: 180, Training Loss: 0.04806432276964188, LR: 0.010000000000000002
Time, 2019-01-01T17:14:07, Epoch: 18, Batch: 190, Training Loss: 0.05896289497613907, LR: 0.010000000000000002
Time, 2019-01-01T17:14:08, Epoch: 18, Batch: 200, Training Loss: 0.07061950117349625, LR: 0.010000000000000002
Time, 2019-01-01T17:14:09, Epoch: 18, Batch: 210, Training Loss: 0.047600970789790156, LR: 0.010000000000000002
Time, 2019-01-01T17:14:09, Epoch: 18, Batch: 220, Training Loss: 0.07986020259559154, LR: 0.010000000000000002
Time, 2019-01-01T17:14:10, Epoch: 18, Batch: 230, Training Loss: 0.07008333317935467, LR: 0.010000000000000002
Time, 2019-01-01T17:14:11, Epoch: 18, Batch: 240, Training Loss: 0.05741205811500549, LR: 0.010000000000000002
Time, 2019-01-01T17:14:12, Epoch: 18, Batch: 250, Training Loss: 0.07006794512271881, LR: 0.010000000000000002
Time, 2019-01-01T17:14:12, Epoch: 18, Batch: 260, Training Loss: 0.061207402125000955, LR: 0.010000000000000002
Time, 2019-01-01T17:14:13, Epoch: 18, Batch: 270, Training Loss: 0.056428379565477374, LR: 0.010000000000000002
Time, 2019-01-01T17:14:14, Epoch: 18, Batch: 280, Training Loss: 0.062149572372436526, LR: 0.010000000000000002
Time, 2019-01-01T17:14:14, Epoch: 18, Batch: 290, Training Loss: 0.054078252986073494, LR: 0.010000000000000002
Time, 2019-01-01T17:14:15, Epoch: 18, Batch: 300, Training Loss: 0.06811579912900925, LR: 0.010000000000000002
Time, 2019-01-01T17:14:16, Epoch: 18, Batch: 310, Training Loss: 0.06496111266314983, LR: 0.010000000000000002
Time, 2019-01-01T17:14:17, Epoch: 18, Batch: 320, Training Loss: 0.06618858650326728, LR: 0.010000000000000002
Time, 2019-01-01T17:14:17, Epoch: 18, Batch: 330, Training Loss: 0.11348960101604462, LR: 0.010000000000000002
Time, 2019-01-01T17:14:18, Epoch: 18, Batch: 340, Training Loss: 0.05727076753973961, LR: 0.010000000000000002
Time, 2019-01-01T17:14:19, Epoch: 18, Batch: 350, Training Loss: 0.06093589439988136, LR: 0.010000000000000002
Time, 2019-01-01T17:14:20, Epoch: 18, Batch: 360, Training Loss: 0.06347180306911468, LR: 0.010000000000000002
Time, 2019-01-01T17:14:20, Epoch: 18, Batch: 370, Training Loss: 0.11387813687324524, LR: 0.010000000000000002
Time, 2019-01-01T17:14:21, Epoch: 18, Batch: 380, Training Loss: 0.0674588531255722, LR: 0.010000000000000002
Time, 2019-01-01T17:14:22, Epoch: 18, Batch: 390, Training Loss: 0.07304404266178607, LR: 0.010000000000000002
Time, 2019-01-01T17:14:22, Epoch: 18, Batch: 400, Training Loss: 0.05359740480780602, LR: 0.010000000000000002
Time, 2019-01-01T17:14:23, Epoch: 18, Batch: 410, Training Loss: 0.09719226583838463, LR: 0.010000000000000002
Time, 2019-01-01T17:14:24, Epoch: 18, Batch: 420, Training Loss: 0.07622494548559189, LR: 0.010000000000000002
Time, 2019-01-01T17:14:25, Epoch: 18, Batch: 430, Training Loss: 0.07734356075525284, LR: 0.010000000000000002
Time, 2019-01-01T17:14:25, Epoch: 18, Batch: 440, Training Loss: 0.07117175385355949, LR: 0.010000000000000002
Time, 2019-01-01T17:14:26, Epoch: 18, Batch: 450, Training Loss: 0.06826514564454556, LR: 0.010000000000000002
Time, 2019-01-01T17:14:27, Epoch: 18, Batch: 460, Training Loss: 0.11621635556221008, LR: 0.010000000000000002
Time, 2019-01-01T17:14:27, Epoch: 18, Batch: 470, Training Loss: 0.10411638393998146, LR: 0.010000000000000002
Time, 2019-01-01T17:14:28, Epoch: 18, Batch: 480, Training Loss: 0.08785701543092728, LR: 0.010000000000000002
Time, 2019-01-01T17:14:29, Epoch: 18, Batch: 490, Training Loss: 0.06984337754547595, LR: 0.010000000000000002
Time, 2019-01-01T17:14:30, Epoch: 18, Batch: 500, Training Loss: 0.08725747093558311, LR: 0.010000000000000002
Time, 2019-01-01T17:14:30, Epoch: 18, Batch: 510, Training Loss: 0.03883252516388893, LR: 0.010000000000000002
Time, 2019-01-01T17:14:31, Epoch: 18, Batch: 520, Training Loss: 0.07294014059007167, LR: 0.010000000000000002
Time, 2019-01-01T17:14:32, Epoch: 18, Batch: 530, Training Loss: 0.08109058812260628, LR: 0.010000000000000002
Time, 2019-01-01T17:14:33, Epoch: 18, Batch: 540, Training Loss: 0.07314757592976093, LR: 0.010000000000000002
Time, 2019-01-01T17:14:33, Epoch: 18, Batch: 550, Training Loss: 0.0667764913290739, LR: 0.010000000000000002
Time, 2019-01-01T17:14:34, Epoch: 18, Batch: 560, Training Loss: 0.0773662768304348, LR: 0.010000000000000002
Time, 2019-01-01T17:14:35, Epoch: 18, Batch: 570, Training Loss: 0.06962084099650383, LR: 0.010000000000000002
Time, 2019-01-01T17:14:35, Epoch: 18, Batch: 580, Training Loss: 0.08345447853207588, LR: 0.010000000000000002
Time, 2019-01-01T17:14:36, Epoch: 18, Batch: 590, Training Loss: 0.07225477322936058, LR: 0.010000000000000002
Time, 2019-01-01T17:14:37, Epoch: 18, Batch: 600, Training Loss: 0.07277165576815606, LR: 0.010000000000000002
Time, 2019-01-01T17:14:38, Epoch: 18, Batch: 610, Training Loss: 0.061881594359874725, LR: 0.010000000000000002
Time, 2019-01-01T17:14:38, Epoch: 18, Batch: 620, Training Loss: 0.08293326236307622, LR: 0.010000000000000002
Time, 2019-01-01T17:14:39, Epoch: 18, Batch: 630, Training Loss: 0.051847420260310174, LR: 0.010000000000000002
Time, 2019-01-01T17:14:40, Epoch: 18, Batch: 640, Training Loss: 0.07951971143484116, LR: 0.010000000000000002
Time, 2019-01-01T17:14:40, Epoch: 18, Batch: 650, Training Loss: 0.06054660677909851, LR: 0.010000000000000002
Time, 2019-01-01T17:14:41, Epoch: 18, Batch: 660, Training Loss: 0.06689050793647766, LR: 0.010000000000000002
Time, 2019-01-01T17:14:42, Epoch: 18, Batch: 670, Training Loss: 0.0885154876857996, LR: 0.010000000000000002
Time, 2019-01-01T17:14:43, Epoch: 18, Batch: 680, Training Loss: 0.0685376662760973, LR: 0.010000000000000002
Time, 2019-01-01T17:14:44, Epoch: 18, Batch: 690, Training Loss: 0.10045077130198479, LR: 0.010000000000000002
Time, 2019-01-01T17:14:44, Epoch: 18, Batch: 700, Training Loss: 0.08041731528937816, LR: 0.010000000000000002
Time, 2019-01-01T17:14:45, Epoch: 18, Batch: 710, Training Loss: 0.09943176656961442, LR: 0.010000000000000002
Time, 2019-01-01T17:14:46, Epoch: 18, Batch: 720, Training Loss: 0.08263764679431915, LR: 0.010000000000000002
Time, 2019-01-01T17:14:46, Epoch: 18, Batch: 730, Training Loss: 0.08412865102291107, LR: 0.010000000000000002
Time, 2019-01-01T17:14:47, Epoch: 18, Batch: 740, Training Loss: 0.05373960137367249, LR: 0.010000000000000002
Time, 2019-01-01T17:14:48, Epoch: 18, Batch: 750, Training Loss: 0.06304128654301167, LR: 0.010000000000000002
Time, 2019-01-01T17:14:49, Epoch: 18, Batch: 760, Training Loss: 0.0709729965776205, LR: 0.010000000000000002
Time, 2019-01-01T17:14:49, Epoch: 18, Batch: 770, Training Loss: 0.06305761747062207, LR: 0.010000000000000002
Time, 2019-01-01T17:14:50, Epoch: 18, Batch: 780, Training Loss: 0.08522552363574505, LR: 0.010000000000000002
Time, 2019-01-01T17:14:51, Epoch: 18, Batch: 790, Training Loss: 0.07603610269725322, LR: 0.010000000000000002
Time, 2019-01-01T17:14:51, Epoch: 18, Batch: 800, Training Loss: 0.06014095395803452, LR: 0.010000000000000002
Time, 2019-01-01T17:14:52, Epoch: 18, Batch: 810, Training Loss: 0.07634495049715043, LR: 0.010000000000000002
Time, 2019-01-01T17:14:53, Epoch: 18, Batch: 820, Training Loss: 0.050977309420704844, LR: 0.010000000000000002
Time, 2019-01-01T17:14:54, Epoch: 18, Batch: 830, Training Loss: 0.08128580451011658, LR: 0.010000000000000002
Time, 2019-01-01T17:14:54, Epoch: 18, Batch: 840, Training Loss: 0.05412307344377041, LR: 0.010000000000000002
Time, 2019-01-01T17:14:55, Epoch: 18, Batch: 850, Training Loss: 0.06206487864255905, LR: 0.010000000000000002
Time, 2019-01-01T17:14:56, Epoch: 18, Batch: 860, Training Loss: 0.05765924900770188, LR: 0.010000000000000002
Time, 2019-01-01T17:14:57, Epoch: 18, Batch: 870, Training Loss: 0.062177371233701706, LR: 0.010000000000000002
Time, 2019-01-01T17:14:57, Epoch: 18, Batch: 880, Training Loss: 0.048444414511322975, LR: 0.010000000000000002
Time, 2019-01-01T17:14:58, Epoch: 18, Batch: 890, Training Loss: 0.05426705107092857, LR: 0.010000000000000002
Time, 2019-01-01T17:14:59, Epoch: 18, Batch: 900, Training Loss: 0.07625382095575332, LR: 0.010000000000000002
Time, 2019-01-01T17:14:59, Epoch: 18, Batch: 910, Training Loss: 0.053396884351968765, LR: 0.010000000000000002
Time, 2019-01-01T17:15:00, Epoch: 18, Batch: 920, Training Loss: 0.06466262079775334, LR: 0.010000000000000002
Time, 2019-01-01T17:15:01, Epoch: 18, Batch: 930, Training Loss: 0.05937411896884441, LR: 0.010000000000000002
Epoch: 18, Validation Top 1 acc: 97.83782196044922
Epoch: 18, Validation Top 5 acc: 99.97001647949219
Epoch: 18, Validation Set Loss: 0.07142982631921768
Start training epoch 19
Time, 2019-01-01T17:15:26, Epoch: 19, Batch: 10, Training Loss: 0.05506043024361133, LR: 0.010000000000000002
Time, 2019-01-01T17:15:27, Epoch: 19, Batch: 20, Training Loss: 0.08280038461089134, LR: 0.010000000000000002
Time, 2019-01-01T17:15:28, Epoch: 19, Batch: 30, Training Loss: 0.0602083969861269, LR: 0.010000000000000002
Time, 2019-01-01T17:15:28, Epoch: 19, Batch: 40, Training Loss: 0.046885914355516436, LR: 0.010000000000000002
Time, 2019-01-01T17:15:29, Epoch: 19, Batch: 50, Training Loss: 0.07835069708526135, LR: 0.010000000000000002
Time, 2019-01-01T17:15:30, Epoch: 19, Batch: 60, Training Loss: 0.0883073203265667, LR: 0.010000000000000002
Time, 2019-01-01T17:15:31, Epoch: 19, Batch: 70, Training Loss: 0.053282614797353745, LR: 0.010000000000000002
Time, 2019-01-01T17:15:31, Epoch: 19, Batch: 80, Training Loss: 0.07955327555537224, LR: 0.010000000000000002
Time, 2019-01-01T17:15:32, Epoch: 19, Batch: 90, Training Loss: 0.06977528184652329, LR: 0.010000000000000002
Time, 2019-01-01T17:15:33, Epoch: 19, Batch: 100, Training Loss: 0.07197959087789059, LR: 0.010000000000000002
Time, 2019-01-01T17:15:33, Epoch: 19, Batch: 110, Training Loss: 0.07720587588846684, LR: 0.010000000000000002
Time, 2019-01-01T17:15:34, Epoch: 19, Batch: 120, Training Loss: 0.06461526528000831, LR: 0.010000000000000002
Time, 2019-01-01T17:15:35, Epoch: 19, Batch: 130, Training Loss: 0.0829816922545433, LR: 0.010000000000000002
Time, 2019-01-01T17:15:35, Epoch: 19, Batch: 140, Training Loss: 0.08082110621035099, LR: 0.010000000000000002
Time, 2019-01-01T17:15:36, Epoch: 19, Batch: 150, Training Loss: 0.1037507027387619, LR: 0.010000000000000002
Time, 2019-01-01T17:15:37, Epoch: 19, Batch: 160, Training Loss: 0.07433709800243378, LR: 0.010000000000000002
Time, 2019-01-01T17:15:37, Epoch: 19, Batch: 170, Training Loss: 0.06834484934806824, LR: 0.010000000000000002
Time, 2019-01-01T17:15:38, Epoch: 19, Batch: 180, Training Loss: 0.0697929635643959, LR: 0.010000000000000002
Time, 2019-01-01T17:15:39, Epoch: 19, Batch: 190, Training Loss: 0.07410762198269367, LR: 0.010000000000000002
Time, 2019-01-01T17:15:40, Epoch: 19, Batch: 200, Training Loss: 0.07377774193882942, LR: 0.010000000000000002
Time, 2019-01-01T17:15:40, Epoch: 19, Batch: 210, Training Loss: 0.10814934372901916, LR: 0.010000000000000002
Time, 2019-01-01T17:15:41, Epoch: 19, Batch: 220, Training Loss: 0.10266579836606979, LR: 0.010000000000000002
Time, 2019-01-01T17:15:42, Epoch: 19, Batch: 230, Training Loss: 0.0922946311533451, LR: 0.010000000000000002
Time, 2019-01-01T17:15:43, Epoch: 19, Batch: 240, Training Loss: 0.07522116005420684, LR: 0.010000000000000002
Time, 2019-01-01T17:15:43, Epoch: 19, Batch: 250, Training Loss: 0.05270434878766537, LR: 0.010000000000000002
Time, 2019-01-01T17:15:44, Epoch: 19, Batch: 260, Training Loss: 0.09597407281398773, LR: 0.010000000000000002
Time, 2019-01-01T17:15:45, Epoch: 19, Batch: 270, Training Loss: 0.0807255357503891, LR: 0.010000000000000002
Time, 2019-01-01T17:15:46, Epoch: 19, Batch: 280, Training Loss: 0.07373709380626678, LR: 0.010000000000000002
Time, 2019-01-01T17:15:47, Epoch: 19, Batch: 290, Training Loss: 0.07727130465209484, LR: 0.010000000000000002
Time, 2019-01-01T17:15:47, Epoch: 19, Batch: 300, Training Loss: 0.064916617795825, LR: 0.010000000000000002
Time, 2019-01-01T17:15:48, Epoch: 19, Batch: 310, Training Loss: 0.10114713720977306, LR: 0.010000000000000002
Time, 2019-01-01T17:15:49, Epoch: 19, Batch: 320, Training Loss: 0.058755412325263025, LR: 0.010000000000000002
Time, 2019-01-01T17:15:49, Epoch: 19, Batch: 330, Training Loss: 0.07716010585427284, LR: 0.010000000000000002
Time, 2019-01-01T17:15:50, Epoch: 19, Batch: 340, Training Loss: 0.08361819386482239, LR: 0.010000000000000002
Time, 2019-01-01T17:15:51, Epoch: 19, Batch: 350, Training Loss: 0.06528762504458427, LR: 0.010000000000000002
Time, 2019-01-01T17:15:52, Epoch: 19, Batch: 360, Training Loss: 0.0763435922563076, LR: 0.010000000000000002
Time, 2019-01-01T17:15:53, Epoch: 19, Batch: 370, Training Loss: 0.09866925589740276, LR: 0.010000000000000002
Time, 2019-01-01T17:15:53, Epoch: 19, Batch: 380, Training Loss: 0.10739611685276032, LR: 0.010000000000000002
Time, 2019-01-01T17:15:54, Epoch: 19, Batch: 390, Training Loss: 0.08117973394691944, LR: 0.010000000000000002
Time, 2019-01-01T17:15:55, Epoch: 19, Batch: 400, Training Loss: 0.08406845703721047, LR: 0.010000000000000002
Time, 2019-01-01T17:15:55, Epoch: 19, Batch: 410, Training Loss: 0.08578427135944366, LR: 0.010000000000000002
Time, 2019-01-01T17:15:56, Epoch: 19, Batch: 420, Training Loss: 0.060310568660497665, LR: 0.010000000000000002
Time, 2019-01-01T17:15:57, Epoch: 19, Batch: 430, Training Loss: 0.09439250379800797, LR: 0.010000000000000002
Time, 2019-01-01T17:15:58, Epoch: 19, Batch: 440, Training Loss: 0.0832796648144722, LR: 0.010000000000000002
Time, 2019-01-01T17:15:58, Epoch: 19, Batch: 450, Training Loss: 0.06577665582299233, LR: 0.010000000000000002
Time, 2019-01-01T17:15:59, Epoch: 19, Batch: 460, Training Loss: 0.06123397909104824, LR: 0.010000000000000002
Time, 2019-01-01T17:16:00, Epoch: 19, Batch: 470, Training Loss: 0.05764059461653233, LR: 0.010000000000000002
Time, 2019-01-01T17:16:00, Epoch: 19, Batch: 480, Training Loss: 0.08058738932013512, LR: 0.010000000000000002
Time, 2019-01-01T17:16:01, Epoch: 19, Batch: 490, Training Loss: 0.046067187935113905, LR: 0.010000000000000002
Time, 2019-01-01T17:16:02, Epoch: 19, Batch: 500, Training Loss: 0.06823190301656723, LR: 0.010000000000000002
Time, 2019-01-01T17:16:03, Epoch: 19, Batch: 510, Training Loss: 0.06665209159255028, LR: 0.010000000000000002
Time, 2019-01-01T17:16:03, Epoch: 19, Batch: 520, Training Loss: 0.07781324684619903, LR: 0.010000000000000002
Time, 2019-01-01T17:16:04, Epoch: 19, Batch: 530, Training Loss: 0.07434568516910076, LR: 0.010000000000000002
Time, 2019-01-01T17:16:05, Epoch: 19, Batch: 540, Training Loss: 0.05051762871444225, LR: 0.010000000000000002
Time, 2019-01-01T17:16:06, Epoch: 19, Batch: 550, Training Loss: 0.06222164519131183, LR: 0.010000000000000002
Time, 2019-01-01T17:16:06, Epoch: 19, Batch: 560, Training Loss: 0.05316708460450172, LR: 0.010000000000000002
Time, 2019-01-01T17:16:07, Epoch: 19, Batch: 570, Training Loss: 0.06456971652805805, LR: 0.010000000000000002
Time, 2019-01-01T17:16:08, Epoch: 19, Batch: 580, Training Loss: 0.05783774815499783, LR: 0.010000000000000002
Time, 2019-01-01T17:16:08, Epoch: 19, Batch: 590, Training Loss: 0.07294727340340615, LR: 0.010000000000000002
Time, 2019-01-01T17:16:09, Epoch: 19, Batch: 600, Training Loss: 0.06937622949481011, LR: 0.010000000000000002
Time, 2019-01-01T17:16:10, Epoch: 19, Batch: 610, Training Loss: 0.09201706536114215, LR: 0.010000000000000002
Time, 2019-01-01T17:16:10, Epoch: 19, Batch: 620, Training Loss: 0.09046727493405342, LR: 0.010000000000000002
Time, 2019-01-01T17:16:11, Epoch: 19, Batch: 630, Training Loss: 0.07410606667399407, LR: 0.010000000000000002
Time, 2019-01-01T17:16:12, Epoch: 19, Batch: 640, Training Loss: 0.08137018755078315, LR: 0.010000000000000002
Time, 2019-01-01T17:16:13, Epoch: 19, Batch: 650, Training Loss: 0.048939158022403714, LR: 0.010000000000000002
Time, 2019-01-01T17:16:13, Epoch: 19, Batch: 660, Training Loss: 0.05879879370331764, LR: 0.010000000000000002
Time, 2019-01-01T17:16:14, Epoch: 19, Batch: 670, Training Loss: 0.0820687785744667, LR: 0.010000000000000002
Time, 2019-01-01T17:16:15, Epoch: 19, Batch: 680, Training Loss: 0.06935877501964569, LR: 0.010000000000000002
Time, 2019-01-01T17:16:15, Epoch: 19, Batch: 690, Training Loss: 0.07521695531904697, LR: 0.010000000000000002
Time, 2019-01-01T17:16:16, Epoch: 19, Batch: 700, Training Loss: 0.10012964978814125, LR: 0.010000000000000002
Time, 2019-01-01T17:16:17, Epoch: 19, Batch: 710, Training Loss: 0.09354907870292664, LR: 0.010000000000000002
Time, 2019-01-01T17:16:18, Epoch: 19, Batch: 720, Training Loss: 0.05497110560536385, LR: 0.010000000000000002
Time, 2019-01-01T17:16:18, Epoch: 19, Batch: 730, Training Loss: 0.0762674018740654, LR: 0.010000000000000002
Time, 2019-01-01T17:16:19, Epoch: 19, Batch: 740, Training Loss: 0.07424697503447533, LR: 0.010000000000000002
Time, 2019-01-01T17:16:20, Epoch: 19, Batch: 750, Training Loss: 0.05176429376006127, LR: 0.010000000000000002
Time, 2019-01-01T17:16:20, Epoch: 19, Batch: 760, Training Loss: 0.07729359492659568, LR: 0.010000000000000002
Time, 2019-01-01T17:16:21, Epoch: 19, Batch: 770, Training Loss: 0.05144480168819428, LR: 0.010000000000000002
Time, 2019-01-01T17:16:22, Epoch: 19, Batch: 780, Training Loss: 0.06988914161920548, LR: 0.010000000000000002
Time, 2019-01-01T17:16:22, Epoch: 19, Batch: 790, Training Loss: 0.06751162484288216, LR: 0.010000000000000002
Time, 2019-01-01T17:16:23, Epoch: 19, Batch: 800, Training Loss: 0.07655055299401284, LR: 0.010000000000000002
Time, 2019-01-01T17:16:24, Epoch: 19, Batch: 810, Training Loss: 0.05393716096878052, LR: 0.010000000000000002
Time, 2019-01-01T17:16:24, Epoch: 19, Batch: 820, Training Loss: 0.0967764601111412, LR: 0.010000000000000002
Time, 2019-01-01T17:16:25, Epoch: 19, Batch: 830, Training Loss: 0.06372391879558563, LR: 0.010000000000000002
Time, 2019-01-01T17:16:26, Epoch: 19, Batch: 840, Training Loss: 0.07482110559940339, LR: 0.010000000000000002
Time, 2019-01-01T17:16:26, Epoch: 19, Batch: 850, Training Loss: 0.07857908494770527, LR: 0.010000000000000002
Time, 2019-01-01T17:16:27, Epoch: 19, Batch: 860, Training Loss: 0.0821925587952137, LR: 0.010000000000000002
Time, 2019-01-01T17:16:28, Epoch: 19, Batch: 870, Training Loss: 0.08117459565401078, LR: 0.010000000000000002
Time, 2019-01-01T17:16:29, Epoch: 19, Batch: 880, Training Loss: 0.0878620207309723, LR: 0.010000000000000002
Time, 2019-01-01T17:16:29, Epoch: 19, Batch: 890, Training Loss: 0.09236767068505287, LR: 0.010000000000000002
Time, 2019-01-01T17:16:30, Epoch: 19, Batch: 900, Training Loss: 0.08221170976758004, LR: 0.010000000000000002
Time, 2019-01-01T17:16:31, Epoch: 19, Batch: 910, Training Loss: 0.07141361683607102, LR: 0.010000000000000002
Time, 2019-01-01T17:16:31, Epoch: 19, Batch: 920, Training Loss: 0.06511893086135387, LR: 0.010000000000000002
Time, 2019-01-01T17:16:32, Epoch: 19, Batch: 930, Training Loss: 0.06737265139818191, LR: 0.010000000000000002
Epoch: 19, Validation Top 1 acc: 97.67790222167969
Epoch: 19, Validation Top 5 acc: 99.97667694091797
Epoch: 19, Validation Set Loss: 0.07429179549217224
Start training epoch 20
Time, 2019-01-01T17:16:57, Epoch: 20, Batch: 10, Training Loss: 0.06791603229939938, LR: 0.010000000000000002
Time, 2019-01-01T17:16:58, Epoch: 20, Batch: 20, Training Loss: 0.06659472808241844, LR: 0.010000000000000002
Time, 2019-01-01T17:16:59, Epoch: 20, Batch: 30, Training Loss: 0.055756570771336555, LR: 0.010000000000000002
Time, 2019-01-01T17:16:59, Epoch: 20, Batch: 40, Training Loss: 0.08512271419167519, LR: 0.010000000000000002
Time, 2019-01-01T17:17:00, Epoch: 20, Batch: 50, Training Loss: 0.06491473689675331, LR: 0.010000000000000002
Time, 2019-01-01T17:17:01, Epoch: 20, Batch: 60, Training Loss: 0.07872426733374596, LR: 0.010000000000000002
Time, 2019-01-01T17:17:01, Epoch: 20, Batch: 70, Training Loss: 0.06789700612425804, LR: 0.010000000000000002
Time, 2019-01-01T17:17:02, Epoch: 20, Batch: 80, Training Loss: 0.056068798154592515, LR: 0.010000000000000002
Time, 2019-01-01T17:17:03, Epoch: 20, Batch: 90, Training Loss: 0.07803002148866653, LR: 0.010000000000000002
Time, 2019-01-01T17:17:03, Epoch: 20, Batch: 100, Training Loss: 0.0861378289759159, LR: 0.010000000000000002
Time, 2019-01-01T17:17:04, Epoch: 20, Batch: 110, Training Loss: 0.07459929324686528, LR: 0.010000000000000002
Time, 2019-01-01T17:17:05, Epoch: 20, Batch: 120, Training Loss: 0.06317774802446366, LR: 0.010000000000000002
Time, 2019-01-01T17:17:05, Epoch: 20, Batch: 130, Training Loss: 0.0860446948558092, LR: 0.010000000000000002
Time, 2019-01-01T17:17:06, Epoch: 20, Batch: 140, Training Loss: 0.06720725260674953, LR: 0.010000000000000002
Time, 2019-01-01T17:17:07, Epoch: 20, Batch: 150, Training Loss: 0.08437201231718064, LR: 0.010000000000000002
Time, 2019-01-01T17:17:07, Epoch: 20, Batch: 160, Training Loss: 0.05348748564720154, LR: 0.010000000000000002
Time, 2019-01-01T17:17:08, Epoch: 20, Batch: 170, Training Loss: 0.08054205924272537, LR: 0.010000000000000002
Time, 2019-01-01T17:17:09, Epoch: 20, Batch: 180, Training Loss: 0.05433085486292839, LR: 0.010000000000000002
Time, 2019-01-01T17:17:10, Epoch: 20, Batch: 190, Training Loss: 0.057065123319625856, LR: 0.010000000000000002
Time, 2019-01-01T17:17:10, Epoch: 20, Batch: 200, Training Loss: 0.05400103591382503, LR: 0.010000000000000002
Time, 2019-01-01T17:17:11, Epoch: 20, Batch: 210, Training Loss: 0.08037891760468482, LR: 0.010000000000000002
Time, 2019-01-01T17:17:12, Epoch: 20, Batch: 220, Training Loss: 0.04979283101856709, LR: 0.010000000000000002
Time, 2019-01-01T17:17:12, Epoch: 20, Batch: 230, Training Loss: 0.0662310890853405, LR: 0.010000000000000002
Time, 2019-01-01T17:17:13, Epoch: 20, Batch: 240, Training Loss: 0.09843547493219376, LR: 0.010000000000000002
Time, 2019-01-01T17:17:14, Epoch: 20, Batch: 250, Training Loss: 0.08429543375968933, LR: 0.010000000000000002
Time, 2019-01-01T17:17:14, Epoch: 20, Batch: 260, Training Loss: 0.07597079388797283, LR: 0.010000000000000002
Time, 2019-01-01T17:17:15, Epoch: 20, Batch: 270, Training Loss: 0.05753625258803367, LR: 0.010000000000000002
Time, 2019-01-01T17:17:16, Epoch: 20, Batch: 280, Training Loss: 0.067211302369833, LR: 0.010000000000000002
Time, 2019-01-01T17:17:16, Epoch: 20, Batch: 290, Training Loss: 0.07631642371416092, LR: 0.010000000000000002
Time, 2019-01-01T17:17:17, Epoch: 20, Batch: 300, Training Loss: 0.07324473038315774, LR: 0.010000000000000002
Time, 2019-01-01T17:17:18, Epoch: 20, Batch: 310, Training Loss: 0.08409697487950325, LR: 0.010000000000000002
Time, 2019-01-01T17:17:18, Epoch: 20, Batch: 320, Training Loss: 0.04474465847015381, LR: 0.010000000000000002
Time, 2019-01-01T17:17:19, Epoch: 20, Batch: 330, Training Loss: 0.0581609383225441, LR: 0.010000000000000002
Time, 2019-01-01T17:17:20, Epoch: 20, Batch: 340, Training Loss: 0.04745682999491692, LR: 0.010000000000000002
Time, 2019-01-01T17:17:20, Epoch: 20, Batch: 350, Training Loss: 0.062053517624735835, LR: 0.010000000000000002
Time, 2019-01-01T17:17:21, Epoch: 20, Batch: 360, Training Loss: 0.05441079884767532, LR: 0.010000000000000002
Time, 2019-01-01T17:17:22, Epoch: 20, Batch: 370, Training Loss: 0.06753755249083042, LR: 0.010000000000000002
Time, 2019-01-01T17:17:23, Epoch: 20, Batch: 380, Training Loss: 0.06259267553687095, LR: 0.010000000000000002
Time, 2019-01-01T17:17:23, Epoch: 20, Batch: 390, Training Loss: 0.0475037656724453, LR: 0.010000000000000002
Time, 2019-01-01T17:17:24, Epoch: 20, Batch: 400, Training Loss: 0.06743769496679305, LR: 0.010000000000000002
Time, 2019-01-01T17:17:25, Epoch: 20, Batch: 410, Training Loss: 0.10190609730780124, LR: 0.010000000000000002
Time, 2019-01-01T17:17:25, Epoch: 20, Batch: 420, Training Loss: 0.11518247276544571, LR: 0.010000000000000002
Time, 2019-01-01T17:17:26, Epoch: 20, Batch: 430, Training Loss: 0.05570764541625976, LR: 0.010000000000000002
Time, 2019-01-01T17:17:27, Epoch: 20, Batch: 440, Training Loss: 0.06537784487009049, LR: 0.010000000000000002
Time, 2019-01-01T17:17:27, Epoch: 20, Batch: 450, Training Loss: 0.060252901166677475, LR: 0.010000000000000002
Time, 2019-01-01T17:17:28, Epoch: 20, Batch: 460, Training Loss: 0.07805219292640686, LR: 0.010000000000000002
Time, 2019-01-01T17:17:29, Epoch: 20, Batch: 470, Training Loss: 0.06463864222168922, LR: 0.010000000000000002
Time, 2019-01-01T17:17:29, Epoch: 20, Batch: 480, Training Loss: 0.06682249084115029, LR: 0.010000000000000002
Time, 2019-01-01T17:17:30, Epoch: 20, Batch: 490, Training Loss: 0.09113728776574134, LR: 0.010000000000000002
Time, 2019-01-01T17:17:31, Epoch: 20, Batch: 500, Training Loss: 0.0639965370297432, LR: 0.010000000000000002
Time, 2019-01-01T17:17:31, Epoch: 20, Batch: 510, Training Loss: 0.06835608780384064, LR: 0.010000000000000002
Time, 2019-01-01T17:17:32, Epoch: 20, Batch: 520, Training Loss: 0.057462071627378465, LR: 0.010000000000000002
Time, 2019-01-01T17:17:33, Epoch: 20, Batch: 530, Training Loss: 0.07283264100551605, LR: 0.010000000000000002
Time, 2019-01-01T17:17:34, Epoch: 20, Batch: 540, Training Loss: 0.04969610422849655, LR: 0.010000000000000002
Time, 2019-01-01T17:17:34, Epoch: 20, Batch: 550, Training Loss: 0.061034337431192395, LR: 0.010000000000000002
Time, 2019-01-01T17:17:35, Epoch: 20, Batch: 560, Training Loss: 0.08330112360417843, LR: 0.010000000000000002
Time, 2019-01-01T17:17:36, Epoch: 20, Batch: 570, Training Loss: 0.060431980341672895, LR: 0.010000000000000002
Time, 2019-01-01T17:17:36, Epoch: 20, Batch: 580, Training Loss: 0.07590069733560086, LR: 0.010000000000000002
Time, 2019-01-01T17:17:37, Epoch: 20, Batch: 590, Training Loss: 0.090587642416358, LR: 0.010000000000000002
Time, 2019-01-01T17:17:38, Epoch: 20, Batch: 600, Training Loss: 0.055478660389781, LR: 0.010000000000000002
Time, 2019-01-01T17:17:38, Epoch: 20, Batch: 610, Training Loss: 0.056416959315538404, LR: 0.010000000000000002
Time, 2019-01-01T17:17:39, Epoch: 20, Batch: 620, Training Loss: 0.07782761603593827, LR: 0.010000000000000002
Time, 2019-01-01T17:17:40, Epoch: 20, Batch: 630, Training Loss: 0.07297260239720345, LR: 0.010000000000000002
Time, 2019-01-01T17:17:40, Epoch: 20, Batch: 640, Training Loss: 0.052285943925380704, LR: 0.010000000000000002
Time, 2019-01-01T17:17:41, Epoch: 20, Batch: 650, Training Loss: 0.07550285421311856, LR: 0.010000000000000002
Time, 2019-01-01T17:17:42, Epoch: 20, Batch: 660, Training Loss: 0.06151825711131096, LR: 0.010000000000000002
Time, 2019-01-01T17:17:42, Epoch: 20, Batch: 670, Training Loss: 0.07645358741283417, LR: 0.010000000000000002
Time, 2019-01-01T17:17:43, Epoch: 20, Batch: 680, Training Loss: 0.056088628619909285, LR: 0.010000000000000002
Time, 2019-01-01T17:17:44, Epoch: 20, Batch: 690, Training Loss: 0.04895914308726788, LR: 0.010000000000000002
Time, 2019-01-01T17:17:44, Epoch: 20, Batch: 700, Training Loss: 0.06684183068573475, LR: 0.010000000000000002
Time, 2019-01-01T17:17:45, Epoch: 20, Batch: 710, Training Loss: 0.06942708007991313, LR: 0.010000000000000002
Time, 2019-01-01T17:17:46, Epoch: 20, Batch: 720, Training Loss: 0.10965799689292907, LR: 0.010000000000000002
Time, 2019-01-01T17:17:47, Epoch: 20, Batch: 730, Training Loss: 0.09119393229484558, LR: 0.010000000000000002
Time, 2019-01-01T17:17:47, Epoch: 20, Batch: 740, Training Loss: 0.09369255676865577, LR: 0.010000000000000002
Time, 2019-01-01T17:17:48, Epoch: 20, Batch: 750, Training Loss: 0.0820849385112524, LR: 0.010000000000000002
Time, 2019-01-01T17:17:49, Epoch: 20, Batch: 760, Training Loss: 0.06284392587840557, LR: 0.010000000000000002
Time, 2019-01-01T17:17:49, Epoch: 20, Batch: 770, Training Loss: 0.058614657074213025, LR: 0.010000000000000002
Time, 2019-01-01T17:17:50, Epoch: 20, Batch: 780, Training Loss: 0.06545906737446786, LR: 0.010000000000000002
Time, 2019-01-01T17:17:51, Epoch: 20, Batch: 790, Training Loss: 0.04816296845674515, LR: 0.010000000000000002
Time, 2019-01-01T17:17:51, Epoch: 20, Batch: 800, Training Loss: 0.06405962333083152, LR: 0.010000000000000002
Time, 2019-01-01T17:17:52, Epoch: 20, Batch: 810, Training Loss: 0.10862302407622337, LR: 0.010000000000000002
Time, 2019-01-01T17:17:53, Epoch: 20, Batch: 820, Training Loss: 0.07174934409558772, LR: 0.010000000000000002
Time, 2019-01-01T17:17:53, Epoch: 20, Batch: 830, Training Loss: 0.06659555435180664, LR: 0.010000000000000002
Time, 2019-01-01T17:17:54, Epoch: 20, Batch: 840, Training Loss: 0.07227271236479282, LR: 0.010000000000000002
Time, 2019-01-01T17:17:55, Epoch: 20, Batch: 850, Training Loss: 0.08021964989602566, LR: 0.010000000000000002
Time, 2019-01-01T17:17:55, Epoch: 20, Batch: 860, Training Loss: 0.05652791857719421, LR: 0.010000000000000002
Time, 2019-01-01T17:17:56, Epoch: 20, Batch: 870, Training Loss: 0.053669290989637373, LR: 0.010000000000000002
Time, 2019-01-01T17:17:57, Epoch: 20, Batch: 880, Training Loss: 0.05120630115270615, LR: 0.010000000000000002
Time, 2019-01-01T17:17:58, Epoch: 20, Batch: 890, Training Loss: 0.07682961449027062, LR: 0.010000000000000002
Time, 2019-01-01T17:17:58, Epoch: 20, Batch: 900, Training Loss: 0.0753425408154726, LR: 0.010000000000000002
Time, 2019-01-01T17:17:59, Epoch: 20, Batch: 910, Training Loss: 0.04415348246693611, LR: 0.010000000000000002
Time, 2019-01-01T17:18:00, Epoch: 20, Batch: 920, Training Loss: 0.08471234403550625, LR: 0.010000000000000002
Time, 2019-01-01T17:18:00, Epoch: 20, Batch: 930, Training Loss: 0.12414904609322548, LR: 0.010000000000000002
Epoch: 20, Validation Top 1 acc: 98.17097473144531
Epoch: 20, Validation Top 5 acc: 99.98167419433594
Epoch: 20, Validation Set Loss: 0.061207398772239685
Start training epoch 21
Time, 2019-01-01T17:18:26, Epoch: 21, Batch: 10, Training Loss: 0.05968809872865677, LR: 0.010000000000000002
Time, 2019-01-01T17:18:27, Epoch: 21, Batch: 20, Training Loss: 0.07023050673305989, LR: 0.010000000000000002
Time, 2019-01-01T17:18:27, Epoch: 21, Batch: 30, Training Loss: 0.05937451645731926, LR: 0.010000000000000002
Time, 2019-01-01T17:18:28, Epoch: 21, Batch: 40, Training Loss: 0.05150824896991253, LR: 0.010000000000000002
Time, 2019-01-01T17:18:29, Epoch: 21, Batch: 50, Training Loss: 0.07896120995283126, LR: 0.010000000000000002
Time, 2019-01-01T17:18:29, Epoch: 21, Batch: 60, Training Loss: 0.05628953501582146, LR: 0.010000000000000002
Time, 2019-01-01T17:18:30, Epoch: 21, Batch: 70, Training Loss: 0.05114087052643299, LR: 0.010000000000000002
Time, 2019-01-01T17:18:31, Epoch: 21, Batch: 80, Training Loss: 0.04337181076407433, LR: 0.010000000000000002
Time, 2019-01-01T17:18:31, Epoch: 21, Batch: 90, Training Loss: 0.04902223758399486, LR: 0.010000000000000002
Time, 2019-01-01T17:18:32, Epoch: 21, Batch: 100, Training Loss: 0.09776443466544152, LR: 0.010000000000000002
Time, 2019-01-01T17:18:33, Epoch: 21, Batch: 110, Training Loss: 0.05811189971864224, LR: 0.010000000000000002
Time, 2019-01-01T17:18:34, Epoch: 21, Batch: 120, Training Loss: 0.05783537551760674, LR: 0.010000000000000002
Time, 2019-01-01T17:18:34, Epoch: 21, Batch: 130, Training Loss: 0.07358560711145401, LR: 0.010000000000000002
Time, 2019-01-01T17:18:35, Epoch: 21, Batch: 140, Training Loss: 0.09887160658836365, LR: 0.010000000000000002
Time, 2019-01-01T17:18:36, Epoch: 21, Batch: 150, Training Loss: 0.06676101237535477, LR: 0.010000000000000002
Time, 2019-01-01T17:18:36, Epoch: 21, Batch: 160, Training Loss: 0.061732527986168864, LR: 0.010000000000000002
Time, 2019-01-01T17:18:37, Epoch: 21, Batch: 170, Training Loss: 0.04142244793474674, LR: 0.010000000000000002
Time, 2019-01-01T17:18:38, Epoch: 21, Batch: 180, Training Loss: 0.0495253287255764, LR: 0.010000000000000002
Time, 2019-01-01T17:18:38, Epoch: 21, Batch: 190, Training Loss: 0.05442277565598488, LR: 0.010000000000000002
Time, 2019-01-01T17:18:39, Epoch: 21, Batch: 200, Training Loss: 0.08174520134925842, LR: 0.010000000000000002
Time, 2019-01-01T17:18:40, Epoch: 21, Batch: 210, Training Loss: 0.04854638576507568, LR: 0.010000000000000002
Time, 2019-01-01T17:18:41, Epoch: 21, Batch: 220, Training Loss: 0.05762378796935082, LR: 0.010000000000000002
Time, 2019-01-01T17:18:41, Epoch: 21, Batch: 230, Training Loss: 0.06493299677968026, LR: 0.010000000000000002
Time, 2019-01-01T17:18:42, Epoch: 21, Batch: 240, Training Loss: 0.0707255970686674, LR: 0.010000000000000002
Time, 2019-01-01T17:18:43, Epoch: 21, Batch: 250, Training Loss: 0.06246904358267784, LR: 0.010000000000000002
Time, 2019-01-01T17:18:43, Epoch: 21, Batch: 260, Training Loss: 0.09519653096795082, LR: 0.010000000000000002
Time, 2019-01-01T17:18:44, Epoch: 21, Batch: 270, Training Loss: 0.07171922475099564, LR: 0.010000000000000002
Time, 2019-01-01T17:18:45, Epoch: 21, Batch: 280, Training Loss: 0.06205516308546066, LR: 0.010000000000000002
Time, 2019-01-01T17:18:45, Epoch: 21, Batch: 290, Training Loss: 0.06207631230354309, LR: 0.010000000000000002
Time, 2019-01-01T17:18:46, Epoch: 21, Batch: 300, Training Loss: 0.054711229354143145, LR: 0.010000000000000002
Time, 2019-01-01T17:18:47, Epoch: 21, Batch: 310, Training Loss: 0.060869713127613065, LR: 0.010000000000000002
Time, 2019-01-01T17:18:47, Epoch: 21, Batch: 320, Training Loss: 0.06896273344755173, LR: 0.010000000000000002
Time, 2019-01-01T17:18:48, Epoch: 21, Batch: 330, Training Loss: 0.091200802475214, LR: 0.010000000000000002
Time, 2019-01-01T17:18:49, Epoch: 21, Batch: 340, Training Loss: 0.07573295459151268, LR: 0.010000000000000002
Time, 2019-01-01T17:18:50, Epoch: 21, Batch: 350, Training Loss: 0.04865953475236893, LR: 0.010000000000000002
Time, 2019-01-01T17:18:50, Epoch: 21, Batch: 360, Training Loss: 0.07745350375771523, LR: 0.010000000000000002
Time, 2019-01-01T17:18:51, Epoch: 21, Batch: 370, Training Loss: 0.04958650358021259, LR: 0.010000000000000002
Time, 2019-01-01T17:18:52, Epoch: 21, Batch: 380, Training Loss: 0.0610761396586895, LR: 0.010000000000000002
Time, 2019-01-01T17:18:52, Epoch: 21, Batch: 390, Training Loss: 0.07238666974008083, LR: 0.010000000000000002
Time, 2019-01-01T17:18:53, Epoch: 21, Batch: 400, Training Loss: 0.06104009784758091, LR: 0.010000000000000002
Time, 2019-01-01T17:18:54, Epoch: 21, Batch: 410, Training Loss: 0.05667024552822113, LR: 0.010000000000000002
Time, 2019-01-01T17:18:54, Epoch: 21, Batch: 420, Training Loss: 0.051404766738414764, LR: 0.010000000000000002
Time, 2019-01-01T17:18:55, Epoch: 21, Batch: 430, Training Loss: 0.0861188281327486, LR: 0.010000000000000002
Time, 2019-01-01T17:18:56, Epoch: 21, Batch: 440, Training Loss: 0.07508049309253692, LR: 0.010000000000000002
Time, 2019-01-01T17:18:57, Epoch: 21, Batch: 450, Training Loss: 0.08578388020396233, LR: 0.010000000000000002
Time, 2019-01-01T17:18:57, Epoch: 21, Batch: 460, Training Loss: 0.07579801827669144, LR: 0.010000000000000002
Time, 2019-01-01T17:18:58, Epoch: 21, Batch: 470, Training Loss: 0.06236335784196854, LR: 0.010000000000000002
Time, 2019-01-01T17:18:59, Epoch: 21, Batch: 480, Training Loss: 0.05461451485753059, LR: 0.010000000000000002
Time, 2019-01-01T17:18:59, Epoch: 21, Batch: 490, Training Loss: 0.053090036287903784, LR: 0.010000000000000002
Time, 2019-01-01T17:19:00, Epoch: 21, Batch: 500, Training Loss: 0.04142475575208664, LR: 0.010000000000000002
Time, 2019-01-01T17:19:01, Epoch: 21, Batch: 510, Training Loss: 0.05425472073256969, LR: 0.010000000000000002
Time, 2019-01-01T17:19:01, Epoch: 21, Batch: 520, Training Loss: 0.038479693606495856, LR: 0.010000000000000002
Time, 2019-01-01T17:19:02, Epoch: 21, Batch: 530, Training Loss: 0.10099226012825965, LR: 0.010000000000000002
Time, 2019-01-01T17:19:03, Epoch: 21, Batch: 540, Training Loss: 0.05127054415643215, LR: 0.010000000000000002
Time, 2019-01-01T17:19:03, Epoch: 21, Batch: 550, Training Loss: 0.07692401744425297, LR: 0.010000000000000002
Time, 2019-01-01T17:19:04, Epoch: 21, Batch: 560, Training Loss: 0.04927194751799106, LR: 0.010000000000000002
Time, 2019-01-01T17:19:05, Epoch: 21, Batch: 570, Training Loss: 0.049971777200698855, LR: 0.010000000000000002
Time, 2019-01-01T17:19:06, Epoch: 21, Batch: 580, Training Loss: 0.06228994652628898, LR: 0.010000000000000002
Time, 2019-01-01T17:19:06, Epoch: 21, Batch: 590, Training Loss: 0.05206124000251293, LR: 0.010000000000000002
Time, 2019-01-01T17:19:07, Epoch: 21, Batch: 600, Training Loss: 0.061750154942274094, LR: 0.010000000000000002
Time, 2019-01-01T17:19:08, Epoch: 21, Batch: 610, Training Loss: 0.05082250013947487, LR: 0.010000000000000002
Time, 2019-01-01T17:19:08, Epoch: 21, Batch: 620, Training Loss: 0.0814067531377077, LR: 0.010000000000000002
Time, 2019-01-01T17:19:09, Epoch: 21, Batch: 630, Training Loss: 0.07041414752602577, LR: 0.010000000000000002
Time, 2019-01-01T17:19:10, Epoch: 21, Batch: 640, Training Loss: 0.05300373546779156, LR: 0.010000000000000002
Time, 2019-01-01T17:19:10, Epoch: 21, Batch: 650, Training Loss: 0.07210583873093128, LR: 0.010000000000000002
Time, 2019-01-01T17:19:11, Epoch: 21, Batch: 660, Training Loss: 0.053765563294291496, LR: 0.010000000000000002
Time, 2019-01-01T17:19:12, Epoch: 21, Batch: 670, Training Loss: 0.05506936013698578, LR: 0.010000000000000002
Time, 2019-01-01T17:19:13, Epoch: 21, Batch: 680, Training Loss: 0.06512487046420574, LR: 0.010000000000000002
Time, 2019-01-01T17:19:13, Epoch: 21, Batch: 690, Training Loss: 0.0637589517980814, LR: 0.010000000000000002
Time, 2019-01-01T17:19:14, Epoch: 21, Batch: 700, Training Loss: 0.0764600683003664, LR: 0.010000000000000002
Time, 2019-01-01T17:19:15, Epoch: 21, Batch: 710, Training Loss: 0.04716824665665627, LR: 0.010000000000000002
Time, 2019-01-01T17:19:15, Epoch: 21, Batch: 720, Training Loss: 0.06988872513175011, LR: 0.010000000000000002
Time, 2019-01-01T17:19:16, Epoch: 21, Batch: 730, Training Loss: 0.08837807402014733, LR: 0.010000000000000002
Time, 2019-01-01T17:19:17, Epoch: 21, Batch: 740, Training Loss: 0.04989446923136711, LR: 0.010000000000000002
Time, 2019-01-01T17:19:17, Epoch: 21, Batch: 750, Training Loss: 0.040468258410692216, LR: 0.010000000000000002
Time, 2019-01-01T17:19:18, Epoch: 21, Batch: 760, Training Loss: 0.04024392664432526, LR: 0.010000000000000002
Time, 2019-01-01T17:19:19, Epoch: 21, Batch: 770, Training Loss: 0.050871093198657034, LR: 0.010000000000000002
Time, 2019-01-01T17:19:19, Epoch: 21, Batch: 780, Training Loss: 0.09062971472740174, LR: 0.010000000000000002
Time, 2019-01-01T17:19:20, Epoch: 21, Batch: 790, Training Loss: 0.04555216543376446, LR: 0.010000000000000002
Time, 2019-01-01T17:19:21, Epoch: 21, Batch: 800, Training Loss: 0.05933156609535217, LR: 0.010000000000000002
Time, 2019-01-01T17:19:22, Epoch: 21, Batch: 810, Training Loss: 0.06379309818148612, LR: 0.010000000000000002
Time, 2019-01-01T17:19:22, Epoch: 21, Batch: 820, Training Loss: 0.04475608095526695, LR: 0.010000000000000002
Time, 2019-01-01T17:19:23, Epoch: 21, Batch: 830, Training Loss: 0.053172652050852774, LR: 0.010000000000000002
Time, 2019-01-01T17:19:24, Epoch: 21, Batch: 840, Training Loss: 0.06423592716455459, LR: 0.010000000000000002
Time, 2019-01-01T17:19:24, Epoch: 21, Batch: 850, Training Loss: 0.0854637186974287, LR: 0.010000000000000002
Time, 2019-01-01T17:19:25, Epoch: 21, Batch: 860, Training Loss: 0.09147504270076752, LR: 0.010000000000000002
Time, 2019-01-01T17:19:26, Epoch: 21, Batch: 870, Training Loss: 0.07286352142691613, LR: 0.010000000000000002
Time, 2019-01-01T17:19:26, Epoch: 21, Batch: 880, Training Loss: 0.05986417904496193, LR: 0.010000000000000002
Time, 2019-01-01T17:19:27, Epoch: 21, Batch: 890, Training Loss: 0.06858015730977059, LR: 0.010000000000000002
Time, 2019-01-01T17:19:28, Epoch: 21, Batch: 900, Training Loss: 0.05585520565509796, LR: 0.010000000000000002
Time, 2019-01-01T17:19:29, Epoch: 21, Batch: 910, Training Loss: 0.04115751683712006, LR: 0.010000000000000002
Time, 2019-01-01T17:19:29, Epoch: 21, Batch: 920, Training Loss: 0.054590276628732684, LR: 0.010000000000000002
Time, 2019-01-01T17:19:30, Epoch: 21, Batch: 930, Training Loss: 0.04705807268619537, LR: 0.010000000000000002
Epoch: 21, Validation Top 1 acc: 98.12933349609375
Epoch: 21, Validation Top 5 acc: 99.98334503173828
Epoch: 21, Validation Set Loss: 0.06105618178844452
Start training epoch 22
Time, 2019-01-01T17:19:56, Epoch: 22, Batch: 10, Training Loss: 0.0386592123657465, LR: 0.010000000000000002
Time, 2019-01-01T17:19:56, Epoch: 22, Batch: 20, Training Loss: 0.08165886849164963, LR: 0.010000000000000002
Time, 2019-01-01T17:19:57, Epoch: 22, Batch: 30, Training Loss: 0.06769443154335023, LR: 0.010000000000000002
Time, 2019-01-01T17:19:58, Epoch: 22, Batch: 40, Training Loss: 0.05642162561416626, LR: 0.010000000000000002
Time, 2019-01-01T17:19:58, Epoch: 22, Batch: 50, Training Loss: 0.06313054226338863, LR: 0.010000000000000002
Time, 2019-01-01T17:19:59, Epoch: 22, Batch: 60, Training Loss: 0.057379217445850374, LR: 0.010000000000000002
Time, 2019-01-01T17:20:00, Epoch: 22, Batch: 70, Training Loss: 0.0729384571313858, LR: 0.010000000000000002
Time, 2019-01-01T17:20:01, Epoch: 22, Batch: 80, Training Loss: 0.05012634471058845, LR: 0.010000000000000002
Time, 2019-01-01T17:20:01, Epoch: 22, Batch: 90, Training Loss: 0.06618074551224709, LR: 0.010000000000000002
Time, 2019-01-01T17:20:02, Epoch: 22, Batch: 100, Training Loss: 0.054083050787448884, LR: 0.010000000000000002
Time, 2019-01-01T17:20:03, Epoch: 22, Batch: 110, Training Loss: 0.06933827996253968, LR: 0.010000000000000002
Time, 2019-01-01T17:20:03, Epoch: 22, Batch: 120, Training Loss: 0.044091999903321265, LR: 0.010000000000000002
Time, 2019-01-01T17:20:04, Epoch: 22, Batch: 130, Training Loss: 0.08188015297055244, LR: 0.010000000000000002
Time, 2019-01-01T17:20:05, Epoch: 22, Batch: 140, Training Loss: 0.07496845461428166, LR: 0.010000000000000002
Time, 2019-01-01T17:20:05, Epoch: 22, Batch: 150, Training Loss: 0.051632807776331904, LR: 0.010000000000000002
Time, 2019-01-01T17:20:06, Epoch: 22, Batch: 160, Training Loss: 0.06542830541729927, LR: 0.010000000000000002
Time, 2019-01-01T17:20:07, Epoch: 22, Batch: 170, Training Loss: 0.0700696974992752, LR: 0.010000000000000002
Time, 2019-01-01T17:20:08, Epoch: 22, Batch: 180, Training Loss: 0.0578848373144865, LR: 0.010000000000000002
Time, 2019-01-01T17:20:08, Epoch: 22, Batch: 190, Training Loss: 0.0707196556031704, LR: 0.010000000000000002
Time, 2019-01-01T17:20:09, Epoch: 22, Batch: 200, Training Loss: 0.0648812010884285, LR: 0.010000000000000002
Time, 2019-01-01T17:20:10, Epoch: 22, Batch: 210, Training Loss: 0.06569915190339089, LR: 0.010000000000000002
Time, 2019-01-01T17:20:10, Epoch: 22, Batch: 220, Training Loss: 0.0547424990683794, LR: 0.010000000000000002
Time, 2019-01-01T17:20:11, Epoch: 22, Batch: 230, Training Loss: 0.0729314524680376, LR: 0.010000000000000002
Time, 2019-01-01T17:20:12, Epoch: 22, Batch: 240, Training Loss: 0.08204995468258858, LR: 0.010000000000000002
Time, 2019-01-01T17:20:12, Epoch: 22, Batch: 250, Training Loss: 0.06860312595963478, LR: 0.010000000000000002
Time, 2019-01-01T17:20:13, Epoch: 22, Batch: 260, Training Loss: 0.08438731208443642, LR: 0.010000000000000002
Time, 2019-01-01T17:20:14, Epoch: 22, Batch: 270, Training Loss: 0.07562523894011974, LR: 0.010000000000000002
Time, 2019-01-01T17:20:14, Epoch: 22, Batch: 280, Training Loss: 0.05237683616578579, LR: 0.010000000000000002
Time, 2019-01-01T17:20:15, Epoch: 22, Batch: 290, Training Loss: 0.041349902749061584, LR: 0.010000000000000002
Time, 2019-01-01T17:20:16, Epoch: 22, Batch: 300, Training Loss: 0.08684588298201561, LR: 0.010000000000000002
Time, 2019-01-01T17:20:17, Epoch: 22, Batch: 310, Training Loss: 0.07516478933393955, LR: 0.010000000000000002
Time, 2019-01-01T17:20:17, Epoch: 22, Batch: 320, Training Loss: 0.07347769849002361, LR: 0.010000000000000002
Time, 2019-01-01T17:20:18, Epoch: 22, Batch: 330, Training Loss: 0.05251181423664093, LR: 0.010000000000000002
Time, 2019-01-01T17:20:19, Epoch: 22, Batch: 340, Training Loss: 0.06448434442281722, LR: 0.010000000000000002
Time, 2019-01-01T17:20:19, Epoch: 22, Batch: 350, Training Loss: 0.07808167152106762, LR: 0.010000000000000002
Time, 2019-01-01T17:20:20, Epoch: 22, Batch: 360, Training Loss: 0.056563937291502955, LR: 0.010000000000000002
Time, 2019-01-01T17:20:21, Epoch: 22, Batch: 370, Training Loss: 0.07664092630147934, LR: 0.010000000000000002
Time, 2019-01-01T17:20:21, Epoch: 22, Batch: 380, Training Loss: 0.05243373960256577, LR: 0.010000000000000002
Time, 2019-01-01T17:20:22, Epoch: 22, Batch: 390, Training Loss: 0.0642369769513607, LR: 0.010000000000000002
Time, 2019-01-01T17:20:23, Epoch: 22, Batch: 400, Training Loss: 0.06449282430112362, LR: 0.010000000000000002
Time, 2019-01-01T17:20:23, Epoch: 22, Batch: 410, Training Loss: 0.08001460433006287, LR: 0.010000000000000002
Time, 2019-01-01T17:20:24, Epoch: 22, Batch: 420, Training Loss: 0.05861765667796135, LR: 0.010000000000000002
Time, 2019-01-01T17:20:25, Epoch: 22, Batch: 430, Training Loss: 0.06573275998234748, LR: 0.010000000000000002
Time, 2019-01-01T17:20:25, Epoch: 22, Batch: 440, Training Loss: 0.04193981625139713, LR: 0.010000000000000002
Time, 2019-01-01T17:20:26, Epoch: 22, Batch: 450, Training Loss: 0.08736754395067692, LR: 0.010000000000000002
Time, 2019-01-01T17:20:27, Epoch: 22, Batch: 460, Training Loss: 0.06085161790251732, LR: 0.010000000000000002
Time, 2019-01-01T17:20:28, Epoch: 22, Batch: 470, Training Loss: 0.06481813304126263, LR: 0.010000000000000002
Time, 2019-01-01T17:20:29, Epoch: 22, Batch: 480, Training Loss: 0.08359095454216003, LR: 0.010000000000000002
Time, 2019-01-01T17:20:29, Epoch: 22, Batch: 490, Training Loss: 0.06675561144948006, LR: 0.010000000000000002
Time, 2019-01-01T17:20:30, Epoch: 22, Batch: 500, Training Loss: 0.07147612795233727, LR: 0.010000000000000002
Time, 2019-01-01T17:20:31, Epoch: 22, Batch: 510, Training Loss: 0.07215904667973519, LR: 0.010000000000000002
Time, 2019-01-01T17:20:31, Epoch: 22, Batch: 520, Training Loss: 0.08292564712464809, LR: 0.010000000000000002
Time, 2019-01-01T17:20:32, Epoch: 22, Batch: 530, Training Loss: 0.07221611216664314, LR: 0.010000000000000002
Time, 2019-01-01T17:20:33, Epoch: 22, Batch: 540, Training Loss: 0.06482048369944096, LR: 0.010000000000000002
Time, 2019-01-01T17:20:34, Epoch: 22, Batch: 550, Training Loss: 0.06748402267694473, LR: 0.010000000000000002
Time, 2019-01-01T17:20:34, Epoch: 22, Batch: 560, Training Loss: 0.06754705272614955, LR: 0.010000000000000002
Time, 2019-01-01T17:20:35, Epoch: 22, Batch: 570, Training Loss: 0.05244581513106823, LR: 0.010000000000000002
Time, 2019-01-01T17:20:36, Epoch: 22, Batch: 580, Training Loss: 0.05284740403294563, LR: 0.010000000000000002
Time, 2019-01-01T17:20:37, Epoch: 22, Batch: 590, Training Loss: 0.04022647105157375, LR: 0.010000000000000002
Time, 2019-01-01T17:20:37, Epoch: 22, Batch: 600, Training Loss: 0.05528434701263905, LR: 0.010000000000000002
Time, 2019-01-01T17:20:38, Epoch: 22, Batch: 610, Training Loss: 0.04285524934530258, LR: 0.010000000000000002
Time, 2019-01-01T17:20:39, Epoch: 22, Batch: 620, Training Loss: 0.06555007137358189, LR: 0.010000000000000002
Time, 2019-01-01T17:20:39, Epoch: 22, Batch: 630, Training Loss: 0.05099011808633804, LR: 0.010000000000000002
Time, 2019-01-01T17:20:40, Epoch: 22, Batch: 640, Training Loss: 0.04386105015873909, LR: 0.010000000000000002
Time, 2019-01-01T17:20:41, Epoch: 22, Batch: 650, Training Loss: 0.061990974470973015, LR: 0.010000000000000002
Time, 2019-01-01T17:20:41, Epoch: 22, Batch: 660, Training Loss: 0.05610581114888191, LR: 0.010000000000000002
Time, 2019-01-01T17:20:42, Epoch: 22, Batch: 670, Training Loss: 0.04116802774369717, LR: 0.010000000000000002
Time, 2019-01-01T17:20:43, Epoch: 22, Batch: 680, Training Loss: 0.07346240021288394, LR: 0.010000000000000002
Time, 2019-01-01T17:20:44, Epoch: 22, Batch: 690, Training Loss: 0.07941180095076561, LR: 0.010000000000000002
Time, 2019-01-01T17:20:44, Epoch: 22, Batch: 700, Training Loss: 0.05991731621325016, LR: 0.010000000000000002
Time, 2019-01-01T17:20:45, Epoch: 22, Batch: 710, Training Loss: 0.06162731535732746, LR: 0.010000000000000002
Time, 2019-01-01T17:20:46, Epoch: 22, Batch: 720, Training Loss: 0.06527116261422634, LR: 0.010000000000000002
Time, 2019-01-01T17:20:46, Epoch: 22, Batch: 730, Training Loss: 0.05367331951856613, LR: 0.010000000000000002
Time, 2019-01-01T17:20:47, Epoch: 22, Batch: 740, Training Loss: 0.06105172149837017, LR: 0.010000000000000002
Time, 2019-01-01T17:20:48, Epoch: 22, Batch: 750, Training Loss: 0.0536716990172863, LR: 0.010000000000000002
Time, 2019-01-01T17:20:49, Epoch: 22, Batch: 760, Training Loss: 0.06140075959265232, LR: 0.010000000000000002
Time, 2019-01-01T17:20:49, Epoch: 22, Batch: 770, Training Loss: 0.04823109582066536, LR: 0.010000000000000002
Time, 2019-01-01T17:20:50, Epoch: 22, Batch: 780, Training Loss: 0.05870423652231693, LR: 0.010000000000000002
Time, 2019-01-01T17:20:51, Epoch: 22, Batch: 790, Training Loss: 0.07665674313902855, LR: 0.010000000000000002
Time, 2019-01-01T17:20:51, Epoch: 22, Batch: 800, Training Loss: 0.0561551034450531, LR: 0.010000000000000002
Time, 2019-01-01T17:20:52, Epoch: 22, Batch: 810, Training Loss: 0.0489375252276659, LR: 0.010000000000000002
Time, 2019-01-01T17:20:53, Epoch: 22, Batch: 820, Training Loss: 0.08060540333390236, LR: 0.010000000000000002
Time, 2019-01-01T17:20:53, Epoch: 22, Batch: 830, Training Loss: 0.06381330043077468, LR: 0.010000000000000002
Time, 2019-01-01T17:20:54, Epoch: 22, Batch: 840, Training Loss: 0.05157949849963188, LR: 0.010000000000000002
Time, 2019-01-01T17:20:55, Epoch: 22, Batch: 850, Training Loss: 0.05942749232053757, LR: 0.010000000000000002
Time, 2019-01-01T17:20:56, Epoch: 22, Batch: 860, Training Loss: 0.06680029183626175, LR: 0.010000000000000002
Time, 2019-01-01T17:20:56, Epoch: 22, Batch: 870, Training Loss: 0.07963528484106064, LR: 0.010000000000000002
Time, 2019-01-01T17:20:57, Epoch: 22, Batch: 880, Training Loss: 0.045037314668297766, LR: 0.010000000000000002
Time, 2019-01-01T17:20:58, Epoch: 22, Batch: 890, Training Loss: 0.06774225868284703, LR: 0.010000000000000002
Time, 2019-01-01T17:20:58, Epoch: 22, Batch: 900, Training Loss: 0.054856544360518456, LR: 0.010000000000000002
Time, 2019-01-01T17:20:59, Epoch: 22, Batch: 910, Training Loss: 0.04080136828124523, LR: 0.010000000000000002
Time, 2019-01-01T17:21:00, Epoch: 22, Batch: 920, Training Loss: 0.05280763357877731, LR: 0.010000000000000002
Time, 2019-01-01T17:21:01, Epoch: 22, Batch: 930, Training Loss: 0.061455176398158076, LR: 0.010000000000000002
Epoch: 22, Validation Top 1 acc: 98.30590057373047
Epoch: 22, Validation Top 5 acc: 99.98334503173828
Epoch: 22, Validation Set Loss: 0.05863356590270996
Start training epoch 23
Time, 2019-01-01T17:21:26, Epoch: 23, Batch: 10, Training Loss: 0.04909383729100227, LR: 0.010000000000000002
Time, 2019-01-01T17:21:27, Epoch: 23, Batch: 20, Training Loss: 0.05106753669679165, LR: 0.010000000000000002
Time, 2019-01-01T17:21:28, Epoch: 23, Batch: 30, Training Loss: 0.10794428996741771, LR: 0.010000000000000002
Time, 2019-01-01T17:21:29, Epoch: 23, Batch: 40, Training Loss: 0.05028738044202328, LR: 0.010000000000000002
Time, 2019-01-01T17:21:29, Epoch: 23, Batch: 50, Training Loss: 0.06445402428507804, LR: 0.010000000000000002
Time, 2019-01-01T17:21:30, Epoch: 23, Batch: 60, Training Loss: 0.05094196535646915, LR: 0.010000000000000002
Time, 2019-01-01T17:21:31, Epoch: 23, Batch: 70, Training Loss: 0.057872899994254115, LR: 0.010000000000000002
Time, 2019-01-01T17:21:31, Epoch: 23, Batch: 80, Training Loss: 0.056648320704698565, LR: 0.010000000000000002
Time, 2019-01-01T17:21:32, Epoch: 23, Batch: 90, Training Loss: 0.04891444779932499, LR: 0.010000000000000002
Time, 2019-01-01T17:21:33, Epoch: 23, Batch: 100, Training Loss: 0.05310595594346523, LR: 0.010000000000000002
Time, 2019-01-01T17:21:34, Epoch: 23, Batch: 110, Training Loss: 0.0648985855281353, LR: 0.010000000000000002
Time, 2019-01-01T17:21:34, Epoch: 23, Batch: 120, Training Loss: 0.05050566270947456, LR: 0.010000000000000002
Time, 2019-01-01T17:21:35, Epoch: 23, Batch: 130, Training Loss: 0.061645596846938135, LR: 0.010000000000000002
Time, 2019-01-01T17:21:36, Epoch: 23, Batch: 140, Training Loss: 0.08265676237642765, LR: 0.010000000000000002
Time, 2019-01-01T17:21:36, Epoch: 23, Batch: 150, Training Loss: 0.060445699095726016, LR: 0.010000000000000002
Time, 2019-01-01T17:21:37, Epoch: 23, Batch: 160, Training Loss: 0.0945177972316742, LR: 0.010000000000000002
Time, 2019-01-01T17:21:38, Epoch: 23, Batch: 170, Training Loss: 0.06790655292570591, LR: 0.010000000000000002
Time, 2019-01-01T17:21:39, Epoch: 23, Batch: 180, Training Loss: 0.06109454855322838, LR: 0.010000000000000002
Time, 2019-01-01T17:21:39, Epoch: 23, Batch: 190, Training Loss: 0.060388782620429994, LR: 0.010000000000000002
Time, 2019-01-01T17:21:40, Epoch: 23, Batch: 200, Training Loss: 0.07454452402889729, LR: 0.010000000000000002
Time, 2019-01-01T17:21:41, Epoch: 23, Batch: 210, Training Loss: 0.05254385620355606, LR: 0.010000000000000002
Time, 2019-01-01T17:21:41, Epoch: 23, Batch: 220, Training Loss: 0.052187691628932956, LR: 0.010000000000000002
Time, 2019-01-01T17:21:42, Epoch: 23, Batch: 230, Training Loss: 0.09454902708530426, LR: 0.010000000000000002
Time, 2019-01-01T17:21:43, Epoch: 23, Batch: 240, Training Loss: 0.06016823947429657, LR: 0.010000000000000002
Time, 2019-01-01T17:21:43, Epoch: 23, Batch: 250, Training Loss: 0.07825096994638443, LR: 0.010000000000000002
Time, 2019-01-01T17:21:44, Epoch: 23, Batch: 260, Training Loss: 0.05716622807085514, LR: 0.010000000000000002
Time, 2019-01-01T17:21:45, Epoch: 23, Batch: 270, Training Loss: 0.06830286122858524, LR: 0.010000000000000002
Time, 2019-01-01T17:21:46, Epoch: 23, Batch: 280, Training Loss: 0.05242104604840279, LR: 0.010000000000000002
Time, 2019-01-01T17:21:46, Epoch: 23, Batch: 290, Training Loss: 0.05873132310807705, LR: 0.010000000000000002
Time, 2019-01-01T17:21:47, Epoch: 23, Batch: 300, Training Loss: 0.05155816785991192, LR: 0.010000000000000002
Time, 2019-01-01T17:21:48, Epoch: 23, Batch: 310, Training Loss: 0.06363696604967117, LR: 0.010000000000000002
Time, 2019-01-01T17:21:48, Epoch: 23, Batch: 320, Training Loss: 0.061599505320191386, LR: 0.010000000000000002
Time, 2019-01-01T17:21:49, Epoch: 23, Batch: 330, Training Loss: 0.06579423658549785, LR: 0.010000000000000002
Time, 2019-01-01T17:21:50, Epoch: 23, Batch: 340, Training Loss: 0.061042191833257674, LR: 0.010000000000000002
Time, 2019-01-01T17:21:51, Epoch: 23, Batch: 350, Training Loss: 0.057619647681713106, LR: 0.010000000000000002
Time, 2019-01-01T17:21:51, Epoch: 23, Batch: 360, Training Loss: 0.06646168120205402, LR: 0.010000000000000002
Time, 2019-01-01T17:21:52, Epoch: 23, Batch: 370, Training Loss: 0.07536406144499778, LR: 0.010000000000000002
Time, 2019-01-01T17:21:53, Epoch: 23, Batch: 380, Training Loss: 0.07526536248624324, LR: 0.010000000000000002
Time, 2019-01-01T17:21:53, Epoch: 23, Batch: 390, Training Loss: 0.1080933753401041, LR: 0.010000000000000002
Time, 2019-01-01T17:21:54, Epoch: 23, Batch: 400, Training Loss: 0.07550700642168522, LR: 0.010000000000000002
Time, 2019-01-01T17:21:55, Epoch: 23, Batch: 410, Training Loss: 0.06172778829932213, LR: 0.010000000000000002
Time, 2019-01-01T17:21:56, Epoch: 23, Batch: 420, Training Loss: 0.06533369049429893, LR: 0.010000000000000002
Time, 2019-01-01T17:21:56, Epoch: 23, Batch: 430, Training Loss: 0.07404185272753239, LR: 0.010000000000000002
Time, 2019-01-01T17:21:57, Epoch: 23, Batch: 440, Training Loss: 0.055861986428499225, LR: 0.010000000000000002
Time, 2019-01-01T17:21:58, Epoch: 23, Batch: 450, Training Loss: 0.05490083247423172, LR: 0.010000000000000002
Time, 2019-01-01T17:21:58, Epoch: 23, Batch: 460, Training Loss: 0.08062356561422349, LR: 0.010000000000000002
Time, 2019-01-01T17:21:59, Epoch: 23, Batch: 470, Training Loss: 0.04005304500460625, LR: 0.010000000000000002
Time, 2019-01-01T17:22:00, Epoch: 23, Batch: 480, Training Loss: 0.06173948273062706, LR: 0.010000000000000002
Time, 2019-01-01T17:22:01, Epoch: 23, Batch: 490, Training Loss: 0.09025270454585552, LR: 0.010000000000000002
Time, 2019-01-01T17:22:02, Epoch: 23, Batch: 500, Training Loss: 0.052344056963920596, LR: 0.010000000000000002
Time, 2019-01-01T17:22:02, Epoch: 23, Batch: 510, Training Loss: 0.051628006622195244, LR: 0.010000000000000002
Time, 2019-01-01T17:22:03, Epoch: 23, Batch: 520, Training Loss: 0.05718415752053261, LR: 0.010000000000000002
Time, 2019-01-01T17:22:04, Epoch: 23, Batch: 530, Training Loss: 0.061846477538347246, LR: 0.010000000000000002
Time, 2019-01-01T17:22:04, Epoch: 23, Batch: 540, Training Loss: 0.043162976577878, LR: 0.010000000000000002
Time, 2019-01-01T17:22:05, Epoch: 23, Batch: 550, Training Loss: 0.061415646225214005, LR: 0.010000000000000002
Time, 2019-01-01T17:22:06, Epoch: 23, Batch: 560, Training Loss: 0.06732622869312763, LR: 0.010000000000000002
Time, 2019-01-01T17:22:07, Epoch: 23, Batch: 570, Training Loss: 0.04409684911370278, LR: 0.010000000000000002
Time, 2019-01-01T17:22:07, Epoch: 23, Batch: 580, Training Loss: 0.046964448690414426, LR: 0.010000000000000002
Time, 2019-01-01T17:22:08, Epoch: 23, Batch: 590, Training Loss: 0.06897760517895221, LR: 0.010000000000000002
Time, 2019-01-01T17:22:09, Epoch: 23, Batch: 600, Training Loss: 0.04491797238588333, LR: 0.010000000000000002
Time, 2019-01-01T17:22:09, Epoch: 23, Batch: 610, Training Loss: 0.0601643018424511, LR: 0.010000000000000002
Time, 2019-01-01T17:22:10, Epoch: 23, Batch: 620, Training Loss: 0.07190232016146184, LR: 0.010000000000000002
Time, 2019-01-01T17:22:11, Epoch: 23, Batch: 630, Training Loss: 0.05266626924276352, LR: 0.010000000000000002
Time, 2019-01-01T17:22:12, Epoch: 23, Batch: 640, Training Loss: 0.0710836872458458, LR: 0.010000000000000002
Time, 2019-01-01T17:22:12, Epoch: 23, Batch: 650, Training Loss: 0.08264734894037247, LR: 0.010000000000000002
Time, 2019-01-01T17:22:13, Epoch: 23, Batch: 660, Training Loss: 0.050276680290699004, LR: 0.010000000000000002
Time, 2019-01-01T17:22:14, Epoch: 23, Batch: 670, Training Loss: 0.09047799073159694, LR: 0.010000000000000002
Time, 2019-01-01T17:22:14, Epoch: 23, Batch: 680, Training Loss: 0.06133734136819839, LR: 0.010000000000000002
Time, 2019-01-01T17:22:15, Epoch: 23, Batch: 690, Training Loss: 0.05444650873541832, LR: 0.010000000000000002
Time, 2019-01-01T17:22:16, Epoch: 23, Batch: 700, Training Loss: 0.08703109323978424, LR: 0.010000000000000002
Time, 2019-01-01T17:22:17, Epoch: 23, Batch: 710, Training Loss: 0.09000080190598965, LR: 0.010000000000000002
Time, 2019-01-01T17:22:17, Epoch: 23, Batch: 720, Training Loss: 0.06553239338099956, LR: 0.010000000000000002
Time, 2019-01-01T17:22:18, Epoch: 23, Batch: 730, Training Loss: 0.05535469427704811, LR: 0.010000000000000002
Time, 2019-01-01T17:22:19, Epoch: 23, Batch: 740, Training Loss: 0.058736078441143036, LR: 0.010000000000000002
Time, 2019-01-01T17:22:19, Epoch: 23, Batch: 750, Training Loss: 0.07835146188735961, LR: 0.010000000000000002
Time, 2019-01-01T17:22:20, Epoch: 23, Batch: 760, Training Loss: 0.05464313328266144, LR: 0.010000000000000002
Time, 2019-01-01T17:22:21, Epoch: 23, Batch: 770, Training Loss: 0.06793376058340073, LR: 0.010000000000000002
Time, 2019-01-01T17:22:21, Epoch: 23, Batch: 780, Training Loss: 0.0609269991517067, LR: 0.010000000000000002
Time, 2019-01-01T17:22:22, Epoch: 23, Batch: 790, Training Loss: 0.06947712413966656, LR: 0.010000000000000002
Time, 2019-01-01T17:22:23, Epoch: 23, Batch: 800, Training Loss: 0.07587177269160747, LR: 0.010000000000000002
Time, 2019-01-01T17:22:24, Epoch: 23, Batch: 810, Training Loss: 0.04964615441858768, LR: 0.010000000000000002
Time, 2019-01-01T17:22:24, Epoch: 23, Batch: 820, Training Loss: 0.062966238707304, LR: 0.010000000000000002
Time, 2019-01-01T17:22:25, Epoch: 23, Batch: 830, Training Loss: 0.05692427381873131, LR: 0.010000000000000002
Time, 2019-01-01T17:22:26, Epoch: 23, Batch: 840, Training Loss: 0.06785208992660045, LR: 0.010000000000000002
Time, 2019-01-01T17:22:26, Epoch: 23, Batch: 850, Training Loss: 0.0812068410217762, LR: 0.010000000000000002
Time, 2019-01-01T17:22:27, Epoch: 23, Batch: 860, Training Loss: 0.07012881711125374, LR: 0.010000000000000002
Time, 2019-01-01T17:22:28, Epoch: 23, Batch: 870, Training Loss: 0.056472280994057655, LR: 0.010000000000000002
Time, 2019-01-01T17:22:29, Epoch: 23, Batch: 880, Training Loss: 0.06859940327703953, LR: 0.010000000000000002
Time, 2019-01-01T17:22:29, Epoch: 23, Batch: 890, Training Loss: 0.07873947769403458, LR: 0.010000000000000002
Time, 2019-01-01T17:22:30, Epoch: 23, Batch: 900, Training Loss: 0.07758249342441559, LR: 0.010000000000000002
Time, 2019-01-01T17:22:31, Epoch: 23, Batch: 910, Training Loss: 0.07054924815893174, LR: 0.010000000000000002
Time, 2019-01-01T17:22:32, Epoch: 23, Batch: 920, Training Loss: 0.04658595994114876, LR: 0.010000000000000002
Time, 2019-01-01T17:22:32, Epoch: 23, Batch: 930, Training Loss: 0.05596461221575737, LR: 0.010000000000000002
Epoch: 23, Validation Top 1 acc: 98.24593353271484
Epoch: 23, Validation Top 5 acc: 99.98001098632812
Epoch: 23, Validation Set Loss: 0.05943765863776207
Start training epoch 24
Time, 2019-01-01T17:22:59, Epoch: 24, Batch: 10, Training Loss: 0.04492674171924591, LR: 0.010000000000000002
Time, 2019-01-01T17:22:59, Epoch: 24, Batch: 20, Training Loss: 0.06313706561923027, LR: 0.010000000000000002
Time, 2019-01-01T17:23:00, Epoch: 24, Batch: 30, Training Loss: 0.05869631320238113, LR: 0.010000000000000002
Time, 2019-01-01T17:23:01, Epoch: 24, Batch: 40, Training Loss: 0.06563422307372094, LR: 0.010000000000000002
Time, 2019-01-01T17:23:02, Epoch: 24, Batch: 50, Training Loss: 0.07510748095810413, LR: 0.010000000000000002
Time, 2019-01-01T17:23:02, Epoch: 24, Batch: 60, Training Loss: 0.06207264363765717, LR: 0.010000000000000002
Time, 2019-01-01T17:23:03, Epoch: 24, Batch: 70, Training Loss: 0.04940546117722988, LR: 0.010000000000000002
Time, 2019-01-01T17:23:04, Epoch: 24, Batch: 80, Training Loss: 0.05968778096139431, LR: 0.010000000000000002
Time, 2019-01-01T17:23:04, Epoch: 24, Batch: 90, Training Loss: 0.09817380085587502, LR: 0.010000000000000002
Time, 2019-01-01T17:23:05, Epoch: 24, Batch: 100, Training Loss: 0.09298133254051208, LR: 0.010000000000000002
Time, 2019-01-01T17:23:06, Epoch: 24, Batch: 110, Training Loss: 0.060957107692956924, LR: 0.010000000000000002
Time, 2019-01-01T17:23:07, Epoch: 24, Batch: 120, Training Loss: 0.08902522288262844, LR: 0.010000000000000002
Time, 2019-01-01T17:23:07, Epoch: 24, Batch: 130, Training Loss: 0.08794652670621872, LR: 0.010000000000000002
Time, 2019-01-01T17:23:08, Epoch: 24, Batch: 140, Training Loss: 0.06755631528794766, LR: 0.010000000000000002
Time, 2019-01-01T17:23:09, Epoch: 24, Batch: 150, Training Loss: 0.05968952849507332, LR: 0.010000000000000002
Time, 2019-01-01T17:23:09, Epoch: 24, Batch: 160, Training Loss: 0.07476280257105827, LR: 0.010000000000000002
Time, 2019-01-01T17:23:10, Epoch: 24, Batch: 170, Training Loss: 0.05063783898949623, LR: 0.010000000000000002
Time, 2019-01-01T17:23:11, Epoch: 24, Batch: 180, Training Loss: 0.0850952461361885, LR: 0.010000000000000002
Time, 2019-01-01T17:23:12, Epoch: 24, Batch: 190, Training Loss: 0.04685732647776604, LR: 0.010000000000000002
Time, 2019-01-01T17:23:12, Epoch: 24, Batch: 200, Training Loss: 0.05248429700732231, LR: 0.010000000000000002
Time, 2019-01-01T17:23:13, Epoch: 24, Batch: 210, Training Loss: 0.03567783124744892, LR: 0.010000000000000002
Time, 2019-01-01T17:23:14, Epoch: 24, Batch: 220, Training Loss: 0.07197338193655015, LR: 0.010000000000000002
Time, 2019-01-01T17:23:14, Epoch: 24, Batch: 230, Training Loss: 0.048704835772514346, LR: 0.010000000000000002
Time, 2019-01-01T17:23:15, Epoch: 24, Batch: 240, Training Loss: 0.042626678571105005, LR: 0.010000000000000002
Time, 2019-01-01T17:23:16, Epoch: 24, Batch: 250, Training Loss: 0.06431320533156396, LR: 0.010000000000000002
Time, 2019-01-01T17:23:17, Epoch: 24, Batch: 260, Training Loss: 0.0543681338429451, LR: 0.010000000000000002
Time, 2019-01-01T17:23:17, Epoch: 24, Batch: 270, Training Loss: 0.06565892398357391, LR: 0.010000000000000002
Time, 2019-01-01T17:23:18, Epoch: 24, Batch: 280, Training Loss: 0.06117257140576839, LR: 0.010000000000000002
Time, 2019-01-01T17:23:19, Epoch: 24, Batch: 290, Training Loss: 0.05509168915450573, LR: 0.010000000000000002
Time, 2019-01-01T17:23:19, Epoch: 24, Batch: 300, Training Loss: 0.051873098313808444, LR: 0.010000000000000002
Time, 2019-01-01T17:23:20, Epoch: 24, Batch: 310, Training Loss: 0.045634014531970024, LR: 0.010000000000000002
Time, 2019-01-01T17:23:21, Epoch: 24, Batch: 320, Training Loss: 0.05953591726720333, LR: 0.010000000000000002
Time, 2019-01-01T17:23:22, Epoch: 24, Batch: 330, Training Loss: 0.07594979517161846, LR: 0.010000000000000002
Time, 2019-01-01T17:23:22, Epoch: 24, Batch: 340, Training Loss: 0.05501144044101238, LR: 0.010000000000000002
Time, 2019-01-01T17:23:23, Epoch: 24, Batch: 350, Training Loss: 0.07840655632317066, LR: 0.010000000000000002
Time, 2019-01-01T17:23:24, Epoch: 24, Batch: 360, Training Loss: 0.08278492949903012, LR: 0.010000000000000002
Time, 2019-01-01T17:23:25, Epoch: 24, Batch: 370, Training Loss: 0.05192303061485291, LR: 0.010000000000000002
Time, 2019-01-01T17:23:25, Epoch: 24, Batch: 380, Training Loss: 0.06407257169485092, LR: 0.010000000000000002
Time, 2019-01-01T17:23:26, Epoch: 24, Batch: 390, Training Loss: 0.05884208530187607, LR: 0.010000000000000002
Time, 2019-01-01T17:23:27, Epoch: 24, Batch: 400, Training Loss: 0.09556532502174378, LR: 0.010000000000000002
Time, 2019-01-01T17:23:27, Epoch: 24, Batch: 410, Training Loss: 0.04495906420052052, LR: 0.010000000000000002
Time, 2019-01-01T17:23:28, Epoch: 24, Batch: 420, Training Loss: 0.06746226474642754, LR: 0.010000000000000002
Time, 2019-01-01T17:23:29, Epoch: 24, Batch: 430, Training Loss: 0.06901117525994778, LR: 0.010000000000000002
Time, 2019-01-01T17:23:30, Epoch: 24, Batch: 440, Training Loss: 0.04892609566450119, LR: 0.010000000000000002
Time, 2019-01-01T17:23:30, Epoch: 24, Batch: 450, Training Loss: 0.07253150753676892, LR: 0.010000000000000002
Time, 2019-01-01T17:23:31, Epoch: 24, Batch: 460, Training Loss: 0.060974343493580815, LR: 0.010000000000000002
Time, 2019-01-01T17:23:32, Epoch: 24, Batch: 470, Training Loss: 0.07822933346033097, LR: 0.010000000000000002
Time, 2019-01-01T17:23:33, Epoch: 24, Batch: 480, Training Loss: 0.09022328183054924, LR: 0.010000000000000002
Time, 2019-01-01T17:23:33, Epoch: 24, Batch: 490, Training Loss: 0.06969363689422607, LR: 0.010000000000000002
Time, 2019-01-01T17:23:34, Epoch: 24, Batch: 500, Training Loss: 0.07258976325392723, LR: 0.010000000000000002
Time, 2019-01-01T17:23:35, Epoch: 24, Batch: 510, Training Loss: 0.06147075816988945, LR: 0.010000000000000002
Time, 2019-01-01T17:23:35, Epoch: 24, Batch: 520, Training Loss: 0.05856118947267532, LR: 0.010000000000000002
Time, 2019-01-01T17:23:36, Epoch: 24, Batch: 530, Training Loss: 0.055759462714195254, LR: 0.010000000000000002
Time, 2019-01-01T17:23:37, Epoch: 24, Batch: 540, Training Loss: 0.07937123589217662, LR: 0.010000000000000002
Time, 2019-01-01T17:23:38, Epoch: 24, Batch: 550, Training Loss: 0.09122292548418046, LR: 0.010000000000000002
Time, 2019-01-01T17:23:38, Epoch: 24, Batch: 560, Training Loss: 0.0662869706749916, LR: 0.010000000000000002
Time, 2019-01-01T17:23:39, Epoch: 24, Batch: 570, Training Loss: 0.06691415384411811, LR: 0.010000000000000002
Time, 2019-01-01T17:23:40, Epoch: 24, Batch: 580, Training Loss: 0.09658591598272323, LR: 0.010000000000000002
Time, 2019-01-01T17:23:41, Epoch: 24, Batch: 590, Training Loss: 0.0812893606722355, LR: 0.010000000000000002
Time, 2019-01-01T17:23:41, Epoch: 24, Batch: 600, Training Loss: 0.05269100069999695, LR: 0.010000000000000002
Time, 2019-01-01T17:23:42, Epoch: 24, Batch: 610, Training Loss: 0.07031301781535149, LR: 0.010000000000000002
Time, 2019-01-01T17:23:43, Epoch: 24, Batch: 620, Training Loss: 0.06571949794888496, LR: 0.010000000000000002
Time, 2019-01-01T17:23:44, Epoch: 24, Batch: 630, Training Loss: 0.1007812898606062, LR: 0.010000000000000002
Time, 2019-01-01T17:23:44, Epoch: 24, Batch: 640, Training Loss: 0.060849529877305034, LR: 0.010000000000000002
Time, 2019-01-01T17:23:45, Epoch: 24, Batch: 650, Training Loss: 0.07110303081572056, LR: 0.010000000000000002
Time, 2019-01-01T17:23:46, Epoch: 24, Batch: 660, Training Loss: 0.07495400086045265, LR: 0.010000000000000002
Time, 2019-01-01T17:23:46, Epoch: 24, Batch: 670, Training Loss: 0.0845290120691061, LR: 0.010000000000000002
Time, 2019-01-01T17:23:47, Epoch: 24, Batch: 680, Training Loss: 0.10442667305469513, LR: 0.010000000000000002
Time, 2019-01-01T17:23:48, Epoch: 24, Batch: 690, Training Loss: 0.06980837509036064, LR: 0.010000000000000002
Time, 2019-01-01T17:23:49, Epoch: 24, Batch: 700, Training Loss: 0.04772158600389957, LR: 0.010000000000000002
Time, 2019-01-01T17:23:49, Epoch: 24, Batch: 710, Training Loss: 0.0726959940046072, LR: 0.010000000000000002
Time, 2019-01-01T17:23:50, Epoch: 24, Batch: 720, Training Loss: 0.03896213509142399, LR: 0.010000000000000002
Time, 2019-01-01T17:23:51, Epoch: 24, Batch: 730, Training Loss: 0.06419760808348655, LR: 0.010000000000000002
Time, 2019-01-01T17:23:52, Epoch: 24, Batch: 740, Training Loss: 0.12831191644072532, LR: 0.010000000000000002
Time, 2019-01-01T17:23:52, Epoch: 24, Batch: 750, Training Loss: 0.06741805225610734, LR: 0.010000000000000002
Time, 2019-01-01T17:23:53, Epoch: 24, Batch: 760, Training Loss: 0.06398836374282837, LR: 0.010000000000000002
Time, 2019-01-01T17:23:54, Epoch: 24, Batch: 770, Training Loss: 0.06791205443441868, LR: 0.010000000000000002
Time, 2019-01-01T17:23:55, Epoch: 24, Batch: 780, Training Loss: 0.058159364759922026, LR: 0.010000000000000002
Time, 2019-01-01T17:23:55, Epoch: 24, Batch: 790, Training Loss: 0.0543687142431736, LR: 0.010000000000000002
Time, 2019-01-01T17:23:56, Epoch: 24, Batch: 800, Training Loss: 0.05947878472507, LR: 0.010000000000000002
Time, 2019-01-01T17:23:57, Epoch: 24, Batch: 810, Training Loss: 0.05938027687370777, LR: 0.010000000000000002
Time, 2019-01-01T17:23:57, Epoch: 24, Batch: 820, Training Loss: 0.05829378217458725, LR: 0.010000000000000002
Time, 2019-01-01T17:23:58, Epoch: 24, Batch: 830, Training Loss: 0.04595072530210018, LR: 0.010000000000000002
Time, 2019-01-01T17:23:59, Epoch: 24, Batch: 840, Training Loss: 0.06505633406341076, LR: 0.010000000000000002
Time, 2019-01-01T17:24:00, Epoch: 24, Batch: 850, Training Loss: 0.06075066849589348, LR: 0.010000000000000002
Time, 2019-01-01T17:24:00, Epoch: 24, Batch: 860, Training Loss: 0.05707405135035515, LR: 0.010000000000000002
Time, 2019-01-01T17:24:01, Epoch: 24, Batch: 870, Training Loss: 0.047673606872558595, LR: 0.010000000000000002
Time, 2019-01-01T17:24:02, Epoch: 24, Batch: 880, Training Loss: 0.038801535964012146, LR: 0.010000000000000002
Time, 2019-01-01T17:24:03, Epoch: 24, Batch: 890, Training Loss: 0.060149367153644565, LR: 0.010000000000000002
Time, 2019-01-01T17:24:03, Epoch: 24, Batch: 900, Training Loss: 0.039984260499477384, LR: 0.010000000000000002
Time, 2019-01-01T17:24:04, Epoch: 24, Batch: 910, Training Loss: 0.1132079690694809, LR: 0.010000000000000002
Time, 2019-01-01T17:24:05, Epoch: 24, Batch: 920, Training Loss: 0.05194331146776676, LR: 0.010000000000000002
Time, 2019-01-01T17:24:06, Epoch: 24, Batch: 930, Training Loss: 0.06929301396012306, LR: 0.010000000000000002
Epoch: 24, Validation Top 1 acc: 97.76786041259766
Epoch: 24, Validation Top 5 acc: 99.9850082397461
Epoch: 24, Validation Set Loss: 0.07031319290399551
Start training epoch 25
Time, 2019-01-01T17:24:32, Epoch: 25, Batch: 10, Training Loss: 0.08588381856679916, LR: 0.010000000000000002
Time, 2019-01-01T17:24:33, Epoch: 25, Batch: 20, Training Loss: 0.059243740141391756, LR: 0.010000000000000002
Time, 2019-01-01T17:24:33, Epoch: 25, Batch: 30, Training Loss: 0.06519161760807038, LR: 0.010000000000000002
Time, 2019-01-01T17:24:34, Epoch: 25, Batch: 40, Training Loss: 0.06756875589489937, LR: 0.010000000000000002
Time, 2019-01-01T17:24:35, Epoch: 25, Batch: 50, Training Loss: 0.05298207327723503, LR: 0.010000000000000002
Time, 2019-01-01T17:24:36, Epoch: 25, Batch: 60, Training Loss: 0.06720131933689118, LR: 0.010000000000000002
Time, 2019-01-01T17:24:36, Epoch: 25, Batch: 70, Training Loss: 0.07631561607122421, LR: 0.010000000000000002
Time, 2019-01-01T17:24:37, Epoch: 25, Batch: 80, Training Loss: 0.04935482628643513, LR: 0.010000000000000002
Time, 2019-01-01T17:24:38, Epoch: 25, Batch: 90, Training Loss: 0.06515843793749809, LR: 0.010000000000000002
Time, 2019-01-01T17:24:39, Epoch: 25, Batch: 100, Training Loss: 0.04980290606617928, LR: 0.010000000000000002
Time, 2019-01-01T17:24:39, Epoch: 25, Batch: 110, Training Loss: 0.053605330735445024, LR: 0.010000000000000002
Time, 2019-01-01T17:24:40, Epoch: 25, Batch: 120, Training Loss: 0.07744110487401486, LR: 0.010000000000000002
Time, 2019-01-01T17:24:41, Epoch: 25, Batch: 130, Training Loss: 0.07025759033858776, LR: 0.010000000000000002
Time, 2019-01-01T17:24:41, Epoch: 25, Batch: 140, Training Loss: 0.057202840223908424, LR: 0.010000000000000002
Time, 2019-01-01T17:24:42, Epoch: 25, Batch: 150, Training Loss: 0.0634555995464325, LR: 0.010000000000000002
Time, 2019-01-01T17:24:43, Epoch: 25, Batch: 160, Training Loss: 0.07215161174535752, LR: 0.010000000000000002
Time, 2019-01-01T17:24:44, Epoch: 25, Batch: 170, Training Loss: 0.08372307047247887, LR: 0.010000000000000002
Time, 2019-01-01T17:24:44, Epoch: 25, Batch: 180, Training Loss: 0.07651053704321384, LR: 0.010000000000000002
Time, 2019-01-01T17:24:45, Epoch: 25, Batch: 190, Training Loss: 0.09112100675702095, LR: 0.010000000000000002
Time, 2019-01-01T17:24:46, Epoch: 25, Batch: 200, Training Loss: 0.052476761117577556, LR: 0.010000000000000002
Time, 2019-01-01T17:24:47, Epoch: 25, Batch: 210, Training Loss: 0.04210021495819092, LR: 0.010000000000000002
Time, 2019-01-01T17:24:47, Epoch: 25, Batch: 220, Training Loss: 0.06281921714544296, LR: 0.010000000000000002
Time, 2019-01-01T17:24:48, Epoch: 25, Batch: 230, Training Loss: 0.07396838255226612, LR: 0.010000000000000002
Time, 2019-01-01T17:24:49, Epoch: 25, Batch: 240, Training Loss: 0.05887527503073216, LR: 0.010000000000000002
Time, 2019-01-01T17:24:49, Epoch: 25, Batch: 250, Training Loss: 0.06338965743780137, LR: 0.010000000000000002
Time, 2019-01-01T17:24:50, Epoch: 25, Batch: 260, Training Loss: 0.06329095549881458, LR: 0.010000000000000002
Time, 2019-01-01T17:24:51, Epoch: 25, Batch: 270, Training Loss: 0.08608981519937516, LR: 0.010000000000000002
Time, 2019-01-01T17:24:52, Epoch: 25, Batch: 280, Training Loss: 0.05907414443790913, LR: 0.010000000000000002
Time, 2019-01-01T17:24:52, Epoch: 25, Batch: 290, Training Loss: 0.047973882779479025, LR: 0.010000000000000002
Time, 2019-01-01T17:24:53, Epoch: 25, Batch: 300, Training Loss: 0.06289793848991394, LR: 0.010000000000000002
Time, 2019-01-01T17:24:54, Epoch: 25, Batch: 310, Training Loss: 0.07189316377043724, LR: 0.010000000000000002
Time, 2019-01-01T17:24:55, Epoch: 25, Batch: 320, Training Loss: 0.0384366575628519, LR: 0.010000000000000002
Time, 2019-01-01T17:24:55, Epoch: 25, Batch: 330, Training Loss: 0.056178735196590425, LR: 0.010000000000000002
Time, 2019-01-01T17:24:56, Epoch: 25, Batch: 340, Training Loss: 0.04746772460639477, LR: 0.010000000000000002
Time, 2019-01-01T17:24:57, Epoch: 25, Batch: 350, Training Loss: 0.07156152874231339, LR: 0.010000000000000002
Time, 2019-01-01T17:24:57, Epoch: 25, Batch: 360, Training Loss: 0.06337341628968715, LR: 0.010000000000000002
Time, 2019-01-01T17:24:58, Epoch: 25, Batch: 370, Training Loss: 0.04458396248519421, LR: 0.010000000000000002
Time, 2019-01-01T17:24:59, Epoch: 25, Batch: 380, Training Loss: 0.0801079086959362, LR: 0.010000000000000002
Time, 2019-01-01T17:25:00, Epoch: 25, Batch: 390, Training Loss: 0.07264757230877876, LR: 0.010000000000000002
Time, 2019-01-01T17:25:00, Epoch: 25, Batch: 400, Training Loss: 0.059404737502336505, LR: 0.010000000000000002
Time, 2019-01-01T17:25:01, Epoch: 25, Batch: 410, Training Loss: 0.051903621107339856, LR: 0.010000000000000002
Time, 2019-01-01T17:25:02, Epoch: 25, Batch: 420, Training Loss: 0.06034619174897671, LR: 0.010000000000000002
Time, 2019-01-01T17:25:02, Epoch: 25, Batch: 430, Training Loss: 0.05832997784018516, LR: 0.010000000000000002
Time, 2019-01-01T17:25:03, Epoch: 25, Batch: 440, Training Loss: 0.0641615279018879, LR: 0.010000000000000002
Time, 2019-01-01T17:25:04, Epoch: 25, Batch: 450, Training Loss: 0.054753709211945535, LR: 0.010000000000000002
Time, 2019-01-01T17:25:05, Epoch: 25, Batch: 460, Training Loss: 0.05057790987193585, LR: 0.010000000000000002
Time, 2019-01-01T17:25:05, Epoch: 25, Batch: 470, Training Loss: 0.07003478556871415, LR: 0.010000000000000002
Time, 2019-01-01T17:25:06, Epoch: 25, Batch: 480, Training Loss: 0.04441750608384609, LR: 0.010000000000000002
Time, 2019-01-01T17:25:07, Epoch: 25, Batch: 490, Training Loss: 0.06342249214649201, LR: 0.010000000000000002
Time, 2019-01-01T17:25:08, Epoch: 25, Batch: 500, Training Loss: 0.07925385273993016, LR: 0.010000000000000002
Time, 2019-01-01T17:25:08, Epoch: 25, Batch: 510, Training Loss: 0.04115729555487633, LR: 0.010000000000000002
Time, 2019-01-01T17:25:09, Epoch: 25, Batch: 520, Training Loss: 0.09234956838190556, LR: 0.010000000000000002
Time, 2019-01-01T17:25:10, Epoch: 25, Batch: 530, Training Loss: 0.04581912159919739, LR: 0.010000000000000002
Time, 2019-01-01T17:25:10, Epoch: 25, Batch: 540, Training Loss: 0.05496629402041435, LR: 0.010000000000000002
Time, 2019-01-01T17:25:11, Epoch: 25, Batch: 550, Training Loss: 0.031040070205926897, LR: 0.010000000000000002
Time, 2019-01-01T17:25:12, Epoch: 25, Batch: 560, Training Loss: 0.04556632786989212, LR: 0.010000000000000002
Time, 2019-01-01T17:25:12, Epoch: 25, Batch: 570, Training Loss: 0.03992107212543487, LR: 0.010000000000000002
Time, 2019-01-01T17:25:13, Epoch: 25, Batch: 580, Training Loss: 0.051577524095773694, LR: 0.010000000000000002
Time, 2019-01-01T17:25:14, Epoch: 25, Batch: 590, Training Loss: 0.06488748975098133, LR: 0.010000000000000002
Time, 2019-01-01T17:25:15, Epoch: 25, Batch: 600, Training Loss: 0.060794968903064725, LR: 0.010000000000000002
Time, 2019-01-01T17:25:15, Epoch: 25, Batch: 610, Training Loss: 0.0532754685729742, LR: 0.010000000000000002
Time, 2019-01-01T17:25:16, Epoch: 25, Batch: 620, Training Loss: 0.05385509692132473, LR: 0.010000000000000002
Time, 2019-01-01T17:25:17, Epoch: 25, Batch: 630, Training Loss: 0.06712284758687019, LR: 0.010000000000000002
Time, 2019-01-01T17:25:17, Epoch: 25, Batch: 640, Training Loss: 0.058425509929656984, LR: 0.010000000000000002
Time, 2019-01-01T17:25:18, Epoch: 25, Batch: 650, Training Loss: 0.05399929955601692, LR: 0.010000000000000002
Time, 2019-01-01T17:25:19, Epoch: 25, Batch: 660, Training Loss: 0.06794755719602108, LR: 0.010000000000000002
Time, 2019-01-01T17:25:20, Epoch: 25, Batch: 670, Training Loss: 0.10385705381631852, LR: 0.010000000000000002
Time, 2019-01-01T17:25:20, Epoch: 25, Batch: 680, Training Loss: 0.07317969910800456, LR: 0.010000000000000002
Time, 2019-01-01T17:25:21, Epoch: 25, Batch: 690, Training Loss: 0.07002894543111324, LR: 0.010000000000000002
Time, 2019-01-01T17:25:22, Epoch: 25, Batch: 700, Training Loss: 0.04652698785066604, LR: 0.010000000000000002
Time, 2019-01-01T17:25:22, Epoch: 25, Batch: 710, Training Loss: 0.04952959306538105, LR: 0.010000000000000002
Time, 2019-01-01T17:25:23, Epoch: 25, Batch: 720, Training Loss: 0.05446442812681198, LR: 0.010000000000000002
Time, 2019-01-01T17:25:24, Epoch: 25, Batch: 730, Training Loss: 0.06213744319975376, LR: 0.010000000000000002
Time, 2019-01-01T17:25:25, Epoch: 25, Batch: 740, Training Loss: 0.049660774320364, LR: 0.010000000000000002
Time, 2019-01-01T17:25:25, Epoch: 25, Batch: 750, Training Loss: 0.0958730436861515, LR: 0.010000000000000002
Time, 2019-01-01T17:25:26, Epoch: 25, Batch: 760, Training Loss: 0.0727700013667345, LR: 0.010000000000000002
Time, 2019-01-01T17:25:27, Epoch: 25, Batch: 770, Training Loss: 0.06466796360909939, LR: 0.010000000000000002
Time, 2019-01-01T17:25:27, Epoch: 25, Batch: 780, Training Loss: 0.07249512448906899, LR: 0.010000000000000002
Time, 2019-01-01T17:25:28, Epoch: 25, Batch: 790, Training Loss: 0.05170594118535519, LR: 0.010000000000000002
Time, 2019-01-01T17:25:29, Epoch: 25, Batch: 800, Training Loss: 0.07334016226232051, LR: 0.010000000000000002
Time, 2019-01-01T17:25:30, Epoch: 25, Batch: 810, Training Loss: 0.04858827963471413, LR: 0.010000000000000002
Time, 2019-01-01T17:25:30, Epoch: 25, Batch: 820, Training Loss: 0.04950941726565361, LR: 0.010000000000000002
Time, 2019-01-01T17:25:31, Epoch: 25, Batch: 830, Training Loss: 0.05068881325423717, LR: 0.010000000000000002
Time, 2019-01-01T17:25:32, Epoch: 25, Batch: 840, Training Loss: 0.06477298513054848, LR: 0.010000000000000002
Time, 2019-01-01T17:25:33, Epoch: 25, Batch: 850, Training Loss: 0.07144259959459305, LR: 0.010000000000000002
Time, 2019-01-01T17:25:33, Epoch: 25, Batch: 860, Training Loss: 0.07294402569532395, LR: 0.010000000000000002
Time, 2019-01-01T17:25:34, Epoch: 25, Batch: 870, Training Loss: 0.05613954775035381, LR: 0.010000000000000002
Time, 2019-01-01T17:25:35, Epoch: 25, Batch: 880, Training Loss: 0.10369004309177399, LR: 0.010000000000000002
Time, 2019-01-01T17:25:35, Epoch: 25, Batch: 890, Training Loss: 0.0823451541364193, LR: 0.010000000000000002
Time, 2019-01-01T17:25:36, Epoch: 25, Batch: 900, Training Loss: 0.05484638139605522, LR: 0.010000000000000002
Time, 2019-01-01T17:25:37, Epoch: 25, Batch: 910, Training Loss: 0.04728578552603722, LR: 0.010000000000000002
Time, 2019-01-01T17:25:38, Epoch: 25, Batch: 920, Training Loss: 0.06654374003410339, LR: 0.010000000000000002
Time, 2019-01-01T17:25:38, Epoch: 25, Batch: 930, Training Loss: 0.07067423984408379, LR: 0.010000000000000002
Epoch: 25, Validation Top 1 acc: 98.37919616699219
Epoch: 25, Validation Top 5 acc: 99.99166870117188
Epoch: 25, Validation Set Loss: 0.05561530217528343
Start training epoch 26
Time, 2019-01-01T17:26:04, Epoch: 26, Batch: 10, Training Loss: 0.05391091406345368, LR: 0.010000000000000002
Time, 2019-01-01T17:26:05, Epoch: 26, Batch: 20, Training Loss: 0.05183444581925869, LR: 0.010000000000000002
Time, 2019-01-01T17:26:06, Epoch: 26, Batch: 30, Training Loss: 0.0619362261146307, LR: 0.010000000000000002
Time, 2019-01-01T17:26:06, Epoch: 26, Batch: 40, Training Loss: 0.07525756619870663, LR: 0.010000000000000002
Time, 2019-01-01T17:26:07, Epoch: 26, Batch: 50, Training Loss: 0.0512852780520916, LR: 0.010000000000000002
Time, 2019-01-01T17:26:08, Epoch: 26, Batch: 60, Training Loss: 0.058541398495435715, LR: 0.010000000000000002
Time, 2019-01-01T17:26:08, Epoch: 26, Batch: 70, Training Loss: 0.08654182739555835, LR: 0.010000000000000002
Time, 2019-01-01T17:26:09, Epoch: 26, Batch: 80, Training Loss: 0.07093859389424324, LR: 0.010000000000000002
Time, 2019-01-01T17:26:10, Epoch: 26, Batch: 90, Training Loss: 0.03782665692269802, LR: 0.010000000000000002
Time, 2019-01-01T17:26:11, Epoch: 26, Batch: 100, Training Loss: 0.04421779252588749, LR: 0.010000000000000002
Time, 2019-01-01T17:26:11, Epoch: 26, Batch: 110, Training Loss: 0.05540640689432621, LR: 0.010000000000000002
Time, 2019-01-01T17:26:12, Epoch: 26, Batch: 120, Training Loss: 0.06477491669356823, LR: 0.010000000000000002
Time, 2019-01-01T17:26:13, Epoch: 26, Batch: 130, Training Loss: 0.06755951717495919, LR: 0.010000000000000002
Time, 2019-01-01T17:26:13, Epoch: 26, Batch: 140, Training Loss: 0.05652177222073078, LR: 0.010000000000000002
Time, 2019-01-01T17:26:14, Epoch: 26, Batch: 150, Training Loss: 0.08494661301374436, LR: 0.010000000000000002
Time, 2019-01-01T17:26:15, Epoch: 26, Batch: 160, Training Loss: 0.06809512004256249, LR: 0.010000000000000002
Time, 2019-01-01T17:26:16, Epoch: 26, Batch: 170, Training Loss: 0.06897853761911392, LR: 0.010000000000000002
Time, 2019-01-01T17:26:16, Epoch: 26, Batch: 180, Training Loss: 0.04928402192890644, LR: 0.010000000000000002
Time, 2019-01-01T17:26:17, Epoch: 26, Batch: 190, Training Loss: 0.05119647458195686, LR: 0.010000000000000002
Time, 2019-01-01T17:26:18, Epoch: 26, Batch: 200, Training Loss: 0.05159730650484562, LR: 0.010000000000000002
Time, 2019-01-01T17:26:18, Epoch: 26, Batch: 210, Training Loss: 0.07384624853730201, LR: 0.010000000000000002
Time, 2019-01-01T17:26:19, Epoch: 26, Batch: 220, Training Loss: 0.0732637483626604, LR: 0.010000000000000002
Time, 2019-01-01T17:26:20, Epoch: 26, Batch: 230, Training Loss: 0.03959171660244465, LR: 0.010000000000000002
Time, 2019-01-01T17:26:20, Epoch: 26, Batch: 240, Training Loss: 0.06975129544734955, LR: 0.010000000000000002
Time, 2019-01-01T17:26:21, Epoch: 26, Batch: 250, Training Loss: 0.062032625824213025, LR: 0.010000000000000002
Time, 2019-01-01T17:26:22, Epoch: 26, Batch: 260, Training Loss: 0.050725963339209555, LR: 0.010000000000000002
Time, 2019-01-01T17:26:23, Epoch: 26, Batch: 270, Training Loss: 0.03967891409993172, LR: 0.010000000000000002
Time, 2019-01-01T17:26:23, Epoch: 26, Batch: 280, Training Loss: 0.04822676926851273, LR: 0.010000000000000002
Time, 2019-01-01T17:26:24, Epoch: 26, Batch: 290, Training Loss: 0.0636240191757679, LR: 0.010000000000000002
Time, 2019-01-01T17:26:25, Epoch: 26, Batch: 300, Training Loss: 0.08193240985274315, LR: 0.010000000000000002
Time, 2019-01-01T17:26:25, Epoch: 26, Batch: 310, Training Loss: 0.06078226044774056, LR: 0.010000000000000002
Time, 2019-01-01T17:26:26, Epoch: 26, Batch: 320, Training Loss: 0.06025591865181923, LR: 0.010000000000000002
Time, 2019-01-01T17:26:27, Epoch: 26, Batch: 330, Training Loss: 0.05038309209048748, LR: 0.010000000000000002
Time, 2019-01-01T17:26:28, Epoch: 26, Batch: 340, Training Loss: 0.06804200857877732, LR: 0.010000000000000002
Time, 2019-01-01T17:26:28, Epoch: 26, Batch: 350, Training Loss: 0.06883605606853962, LR: 0.010000000000000002
Time, 2019-01-01T17:26:29, Epoch: 26, Batch: 360, Training Loss: 0.04365660101175308, LR: 0.010000000000000002
Time, 2019-01-01T17:26:30, Epoch: 26, Batch: 370, Training Loss: 0.060715223848819735, LR: 0.010000000000000002
Time, 2019-01-01T17:26:30, Epoch: 26, Batch: 380, Training Loss: 0.05820041000843048, LR: 0.010000000000000002
Time, 2019-01-01T17:26:31, Epoch: 26, Batch: 390, Training Loss: 0.07517868205904961, LR: 0.010000000000000002
Time, 2019-01-01T17:26:32, Epoch: 26, Batch: 400, Training Loss: 0.05807611308991909, LR: 0.010000000000000002
Time, 2019-01-01T17:26:33, Epoch: 26, Batch: 410, Training Loss: 0.0789266474545002, LR: 0.010000000000000002
Time, 2019-01-01T17:26:33, Epoch: 26, Batch: 420, Training Loss: 0.05687747485935688, LR: 0.010000000000000002
Time, 2019-01-01T17:26:34, Epoch: 26, Batch: 430, Training Loss: 0.04561955519020557, LR: 0.010000000000000002
Time, 2019-01-01T17:26:35, Epoch: 26, Batch: 440, Training Loss: 0.06595471426844597, LR: 0.010000000000000002
Time, 2019-01-01T17:26:35, Epoch: 26, Batch: 450, Training Loss: 0.041163190826773645, LR: 0.010000000000000002
Time, 2019-01-01T17:26:36, Epoch: 26, Batch: 460, Training Loss: 0.03577469773590565, LR: 0.010000000000000002
Time, 2019-01-01T17:26:37, Epoch: 26, Batch: 470, Training Loss: 0.050420019775629044, LR: 0.010000000000000002
Time, 2019-01-01T17:26:37, Epoch: 26, Batch: 480, Training Loss: 0.07335861697793007, LR: 0.010000000000000002
Time, 2019-01-01T17:26:38, Epoch: 26, Batch: 490, Training Loss: 0.08013086430728436, LR: 0.010000000000000002
Time, 2019-01-01T17:26:39, Epoch: 26, Batch: 500, Training Loss: 0.09373837187886239, LR: 0.010000000000000002
Time, 2019-01-01T17:26:40, Epoch: 26, Batch: 510, Training Loss: 0.07047330923378467, LR: 0.010000000000000002
Time, 2019-01-01T17:26:40, Epoch: 26, Batch: 520, Training Loss: 0.05328566133975983, LR: 0.010000000000000002
Time, 2019-01-01T17:26:41, Epoch: 26, Batch: 530, Training Loss: 0.054796024784445764, LR: 0.010000000000000002
Time, 2019-01-01T17:26:42, Epoch: 26, Batch: 540, Training Loss: 0.08916491270065308, LR: 0.010000000000000002
Time, 2019-01-01T17:26:42, Epoch: 26, Batch: 550, Training Loss: 0.05837762206792831, LR: 0.010000000000000002
Time, 2019-01-01T17:26:43, Epoch: 26, Batch: 560, Training Loss: 0.05470133721828461, LR: 0.010000000000000002
Time, 2019-01-01T17:26:44, Epoch: 26, Batch: 570, Training Loss: 0.048742858693003654, LR: 0.010000000000000002
Time, 2019-01-01T17:26:45, Epoch: 26, Batch: 580, Training Loss: 0.056485769152641294, LR: 0.010000000000000002
Time, 2019-01-01T17:26:45, Epoch: 26, Batch: 590, Training Loss: 0.039011276140809056, LR: 0.010000000000000002
Time, 2019-01-01T17:26:46, Epoch: 26, Batch: 600, Training Loss: 0.05594454407691955, LR: 0.010000000000000002
Time, 2019-01-01T17:26:47, Epoch: 26, Batch: 610, Training Loss: 0.05282161869108677, LR: 0.010000000000000002
Time, 2019-01-01T17:26:47, Epoch: 26, Batch: 620, Training Loss: 0.05956591591238976, LR: 0.010000000000000002
Time, 2019-01-01T17:26:48, Epoch: 26, Batch: 630, Training Loss: 0.06458279564976692, LR: 0.010000000000000002
Time, 2019-01-01T17:26:49, Epoch: 26, Batch: 640, Training Loss: 0.04371420368552208, LR: 0.010000000000000002
Time, 2019-01-01T17:26:50, Epoch: 26, Batch: 650, Training Loss: 0.055175496637821196, LR: 0.010000000000000002
Time, 2019-01-01T17:26:50, Epoch: 26, Batch: 660, Training Loss: 0.047772178798913954, LR: 0.010000000000000002
Time, 2019-01-01T17:26:51, Epoch: 26, Batch: 670, Training Loss: 0.05779977440834046, LR: 0.010000000000000002
Time, 2019-01-01T17:26:52, Epoch: 26, Batch: 680, Training Loss: 0.06327342167496681, LR: 0.010000000000000002
Time, 2019-01-01T17:26:52, Epoch: 26, Batch: 690, Training Loss: 0.06859366297721863, LR: 0.010000000000000002
Time, 2019-01-01T17:26:53, Epoch: 26, Batch: 700, Training Loss: 0.05185370445251465, LR: 0.010000000000000002
Time, 2019-01-01T17:26:54, Epoch: 26, Batch: 710, Training Loss: 0.07901146933436394, LR: 0.010000000000000002
Time, 2019-01-01T17:26:55, Epoch: 26, Batch: 720, Training Loss: 0.058069304749369624, LR: 0.010000000000000002
Time, 2019-01-01T17:26:55, Epoch: 26, Batch: 730, Training Loss: 0.053099234402179715, LR: 0.010000000000000002
Time, 2019-01-01T17:26:56, Epoch: 26, Batch: 740, Training Loss: 0.043594880402088164, LR: 0.010000000000000002
Time, 2019-01-01T17:26:57, Epoch: 26, Batch: 750, Training Loss: 0.08161496482789517, LR: 0.010000000000000002
Time, 2019-01-01T17:26:57, Epoch: 26, Batch: 760, Training Loss: 0.059064092487096785, LR: 0.010000000000000002
Time, 2019-01-01T17:26:58, Epoch: 26, Batch: 770, Training Loss: 0.04399549029767513, LR: 0.010000000000000002
Time, 2019-01-01T17:26:59, Epoch: 26, Batch: 780, Training Loss: 0.053542366996407506, LR: 0.010000000000000002
Time, 2019-01-01T17:27:00, Epoch: 26, Batch: 790, Training Loss: 0.07636360637843609, LR: 0.010000000000000002
Time, 2019-01-01T17:27:00, Epoch: 26, Batch: 800, Training Loss: 0.04438065998256206, LR: 0.010000000000000002
Time, 2019-01-01T17:27:01, Epoch: 26, Batch: 810, Training Loss: 0.07166995853185654, LR: 0.010000000000000002
Time, 2019-01-01T17:27:02, Epoch: 26, Batch: 820, Training Loss: 0.043009179085493086, LR: 0.010000000000000002
Time, 2019-01-01T17:27:02, Epoch: 26, Batch: 830, Training Loss: 0.07750046476721764, LR: 0.010000000000000002
Time, 2019-01-01T17:27:03, Epoch: 26, Batch: 840, Training Loss: 0.0643464669585228, LR: 0.010000000000000002
Time, 2019-01-01T17:27:04, Epoch: 26, Batch: 850, Training Loss: 0.05582388937473297, LR: 0.010000000000000002
Time, 2019-01-01T17:27:05, Epoch: 26, Batch: 860, Training Loss: 0.04778643473982811, LR: 0.010000000000000002
Time, 2019-01-01T17:27:05, Epoch: 26, Batch: 870, Training Loss: 0.06642603017389774, LR: 0.010000000000000002
Time, 2019-01-01T17:27:06, Epoch: 26, Batch: 880, Training Loss: 0.04315694272518158, LR: 0.010000000000000002
Time, 2019-01-01T17:27:07, Epoch: 26, Batch: 890, Training Loss: 0.05451913774013519, LR: 0.010000000000000002
Time, 2019-01-01T17:27:07, Epoch: 26, Batch: 900, Training Loss: 0.07100996151566505, LR: 0.010000000000000002
Time, 2019-01-01T17:27:08, Epoch: 26, Batch: 910, Training Loss: 0.05538000166416168, LR: 0.010000000000000002
Time, 2019-01-01T17:27:09, Epoch: 26, Batch: 920, Training Loss: 0.05068148374557495, LR: 0.010000000000000002
Time, 2019-01-01T17:27:10, Epoch: 26, Batch: 930, Training Loss: 0.04976453557610512, LR: 0.010000000000000002
Epoch: 26, Validation Top 1 acc: 98.3575439453125
Epoch: 26, Validation Top 5 acc: 99.98167419433594
Epoch: 26, Validation Set Loss: 0.05526178702712059
Start training epoch 27
Time, 2019-01-01T17:27:35, Epoch: 27, Batch: 10, Training Loss: 0.056310543417930604, LR: 0.010000000000000002
Time, 2019-01-01T17:27:36, Epoch: 27, Batch: 20, Training Loss: 0.05626333951950073, LR: 0.010000000000000002
Time, 2019-01-01T17:27:37, Epoch: 27, Batch: 30, Training Loss: 0.057917908579111096, LR: 0.010000000000000002
Time, 2019-01-01T17:27:38, Epoch: 27, Batch: 40, Training Loss: 0.03863574005663395, LR: 0.010000000000000002
Time, 2019-01-01T17:27:38, Epoch: 27, Batch: 50, Training Loss: 0.04616693668067455, LR: 0.010000000000000002
Time, 2019-01-01T17:27:39, Epoch: 27, Batch: 60, Training Loss: 0.045331360027194026, LR: 0.010000000000000002
Time, 2019-01-01T17:27:40, Epoch: 27, Batch: 70, Training Loss: 0.06118966713547706, LR: 0.010000000000000002
Time, 2019-01-01T17:27:41, Epoch: 27, Batch: 80, Training Loss: 0.08872478120028973, LR: 0.010000000000000002
Time, 2019-01-01T17:27:41, Epoch: 27, Batch: 90, Training Loss: 0.04974849931895733, LR: 0.010000000000000002
Time, 2019-01-01T17:27:42, Epoch: 27, Batch: 100, Training Loss: 0.08004780225455761, LR: 0.010000000000000002
Time, 2019-01-01T17:27:43, Epoch: 27, Batch: 110, Training Loss: 0.04059683829545975, LR: 0.010000000000000002
Time, 2019-01-01T17:27:43, Epoch: 27, Batch: 120, Training Loss: 0.07150720097124577, LR: 0.010000000000000002
Time, 2019-01-01T17:27:44, Epoch: 27, Batch: 130, Training Loss: 0.06907520517706871, LR: 0.010000000000000002
Time, 2019-01-01T17:27:45, Epoch: 27, Batch: 140, Training Loss: 0.047093576192855834, LR: 0.010000000000000002
Time, 2019-01-01T17:27:45, Epoch: 27, Batch: 150, Training Loss: 0.06637265235185623, LR: 0.010000000000000002
Time, 2019-01-01T17:27:46, Epoch: 27, Batch: 160, Training Loss: 0.05842229425907135, LR: 0.010000000000000002
Time, 2019-01-01T17:27:47, Epoch: 27, Batch: 170, Training Loss: 0.053647417947649954, LR: 0.010000000000000002
Time, 2019-01-01T17:27:48, Epoch: 27, Batch: 180, Training Loss: 0.0446210790425539, LR: 0.010000000000000002
Time, 2019-01-01T17:27:48, Epoch: 27, Batch: 190, Training Loss: 0.037159107998013495, LR: 0.010000000000000002
Time, 2019-01-01T17:27:49, Epoch: 27, Batch: 200, Training Loss: 0.06095726788043976, LR: 0.010000000000000002
Time, 2019-01-01T17:27:50, Epoch: 27, Batch: 210, Training Loss: 0.05013468489050865, LR: 0.010000000000000002
Time, 2019-01-01T17:27:50, Epoch: 27, Batch: 220, Training Loss: 0.05273273587226868, LR: 0.010000000000000002
Time, 2019-01-01T17:27:51, Epoch: 27, Batch: 230, Training Loss: 0.05059018097817898, LR: 0.010000000000000002
Time, 2019-01-01T17:27:52, Epoch: 27, Batch: 240, Training Loss: 0.030499141663312912, LR: 0.010000000000000002
Time, 2019-01-01T17:27:53, Epoch: 27, Batch: 250, Training Loss: 0.05793753080070019, LR: 0.010000000000000002
Time, 2019-01-01T17:27:53, Epoch: 27, Batch: 260, Training Loss: 0.06753075644373893, LR: 0.010000000000000002
Time, 2019-01-01T17:27:54, Epoch: 27, Batch: 270, Training Loss: 0.07307989448308945, LR: 0.010000000000000002
Time, 2019-01-01T17:27:55, Epoch: 27, Batch: 280, Training Loss: 0.0836564101278782, LR: 0.010000000000000002
Time, 2019-01-01T17:27:55, Epoch: 27, Batch: 290, Training Loss: 0.06121277213096619, LR: 0.010000000000000002
Time, 2019-01-01T17:27:56, Epoch: 27, Batch: 300, Training Loss: 0.05483497716486454, LR: 0.010000000000000002
Time, 2019-01-01T17:27:57, Epoch: 27, Batch: 310, Training Loss: 0.07203086763620377, LR: 0.010000000000000002
Time, 2019-01-01T17:27:58, Epoch: 27, Batch: 320, Training Loss: 0.08172567673027516, LR: 0.010000000000000002
Time, 2019-01-01T17:27:58, Epoch: 27, Batch: 330, Training Loss: 0.04858618713915348, LR: 0.010000000000000002
Time, 2019-01-01T17:27:59, Epoch: 27, Batch: 340, Training Loss: 0.04898211508989334, LR: 0.010000000000000002
Time, 2019-01-01T17:28:00, Epoch: 27, Batch: 350, Training Loss: 0.061897077783942225, LR: 0.010000000000000002
Time, 2019-01-01T17:28:00, Epoch: 27, Batch: 360, Training Loss: 0.06907791942358017, LR: 0.010000000000000002
Time, 2019-01-01T17:28:01, Epoch: 27, Batch: 370, Training Loss: 0.05500954613089561, LR: 0.010000000000000002
Time, 2019-01-01T17:28:02, Epoch: 27, Batch: 380, Training Loss: 0.07209266610443592, LR: 0.010000000000000002
Time, 2019-01-01T17:28:03, Epoch: 27, Batch: 390, Training Loss: 0.0518924281001091, LR: 0.010000000000000002
Time, 2019-01-01T17:28:03, Epoch: 27, Batch: 400, Training Loss: 0.049346039444208144, LR: 0.010000000000000002
Time, 2019-01-01T17:28:04, Epoch: 27, Batch: 410, Training Loss: 0.0568669106811285, LR: 0.010000000000000002
Time, 2019-01-01T17:28:05, Epoch: 27, Batch: 420, Training Loss: 0.09100876189768314, LR: 0.010000000000000002
Time, 2019-01-01T17:28:06, Epoch: 27, Batch: 430, Training Loss: 0.0588144663721323, LR: 0.010000000000000002
Time, 2019-01-01T17:28:06, Epoch: 27, Batch: 440, Training Loss: 0.04937901459634304, LR: 0.010000000000000002
Time, 2019-01-01T17:28:07, Epoch: 27, Batch: 450, Training Loss: 0.06164964139461517, LR: 0.010000000000000002
Time, 2019-01-01T17:28:08, Epoch: 27, Batch: 460, Training Loss: 0.08223377093672753, LR: 0.010000000000000002
Time, 2019-01-01T17:28:08, Epoch: 27, Batch: 470, Training Loss: 0.06347814202308655, LR: 0.010000000000000002
Time, 2019-01-01T17:28:09, Epoch: 27, Batch: 480, Training Loss: 0.04284100644290447, LR: 0.010000000000000002
Time, 2019-01-01T17:28:10, Epoch: 27, Batch: 490, Training Loss: 0.03259787037968635, LR: 0.010000000000000002
Time, 2019-01-01T17:28:11, Epoch: 27, Batch: 500, Training Loss: 0.06632000580430031, LR: 0.010000000000000002
Time, 2019-01-01T17:28:11, Epoch: 27, Batch: 510, Training Loss: 0.05543306544423103, LR: 0.010000000000000002
Time, 2019-01-01T17:28:12, Epoch: 27, Batch: 520, Training Loss: 0.06351164728403091, LR: 0.010000000000000002
Time, 2019-01-01T17:28:13, Epoch: 27, Batch: 530, Training Loss: 0.04145938195288181, LR: 0.010000000000000002
Time, 2019-01-01T17:28:13, Epoch: 27, Batch: 540, Training Loss: 0.045333518832921985, LR: 0.010000000000000002
Time, 2019-01-01T17:28:14, Epoch: 27, Batch: 550, Training Loss: 0.05151346102356911, LR: 0.010000000000000002
Time, 2019-01-01T17:28:15, Epoch: 27, Batch: 560, Training Loss: 0.04946929514408112, LR: 0.010000000000000002
Time, 2019-01-01T17:28:16, Epoch: 27, Batch: 570, Training Loss: 0.051372550427913666, LR: 0.010000000000000002
Time, 2019-01-01T17:28:16, Epoch: 27, Batch: 580, Training Loss: 0.05457567349076271, LR: 0.010000000000000002
Time, 2019-01-01T17:28:17, Epoch: 27, Batch: 590, Training Loss: 0.049210111051797865, LR: 0.010000000000000002
Time, 2019-01-01T17:28:18, Epoch: 27, Batch: 600, Training Loss: 0.05094452872872353, LR: 0.010000000000000002
Time, 2019-01-01T17:28:18, Epoch: 27, Batch: 610, Training Loss: 0.046626444160938266, LR: 0.010000000000000002
Time, 2019-01-01T17:28:19, Epoch: 27, Batch: 620, Training Loss: 0.04836610965430736, LR: 0.010000000000000002
Time, 2019-01-01T17:28:20, Epoch: 27, Batch: 630, Training Loss: 0.032957293838262555, LR: 0.010000000000000002
Time, 2019-01-01T17:28:20, Epoch: 27, Batch: 640, Training Loss: 0.06636523157358169, LR: 0.010000000000000002
Time, 2019-01-01T17:28:21, Epoch: 27, Batch: 650, Training Loss: 0.05918418690562248, LR: 0.010000000000000002
Time, 2019-01-01T17:28:22, Epoch: 27, Batch: 660, Training Loss: 0.05974063985049725, LR: 0.010000000000000002
Time, 2019-01-01T17:28:23, Epoch: 27, Batch: 670, Training Loss: 0.055361334979534146, LR: 0.010000000000000002
Time, 2019-01-01T17:28:23, Epoch: 27, Batch: 680, Training Loss: 0.05891963988542557, LR: 0.010000000000000002
Time, 2019-01-01T17:28:24, Epoch: 27, Batch: 690, Training Loss: 0.049333617091178894, LR: 0.010000000000000002
Time, 2019-01-01T17:28:25, Epoch: 27, Batch: 700, Training Loss: 0.07506524845957756, LR: 0.010000000000000002
Time, 2019-01-01T17:28:25, Epoch: 27, Batch: 710, Training Loss: 0.05732727088034153, LR: 0.010000000000000002
Time, 2019-01-01T17:28:26, Epoch: 27, Batch: 720, Training Loss: 0.07696135938167573, LR: 0.010000000000000002
Time, 2019-01-01T17:28:27, Epoch: 27, Batch: 730, Training Loss: 0.06004728563129902, LR: 0.010000000000000002
Time, 2019-01-01T17:28:28, Epoch: 27, Batch: 740, Training Loss: 0.05936664268374443, LR: 0.010000000000000002
Time, 2019-01-01T17:28:28, Epoch: 27, Batch: 750, Training Loss: 0.06351861283183098, LR: 0.010000000000000002
Time, 2019-01-01T17:28:29, Epoch: 27, Batch: 760, Training Loss: 0.07262577153742314, LR: 0.010000000000000002
Time, 2019-01-01T17:28:30, Epoch: 27, Batch: 770, Training Loss: 0.04796794764697552, LR: 0.010000000000000002
Time, 2019-01-01T17:28:30, Epoch: 27, Batch: 780, Training Loss: 0.0316358283162117, LR: 0.010000000000000002
Time, 2019-01-01T17:28:31, Epoch: 27, Batch: 790, Training Loss: 0.06689608097076416, LR: 0.010000000000000002
Time, 2019-01-01T17:28:32, Epoch: 27, Batch: 800, Training Loss: 0.04288610443472862, LR: 0.010000000000000002
Time, 2019-01-01T17:28:33, Epoch: 27, Batch: 810, Training Loss: 0.041691165417432785, LR: 0.010000000000000002
Time, 2019-01-01T17:28:33, Epoch: 27, Batch: 820, Training Loss: 0.058360129594802856, LR: 0.010000000000000002
Time, 2019-01-01T17:28:34, Epoch: 27, Batch: 830, Training Loss: 0.05695882700383663, LR: 0.010000000000000002
Time, 2019-01-01T17:28:35, Epoch: 27, Batch: 840, Training Loss: 0.04525732696056366, LR: 0.010000000000000002
Time, 2019-01-01T17:28:35, Epoch: 27, Batch: 850, Training Loss: 0.09279339089989662, LR: 0.010000000000000002
Time, 2019-01-01T17:28:36, Epoch: 27, Batch: 860, Training Loss: 0.06570810228586196, LR: 0.010000000000000002
Time, 2019-01-01T17:28:37, Epoch: 27, Batch: 870, Training Loss: 0.05433375835418701, LR: 0.010000000000000002
Time, 2019-01-01T17:28:37, Epoch: 27, Batch: 880, Training Loss: 0.06534098759293556, LR: 0.010000000000000002
Time, 2019-01-01T17:28:38, Epoch: 27, Batch: 890, Training Loss: 0.07267564982175827, LR: 0.010000000000000002
Time, 2019-01-01T17:28:39, Epoch: 27, Batch: 900, Training Loss: 0.04740865677595139, LR: 0.010000000000000002
Time, 2019-01-01T17:28:40, Epoch: 27, Batch: 910, Training Loss: 0.056379522383213046, LR: 0.010000000000000002
Time, 2019-01-01T17:28:40, Epoch: 27, Batch: 920, Training Loss: 0.04722809679806232, LR: 0.010000000000000002
Time, 2019-01-01T17:28:41, Epoch: 27, Batch: 930, Training Loss: 0.08905933164060116, LR: 0.010000000000000002
Epoch: 27, Validation Top 1 acc: 98.24093627929688
Epoch: 27, Validation Top 5 acc: 99.99000549316406
Epoch: 27, Validation Set Loss: 0.05781642347574234
Start training epoch 28
Time, 2019-01-01T17:29:07, Epoch: 28, Batch: 10, Training Loss: 0.07512281760573387, LR: 0.010000000000000002
Time, 2019-01-01T17:29:07, Epoch: 28, Batch: 20, Training Loss: 0.045257555320858955, LR: 0.010000000000000002
Time, 2019-01-01T17:29:08, Epoch: 28, Batch: 30, Training Loss: 0.055461161211133, LR: 0.010000000000000002
Time, 2019-01-01T17:29:09, Epoch: 28, Batch: 40, Training Loss: 0.04769914075732231, LR: 0.010000000000000002
Time, 2019-01-01T17:29:10, Epoch: 28, Batch: 50, Training Loss: 0.03790741115808487, LR: 0.010000000000000002
Time, 2019-01-01T17:29:10, Epoch: 28, Batch: 60, Training Loss: 0.05569732300937176, LR: 0.010000000000000002
Time, 2019-01-01T17:29:11, Epoch: 28, Batch: 70, Training Loss: 0.04616633057594299, LR: 0.010000000000000002
Time, 2019-01-01T17:29:12, Epoch: 28, Batch: 80, Training Loss: 0.07227885015308858, LR: 0.010000000000000002
Time, 2019-01-01T17:29:12, Epoch: 28, Batch: 90, Training Loss: 0.08423119895160198, LR: 0.010000000000000002
Time, 2019-01-01T17:29:13, Epoch: 28, Batch: 100, Training Loss: 0.05926792696118355, LR: 0.010000000000000002
Time, 2019-01-01T17:29:14, Epoch: 28, Batch: 110, Training Loss: 0.04820789285004139, LR: 0.010000000000000002
Time, 2019-01-01T17:29:15, Epoch: 28, Batch: 120, Training Loss: 0.05589216575026512, LR: 0.010000000000000002
Time, 2019-01-01T17:29:15, Epoch: 28, Batch: 130, Training Loss: 0.050675195455551145, LR: 0.010000000000000002
Time, 2019-01-01T17:29:16, Epoch: 28, Batch: 140, Training Loss: 0.052235252037644384, LR: 0.010000000000000002
Time, 2019-01-01T17:29:17, Epoch: 28, Batch: 150, Training Loss: 0.04903759844601154, LR: 0.010000000000000002
Time, 2019-01-01T17:29:18, Epoch: 28, Batch: 160, Training Loss: 0.03392099402844906, LR: 0.010000000000000002
Time, 2019-01-01T17:29:18, Epoch: 28, Batch: 170, Training Loss: 0.05478047095239162, LR: 0.010000000000000002
Time, 2019-01-01T17:29:19, Epoch: 28, Batch: 180, Training Loss: 0.06868367567658425, LR: 0.010000000000000002
Time, 2019-01-01T17:29:20, Epoch: 28, Batch: 190, Training Loss: 0.08536176048219205, LR: 0.010000000000000002
Time, 2019-01-01T17:29:21, Epoch: 28, Batch: 200, Training Loss: 0.043165550008416174, LR: 0.010000000000000002
Time, 2019-01-01T17:29:21, Epoch: 28, Batch: 210, Training Loss: 0.06575471125543117, LR: 0.010000000000000002
Time, 2019-01-01T17:29:22, Epoch: 28, Batch: 220, Training Loss: 0.0639979187399149, LR: 0.010000000000000002
Time, 2019-01-01T17:29:23, Epoch: 28, Batch: 230, Training Loss: 0.08872657641768456, LR: 0.010000000000000002
Time, 2019-01-01T17:29:24, Epoch: 28, Batch: 240, Training Loss: 0.0612819142639637, LR: 0.010000000000000002
Time, 2019-01-01T17:29:25, Epoch: 28, Batch: 250, Training Loss: 0.05010744258761406, LR: 0.010000000000000002
Time, 2019-01-01T17:29:25, Epoch: 28, Batch: 260, Training Loss: 0.06543295197188854, LR: 0.010000000000000002
Time, 2019-01-01T17:29:26, Epoch: 28, Batch: 270, Training Loss: 0.051641684770584104, LR: 0.010000000000000002
Time, 2019-01-01T17:29:27, Epoch: 28, Batch: 280, Training Loss: 0.04642640687525272, LR: 0.010000000000000002
Time, 2019-01-01T17:29:28, Epoch: 28, Batch: 290, Training Loss: 0.05387898087501526, LR: 0.010000000000000002
Time, 2019-01-01T17:29:28, Epoch: 28, Batch: 300, Training Loss: 0.07890336960554123, LR: 0.010000000000000002
Time, 2019-01-01T17:29:29, Epoch: 28, Batch: 310, Training Loss: 0.07371902316808701, LR: 0.010000000000000002
Time, 2019-01-01T17:29:30, Epoch: 28, Batch: 320, Training Loss: 0.058431435376405716, LR: 0.010000000000000002
Time, 2019-01-01T17:29:31, Epoch: 28, Batch: 330, Training Loss: 0.05220941863954067, LR: 0.010000000000000002
Time, 2019-01-01T17:29:31, Epoch: 28, Batch: 340, Training Loss: 0.03854159638285637, LR: 0.010000000000000002
Time, 2019-01-01T17:29:32, Epoch: 28, Batch: 350, Training Loss: 0.07199809439480305, LR: 0.010000000000000002
Time, 2019-01-01T17:29:33, Epoch: 28, Batch: 360, Training Loss: 0.056789015978574754, LR: 0.010000000000000002
Time, 2019-01-01T17:29:33, Epoch: 28, Batch: 370, Training Loss: 0.06613466069102288, LR: 0.010000000000000002
Time, 2019-01-01T17:29:34, Epoch: 28, Batch: 380, Training Loss: 0.053857112675905226, LR: 0.010000000000000002
Time, 2019-01-01T17:29:35, Epoch: 28, Batch: 390, Training Loss: 0.04596090242266655, LR: 0.010000000000000002
Time, 2019-01-01T17:29:35, Epoch: 28, Batch: 400, Training Loss: 0.06169798672199249, LR: 0.010000000000000002
Time, 2019-01-01T17:29:36, Epoch: 28, Batch: 410, Training Loss: 0.0366208516061306, LR: 0.010000000000000002
Time, 2019-01-01T17:29:37, Epoch: 28, Batch: 420, Training Loss: 0.050199997052550314, LR: 0.010000000000000002
Time, 2019-01-01T17:29:38, Epoch: 28, Batch: 430, Training Loss: 0.06058917120099068, LR: 0.010000000000000002
Time, 2019-01-01T17:29:38, Epoch: 28, Batch: 440, Training Loss: 0.06034639179706573, LR: 0.010000000000000002
Time, 2019-01-01T17:29:39, Epoch: 28, Batch: 450, Training Loss: 0.0734484538435936, LR: 0.010000000000000002
Time, 2019-01-01T17:29:40, Epoch: 28, Batch: 460, Training Loss: 0.0670492123812437, LR: 0.010000000000000002
Time, 2019-01-01T17:29:40, Epoch: 28, Batch: 470, Training Loss: 0.05084589496254921, LR: 0.010000000000000002
Time, 2019-01-01T17:29:41, Epoch: 28, Batch: 480, Training Loss: 0.06447212770581245, LR: 0.010000000000000002
Time, 2019-01-01T17:29:42, Epoch: 28, Batch: 490, Training Loss: 0.05814911797642708, LR: 0.010000000000000002
Time, 2019-01-01T17:29:42, Epoch: 28, Batch: 500, Training Loss: 0.05498280636966228, LR: 0.010000000000000002
Time, 2019-01-01T17:29:43, Epoch: 28, Batch: 510, Training Loss: 0.06233256496489048, LR: 0.010000000000000002
Time, 2019-01-01T17:29:44, Epoch: 28, Batch: 520, Training Loss: 0.07226139716804028, LR: 0.010000000000000002
Time, 2019-01-01T17:29:45, Epoch: 28, Batch: 530, Training Loss: 0.0577755767852068, LR: 0.010000000000000002
Time, 2019-01-01T17:29:45, Epoch: 28, Batch: 540, Training Loss: 0.07012014351785183, LR: 0.010000000000000002
Time, 2019-01-01T17:29:46, Epoch: 28, Batch: 550, Training Loss: 0.054951530694961545, LR: 0.010000000000000002
Time, 2019-01-01T17:29:47, Epoch: 28, Batch: 560, Training Loss: 0.03789915703237057, LR: 0.010000000000000002
Time, 2019-01-01T17:29:47, Epoch: 28, Batch: 570, Training Loss: 0.05065995268523693, LR: 0.010000000000000002
Time, 2019-01-01T17:29:48, Epoch: 28, Batch: 580, Training Loss: 0.045549276471138, LR: 0.010000000000000002
Time, 2019-01-01T17:29:49, Epoch: 28, Batch: 590, Training Loss: 0.04739040806889534, LR: 0.010000000000000002
Time, 2019-01-01T17:29:49, Epoch: 28, Batch: 600, Training Loss: 0.04790589213371277, LR: 0.010000000000000002
Time, 2019-01-01T17:29:50, Epoch: 28, Batch: 610, Training Loss: 0.027308904752135276, LR: 0.010000000000000002
Time, 2019-01-01T17:29:51, Epoch: 28, Batch: 620, Training Loss: 0.054987916350364686, LR: 0.010000000000000002
Time, 2019-01-01T17:29:52, Epoch: 28, Batch: 630, Training Loss: 0.06154732406139374, LR: 0.010000000000000002
Time, 2019-01-01T17:29:52, Epoch: 28, Batch: 640, Training Loss: 0.05935889109969139, LR: 0.010000000000000002
Time, 2019-01-01T17:29:53, Epoch: 28, Batch: 650, Training Loss: 0.09307596497237683, LR: 0.010000000000000002
Time, 2019-01-01T17:29:54, Epoch: 28, Batch: 660, Training Loss: 0.05111167319118977, LR: 0.010000000000000002
Time, 2019-01-01T17:29:54, Epoch: 28, Batch: 670, Training Loss: 0.04724189974367619, LR: 0.010000000000000002
Time, 2019-01-01T17:29:55, Epoch: 28, Batch: 680, Training Loss: 0.06385528296232224, LR: 0.010000000000000002
Time, 2019-01-01T17:29:56, Epoch: 28, Batch: 690, Training Loss: 0.05403433702886105, LR: 0.010000000000000002
Time, 2019-01-01T17:29:56, Epoch: 28, Batch: 700, Training Loss: 0.05106939822435379, LR: 0.010000000000000002
Time, 2019-01-01T17:29:57, Epoch: 28, Batch: 710, Training Loss: 0.07963560856878757, LR: 0.010000000000000002
Time, 2019-01-01T17:29:58, Epoch: 28, Batch: 720, Training Loss: 0.06134093217551708, LR: 0.010000000000000002
Time, 2019-01-01T17:29:59, Epoch: 28, Batch: 730, Training Loss: 0.04372229129076004, LR: 0.010000000000000002
Time, 2019-01-01T17:29:59, Epoch: 28, Batch: 740, Training Loss: 0.07043904066085815, LR: 0.010000000000000002
Time, 2019-01-01T17:30:00, Epoch: 28, Batch: 750, Training Loss: 0.053194839507341385, LR: 0.010000000000000002
Time, 2019-01-01T17:30:01, Epoch: 28, Batch: 760, Training Loss: 0.07098028138279915, LR: 0.010000000000000002
Time, 2019-01-01T17:30:01, Epoch: 28, Batch: 770, Training Loss: 0.05482705160975456, LR: 0.010000000000000002
Time, 2019-01-01T17:30:02, Epoch: 28, Batch: 780, Training Loss: 0.05405332259833813, LR: 0.010000000000000002
Time, 2019-01-01T17:30:03, Epoch: 28, Batch: 790, Training Loss: 0.047223415970802304, LR: 0.010000000000000002
Time, 2019-01-01T17:30:03, Epoch: 28, Batch: 800, Training Loss: 0.06295762062072754, LR: 0.010000000000000002
Time, 2019-01-01T17:30:04, Epoch: 28, Batch: 810, Training Loss: 0.05310065746307373, LR: 0.010000000000000002
Time, 2019-01-01T17:30:05, Epoch: 28, Batch: 820, Training Loss: 0.05466796718537807, LR: 0.010000000000000002
Time, 2019-01-01T17:30:06, Epoch: 28, Batch: 830, Training Loss: 0.04799702912569046, LR: 0.010000000000000002
Time, 2019-01-01T17:30:06, Epoch: 28, Batch: 840, Training Loss: 0.04067657142877579, LR: 0.010000000000000002
Time, 2019-01-01T17:30:07, Epoch: 28, Batch: 850, Training Loss: 0.05357246920466423, LR: 0.010000000000000002
Time, 2019-01-01T17:30:08, Epoch: 28, Batch: 860, Training Loss: 0.056572021543979646, LR: 0.010000000000000002
Time, 2019-01-01T17:30:08, Epoch: 28, Batch: 870, Training Loss: 0.08376211524009705, LR: 0.010000000000000002
Time, 2019-01-01T17:30:09, Epoch: 28, Batch: 880, Training Loss: 0.07221061252057552, LR: 0.010000000000000002
Time, 2019-01-01T17:30:10, Epoch: 28, Batch: 890, Training Loss: 0.07202233411371708, LR: 0.010000000000000002
Time, 2019-01-01T17:30:11, Epoch: 28, Batch: 900, Training Loss: 0.0440546914935112, LR: 0.010000000000000002
Time, 2019-01-01T17:30:11, Epoch: 28, Batch: 910, Training Loss: 0.07400750741362572, LR: 0.010000000000000002
Time, 2019-01-01T17:30:12, Epoch: 28, Batch: 920, Training Loss: 0.06083878427743912, LR: 0.010000000000000002
Time, 2019-01-01T17:30:13, Epoch: 28, Batch: 930, Training Loss: 0.057699310779571536, LR: 0.010000000000000002
Epoch: 28, Validation Top 1 acc: 98.50746154785156
Epoch: 28, Validation Top 5 acc: 99.98001098632812
Epoch: 28, Validation Set Loss: 0.05261234566569328
Start training epoch 29
Time, 2019-01-01T17:30:38, Epoch: 29, Batch: 10, Training Loss: 0.05704282931983471, LR: 0.010000000000000002
Time, 2019-01-01T17:30:39, Epoch: 29, Batch: 20, Training Loss: 0.0677238091826439, LR: 0.010000000000000002
Time, 2019-01-01T17:30:40, Epoch: 29, Batch: 30, Training Loss: 0.04939231313765049, LR: 0.010000000000000002
Time, 2019-01-01T17:30:40, Epoch: 29, Batch: 40, Training Loss: 0.0717378832399845, LR: 0.010000000000000002
Time, 2019-01-01T17:30:41, Epoch: 29, Batch: 50, Training Loss: 0.0574623454362154, LR: 0.010000000000000002
Time, 2019-01-01T17:30:42, Epoch: 29, Batch: 60, Training Loss: 0.05468375347554684, LR: 0.010000000000000002
Time, 2019-01-01T17:30:43, Epoch: 29, Batch: 70, Training Loss: 0.05100217238068581, LR: 0.010000000000000002
Time, 2019-01-01T17:30:43, Epoch: 29, Batch: 80, Training Loss: 0.04498351700603962, LR: 0.010000000000000002
Time, 2019-01-01T17:30:44, Epoch: 29, Batch: 90, Training Loss: 0.051455986499786374, LR: 0.010000000000000002
Time, 2019-01-01T17:30:45, Epoch: 29, Batch: 100, Training Loss: 0.05971543863415718, LR: 0.010000000000000002
Time, 2019-01-01T17:30:45, Epoch: 29, Batch: 110, Training Loss: 0.04843139611184597, LR: 0.010000000000000002
Time, 2019-01-01T17:30:46, Epoch: 29, Batch: 120, Training Loss: 0.07578456550836563, LR: 0.010000000000000002
Time, 2019-01-01T17:30:47, Epoch: 29, Batch: 130, Training Loss: 0.0679839450865984, LR: 0.010000000000000002
Time, 2019-01-01T17:30:47, Epoch: 29, Batch: 140, Training Loss: 0.09673252925276757, LR: 0.010000000000000002
Time, 2019-01-01T17:30:48, Epoch: 29, Batch: 150, Training Loss: 0.1005682397633791, LR: 0.010000000000000002
Time, 2019-01-01T17:30:49, Epoch: 29, Batch: 160, Training Loss: 0.12769804298877716, LR: 0.010000000000000002
Time, 2019-01-01T17:30:50, Epoch: 29, Batch: 170, Training Loss: 0.12607688531279565, LR: 0.010000000000000002
Time, 2019-01-01T17:30:50, Epoch: 29, Batch: 180, Training Loss: 0.10266147032380105, LR: 0.010000000000000002
Time, 2019-01-01T17:30:51, Epoch: 29, Batch: 190, Training Loss: 0.10542904026806355, LR: 0.010000000000000002
Time, 2019-01-01T17:30:52, Epoch: 29, Batch: 200, Training Loss: 0.11892896853387355, LR: 0.010000000000000002
Time, 2019-01-01T17:30:52, Epoch: 29, Batch: 210, Training Loss: 0.08011323437094689, LR: 0.010000000000000002
Time, 2019-01-01T17:30:53, Epoch: 29, Batch: 220, Training Loss: 0.0880288042128086, LR: 0.010000000000000002
Time, 2019-01-01T17:30:54, Epoch: 29, Batch: 230, Training Loss: 0.07512759864330291, LR: 0.010000000000000002
Time, 2019-01-01T17:30:54, Epoch: 29, Batch: 240, Training Loss: 0.08864127919077873, LR: 0.010000000000000002
Time, 2019-01-01T17:30:55, Epoch: 29, Batch: 250, Training Loss: 0.08257013186812401, LR: 0.010000000000000002
Time, 2019-01-01T17:30:56, Epoch: 29, Batch: 260, Training Loss: 0.07371256723999978, LR: 0.010000000000000002
Time, 2019-01-01T17:30:56, Epoch: 29, Batch: 270, Training Loss: 0.08694978132843971, LR: 0.010000000000000002
Time, 2019-01-01T17:30:57, Epoch: 29, Batch: 280, Training Loss: 0.08667943328619003, LR: 0.010000000000000002
Time, 2019-01-01T17:30:57, Epoch: 29, Batch: 290, Training Loss: 0.09372279271483422, LR: 0.010000000000000002
Time, 2019-01-01T17:30:58, Epoch: 29, Batch: 300, Training Loss: 0.07380731254816056, LR: 0.010000000000000002
Time, 2019-01-01T17:30:59, Epoch: 29, Batch: 310, Training Loss: 0.11583712846040725, LR: 0.010000000000000002
Time, 2019-01-01T17:30:59, Epoch: 29, Batch: 320, Training Loss: 0.0630987398326397, LR: 0.010000000000000002
Time, 2019-01-01T17:31:00, Epoch: 29, Batch: 330, Training Loss: 0.07639276757836341, LR: 0.010000000000000002
Time, 2019-01-01T17:31:01, Epoch: 29, Batch: 340, Training Loss: 0.06772724278271199, LR: 0.010000000000000002
Time, 2019-01-01T17:31:01, Epoch: 29, Batch: 350, Training Loss: 0.06884014196693897, LR: 0.010000000000000002
Time, 2019-01-01T17:31:02, Epoch: 29, Batch: 360, Training Loss: 0.0824931163340807, LR: 0.010000000000000002
Time, 2019-01-01T17:31:03, Epoch: 29, Batch: 370, Training Loss: 0.11152442172169685, LR: 0.010000000000000002
Time, 2019-01-01T17:31:03, Epoch: 29, Batch: 380, Training Loss: 0.09196041151881218, LR: 0.010000000000000002
Time, 2019-01-01T17:31:04, Epoch: 29, Batch: 390, Training Loss: 0.08991527408361435, LR: 0.010000000000000002
Time, 2019-01-01T17:31:05, Epoch: 29, Batch: 400, Training Loss: 0.06941832825541497, LR: 0.010000000000000002
Time, 2019-01-01T17:31:05, Epoch: 29, Batch: 410, Training Loss: 0.07543024569749832, LR: 0.010000000000000002
Time, 2019-01-01T17:31:06, Epoch: 29, Batch: 420, Training Loss: 0.09291285574436188, LR: 0.010000000000000002
Time, 2019-01-01T17:31:07, Epoch: 29, Batch: 430, Training Loss: 0.07279771715402603, LR: 0.010000000000000002
Time, 2019-01-01T17:31:07, Epoch: 29, Batch: 440, Training Loss: 0.05154036432504654, LR: 0.010000000000000002
Time, 2019-01-01T17:31:08, Epoch: 29, Batch: 450, Training Loss: 0.0850194051861763, LR: 0.010000000000000002
Time, 2019-01-01T17:31:09, Epoch: 29, Batch: 460, Training Loss: 0.05902344137430191, LR: 0.010000000000000002
Time, 2019-01-01T17:31:09, Epoch: 29, Batch: 470, Training Loss: 0.09365100041031837, LR: 0.010000000000000002
Time, 2019-01-01T17:31:10, Epoch: 29, Batch: 480, Training Loss: 0.07580203376710415, LR: 0.010000000000000002
Time, 2019-01-01T17:31:11, Epoch: 29, Batch: 490, Training Loss: 0.06443499475717544, LR: 0.010000000000000002
Time, 2019-01-01T17:31:12, Epoch: 29, Batch: 500, Training Loss: 0.08941996321082116, LR: 0.010000000000000002
Time, 2019-01-01T17:31:12, Epoch: 29, Batch: 510, Training Loss: 0.0659397415816784, LR: 0.010000000000000002
Time, 2019-01-01T17:31:13, Epoch: 29, Batch: 520, Training Loss: 0.06909963041543961, LR: 0.010000000000000002
Time, 2019-01-01T17:31:14, Epoch: 29, Batch: 530, Training Loss: 0.07359825037419795, LR: 0.010000000000000002
Time, 2019-01-01T17:31:14, Epoch: 29, Batch: 540, Training Loss: 0.07957057021558285, LR: 0.010000000000000002
Time, 2019-01-01T17:31:15, Epoch: 29, Batch: 550, Training Loss: 0.07224208414554596, LR: 0.010000000000000002
Time, 2019-01-01T17:31:16, Epoch: 29, Batch: 560, Training Loss: 0.06735676303505897, LR: 0.010000000000000002
Time, 2019-01-01T17:31:16, Epoch: 29, Batch: 570, Training Loss: 0.09712351113557816, LR: 0.010000000000000002
Time, 2019-01-01T17:31:17, Epoch: 29, Batch: 580, Training Loss: 0.07798242792487145, LR: 0.010000000000000002
Time, 2019-01-01T17:31:18, Epoch: 29, Batch: 590, Training Loss: 0.0758893609046936, LR: 0.010000000000000002
Time, 2019-01-01T17:31:18, Epoch: 29, Batch: 600, Training Loss: 0.07826631292700767, LR: 0.010000000000000002
Time, 2019-01-01T17:31:19, Epoch: 29, Batch: 610, Training Loss: 0.07046510949730873, LR: 0.010000000000000002
Time, 2019-01-01T17:31:20, Epoch: 29, Batch: 620, Training Loss: 0.07628521993756295, LR: 0.010000000000000002
Time, 2019-01-01T17:31:20, Epoch: 29, Batch: 630, Training Loss: 0.08008047044277192, LR: 0.010000000000000002
Time, 2019-01-01T17:31:21, Epoch: 29, Batch: 640, Training Loss: 0.09478469118475914, LR: 0.010000000000000002
Time, 2019-01-01T17:31:22, Epoch: 29, Batch: 650, Training Loss: 0.10556710958480835, LR: 0.010000000000000002
Time, 2019-01-01T17:31:22, Epoch: 29, Batch: 660, Training Loss: 0.09523320198059082, LR: 0.010000000000000002
Time, 2019-01-01T17:31:23, Epoch: 29, Batch: 670, Training Loss: 0.0768033031374216, LR: 0.010000000000000002
Time, 2019-01-01T17:31:24, Epoch: 29, Batch: 680, Training Loss: 0.0666785828769207, LR: 0.010000000000000002
Time, 2019-01-01T17:31:24, Epoch: 29, Batch: 690, Training Loss: 0.07263806648552418, LR: 0.010000000000000002
Time, 2019-01-01T17:31:25, Epoch: 29, Batch: 700, Training Loss: 0.08240578882396221, LR: 0.010000000000000002
Time, 2019-01-01T17:31:26, Epoch: 29, Batch: 710, Training Loss: 0.08428565487265587, LR: 0.010000000000000002
Time, 2019-01-01T17:31:26, Epoch: 29, Batch: 720, Training Loss: 0.07251989804208278, LR: 0.010000000000000002
Time, 2019-01-01T17:31:27, Epoch: 29, Batch: 730, Training Loss: 0.05390857346355915, LR: 0.010000000000000002
Time, 2019-01-01T17:31:28, Epoch: 29, Batch: 740, Training Loss: 0.06125342845916748, LR: 0.010000000000000002
Time, 2019-01-01T17:31:28, Epoch: 29, Batch: 750, Training Loss: 0.08333326578140259, LR: 0.010000000000000002
Time, 2019-01-01T17:31:29, Epoch: 29, Batch: 760, Training Loss: 0.07946671769022942, LR: 0.010000000000000002
Time, 2019-01-01T17:31:30, Epoch: 29, Batch: 770, Training Loss: 0.0761775329709053, LR: 0.010000000000000002
Time, 2019-01-01T17:31:30, Epoch: 29, Batch: 780, Training Loss: 0.07849197387695313, LR: 0.010000000000000002
Time, 2019-01-01T17:31:31, Epoch: 29, Batch: 790, Training Loss: 0.07964211478829383, LR: 0.010000000000000002
Time, 2019-01-01T17:31:32, Epoch: 29, Batch: 800, Training Loss: 0.09035683944821357, LR: 0.010000000000000002
Time, 2019-01-01T17:31:32, Epoch: 29, Batch: 810, Training Loss: 0.07349678352475167, LR: 0.010000000000000002
Time, 2019-01-01T17:31:33, Epoch: 29, Batch: 820, Training Loss: 0.09421201758086681, LR: 0.010000000000000002
Time, 2019-01-01T17:31:34, Epoch: 29, Batch: 830, Training Loss: 0.057633377611637115, LR: 0.010000000000000002
Time, 2019-01-01T17:31:34, Epoch: 29, Batch: 840, Training Loss: 0.064964160323143, LR: 0.010000000000000002
Time, 2019-01-01T17:31:35, Epoch: 29, Batch: 850, Training Loss: 0.07031946405768394, LR: 0.010000000000000002
Time, 2019-01-01T17:31:36, Epoch: 29, Batch: 860, Training Loss: 0.0640085831284523, LR: 0.010000000000000002
Time, 2019-01-01T17:31:36, Epoch: 29, Batch: 870, Training Loss: 0.08241834118962288, LR: 0.010000000000000002
Time, 2019-01-01T17:31:37, Epoch: 29, Batch: 880, Training Loss: 0.0711615014821291, LR: 0.010000000000000002
Time, 2019-01-01T17:31:38, Epoch: 29, Batch: 890, Training Loss: 0.06709954775869846, LR: 0.010000000000000002
Time, 2019-01-01T17:31:38, Epoch: 29, Batch: 900, Training Loss: 0.054135295748710635, LR: 0.010000000000000002
Time, 2019-01-01T17:31:39, Epoch: 29, Batch: 910, Training Loss: 0.07470666579902172, LR: 0.010000000000000002
Time, 2019-01-01T17:31:40, Epoch: 29, Batch: 920, Training Loss: 0.08169821612536907, LR: 0.010000000000000002
Time, 2019-01-01T17:31:41, Epoch: 29, Batch: 930, Training Loss: 0.06685548014938832, LR: 0.010000000000000002
Epoch: 29, Validation Top 1 acc: 98.22927856445312
Epoch: 29, Validation Top 5 acc: 99.98334503173828
Epoch: 29, Validation Set Loss: 0.060923803597688675
Start training epoch 30
Time, 2019-01-01T17:32:06, Epoch: 30, Batch: 10, Training Loss: 0.05888806916773319, LR: 0.010000000000000002
Time, 2019-01-01T17:32:07, Epoch: 30, Batch: 20, Training Loss: 0.06388774961233139, LR: 0.010000000000000002
Time, 2019-01-01T17:32:07, Epoch: 30, Batch: 30, Training Loss: 0.04389069750905037, LR: 0.010000000000000002
Time, 2019-01-01T17:32:08, Epoch: 30, Batch: 40, Training Loss: 0.07215450182557107, LR: 0.010000000000000002
Time, 2019-01-01T17:32:09, Epoch: 30, Batch: 50, Training Loss: 0.056442777067422865, LR: 0.010000000000000002
Time, 2019-01-01T17:32:09, Epoch: 30, Batch: 60, Training Loss: 0.051945296302437785, LR: 0.010000000000000002
Time, 2019-01-01T17:32:10, Epoch: 30, Batch: 70, Training Loss: 0.08353627249598503, LR: 0.010000000000000002
Time, 2019-01-01T17:32:11, Epoch: 30, Batch: 80, Training Loss: 0.07759275622665882, LR: 0.010000000000000002
Time, 2019-01-01T17:32:11, Epoch: 30, Batch: 90, Training Loss: 0.050646917149424556, LR: 0.010000000000000002
Time, 2019-01-01T17:32:12, Epoch: 30, Batch: 100, Training Loss: 0.07946465872228145, LR: 0.010000000000000002
Time, 2019-01-01T17:32:13, Epoch: 30, Batch: 110, Training Loss: 0.06387931145727635, LR: 0.010000000000000002
Time, 2019-01-01T17:32:13, Epoch: 30, Batch: 120, Training Loss: 0.06027215495705605, LR: 0.010000000000000002
Time, 2019-01-01T17:32:14, Epoch: 30, Batch: 130, Training Loss: 0.07433359436690808, LR: 0.010000000000000002
Time, 2019-01-01T17:32:15, Epoch: 30, Batch: 140, Training Loss: 0.07991770431399345, LR: 0.010000000000000002
Time, 2019-01-01T17:32:15, Epoch: 30, Batch: 150, Training Loss: 0.07204801253974438, LR: 0.010000000000000002
Time, 2019-01-01T17:32:16, Epoch: 30, Batch: 160, Training Loss: 0.08017116263508797, LR: 0.010000000000000002
Time, 2019-01-01T17:32:17, Epoch: 30, Batch: 170, Training Loss: 0.11136651039123535, LR: 0.010000000000000002
Time, 2019-01-01T17:32:17, Epoch: 30, Batch: 180, Training Loss: 0.09312095195055008, LR: 0.010000000000000002
Time, 2019-01-01T17:32:18, Epoch: 30, Batch: 190, Training Loss: 0.07816125825047493, LR: 0.010000000000000002
Time, 2019-01-01T17:32:19, Epoch: 30, Batch: 200, Training Loss: 0.06762578450143338, LR: 0.010000000000000002
Time, 2019-01-01T17:32:19, Epoch: 30, Batch: 210, Training Loss: 0.06974159143865108, LR: 0.010000000000000002
Time, 2019-01-01T17:32:20, Epoch: 30, Batch: 220, Training Loss: 0.05286404192447662, LR: 0.010000000000000002
Time, 2019-01-01T17:32:21, Epoch: 30, Batch: 230, Training Loss: 0.10007360726594924, LR: 0.010000000000000002
Time, 2019-01-01T17:32:21, Epoch: 30, Batch: 240, Training Loss: 0.072346431016922, LR: 0.010000000000000002
Time, 2019-01-01T17:32:22, Epoch: 30, Batch: 250, Training Loss: 0.0641033411026001, LR: 0.010000000000000002
Time, 2019-01-01T17:32:23, Epoch: 30, Batch: 260, Training Loss: 0.06860945522785186, LR: 0.010000000000000002
Time, 2019-01-01T17:32:23, Epoch: 30, Batch: 270, Training Loss: 0.06415153257548808, LR: 0.010000000000000002
Time, 2019-01-01T17:32:24, Epoch: 30, Batch: 280, Training Loss: 0.08011795729398727, LR: 0.010000000000000002
Time, 2019-01-01T17:32:25, Epoch: 30, Batch: 290, Training Loss: 0.06103809475898743, LR: 0.010000000000000002
Time, 2019-01-01T17:32:25, Epoch: 30, Batch: 300, Training Loss: 0.05540241301059723, LR: 0.010000000000000002
Time, 2019-01-01T17:32:26, Epoch: 30, Batch: 310, Training Loss: 0.06833832412958145, LR: 0.010000000000000002
Time, 2019-01-01T17:32:27, Epoch: 30, Batch: 320, Training Loss: 0.06215210556983948, LR: 0.010000000000000002
Time, 2019-01-01T17:32:27, Epoch: 30, Batch: 330, Training Loss: 0.07584346309304238, LR: 0.010000000000000002
Time, 2019-01-01T17:32:28, Epoch: 30, Batch: 340, Training Loss: 0.05721440427005291, LR: 0.010000000000000002
Time, 2019-01-01T17:32:29, Epoch: 30, Batch: 350, Training Loss: 0.10568142533302308, LR: 0.010000000000000002
Time, 2019-01-01T17:32:29, Epoch: 30, Batch: 360, Training Loss: 0.07290614545345306, LR: 0.010000000000000002
Time, 2019-01-01T17:32:30, Epoch: 30, Batch: 370, Training Loss: 0.0750336203724146, LR: 0.010000000000000002
Time, 2019-01-01T17:32:31, Epoch: 30, Batch: 380, Training Loss: 0.057030781731009485, LR: 0.010000000000000002
Time, 2019-01-01T17:32:31, Epoch: 30, Batch: 390, Training Loss: 0.07283477559685707, LR: 0.010000000000000002
Time, 2019-01-01T17:32:32, Epoch: 30, Batch: 400, Training Loss: 0.05659231804311275, LR: 0.010000000000000002
Time, 2019-01-01T17:32:33, Epoch: 30, Batch: 410, Training Loss: 0.05947637595236301, LR: 0.010000000000000002
Time, 2019-01-01T17:32:33, Epoch: 30, Batch: 420, Training Loss: 0.06954347267746926, LR: 0.010000000000000002
Time, 2019-01-01T17:32:34, Epoch: 30, Batch: 430, Training Loss: 0.06471271738409996, LR: 0.010000000000000002
Time, 2019-01-01T17:32:35, Epoch: 30, Batch: 440, Training Loss: 0.049012935161590575, LR: 0.010000000000000002
Time, 2019-01-01T17:32:36, Epoch: 30, Batch: 450, Training Loss: 0.054412524402141574, LR: 0.010000000000000002
Time, 2019-01-01T17:32:36, Epoch: 30, Batch: 460, Training Loss: 0.06508400030434132, LR: 0.010000000000000002
Time, 2019-01-01T17:32:37, Epoch: 30, Batch: 470, Training Loss: 0.05837434269487858, LR: 0.010000000000000002
Time, 2019-01-01T17:32:38, Epoch: 30, Batch: 480, Training Loss: 0.05835611745715141, LR: 0.010000000000000002
Time, 2019-01-01T17:32:38, Epoch: 30, Batch: 490, Training Loss: 0.038554136827588084, LR: 0.010000000000000002
Time, 2019-01-01T17:32:39, Epoch: 30, Batch: 500, Training Loss: 0.0731735073029995, LR: 0.010000000000000002
Time, 2019-01-01T17:32:40, Epoch: 30, Batch: 510, Training Loss: 0.040677720308303834, LR: 0.010000000000000002
Time, 2019-01-01T17:32:40, Epoch: 30, Batch: 520, Training Loss: 0.06337045431137085, LR: 0.010000000000000002
Time, 2019-01-01T17:32:41, Epoch: 30, Batch: 530, Training Loss: 0.05720234587788582, LR: 0.010000000000000002
Time, 2019-01-01T17:32:42, Epoch: 30, Batch: 540, Training Loss: 0.05644095987081528, LR: 0.010000000000000002
Time, 2019-01-01T17:32:42, Epoch: 30, Batch: 550, Training Loss: 0.08712360709905624, LR: 0.010000000000000002
Time, 2019-01-01T17:32:43, Epoch: 30, Batch: 560, Training Loss: 0.061863484978675845, LR: 0.010000000000000002
Time, 2019-01-01T17:32:44, Epoch: 30, Batch: 570, Training Loss: 0.06720168180763722, LR: 0.010000000000000002
Time, 2019-01-01T17:32:44, Epoch: 30, Batch: 580, Training Loss: 0.05902155861258507, LR: 0.010000000000000002
Time, 2019-01-01T17:32:45, Epoch: 30, Batch: 590, Training Loss: 0.062077509611845015, LR: 0.010000000000000002
Time, 2019-01-01T17:32:46, Epoch: 30, Batch: 600, Training Loss: 0.056622724235057834, LR: 0.010000000000000002
Time, 2019-01-01T17:32:47, Epoch: 30, Batch: 610, Training Loss: 0.04076870679855347, LR: 0.010000000000000002
Time, 2019-01-01T17:32:47, Epoch: 30, Batch: 620, Training Loss: 0.04138748683035374, LR: 0.010000000000000002
Time, 2019-01-01T17:32:48, Epoch: 30, Batch: 630, Training Loss: 0.09371755793690681, LR: 0.010000000000000002
Time, 2019-01-01T17:32:49, Epoch: 30, Batch: 640, Training Loss: 0.06891086399555206, LR: 0.010000000000000002
Time, 2019-01-01T17:32:49, Epoch: 30, Batch: 650, Training Loss: 0.06282815597951412, LR: 0.010000000000000002
Time, 2019-01-01T17:32:50, Epoch: 30, Batch: 660, Training Loss: 0.04864352196455002, LR: 0.010000000000000002
Time, 2019-01-01T17:32:51, Epoch: 30, Batch: 670, Training Loss: 0.04648107700049877, LR: 0.010000000000000002
Time, 2019-01-01T17:32:51, Epoch: 30, Batch: 680, Training Loss: 0.07923478744924069, LR: 0.010000000000000002
Time, 2019-01-01T17:32:52, Epoch: 30, Batch: 690, Training Loss: 0.06405089870095253, LR: 0.010000000000000002
Time, 2019-01-01T17:32:53, Epoch: 30, Batch: 700, Training Loss: 0.060154089331626893, LR: 0.010000000000000002
Time, 2019-01-01T17:32:54, Epoch: 30, Batch: 710, Training Loss: 0.05839145928621292, LR: 0.010000000000000002
Time, 2019-01-01T17:32:54, Epoch: 30, Batch: 720, Training Loss: 0.07609984427690505, LR: 0.010000000000000002
Time, 2019-01-01T17:32:55, Epoch: 30, Batch: 730, Training Loss: 0.05351684242486954, LR: 0.010000000000000002
Time, 2019-01-01T17:32:56, Epoch: 30, Batch: 740, Training Loss: 0.04929170943796635, LR: 0.010000000000000002
Time, 2019-01-01T17:32:56, Epoch: 30, Batch: 750, Training Loss: 0.053374356031417845, LR: 0.010000000000000002
Time, 2019-01-01T17:32:57, Epoch: 30, Batch: 760, Training Loss: 0.04983395338058472, LR: 0.010000000000000002
Time, 2019-01-01T17:32:58, Epoch: 30, Batch: 770, Training Loss: 0.07676052153110505, LR: 0.010000000000000002
Time, 2019-01-01T17:32:58, Epoch: 30, Batch: 780, Training Loss: 0.0432875070720911, LR: 0.010000000000000002
Time, 2019-01-01T17:32:59, Epoch: 30, Batch: 790, Training Loss: 0.07489864453673363, LR: 0.010000000000000002
Time, 2019-01-01T17:33:00, Epoch: 30, Batch: 800, Training Loss: 0.05565693937242031, LR: 0.010000000000000002
Time, 2019-01-01T17:33:01, Epoch: 30, Batch: 810, Training Loss: 0.03252126052975655, LR: 0.010000000000000002
Time, 2019-01-01T17:33:01, Epoch: 30, Batch: 820, Training Loss: 0.0726819597184658, LR: 0.010000000000000002
Time, 2019-01-01T17:33:02, Epoch: 30, Batch: 830, Training Loss: 0.05087412893772125, LR: 0.010000000000000002
Time, 2019-01-01T17:33:03, Epoch: 30, Batch: 840, Training Loss: 0.04266587421298027, LR: 0.010000000000000002
Time, 2019-01-01T17:33:03, Epoch: 30, Batch: 850, Training Loss: 0.0500179722905159, LR: 0.010000000000000002
Time, 2019-01-01T17:33:04, Epoch: 30, Batch: 860, Training Loss: 0.05721171908080578, LR: 0.010000000000000002
Time, 2019-01-01T17:33:05, Epoch: 30, Batch: 870, Training Loss: 0.060390747338533404, LR: 0.010000000000000002
Time, 2019-01-01T17:33:05, Epoch: 30, Batch: 880, Training Loss: 0.06607648059725761, LR: 0.010000000000000002
Time, 2019-01-01T17:33:06, Epoch: 30, Batch: 890, Training Loss: 0.06481036581099034, LR: 0.010000000000000002
Time, 2019-01-01T17:33:07, Epoch: 30, Batch: 900, Training Loss: 0.06092190742492676, LR: 0.010000000000000002
Time, 2019-01-01T17:33:07, Epoch: 30, Batch: 910, Training Loss: 0.03674092218279838, LR: 0.010000000000000002
Time, 2019-01-01T17:33:08, Epoch: 30, Batch: 920, Training Loss: 0.06724658831954003, LR: 0.010000000000000002
Time, 2019-01-01T17:33:09, Epoch: 30, Batch: 930, Training Loss: 0.037398752942681315, LR: 0.010000000000000002
Epoch: 30, Validation Top 1 acc: 98.41917419433594
Epoch: 30, Validation Top 5 acc: 99.99000549316406
Epoch: 30, Validation Set Loss: 0.053115054965019226
Start training epoch 31
Time, 2019-01-01T17:33:35, Epoch: 31, Batch: 10, Training Loss: 0.03433726653456688, LR: 0.010000000000000002
Time, 2019-01-01T17:33:35, Epoch: 31, Batch: 20, Training Loss: 0.04034768864512443, LR: 0.010000000000000002
Time, 2019-01-01T17:33:36, Epoch: 31, Batch: 30, Training Loss: 0.04786263033747673, LR: 0.010000000000000002
Time, 2019-01-01T17:33:37, Epoch: 31, Batch: 40, Training Loss: 0.05294862538576126, LR: 0.010000000000000002
Time, 2019-01-01T17:33:38, Epoch: 31, Batch: 50, Training Loss: 0.06524038873612881, LR: 0.010000000000000002
Time, 2019-01-01T17:33:38, Epoch: 31, Batch: 60, Training Loss: 0.0560034453868866, LR: 0.010000000000000002
Time, 2019-01-01T17:33:39, Epoch: 31, Batch: 70, Training Loss: 0.07666918896138668, LR: 0.010000000000000002
Time, 2019-01-01T17:33:40, Epoch: 31, Batch: 80, Training Loss: 0.046197091788053514, LR: 0.010000000000000002
Time, 2019-01-01T17:33:40, Epoch: 31, Batch: 90, Training Loss: 0.04264004081487656, LR: 0.010000000000000002
Time, 2019-01-01T17:33:41, Epoch: 31, Batch: 100, Training Loss: 0.06989448443055153, LR: 0.010000000000000002
Time, 2019-01-01T17:33:42, Epoch: 31, Batch: 110, Training Loss: 0.05597890466451645, LR: 0.010000000000000002
Time, 2019-01-01T17:33:42, Epoch: 31, Batch: 120, Training Loss: 0.041806458681821826, LR: 0.010000000000000002
Time, 2019-01-01T17:33:43, Epoch: 31, Batch: 130, Training Loss: 0.05543003007769585, LR: 0.010000000000000002
Time, 2019-01-01T17:33:44, Epoch: 31, Batch: 140, Training Loss: 0.0604152224957943, LR: 0.010000000000000002
Time, 2019-01-01T17:33:44, Epoch: 31, Batch: 150, Training Loss: 0.06321359500288963, LR: 0.010000000000000002
Time, 2019-01-01T17:33:45, Epoch: 31, Batch: 160, Training Loss: 0.058063970506191255, LR: 0.010000000000000002
Time, 2019-01-01T17:33:46, Epoch: 31, Batch: 170, Training Loss: 0.06142529509961605, LR: 0.010000000000000002
Time, 2019-01-01T17:33:47, Epoch: 31, Batch: 180, Training Loss: 0.05268680900335312, LR: 0.010000000000000002
Time, 2019-01-01T17:33:47, Epoch: 31, Batch: 190, Training Loss: 0.043242672458291054, LR: 0.010000000000000002
Time, 2019-01-01T17:33:48, Epoch: 31, Batch: 200, Training Loss: 0.06914942935109139, LR: 0.010000000000000002
Time, 2019-01-01T17:33:49, Epoch: 31, Batch: 210, Training Loss: 0.07390534691512585, LR: 0.010000000000000002
Time, 2019-01-01T17:33:49, Epoch: 31, Batch: 220, Training Loss: 0.06970206536352634, LR: 0.010000000000000002
Time, 2019-01-01T17:33:50, Epoch: 31, Batch: 230, Training Loss: 0.05648566782474518, LR: 0.010000000000000002
Time, 2019-01-01T17:33:51, Epoch: 31, Batch: 240, Training Loss: 0.04105251617729664, LR: 0.010000000000000002
Time, 2019-01-01T17:33:51, Epoch: 31, Batch: 250, Training Loss: 0.038108164072036745, LR: 0.010000000000000002
Time, 2019-01-01T17:33:52, Epoch: 31, Batch: 260, Training Loss: 0.02993505597114563, LR: 0.010000000000000002
Time, 2019-01-01T17:33:53, Epoch: 31, Batch: 270, Training Loss: 0.07499852888286114, LR: 0.010000000000000002
Time, 2019-01-01T17:33:54, Epoch: 31, Batch: 280, Training Loss: 0.053333282470703125, LR: 0.010000000000000002
Time, 2019-01-01T17:33:54, Epoch: 31, Batch: 290, Training Loss: 0.0522525180131197, LR: 0.010000000000000002
Time, 2019-01-01T17:33:55, Epoch: 31, Batch: 300, Training Loss: 0.04322747066617012, LR: 0.010000000000000002
Time, 2019-01-01T17:33:56, Epoch: 31, Batch: 310, Training Loss: 0.06062363646924496, LR: 0.010000000000000002
Time, 2019-01-01T17:33:56, Epoch: 31, Batch: 320, Training Loss: 0.04771687984466553, LR: 0.010000000000000002
Time, 2019-01-01T17:33:57, Epoch: 31, Batch: 330, Training Loss: 0.04962301775813103, LR: 0.010000000000000002
Time, 2019-01-01T17:33:58, Epoch: 31, Batch: 340, Training Loss: 0.062121573090553286, LR: 0.010000000000000002
Time, 2019-01-01T17:33:58, Epoch: 31, Batch: 350, Training Loss: 0.05017059892416, LR: 0.010000000000000002
Time, 2019-01-01T17:33:59, Epoch: 31, Batch: 360, Training Loss: 0.04471418783068657, LR: 0.010000000000000002
Time, 2019-01-01T17:34:00, Epoch: 31, Batch: 370, Training Loss: 0.03258523792028427, LR: 0.010000000000000002
Time, 2019-01-01T17:34:01, Epoch: 31, Batch: 380, Training Loss: 0.042593537271022795, LR: 0.010000000000000002
Time, 2019-01-01T17:34:01, Epoch: 31, Batch: 390, Training Loss: 0.04768589027225971, LR: 0.010000000000000002
Time, 2019-01-01T17:34:02, Epoch: 31, Batch: 400, Training Loss: 0.04630030281841755, LR: 0.010000000000000002
Time, 2019-01-01T17:34:03, Epoch: 31, Batch: 410, Training Loss: 0.04167553782463074, LR: 0.010000000000000002
Time, 2019-01-01T17:34:03, Epoch: 31, Batch: 420, Training Loss: 0.07047071829438209, LR: 0.010000000000000002
Time, 2019-01-01T17:34:04, Epoch: 31, Batch: 430, Training Loss: 0.05440071672201156, LR: 0.010000000000000002
Time, 2019-01-01T17:34:05, Epoch: 31, Batch: 440, Training Loss: 0.05057881101965904, LR: 0.010000000000000002
Time, 2019-01-01T17:34:05, Epoch: 31, Batch: 450, Training Loss: 0.057419534772634506, LR: 0.010000000000000002
Time, 2019-01-01T17:34:06, Epoch: 31, Batch: 460, Training Loss: 0.06351506151258945, LR: 0.010000000000000002
Time, 2019-01-01T17:34:07, Epoch: 31, Batch: 470, Training Loss: 0.047178153321146964, LR: 0.010000000000000002
Time, 2019-01-01T17:34:07, Epoch: 31, Batch: 480, Training Loss: 0.05487415269017219, LR: 0.010000000000000002
Time, 2019-01-01T17:34:08, Epoch: 31, Batch: 490, Training Loss: 0.05550071746110916, LR: 0.010000000000000002
Time, 2019-01-01T17:34:09, Epoch: 31, Batch: 500, Training Loss: 0.05190286561846733, LR: 0.010000000000000002
Time, 2019-01-01T17:34:10, Epoch: 31, Batch: 510, Training Loss: 0.04264238215982914, LR: 0.010000000000000002
Time, 2019-01-01T17:34:10, Epoch: 31, Batch: 520, Training Loss: 0.06652503535151481, LR: 0.010000000000000002
Time, 2019-01-01T17:34:11, Epoch: 31, Batch: 530, Training Loss: 0.07625014632940293, LR: 0.010000000000000002
Time, 2019-01-01T17:34:12, Epoch: 31, Batch: 540, Training Loss: 0.06937715597450733, LR: 0.010000000000000002
Time, 2019-01-01T17:34:12, Epoch: 31, Batch: 550, Training Loss: 0.07221997864544391, LR: 0.010000000000000002
Time, 2019-01-01T17:34:13, Epoch: 31, Batch: 560, Training Loss: 0.06368889212608338, LR: 0.010000000000000002
Time, 2019-01-01T17:34:14, Epoch: 31, Batch: 570, Training Loss: 0.045234011486172676, LR: 0.010000000000000002
Time, 2019-01-01T17:34:14, Epoch: 31, Batch: 580, Training Loss: 0.06089791171252727, LR: 0.010000000000000002
Time, 2019-01-01T17:34:15, Epoch: 31, Batch: 590, Training Loss: 0.055315320193767545, LR: 0.010000000000000002
Time, 2019-01-01T17:34:16, Epoch: 31, Batch: 600, Training Loss: 0.0628511905670166, LR: 0.010000000000000002
Time, 2019-01-01T17:34:17, Epoch: 31, Batch: 610, Training Loss: 0.058213647827506064, LR: 0.010000000000000002
Time, 2019-01-01T17:34:17, Epoch: 31, Batch: 620, Training Loss: 0.07580667212605477, LR: 0.010000000000000002
Time, 2019-01-01T17:34:18, Epoch: 31, Batch: 630, Training Loss: 0.047518066316843036, LR: 0.010000000000000002
Time, 2019-01-01T17:34:19, Epoch: 31, Batch: 640, Training Loss: 0.06865554191172123, LR: 0.010000000000000002
Time, 2019-01-01T17:34:19, Epoch: 31, Batch: 650, Training Loss: 0.06465620249509811, LR: 0.010000000000000002
Time, 2019-01-01T17:34:20, Epoch: 31, Batch: 660, Training Loss: 0.062049584463238716, LR: 0.010000000000000002
Time, 2019-01-01T17:34:21, Epoch: 31, Batch: 670, Training Loss: 0.055890724435448645, LR: 0.010000000000000002
Time, 2019-01-01T17:34:21, Epoch: 31, Batch: 680, Training Loss: 0.044558314979076384, LR: 0.010000000000000002
Time, 2019-01-01T17:34:22, Epoch: 31, Batch: 690, Training Loss: 0.06324921473860741, LR: 0.010000000000000002
Time, 2019-01-01T17:34:23, Epoch: 31, Batch: 700, Training Loss: 0.04085309728980065, LR: 0.010000000000000002
Time, 2019-01-01T17:34:24, Epoch: 31, Batch: 710, Training Loss: 0.06789188608527183, LR: 0.010000000000000002
Time, 2019-01-01T17:34:24, Epoch: 31, Batch: 720, Training Loss: 0.043730980902910235, LR: 0.010000000000000002
Time, 2019-01-01T17:34:25, Epoch: 31, Batch: 730, Training Loss: 0.05973213836550713, LR: 0.010000000000000002
Time, 2019-01-01T17:34:26, Epoch: 31, Batch: 740, Training Loss: 0.05280380547046661, LR: 0.010000000000000002
Time, 2019-01-01T17:34:26, Epoch: 31, Batch: 750, Training Loss: 0.06163216196000576, LR: 0.010000000000000002
Time, 2019-01-01T17:34:27, Epoch: 31, Batch: 760, Training Loss: 0.058358895033597945, LR: 0.010000000000000002
Time, 2019-01-01T17:34:28, Epoch: 31, Batch: 770, Training Loss: 0.08131433092057705, LR: 0.010000000000000002
Time, 2019-01-01T17:34:29, Epoch: 31, Batch: 780, Training Loss: 0.07767019122838974, LR: 0.010000000000000002
Time, 2019-01-01T17:34:29, Epoch: 31, Batch: 790, Training Loss: 0.06061781011521816, LR: 0.010000000000000002
Time, 2019-01-01T17:34:30, Epoch: 31, Batch: 800, Training Loss: 0.05883350372314453, LR: 0.010000000000000002
Time, 2019-01-01T17:34:31, Epoch: 31, Batch: 810, Training Loss: 0.05950859375298023, LR: 0.010000000000000002
Time, 2019-01-01T17:34:31, Epoch: 31, Batch: 820, Training Loss: 0.0493753869086504, LR: 0.010000000000000002
Time, 2019-01-01T17:34:32, Epoch: 31, Batch: 830, Training Loss: 0.07297561131417751, LR: 0.010000000000000002
Time, 2019-01-01T17:34:33, Epoch: 31, Batch: 840, Training Loss: 0.07018479816615582, LR: 0.010000000000000002
Time, 2019-01-01T17:34:34, Epoch: 31, Batch: 850, Training Loss: 0.04647838920354843, LR: 0.010000000000000002
Time, 2019-01-01T17:34:34, Epoch: 31, Batch: 860, Training Loss: 0.08607467338442802, LR: 0.010000000000000002
Time, 2019-01-01T17:34:35, Epoch: 31, Batch: 870, Training Loss: 0.09157253578305244, LR: 0.010000000000000002
Time, 2019-01-01T17:34:36, Epoch: 31, Batch: 880, Training Loss: 0.09220199957489968, LR: 0.010000000000000002
Time, 2019-01-01T17:34:36, Epoch: 31, Batch: 890, Training Loss: 0.04079089276492596, LR: 0.010000000000000002
Time, 2019-01-01T17:34:37, Epoch: 31, Batch: 900, Training Loss: 0.05950660519301891, LR: 0.010000000000000002
Time, 2019-01-01T17:34:38, Epoch: 31, Batch: 910, Training Loss: 0.07005553357303143, LR: 0.010000000000000002
Time, 2019-01-01T17:34:38, Epoch: 31, Batch: 920, Training Loss: 0.061285820230841635, LR: 0.010000000000000002
Time, 2019-01-01T17:34:39, Epoch: 31, Batch: 930, Training Loss: 0.0705954223871231, LR: 0.010000000000000002
Epoch: 31, Validation Top 1 acc: 98.39252471923828
Epoch: 31, Validation Top 5 acc: 99.98334503173828
Epoch: 31, Validation Set Loss: 0.05541830509901047
Start training epoch 32
Time, 2019-01-01T17:35:05, Epoch: 32, Batch: 10, Training Loss: 0.05224798358976841, LR: 0.010000000000000002
Time, 2019-01-01T17:35:06, Epoch: 32, Batch: 20, Training Loss: 0.07473700270056724, LR: 0.010000000000000002
Time, 2019-01-01T17:35:07, Epoch: 32, Batch: 30, Training Loss: 0.06993069499731064, LR: 0.010000000000000002
Time, 2019-01-01T17:35:07, Epoch: 32, Batch: 40, Training Loss: 0.09615745805203915, LR: 0.010000000000000002
Time, 2019-01-01T17:35:08, Epoch: 32, Batch: 50, Training Loss: 0.07332199960947036, LR: 0.010000000000000002
Time, 2019-01-01T17:35:09, Epoch: 32, Batch: 60, Training Loss: 0.0674450971186161, LR: 0.010000000000000002
Time, 2019-01-01T17:35:10, Epoch: 32, Batch: 70, Training Loss: 0.0718806616961956, LR: 0.010000000000000002
Time, 2019-01-01T17:35:10, Epoch: 32, Batch: 80, Training Loss: 0.06811267286539077, LR: 0.010000000000000002
Time, 2019-01-01T17:35:11, Epoch: 32, Batch: 90, Training Loss: 0.05377684459090233, LR: 0.010000000000000002
Time, 2019-01-01T17:35:12, Epoch: 32, Batch: 100, Training Loss: 0.06627360433340072, LR: 0.010000000000000002
Time, 2019-01-01T17:35:13, Epoch: 32, Batch: 110, Training Loss: 0.08797628432512283, LR: 0.010000000000000002
Time, 2019-01-01T17:35:13, Epoch: 32, Batch: 120, Training Loss: 0.07338206805288791, LR: 0.010000000000000002
Time, 2019-01-01T17:35:14, Epoch: 32, Batch: 130, Training Loss: 0.05553039312362671, LR: 0.010000000000000002
Time, 2019-01-01T17:35:15, Epoch: 32, Batch: 140, Training Loss: 0.06837677359580993, LR: 0.010000000000000002
Time, 2019-01-01T17:35:15, Epoch: 32, Batch: 150, Training Loss: 0.07702430076897145, LR: 0.010000000000000002
Time, 2019-01-01T17:35:16, Epoch: 32, Batch: 160, Training Loss: 0.06466209441423416, LR: 0.010000000000000002
Time, 2019-01-01T17:35:17, Epoch: 32, Batch: 170, Training Loss: 0.05816376693546772, LR: 0.010000000000000002
Time, 2019-01-01T17:35:18, Epoch: 32, Batch: 180, Training Loss: 0.09202635884284974, LR: 0.010000000000000002
Time, 2019-01-01T17:35:18, Epoch: 32, Batch: 190, Training Loss: 0.06589802131056785, LR: 0.010000000000000002
Time, 2019-01-01T17:35:19, Epoch: 32, Batch: 200, Training Loss: 0.04366534948348999, LR: 0.010000000000000002
Time, 2019-01-01T17:35:20, Epoch: 32, Batch: 210, Training Loss: 0.05209472551941872, LR: 0.010000000000000002
Time, 2019-01-01T17:35:20, Epoch: 32, Batch: 220, Training Loss: 0.06330693289637565, LR: 0.010000000000000002
Time, 2019-01-01T17:35:21, Epoch: 32, Batch: 230, Training Loss: 0.06015977002680302, LR: 0.010000000000000002
Time, 2019-01-01T17:35:22, Epoch: 32, Batch: 240, Training Loss: 0.05029747597873211, LR: 0.010000000000000002
Time, 2019-01-01T17:35:22, Epoch: 32, Batch: 250, Training Loss: 0.0601525105535984, LR: 0.010000000000000002
Time, 2019-01-01T17:35:23, Epoch: 32, Batch: 260, Training Loss: 0.051566234603524205, LR: 0.010000000000000002
Time, 2019-01-01T17:35:24, Epoch: 32, Batch: 270, Training Loss: 0.04785217680037022, LR: 0.010000000000000002
Time, 2019-01-01T17:35:25, Epoch: 32, Batch: 280, Training Loss: 0.05317236855626106, LR: 0.010000000000000002
Time, 2019-01-01T17:35:25, Epoch: 32, Batch: 290, Training Loss: 0.055599328875541684, LR: 0.010000000000000002
Time, 2019-01-01T17:35:26, Epoch: 32, Batch: 300, Training Loss: 0.0797511551529169, LR: 0.010000000000000002
Time, 2019-01-01T17:35:27, Epoch: 32, Batch: 310, Training Loss: 0.05025635100901127, LR: 0.010000000000000002
Time, 2019-01-01T17:35:27, Epoch: 32, Batch: 320, Training Loss: 0.04117526076734066, LR: 0.010000000000000002
Time, 2019-01-01T17:35:28, Epoch: 32, Batch: 330, Training Loss: 0.0641495205461979, LR: 0.010000000000000002
Time, 2019-01-01T17:35:29, Epoch: 32, Batch: 340, Training Loss: 0.07142654880881309, LR: 0.010000000000000002
Time, 2019-01-01T17:35:30, Epoch: 32, Batch: 350, Training Loss: 0.05895810127258301, LR: 0.010000000000000002
Time, 2019-01-01T17:35:30, Epoch: 32, Batch: 360, Training Loss: 0.06910624392330647, LR: 0.010000000000000002
Time, 2019-01-01T17:35:31, Epoch: 32, Batch: 370, Training Loss: 0.06488163396716118, LR: 0.010000000000000002
Time, 2019-01-01T17:35:32, Epoch: 32, Batch: 380, Training Loss: 0.060460617393255235, LR: 0.010000000000000002
Time, 2019-01-01T17:35:32, Epoch: 32, Batch: 390, Training Loss: 0.06094641126692295, LR: 0.010000000000000002
Time, 2019-01-01T17:35:33, Epoch: 32, Batch: 400, Training Loss: 0.061037682741880414, LR: 0.010000000000000002
Time, 2019-01-01T17:35:34, Epoch: 32, Batch: 410, Training Loss: 0.06313369013369083, LR: 0.010000000000000002
Time, 2019-01-01T17:35:34, Epoch: 32, Batch: 420, Training Loss: 0.05547417104244232, LR: 0.010000000000000002
Time, 2019-01-01T17:35:35, Epoch: 32, Batch: 430, Training Loss: 0.05249998271465302, LR: 0.010000000000000002
Time, 2019-01-01T17:35:36, Epoch: 32, Batch: 440, Training Loss: 0.050143593549728395, LR: 0.010000000000000002
Time, 2019-01-01T17:35:37, Epoch: 32, Batch: 450, Training Loss: 0.03783626556396484, LR: 0.010000000000000002
Time, 2019-01-01T17:35:37, Epoch: 32, Batch: 460, Training Loss: 0.042965945601463315, LR: 0.010000000000000002
Time, 2019-01-01T17:35:38, Epoch: 32, Batch: 470, Training Loss: 0.06890648603439331, LR: 0.010000000000000002
Time, 2019-01-01T17:35:39, Epoch: 32, Batch: 480, Training Loss: 0.06619438380002976, LR: 0.010000000000000002
Time, 2019-01-01T17:35:39, Epoch: 32, Batch: 490, Training Loss: 0.07030320279300213, LR: 0.010000000000000002
Time, 2019-01-01T17:35:40, Epoch: 32, Batch: 500, Training Loss: 0.047054076194763185, LR: 0.010000000000000002
Time, 2019-01-01T17:35:41, Epoch: 32, Batch: 510, Training Loss: 0.05645936392247677, LR: 0.010000000000000002
Time, 2019-01-01T17:35:41, Epoch: 32, Batch: 520, Training Loss: 0.050561153516173366, LR: 0.010000000000000002
Time, 2019-01-01T17:35:42, Epoch: 32, Batch: 530, Training Loss: 0.05703435838222504, LR: 0.010000000000000002
Time, 2019-01-01T17:35:43, Epoch: 32, Batch: 540, Training Loss: 0.04414101392030716, LR: 0.010000000000000002
Time, 2019-01-01T17:35:44, Epoch: 32, Batch: 550, Training Loss: 0.07675395123660564, LR: 0.010000000000000002
Time, 2019-01-01T17:35:44, Epoch: 32, Batch: 560, Training Loss: 0.04775338508188724, LR: 0.010000000000000002
Time, 2019-01-01T17:35:45, Epoch: 32, Batch: 570, Training Loss: 0.060110364109277725, LR: 0.010000000000000002
Time, 2019-01-01T17:35:46, Epoch: 32, Batch: 580, Training Loss: 0.0489739365875721, LR: 0.010000000000000002
Time, 2019-01-01T17:35:46, Epoch: 32, Batch: 590, Training Loss: 0.07511631138622761, LR: 0.010000000000000002
Time, 2019-01-01T17:35:47, Epoch: 32, Batch: 600, Training Loss: 0.06673422642052174, LR: 0.010000000000000002
Time, 2019-01-01T17:35:48, Epoch: 32, Batch: 610, Training Loss: 0.047390565276145935, LR: 0.010000000000000002
Time, 2019-01-01T17:35:48, Epoch: 32, Batch: 620, Training Loss: 0.044650335982441905, LR: 0.010000000000000002
Time, 2019-01-01T17:35:49, Epoch: 32, Batch: 630, Training Loss: 0.06369452588260174, LR: 0.010000000000000002
Time, 2019-01-01T17:35:50, Epoch: 32, Batch: 640, Training Loss: 0.06440728977322578, LR: 0.010000000000000002
Time, 2019-01-01T17:35:51, Epoch: 32, Batch: 650, Training Loss: 0.06319820731878281, LR: 0.010000000000000002
Time, 2019-01-01T17:35:51, Epoch: 32, Batch: 660, Training Loss: 0.05193797238171101, LR: 0.010000000000000002
Time, 2019-01-01T17:35:52, Epoch: 32, Batch: 670, Training Loss: 0.08090175949037075, LR: 0.010000000000000002
Time, 2019-01-01T17:35:53, Epoch: 32, Batch: 680, Training Loss: 0.08153220862150193, LR: 0.010000000000000002
Time, 2019-01-01T17:35:53, Epoch: 32, Batch: 690, Training Loss: 0.038474994152784346, LR: 0.010000000000000002
Time, 2019-01-01T17:35:54, Epoch: 32, Batch: 700, Training Loss: 0.04725064560770988, LR: 0.010000000000000002
Time, 2019-01-01T17:35:55, Epoch: 32, Batch: 710, Training Loss: 0.06294044889509678, LR: 0.010000000000000002
Time, 2019-01-01T17:35:55, Epoch: 32, Batch: 720, Training Loss: 0.05035718940198421, LR: 0.010000000000000002
Time, 2019-01-01T17:35:56, Epoch: 32, Batch: 730, Training Loss: 0.050782182812690736, LR: 0.010000000000000002
Time, 2019-01-01T17:35:57, Epoch: 32, Batch: 740, Training Loss: 0.06196953020989895, LR: 0.010000000000000002
Time, 2019-01-01T17:35:58, Epoch: 32, Batch: 750, Training Loss: 0.06209257170557976, LR: 0.010000000000000002
Time, 2019-01-01T17:35:58, Epoch: 32, Batch: 760, Training Loss: 0.06443727277219295, LR: 0.010000000000000002
Time, 2019-01-01T17:35:59, Epoch: 32, Batch: 770, Training Loss: 0.047587697952985765, LR: 0.010000000000000002
Time, 2019-01-01T17:36:00, Epoch: 32, Batch: 780, Training Loss: 0.06745770759880543, LR: 0.010000000000000002
Time, 2019-01-01T17:36:00, Epoch: 32, Batch: 790, Training Loss: 0.06710703447461128, LR: 0.010000000000000002
Time, 2019-01-01T17:36:01, Epoch: 32, Batch: 800, Training Loss: 0.07388125285506249, LR: 0.010000000000000002
Time, 2019-01-01T17:36:02, Epoch: 32, Batch: 810, Training Loss: 0.04348000921308994, LR: 0.010000000000000002
Time, 2019-01-01T17:36:02, Epoch: 32, Batch: 820, Training Loss: 0.04288083091378212, LR: 0.010000000000000002
Time, 2019-01-01T17:36:03, Epoch: 32, Batch: 830, Training Loss: 0.058151567727327345, LR: 0.010000000000000002
Time, 2019-01-01T17:36:04, Epoch: 32, Batch: 840, Training Loss: 0.04898780062794685, LR: 0.010000000000000002
Time, 2019-01-01T17:36:04, Epoch: 32, Batch: 850, Training Loss: 0.04495767988264561, LR: 0.010000000000000002
Time, 2019-01-01T17:36:05, Epoch: 32, Batch: 860, Training Loss: 0.07431581914424897, LR: 0.010000000000000002
Time, 2019-01-01T17:36:06, Epoch: 32, Batch: 870, Training Loss: 0.032669684290885924, LR: 0.010000000000000002
Time, 2019-01-01T17:36:07, Epoch: 32, Batch: 880, Training Loss: 0.05212630704045296, LR: 0.010000000000000002
Time, 2019-01-01T17:36:07, Epoch: 32, Batch: 890, Training Loss: 0.0700584352016449, LR: 0.010000000000000002
Time, 2019-01-01T17:36:08, Epoch: 32, Batch: 900, Training Loss: 0.06562244817614556, LR: 0.010000000000000002
Time, 2019-01-01T17:36:09, Epoch: 32, Batch: 910, Training Loss: 0.04372422099113464, LR: 0.010000000000000002
Time, 2019-01-01T17:36:09, Epoch: 32, Batch: 920, Training Loss: 0.04445894882082939, LR: 0.010000000000000002
Time, 2019-01-01T17:36:10, Epoch: 32, Batch: 930, Training Loss: 0.046850628405809405, LR: 0.010000000000000002
Epoch: 32, Validation Top 1 acc: 98.35587310791016
Epoch: 32, Validation Top 5 acc: 99.99333953857422
Epoch: 32, Validation Set Loss: 0.05558112636208534
Start training epoch 33
Time, 2019-01-01T17:36:37, Epoch: 33, Batch: 10, Training Loss: 0.03795597292482853, LR: 0.010000000000000002
Time, 2019-01-01T17:36:38, Epoch: 33, Batch: 20, Training Loss: 0.05691692233085632, LR: 0.010000000000000002
Time, 2019-01-01T17:36:39, Epoch: 33, Batch: 30, Training Loss: 0.04669841714203358, LR: 0.010000000000000002
Time, 2019-01-01T17:36:39, Epoch: 33, Batch: 40, Training Loss: 0.06300662271678448, LR: 0.010000000000000002
Time, 2019-01-01T17:36:40, Epoch: 33, Batch: 50, Training Loss: 0.056412335485219955, LR: 0.010000000000000002
Time, 2019-01-01T17:36:41, Epoch: 33, Batch: 60, Training Loss: 0.03559232093393803, LR: 0.010000000000000002
Time, 2019-01-01T17:36:41, Epoch: 33, Batch: 70, Training Loss: 0.059986136853694916, LR: 0.010000000000000002
Time, 2019-01-01T17:36:42, Epoch: 33, Batch: 80, Training Loss: 0.03503304570913315, LR: 0.010000000000000002
Time, 2019-01-01T17:36:43, Epoch: 33, Batch: 90, Training Loss: 0.05722353123128414, LR: 0.010000000000000002
Time, 2019-01-01T17:36:44, Epoch: 33, Batch: 100, Training Loss: 0.06030637696385384, LR: 0.010000000000000002
Time, 2019-01-01T17:36:44, Epoch: 33, Batch: 110, Training Loss: 0.03399127759039402, LR: 0.010000000000000002
Time, 2019-01-01T17:36:45, Epoch: 33, Batch: 120, Training Loss: 0.04544256739318371, LR: 0.010000000000000002
Time, 2019-01-01T17:36:46, Epoch: 33, Batch: 130, Training Loss: 0.0359992615878582, LR: 0.010000000000000002
Time, 2019-01-01T17:36:46, Epoch: 33, Batch: 140, Training Loss: 0.06658927202224732, LR: 0.010000000000000002
Time, 2019-01-01T17:36:47, Epoch: 33, Batch: 150, Training Loss: 0.07727520242333412, LR: 0.010000000000000002
Time, 2019-01-01T17:36:48, Epoch: 33, Batch: 160, Training Loss: 0.0727219294756651, LR: 0.010000000000000002
Time, 2019-01-01T17:36:48, Epoch: 33, Batch: 170, Training Loss: 0.04070146158337593, LR: 0.010000000000000002
Time, 2019-01-01T17:36:49, Epoch: 33, Batch: 180, Training Loss: 0.046489517390728, LR: 0.010000000000000002
Time, 2019-01-01T17:36:50, Epoch: 33, Batch: 190, Training Loss: 0.04131266288459301, LR: 0.010000000000000002
Time, 2019-01-01T17:36:51, Epoch: 33, Batch: 200, Training Loss: 0.08402899801731109, LR: 0.010000000000000002
Time, 2019-01-01T17:36:51, Epoch: 33, Batch: 210, Training Loss: 0.06094937585294247, LR: 0.010000000000000002
Time, 2019-01-01T17:36:52, Epoch: 33, Batch: 220, Training Loss: 0.07026178427040577, LR: 0.010000000000000002
Time, 2019-01-01T17:36:53, Epoch: 33, Batch: 230, Training Loss: 0.058289573341608045, LR: 0.010000000000000002
Time, 2019-01-01T17:36:53, Epoch: 33, Batch: 240, Training Loss: 0.041835670545697215, LR: 0.010000000000000002
Time, 2019-01-01T17:36:54, Epoch: 33, Batch: 250, Training Loss: 0.05011836253106594, LR: 0.010000000000000002
Time, 2019-01-01T17:36:55, Epoch: 33, Batch: 260, Training Loss: 0.04136220812797546, LR: 0.010000000000000002
Time, 2019-01-01T17:36:55, Epoch: 33, Batch: 270, Training Loss: 0.06255332231521607, LR: 0.010000000000000002
Time, 2019-01-01T17:36:56, Epoch: 33, Batch: 280, Training Loss: 0.04331783391535282, LR: 0.010000000000000002
Time, 2019-01-01T17:36:57, Epoch: 33, Batch: 290, Training Loss: 0.058985594660043716, LR: 0.010000000000000002
Time, 2019-01-01T17:36:58, Epoch: 33, Batch: 300, Training Loss: 0.05841450691223145, LR: 0.010000000000000002
Time, 2019-01-01T17:36:58, Epoch: 33, Batch: 310, Training Loss: 0.07602919451892376, LR: 0.010000000000000002
Time, 2019-01-01T17:36:59, Epoch: 33, Batch: 320, Training Loss: 0.037046779692173, LR: 0.010000000000000002
Time, 2019-01-01T17:37:00, Epoch: 33, Batch: 330, Training Loss: 0.05681653618812561, LR: 0.010000000000000002
Time, 2019-01-01T17:37:00, Epoch: 33, Batch: 340, Training Loss: 0.04898794293403626, LR: 0.010000000000000002
Time, 2019-01-01T17:37:01, Epoch: 33, Batch: 350, Training Loss: 0.03833356201648712, LR: 0.010000000000000002
Time, 2019-01-01T17:37:02, Epoch: 33, Batch: 360, Training Loss: 0.043461421877145766, LR: 0.010000000000000002
Time, 2019-01-01T17:37:02, Epoch: 33, Batch: 370, Training Loss: 0.06132014244794846, LR: 0.010000000000000002
Time, 2019-01-01T17:37:03, Epoch: 33, Batch: 380, Training Loss: 0.09360592365264893, LR: 0.010000000000000002
Time, 2019-01-01T17:37:04, Epoch: 33, Batch: 390, Training Loss: 0.07937913089990616, LR: 0.010000000000000002
Time, 2019-01-01T17:37:05, Epoch: 33, Batch: 400, Training Loss: 0.0775396466255188, LR: 0.010000000000000002
Time, 2019-01-01T17:37:05, Epoch: 33, Batch: 410, Training Loss: 0.05095584057271481, LR: 0.010000000000000002
Time, 2019-01-01T17:37:06, Epoch: 33, Batch: 420, Training Loss: 0.0758136436343193, LR: 0.010000000000000002
Time, 2019-01-01T17:37:07, Epoch: 33, Batch: 430, Training Loss: 0.06578880734741688, LR: 0.010000000000000002
Time, 2019-01-01T17:37:07, Epoch: 33, Batch: 440, Training Loss: 0.07706002481281757, LR: 0.010000000000000002
Time, 2019-01-01T17:37:08, Epoch: 33, Batch: 450, Training Loss: 0.049671710655093196, LR: 0.010000000000000002
Time, 2019-01-01T17:37:09, Epoch: 33, Batch: 460, Training Loss: 0.059755219519138335, LR: 0.010000000000000002
Time, 2019-01-01T17:37:09, Epoch: 33, Batch: 470, Training Loss: 0.07056305296719075, LR: 0.010000000000000002
Time, 2019-01-01T17:37:10, Epoch: 33, Batch: 480, Training Loss: 0.048357687145471576, LR: 0.010000000000000002
Time, 2019-01-01T17:37:11, Epoch: 33, Batch: 490, Training Loss: 0.04065267443656921, LR: 0.010000000000000002
Time, 2019-01-01T17:37:12, Epoch: 33, Batch: 500, Training Loss: 0.07758588157594204, LR: 0.010000000000000002
Time, 2019-01-01T17:37:12, Epoch: 33, Batch: 510, Training Loss: 0.05186464339494705, LR: 0.010000000000000002
Time, 2019-01-01T17:37:13, Epoch: 33, Batch: 520, Training Loss: 0.06857215091586114, LR: 0.010000000000000002
Time, 2019-01-01T17:37:14, Epoch: 33, Batch: 530, Training Loss: 0.05548098459839821, LR: 0.010000000000000002
Time, 2019-01-01T17:37:14, Epoch: 33, Batch: 540, Training Loss: 0.05427773930132389, LR: 0.010000000000000002
Time, 2019-01-01T17:37:15, Epoch: 33, Batch: 550, Training Loss: 0.05900184251368046, LR: 0.010000000000000002
Time, 2019-01-01T17:37:16, Epoch: 33, Batch: 560, Training Loss: 0.054898492991924286, LR: 0.010000000000000002
Time, 2019-01-01T17:37:16, Epoch: 33, Batch: 570, Training Loss: 0.05873040407896042, LR: 0.010000000000000002
Time, 2019-01-01T17:37:17, Epoch: 33, Batch: 580, Training Loss: 0.047338385879993436, LR: 0.010000000000000002
Time, 2019-01-01T17:37:18, Epoch: 33, Batch: 590, Training Loss: 0.04519363082945347, LR: 0.010000000000000002
Time, 2019-01-01T17:37:18, Epoch: 33, Batch: 600, Training Loss: 0.056110172718763354, LR: 0.010000000000000002
Time, 2019-01-01T17:37:19, Epoch: 33, Batch: 610, Training Loss: 0.04703009426593781, LR: 0.010000000000000002
Time, 2019-01-01T17:37:20, Epoch: 33, Batch: 620, Training Loss: 0.07541829012334347, LR: 0.010000000000000002
Time, 2019-01-01T17:37:21, Epoch: 33, Batch: 630, Training Loss: 0.05608659014105797, LR: 0.010000000000000002
Time, 2019-01-01T17:37:21, Epoch: 33, Batch: 640, Training Loss: 0.05638635903596878, LR: 0.010000000000000002
Time, 2019-01-01T17:37:22, Epoch: 33, Batch: 650, Training Loss: 0.06880915313959121, LR: 0.010000000000000002
Time, 2019-01-01T17:37:23, Epoch: 33, Batch: 660, Training Loss: 0.04844728857278824, LR: 0.010000000000000002
Time, 2019-01-01T17:37:23, Epoch: 33, Batch: 670, Training Loss: 0.08473518788814545, LR: 0.010000000000000002
Time, 2019-01-01T17:37:24, Epoch: 33, Batch: 680, Training Loss: 0.04767824076116085, LR: 0.010000000000000002
Time, 2019-01-01T17:37:25, Epoch: 33, Batch: 690, Training Loss: 0.05350839085876942, LR: 0.010000000000000002
Time, 2019-01-01T17:37:26, Epoch: 33, Batch: 700, Training Loss: 0.026593073457479476, LR: 0.010000000000000002
Time, 2019-01-01T17:37:26, Epoch: 33, Batch: 710, Training Loss: 0.06409130543470383, LR: 0.010000000000000002
Time, 2019-01-01T17:37:27, Epoch: 33, Batch: 720, Training Loss: 0.08210791572928429, LR: 0.010000000000000002
Time, 2019-01-01T17:37:28, Epoch: 33, Batch: 730, Training Loss: 0.08088395781815053, LR: 0.010000000000000002
Time, 2019-01-01T17:37:28, Epoch: 33, Batch: 740, Training Loss: 0.03892494216561317, LR: 0.010000000000000002
Time, 2019-01-01T17:37:29, Epoch: 33, Batch: 750, Training Loss: 0.04712218269705772, LR: 0.010000000000000002
Time, 2019-01-01T17:37:30, Epoch: 33, Batch: 760, Training Loss: 0.04469810388982296, LR: 0.010000000000000002
Time, 2019-01-01T17:37:30, Epoch: 33, Batch: 770, Training Loss: 0.04856446124613285, LR: 0.010000000000000002
Time, 2019-01-01T17:37:31, Epoch: 33, Batch: 780, Training Loss: 0.05257419720292091, LR: 0.010000000000000002
Time, 2019-01-01T17:37:32, Epoch: 33, Batch: 790, Training Loss: 0.06142566874623299, LR: 0.010000000000000002
Time, 2019-01-01T17:37:33, Epoch: 33, Batch: 800, Training Loss: 0.07109700068831444, LR: 0.010000000000000002
Time, 2019-01-01T17:37:33, Epoch: 33, Batch: 810, Training Loss: 0.07893584445118904, LR: 0.010000000000000002
Time, 2019-01-01T17:37:34, Epoch: 33, Batch: 820, Training Loss: 0.039614717662334445, LR: 0.010000000000000002
Time, 2019-01-01T17:37:35, Epoch: 33, Batch: 830, Training Loss: 0.05419190749526024, LR: 0.010000000000000002
Time, 2019-01-01T17:37:35, Epoch: 33, Batch: 840, Training Loss: 0.04023305885493755, LR: 0.010000000000000002
Time, 2019-01-01T17:37:36, Epoch: 33, Batch: 850, Training Loss: 0.08285633586347103, LR: 0.010000000000000002
Time, 2019-01-01T17:37:37, Epoch: 33, Batch: 860, Training Loss: 0.06477049365639687, LR: 0.010000000000000002
Time, 2019-01-01T17:37:38, Epoch: 33, Batch: 870, Training Loss: 0.04792087413370609, LR: 0.010000000000000002
Time, 2019-01-01T17:37:38, Epoch: 33, Batch: 880, Training Loss: 0.04826791398227215, LR: 0.010000000000000002
Time, 2019-01-01T17:37:39, Epoch: 33, Batch: 890, Training Loss: 0.08160770609974861, LR: 0.010000000000000002
Time, 2019-01-01T17:37:40, Epoch: 33, Batch: 900, Training Loss: 0.06323457770049572, LR: 0.010000000000000002
Time, 2019-01-01T17:37:40, Epoch: 33, Batch: 910, Training Loss: 0.05613856762647629, LR: 0.010000000000000002
Time, 2019-01-01T17:37:41, Epoch: 33, Batch: 920, Training Loss: 0.07267967946827411, LR: 0.010000000000000002
Time, 2019-01-01T17:37:42, Epoch: 33, Batch: 930, Training Loss: 0.051582133024930955, LR: 0.010000000000000002
Epoch: 33, Validation Top 1 acc: 98.30923461914062
Epoch: 33, Validation Top 5 acc: 99.97834777832031
Epoch: 33, Validation Set Loss: 0.05648866295814514
Start training epoch 34
Time, 2019-01-01T17:38:08, Epoch: 34, Batch: 10, Training Loss: 0.06864962093532086, LR: 0.010000000000000002
Time, 2019-01-01T17:38:09, Epoch: 34, Batch: 20, Training Loss: 0.06726955138146877, LR: 0.010000000000000002
Time, 2019-01-01T17:38:09, Epoch: 34, Batch: 30, Training Loss: 0.05421466454863548, LR: 0.010000000000000002
Time, 2019-01-01T17:38:10, Epoch: 34, Batch: 40, Training Loss: 0.04945714250206947, LR: 0.010000000000000002
Time, 2019-01-01T17:38:11, Epoch: 34, Batch: 50, Training Loss: 0.05118386372923851, LR: 0.010000000000000002
Time, 2019-01-01T17:38:12, Epoch: 34, Batch: 60, Training Loss: 0.06948371902108193, LR: 0.010000000000000002
Time, 2019-01-01T17:38:12, Epoch: 34, Batch: 70, Training Loss: 0.07245967537164688, LR: 0.010000000000000002
Time, 2019-01-01T17:38:13, Epoch: 34, Batch: 80, Training Loss: 0.05059879906475544, LR: 0.010000000000000002
Time, 2019-01-01T17:38:14, Epoch: 34, Batch: 90, Training Loss: 0.0504910834133625, LR: 0.010000000000000002
Time, 2019-01-01T17:38:14, Epoch: 34, Batch: 100, Training Loss: 0.06982080340385437, LR: 0.010000000000000002
Time, 2019-01-01T17:38:15, Epoch: 34, Batch: 110, Training Loss: 0.05777971893548965, LR: 0.010000000000000002
Time, 2019-01-01T17:38:16, Epoch: 34, Batch: 120, Training Loss: 0.0382146842777729, LR: 0.010000000000000002
Time, 2019-01-01T17:38:17, Epoch: 34, Batch: 130, Training Loss: 0.04961326904594898, LR: 0.010000000000000002
Time, 2019-01-01T17:38:17, Epoch: 34, Batch: 140, Training Loss: 0.05448240265250206, LR: 0.010000000000000002
Time, 2019-01-01T17:38:18, Epoch: 34, Batch: 150, Training Loss: 0.06698683984577655, LR: 0.010000000000000002
Time, 2019-01-01T17:38:19, Epoch: 34, Batch: 160, Training Loss: 0.08787568435072898, LR: 0.010000000000000002
Time, 2019-01-01T17:38:19, Epoch: 34, Batch: 170, Training Loss: 0.07848139703273774, LR: 0.010000000000000002
Time, 2019-01-01T17:38:20, Epoch: 34, Batch: 180, Training Loss: 0.03328232541680336, LR: 0.010000000000000002
Time, 2019-01-01T17:38:21, Epoch: 34, Batch: 190, Training Loss: 0.05421268008649349, LR: 0.010000000000000002
Time, 2019-01-01T17:38:22, Epoch: 34, Batch: 200, Training Loss: 0.06146923936903477, LR: 0.010000000000000002
Time, 2019-01-01T17:38:22, Epoch: 34, Batch: 210, Training Loss: 0.041671738028526306, LR: 0.010000000000000002
Time, 2019-01-01T17:38:23, Epoch: 34, Batch: 220, Training Loss: 0.06056839004158974, LR: 0.010000000000000002
Time, 2019-01-01T17:38:24, Epoch: 34, Batch: 230, Training Loss: 0.06038381718099117, LR: 0.010000000000000002
Time, 2019-01-01T17:38:24, Epoch: 34, Batch: 240, Training Loss: 0.06570423506200314, LR: 0.010000000000000002
Time, 2019-01-01T17:38:25, Epoch: 34, Batch: 250, Training Loss: 0.04244078956544399, LR: 0.010000000000000002
Time, 2019-01-01T17:38:26, Epoch: 34, Batch: 260, Training Loss: 0.047187626361846924, LR: 0.010000000000000002
Time, 2019-01-01T17:38:27, Epoch: 34, Batch: 270, Training Loss: 0.06791783198714256, LR: 0.010000000000000002
Time, 2019-01-01T17:38:27, Epoch: 34, Batch: 280, Training Loss: 0.04586753398180008, LR: 0.010000000000000002
Time, 2019-01-01T17:38:28, Epoch: 34, Batch: 290, Training Loss: 0.03997468911111355, LR: 0.010000000000000002
Time, 2019-01-01T17:38:29, Epoch: 34, Batch: 300, Training Loss: 0.0483608603477478, LR: 0.010000000000000002
Time, 2019-01-01T17:38:29, Epoch: 34, Batch: 310, Training Loss: 0.05746258571743965, LR: 0.010000000000000002
Time, 2019-01-01T17:38:30, Epoch: 34, Batch: 320, Training Loss: 0.08891791999340057, LR: 0.010000000000000002
Time, 2019-01-01T17:38:31, Epoch: 34, Batch: 330, Training Loss: 0.07746820598840713, LR: 0.010000000000000002
Time, 2019-01-01T17:38:32, Epoch: 34, Batch: 340, Training Loss: 0.05418757796287536, LR: 0.010000000000000002
Time, 2019-01-01T17:38:32, Epoch: 34, Batch: 350, Training Loss: 0.05912269540131092, LR: 0.010000000000000002
Time, 2019-01-01T17:38:33, Epoch: 34, Batch: 360, Training Loss: 0.05504600405693054, LR: 0.010000000000000002
Time, 2019-01-01T17:38:34, Epoch: 34, Batch: 370, Training Loss: 0.07729833759367466, LR: 0.010000000000000002
Time, 2019-01-01T17:38:34, Epoch: 34, Batch: 380, Training Loss: 0.04116517938673496, LR: 0.010000000000000002
Time, 2019-01-01T17:38:35, Epoch: 34, Batch: 390, Training Loss: 0.05372227132320404, LR: 0.010000000000000002
Time, 2019-01-01T17:38:36, Epoch: 34, Batch: 400, Training Loss: 0.06462717987596989, LR: 0.010000000000000002
Time, 2019-01-01T17:38:37, Epoch: 34, Batch: 410, Training Loss: 0.048503901436924936, LR: 0.010000000000000002
Time, 2019-01-01T17:38:37, Epoch: 34, Batch: 420, Training Loss: 0.059051765501499175, LR: 0.010000000000000002
Time, 2019-01-01T17:38:38, Epoch: 34, Batch: 430, Training Loss: 0.08523803353309631, LR: 0.010000000000000002
Time, 2019-01-01T17:38:39, Epoch: 34, Batch: 440, Training Loss: 0.047193555533885954, LR: 0.010000000000000002
Time, 2019-01-01T17:38:39, Epoch: 34, Batch: 450, Training Loss: 0.04800240993499756, LR: 0.010000000000000002
Time, 2019-01-01T17:38:40, Epoch: 34, Batch: 460, Training Loss: 0.06775590926408767, LR: 0.010000000000000002
Time, 2019-01-01T17:38:41, Epoch: 34, Batch: 470, Training Loss: 0.07240167781710624, LR: 0.010000000000000002
Time, 2019-01-01T17:38:41, Epoch: 34, Batch: 480, Training Loss: 0.05611600764095783, LR: 0.010000000000000002
Time, 2019-01-01T17:38:42, Epoch: 34, Batch: 490, Training Loss: 0.06850067749619485, LR: 0.010000000000000002
Time, 2019-01-01T17:38:43, Epoch: 34, Batch: 500, Training Loss: 0.038864041492342946, LR: 0.010000000000000002
Time, 2019-01-01T17:38:44, Epoch: 34, Batch: 510, Training Loss: 0.03449991755187511, LR: 0.010000000000000002
Time, 2019-01-01T17:38:44, Epoch: 34, Batch: 520, Training Loss: 0.07068597339093685, LR: 0.010000000000000002
Time, 2019-01-01T17:38:45, Epoch: 34, Batch: 530, Training Loss: 0.0752410613000393, LR: 0.010000000000000002
Time, 2019-01-01T17:38:46, Epoch: 34, Batch: 540, Training Loss: 0.05312046930193901, LR: 0.010000000000000002
Time, 2019-01-01T17:38:46, Epoch: 34, Batch: 550, Training Loss: 0.04736284911632538, LR: 0.010000000000000002
Time, 2019-01-01T17:38:47, Epoch: 34, Batch: 560, Training Loss: 0.03936551734805107, LR: 0.010000000000000002
Time, 2019-01-01T17:38:48, Epoch: 34, Batch: 570, Training Loss: 0.059677065908908845, LR: 0.010000000000000002
Time, 2019-01-01T17:38:49, Epoch: 34, Batch: 580, Training Loss: 0.04721641205251217, LR: 0.010000000000000002
Time, 2019-01-01T17:38:49, Epoch: 34, Batch: 590, Training Loss: 0.060912488400936125, LR: 0.010000000000000002
Time, 2019-01-01T17:38:50, Epoch: 34, Batch: 600, Training Loss: 0.06977683082222938, LR: 0.010000000000000002
Time, 2019-01-01T17:38:51, Epoch: 34, Batch: 610, Training Loss: 0.04947745203971863, LR: 0.010000000000000002
Time, 2019-01-01T17:38:51, Epoch: 34, Batch: 620, Training Loss: 0.04648757167160511, LR: 0.010000000000000002
Time, 2019-01-01T17:38:52, Epoch: 34, Batch: 630, Training Loss: 0.0568274587392807, LR: 0.010000000000000002
Time, 2019-01-01T17:38:53, Epoch: 34, Batch: 640, Training Loss: 0.05541031062602997, LR: 0.010000000000000002
Time, 2019-01-01T17:38:53, Epoch: 34, Batch: 650, Training Loss: 0.048499200865626334, LR: 0.010000000000000002
Time, 2019-01-01T17:38:54, Epoch: 34, Batch: 660, Training Loss: 0.07335851117968559, LR: 0.010000000000000002
Time, 2019-01-01T17:38:55, Epoch: 34, Batch: 670, Training Loss: 0.04402712248265743, LR: 0.010000000000000002
Time, 2019-01-01T17:38:56, Epoch: 34, Batch: 680, Training Loss: 0.053063777089118955, LR: 0.010000000000000002
Time, 2019-01-01T17:38:56, Epoch: 34, Batch: 690, Training Loss: 0.04486684501171112, LR: 0.010000000000000002
Time, 2019-01-01T17:38:57, Epoch: 34, Batch: 700, Training Loss: 0.04029425680637359, LR: 0.010000000000000002
Time, 2019-01-01T17:38:58, Epoch: 34, Batch: 710, Training Loss: 0.03821013495326042, LR: 0.010000000000000002
Time, 2019-01-01T17:38:58, Epoch: 34, Batch: 720, Training Loss: 0.032385698333382604, LR: 0.010000000000000002
Time, 2019-01-01T17:38:59, Epoch: 34, Batch: 730, Training Loss: 0.039436555653810504, LR: 0.010000000000000002
Time, 2019-01-01T17:39:00, Epoch: 34, Batch: 740, Training Loss: 0.04719303026795387, LR: 0.010000000000000002
Time, 2019-01-01T17:39:00, Epoch: 34, Batch: 750, Training Loss: 0.08803286403417587, LR: 0.010000000000000002
Time, 2019-01-01T17:39:01, Epoch: 34, Batch: 760, Training Loss: 0.07107880413532257, LR: 0.010000000000000002
Time, 2019-01-01T17:39:02, Epoch: 34, Batch: 770, Training Loss: 0.06621530577540398, LR: 0.010000000000000002
Time, 2019-01-01T17:39:03, Epoch: 34, Batch: 780, Training Loss: 0.05624512284994125, LR: 0.010000000000000002
Time, 2019-01-01T17:39:03, Epoch: 34, Batch: 790, Training Loss: 0.04597266651690006, LR: 0.010000000000000002
Time, 2019-01-01T17:39:04, Epoch: 34, Batch: 800, Training Loss: 0.046541061252355576, LR: 0.010000000000000002
Time, 2019-01-01T17:39:05, Epoch: 34, Batch: 810, Training Loss: 0.06311039887368679, LR: 0.010000000000000002
Time, 2019-01-01T17:39:05, Epoch: 34, Batch: 820, Training Loss: 0.08793961480259896, LR: 0.010000000000000002
Time, 2019-01-01T17:39:06, Epoch: 34, Batch: 830, Training Loss: 0.04129129908978939, LR: 0.010000000000000002
Time, 2019-01-01T17:39:07, Epoch: 34, Batch: 840, Training Loss: 0.056843174248933794, LR: 0.010000000000000002
Time, 2019-01-01T17:39:08, Epoch: 34, Batch: 850, Training Loss: 0.0782455176115036, LR: 0.010000000000000002
Time, 2019-01-01T17:39:08, Epoch: 34, Batch: 860, Training Loss: 0.04799215942621231, LR: 0.010000000000000002
Time, 2019-01-01T17:39:09, Epoch: 34, Batch: 870, Training Loss: 0.05794476307928562, LR: 0.010000000000000002
Time, 2019-01-01T17:39:10, Epoch: 34, Batch: 880, Training Loss: 0.06244436241686344, LR: 0.010000000000000002
Time, 2019-01-01T17:39:10, Epoch: 34, Batch: 890, Training Loss: 0.05560347884893417, LR: 0.010000000000000002
Time, 2019-01-01T17:39:11, Epoch: 34, Batch: 900, Training Loss: 0.06265581361949443, LR: 0.010000000000000002
Time, 2019-01-01T17:39:12, Epoch: 34, Batch: 910, Training Loss: 0.058703222498297694, LR: 0.010000000000000002
Time, 2019-01-01T17:39:12, Epoch: 34, Batch: 920, Training Loss: 0.03765171468257904, LR: 0.010000000000000002
Time, 2019-01-01T17:39:13, Epoch: 34, Batch: 930, Training Loss: 0.05578277744352818, LR: 0.010000000000000002
Epoch: 34, Validation Top 1 acc: 98.39252471923828
Epoch: 34, Validation Top 5 acc: 99.9850082397461
Epoch: 34, Validation Set Loss: 0.0548052042722702
Start training epoch 35
Time, 2019-01-01T17:39:39, Epoch: 35, Batch: 10, Training Loss: 0.052759261429309846, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:40, Epoch: 35, Batch: 20, Training Loss: 0.04511050283908844, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:40, Epoch: 35, Batch: 30, Training Loss: 0.054706921428442, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:41, Epoch: 35, Batch: 40, Training Loss: 0.05353850871324539, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:42, Epoch: 35, Batch: 50, Training Loss: 0.036565642431378366, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:42, Epoch: 35, Batch: 60, Training Loss: 0.054726191237568854, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:43, Epoch: 35, Batch: 70, Training Loss: 0.054951944202184674, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:44, Epoch: 35, Batch: 80, Training Loss: 0.07321618646383285, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:45, Epoch: 35, Batch: 90, Training Loss: 0.035530278086662294, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:45, Epoch: 35, Batch: 100, Training Loss: 0.0592132318764925, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:46, Epoch: 35, Batch: 110, Training Loss: 0.04797485247254372, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:47, Epoch: 35, Batch: 120, Training Loss: 0.06793154440820218, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:47, Epoch: 35, Batch: 130, Training Loss: 0.03907241746783256, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:48, Epoch: 35, Batch: 140, Training Loss: 0.036418850719928744, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:49, Epoch: 35, Batch: 150, Training Loss: 0.05711790546774864, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:49, Epoch: 35, Batch: 160, Training Loss: 0.044744919240474704, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:50, Epoch: 35, Batch: 170, Training Loss: 0.03786718435585499, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:51, Epoch: 35, Batch: 180, Training Loss: 0.0488034799695015, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:52, Epoch: 35, Batch: 190, Training Loss: 0.05746215544641018, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:52, Epoch: 35, Batch: 200, Training Loss: 0.058947866037487986, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:53, Epoch: 35, Batch: 210, Training Loss: 0.048106401786208154, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:54, Epoch: 35, Batch: 220, Training Loss: 0.06493580266833306, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:54, Epoch: 35, Batch: 230, Training Loss: 0.055438751727342604, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:55, Epoch: 35, Batch: 240, Training Loss: 0.05720892511308193, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:56, Epoch: 35, Batch: 250, Training Loss: 0.05692424550652504, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:57, Epoch: 35, Batch: 260, Training Loss: 0.05205337554216385, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:57, Epoch: 35, Batch: 270, Training Loss: 0.04950426667928696, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:58, Epoch: 35, Batch: 280, Training Loss: 0.039223610609769824, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:59, Epoch: 35, Batch: 290, Training Loss: 0.047709254547953606, LR: 0.0010000000000000002
Time, 2019-01-01T17:39:59, Epoch: 35, Batch: 300, Training Loss: 0.06137438118457794, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:00, Epoch: 35, Batch: 310, Training Loss: 0.053478770330548285, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:01, Epoch: 35, Batch: 320, Training Loss: 0.041793692484498025, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:01, Epoch: 35, Batch: 330, Training Loss: 0.041931784525513646, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:02, Epoch: 35, Batch: 340, Training Loss: 0.048643283918499945, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:03, Epoch: 35, Batch: 350, Training Loss: 0.04119052328169346, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:04, Epoch: 35, Batch: 360, Training Loss: 0.0622746791690588, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:04, Epoch: 35, Batch: 370, Training Loss: 0.05356086120009422, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:05, Epoch: 35, Batch: 380, Training Loss: 0.06150432489812374, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:06, Epoch: 35, Batch: 390, Training Loss: 0.06746196262538433, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:06, Epoch: 35, Batch: 400, Training Loss: 0.03850702457129955, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:07, Epoch: 35, Batch: 410, Training Loss: 0.05172811672091484, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:08, Epoch: 35, Batch: 420, Training Loss: 0.048984394967556, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:08, Epoch: 35, Batch: 430, Training Loss: 0.06958593800663948, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:09, Epoch: 35, Batch: 440, Training Loss: 0.05201614573597908, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:10, Epoch: 35, Batch: 450, Training Loss: 0.03812176324427128, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:11, Epoch: 35, Batch: 460, Training Loss: 0.04839858375489712, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:11, Epoch: 35, Batch: 470, Training Loss: 0.07105287313461303, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:12, Epoch: 35, Batch: 480, Training Loss: 0.04278429001569748, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:13, Epoch: 35, Batch: 490, Training Loss: 0.05520225465297699, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:13, Epoch: 35, Batch: 500, Training Loss: 0.0624412652105093, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:14, Epoch: 35, Batch: 510, Training Loss: 0.07119862958788872, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:15, Epoch: 35, Batch: 520, Training Loss: 0.055686593428254126, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:15, Epoch: 35, Batch: 530, Training Loss: 0.05679303333163262, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:16, Epoch: 35, Batch: 540, Training Loss: 0.04663427136838436, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:17, Epoch: 35, Batch: 550, Training Loss: 0.0464858241379261, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:17, Epoch: 35, Batch: 560, Training Loss: 0.07491837292909623, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:18, Epoch: 35, Batch: 570, Training Loss: 0.03281043320894241, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:19, Epoch: 35, Batch: 580, Training Loss: 0.06330971345305443, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:20, Epoch: 35, Batch: 590, Training Loss: 0.041850915551185607, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:20, Epoch: 35, Batch: 600, Training Loss: 0.044931408017873764, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:21, Epoch: 35, Batch: 610, Training Loss: 0.05220664069056511, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:22, Epoch: 35, Batch: 620, Training Loss: 0.049322154745459555, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:22, Epoch: 35, Batch: 630, Training Loss: 0.045520175248384476, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:23, Epoch: 35, Batch: 640, Training Loss: 0.07110107317566872, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:24, Epoch: 35, Batch: 650, Training Loss: 0.056659509986639024, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:24, Epoch: 35, Batch: 660, Training Loss: 0.0546199694275856, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:25, Epoch: 35, Batch: 670, Training Loss: 0.03626923523843288, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:26, Epoch: 35, Batch: 680, Training Loss: 0.03505414165556431, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:27, Epoch: 35, Batch: 690, Training Loss: 0.05980900302529335, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:27, Epoch: 35, Batch: 700, Training Loss: 0.03612737990915775, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:28, Epoch: 35, Batch: 710, Training Loss: 0.07151718810200691, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:29, Epoch: 35, Batch: 720, Training Loss: 0.04357439652085304, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:29, Epoch: 35, Batch: 730, Training Loss: 0.046074995398521425, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:30, Epoch: 35, Batch: 740, Training Loss: 0.03792745172977448, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:31, Epoch: 35, Batch: 750, Training Loss: 0.04117959253489971, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:31, Epoch: 35, Batch: 760, Training Loss: 0.03336973674595356, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:32, Epoch: 35, Batch: 770, Training Loss: 0.03365049883723259, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:33, Epoch: 35, Batch: 780, Training Loss: 0.037490513920783994, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:34, Epoch: 35, Batch: 790, Training Loss: 0.0313540130853653, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:34, Epoch: 35, Batch: 800, Training Loss: 0.05409386418759823, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:35, Epoch: 35, Batch: 810, Training Loss: 0.029934068396687507, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:36, Epoch: 35, Batch: 820, Training Loss: 0.04384455569088459, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:36, Epoch: 35, Batch: 830, Training Loss: 0.05700916461646557, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:37, Epoch: 35, Batch: 840, Training Loss: 0.046283376216888425, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:38, Epoch: 35, Batch: 850, Training Loss: 0.04871697723865509, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:38, Epoch: 35, Batch: 860, Training Loss: 0.05060131177306175, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:39, Epoch: 35, Batch: 870, Training Loss: 0.05029897503554821, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:40, Epoch: 35, Batch: 880, Training Loss: 0.05283517390489578, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:41, Epoch: 35, Batch: 890, Training Loss: 0.041922715678811076, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:41, Epoch: 35, Batch: 900, Training Loss: 0.057677092403173445, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:42, Epoch: 35, Batch: 910, Training Loss: 0.04742591045796871, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:43, Epoch: 35, Batch: 920, Training Loss: 0.053746741637587546, LR: 0.0010000000000000002
Time, 2019-01-01T17:40:43, Epoch: 35, Batch: 930, Training Loss: 0.04238307774066925, LR: 0.0010000000000000002
Epoch: 35, Validation Top 1 acc: 98.57242584228516
Epoch: 35, Validation Top 5 acc: 99.99000549316406
Epoch: 35, Validation Set Loss: 0.04802284017205238
Start training epoch 36
Time, 2019-01-01T17:41:09, Epoch: 36, Batch: 10, Training Loss: 0.07372981086373329, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:10, Epoch: 36, Batch: 20, Training Loss: 0.047679515555500984, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:11, Epoch: 36, Batch: 30, Training Loss: 0.04283237084746361, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:11, Epoch: 36, Batch: 40, Training Loss: 0.04309526421129704, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:12, Epoch: 36, Batch: 50, Training Loss: 0.04939018599689007, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:13, Epoch: 36, Batch: 60, Training Loss: 0.05198612213134766, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:13, Epoch: 36, Batch: 70, Training Loss: 0.042420486360788344, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:14, Epoch: 36, Batch: 80, Training Loss: 0.04142107963562012, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:15, Epoch: 36, Batch: 90, Training Loss: 0.041653506085276605, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:15, Epoch: 36, Batch: 100, Training Loss: 0.03805979937314987, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:16, Epoch: 36, Batch: 110, Training Loss: 0.05470636487007141, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:17, Epoch: 36, Batch: 120, Training Loss: 0.05773843377828598, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:18, Epoch: 36, Batch: 130, Training Loss: 0.04921602420508862, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:18, Epoch: 36, Batch: 140, Training Loss: 0.05494402013719082, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:19, Epoch: 36, Batch: 150, Training Loss: 0.058978278562426566, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:20, Epoch: 36, Batch: 160, Training Loss: 0.05216977074742317, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:20, Epoch: 36, Batch: 170, Training Loss: 0.04363546073436737, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:21, Epoch: 36, Batch: 180, Training Loss: 0.04922481924295426, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:22, Epoch: 36, Batch: 190, Training Loss: 0.03974137604236603, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:22, Epoch: 36, Batch: 200, Training Loss: 0.04854716062545776, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:23, Epoch: 36, Batch: 210, Training Loss: 0.04813072569668293, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:24, Epoch: 36, Batch: 220, Training Loss: 0.04994257241487503, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:25, Epoch: 36, Batch: 230, Training Loss: 0.036162715405225754, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:25, Epoch: 36, Batch: 240, Training Loss: 0.03807827830314636, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:26, Epoch: 36, Batch: 250, Training Loss: 0.051465338468551634, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:27, Epoch: 36, Batch: 260, Training Loss: 0.06464293375611305, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:27, Epoch: 36, Batch: 270, Training Loss: 0.049054055288434026, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:28, Epoch: 36, Batch: 280, Training Loss: 0.055319520831108096, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:29, Epoch: 36, Batch: 290, Training Loss: 0.05290482975542545, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:29, Epoch: 36, Batch: 300, Training Loss: 0.04526013731956482, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:30, Epoch: 36, Batch: 310, Training Loss: 0.04025513157248497, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:31, Epoch: 36, Batch: 320, Training Loss: 0.04872447662055492, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:32, Epoch: 36, Batch: 330, Training Loss: 0.03460204154253006, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:32, Epoch: 36, Batch: 340, Training Loss: 0.04531264267861843, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:33, Epoch: 36, Batch: 350, Training Loss: 0.04662517942488194, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:34, Epoch: 36, Batch: 360, Training Loss: 0.04789900407195091, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:34, Epoch: 36, Batch: 370, Training Loss: 0.05355351492762565, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:35, Epoch: 36, Batch: 380, Training Loss: 0.047290530800819394, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:36, Epoch: 36, Batch: 390, Training Loss: 0.06179569438099861, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:37, Epoch: 36, Batch: 400, Training Loss: 0.0425476212054491, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:37, Epoch: 36, Batch: 410, Training Loss: 0.05588456988334656, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:38, Epoch: 36, Batch: 420, Training Loss: 0.03732494413852692, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:39, Epoch: 36, Batch: 430, Training Loss: 0.03271914049983025, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:39, Epoch: 36, Batch: 440, Training Loss: 0.03025209568440914, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:40, Epoch: 36, Batch: 450, Training Loss: 0.08361904434859753, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:41, Epoch: 36, Batch: 460, Training Loss: 0.04607507511973381, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:42, Epoch: 36, Batch: 470, Training Loss: 0.03689533956348896, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:42, Epoch: 36, Batch: 480, Training Loss: 0.04613480865955353, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:43, Epoch: 36, Batch: 490, Training Loss: 0.046844661235809326, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:44, Epoch: 36, Batch: 500, Training Loss: 0.05887901298701763, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:44, Epoch: 36, Batch: 510, Training Loss: 0.05181231461465359, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:45, Epoch: 36, Batch: 520, Training Loss: 0.045773708075284955, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:46, Epoch: 36, Batch: 530, Training Loss: 0.047674340382218364, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:46, Epoch: 36, Batch: 540, Training Loss: 0.0482257854193449, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:47, Epoch: 36, Batch: 550, Training Loss: 0.04863090552389622, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:48, Epoch: 36, Batch: 560, Training Loss: 0.05429267063736916, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:48, Epoch: 36, Batch: 570, Training Loss: 0.03221932910382748, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:49, Epoch: 36, Batch: 580, Training Loss: 0.031162755191326143, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:50, Epoch: 36, Batch: 590, Training Loss: 0.040115692839026454, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:51, Epoch: 36, Batch: 600, Training Loss: 0.03580105900764465, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:51, Epoch: 36, Batch: 610, Training Loss: 0.0722123108804226, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:52, Epoch: 36, Batch: 620, Training Loss: 0.07907356470823287, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:53, Epoch: 36, Batch: 630, Training Loss: 0.04900563210248947, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:53, Epoch: 36, Batch: 640, Training Loss: 0.05425621718168259, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:54, Epoch: 36, Batch: 650, Training Loss: 0.06205429844558239, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:55, Epoch: 36, Batch: 660, Training Loss: 0.07590572163462639, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:55, Epoch: 36, Batch: 670, Training Loss: 0.05810592658817768, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:56, Epoch: 36, Batch: 680, Training Loss: 0.044421826303005216, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:57, Epoch: 36, Batch: 690, Training Loss: 0.05659823901951313, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:58, Epoch: 36, Batch: 700, Training Loss: 0.04199180752038956, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:58, Epoch: 36, Batch: 710, Training Loss: 0.03363329023122787, LR: 0.0010000000000000002
Time, 2019-01-01T17:41:59, Epoch: 36, Batch: 720, Training Loss: 0.04152496829628945, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:00, Epoch: 36, Batch: 730, Training Loss: 0.04731205813586712, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:00, Epoch: 36, Batch: 740, Training Loss: 0.0695358783006668, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:01, Epoch: 36, Batch: 750, Training Loss: 0.04274089001119137, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:02, Epoch: 36, Batch: 760, Training Loss: 0.0486638031899929, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:03, Epoch: 36, Batch: 770, Training Loss: 0.04361361972987652, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:03, Epoch: 36, Batch: 780, Training Loss: 0.05272061675786972, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:04, Epoch: 36, Batch: 790, Training Loss: 0.046007410436868665, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:05, Epoch: 36, Batch: 800, Training Loss: 0.05443018153309822, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:05, Epoch: 36, Batch: 810, Training Loss: 0.044041887670755384, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:06, Epoch: 36, Batch: 820, Training Loss: 0.04645962566137314, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:07, Epoch: 36, Batch: 830, Training Loss: 0.0546361468732357, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:07, Epoch: 36, Batch: 840, Training Loss: 0.05989788547158241, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:08, Epoch: 36, Batch: 850, Training Loss: 0.036096590012311934, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:09, Epoch: 36, Batch: 860, Training Loss: 0.039066331833601, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:09, Epoch: 36, Batch: 870, Training Loss: 0.04839666858315468, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:10, Epoch: 36, Batch: 880, Training Loss: 0.03969864025712013, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:11, Epoch: 36, Batch: 890, Training Loss: 0.06839461401104927, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:12, Epoch: 36, Batch: 900, Training Loss: 0.04614029973745346, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:12, Epoch: 36, Batch: 910, Training Loss: 0.028276223689317703, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:13, Epoch: 36, Batch: 920, Training Loss: 0.039604275673627856, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:14, Epoch: 36, Batch: 930, Training Loss: 0.05227947384119034, LR: 0.0010000000000000002
Epoch: 36, Validation Top 1 acc: 98.5591049194336
Epoch: 36, Validation Top 5 acc: 99.99000549316406
Epoch: 36, Validation Set Loss: 0.0487355999648571
Start training epoch 37
Time, 2019-01-01T17:42:39, Epoch: 37, Batch: 10, Training Loss: 0.046987703815102574, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:40, Epoch: 37, Batch: 20, Training Loss: 0.051953472569584845, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:41, Epoch: 37, Batch: 30, Training Loss: 0.06377529427409172, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:41, Epoch: 37, Batch: 40, Training Loss: 0.041332533583045006, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:42, Epoch: 37, Batch: 50, Training Loss: 0.06771210879087448, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:43, Epoch: 37, Batch: 60, Training Loss: 0.050689315795898436, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:43, Epoch: 37, Batch: 70, Training Loss: 0.06574678458273411, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:44, Epoch: 37, Batch: 80, Training Loss: 0.050379591062664986, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:45, Epoch: 37, Batch: 90, Training Loss: 0.040244802832603455, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:46, Epoch: 37, Batch: 100, Training Loss: 0.04518281221389771, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:46, Epoch: 37, Batch: 110, Training Loss: 0.05854456350207329, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:47, Epoch: 37, Batch: 120, Training Loss: 0.05599587559700012, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:48, Epoch: 37, Batch: 130, Training Loss: 0.03140994384884834, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:48, Epoch: 37, Batch: 140, Training Loss: 0.0515966035425663, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:49, Epoch: 37, Batch: 150, Training Loss: 0.06661313101649284, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:50, Epoch: 37, Batch: 160, Training Loss: 0.03138292245566845, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:50, Epoch: 37, Batch: 170, Training Loss: 0.05439077168703079, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:51, Epoch: 37, Batch: 180, Training Loss: 0.03561694324016571, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:52, Epoch: 37, Batch: 190, Training Loss: 0.0521170437335968, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:53, Epoch: 37, Batch: 200, Training Loss: 0.0501467440277338, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:53, Epoch: 37, Batch: 210, Training Loss: 0.04514684975147247, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:54, Epoch: 37, Batch: 220, Training Loss: 0.04512157440185547, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:55, Epoch: 37, Batch: 230, Training Loss: 0.0291262648999691, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:55, Epoch: 37, Batch: 240, Training Loss: 0.049552444368600845, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:56, Epoch: 37, Batch: 250, Training Loss: 0.051314835995435716, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:57, Epoch: 37, Batch: 260, Training Loss: 0.06791461184620858, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:57, Epoch: 37, Batch: 270, Training Loss: 0.047999560832977295, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:58, Epoch: 37, Batch: 280, Training Loss: 0.07714782357215881, LR: 0.0010000000000000002
Time, 2019-01-01T17:42:59, Epoch: 37, Batch: 290, Training Loss: 0.0419089213013649, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:00, Epoch: 37, Batch: 300, Training Loss: 0.051040249317884444, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:00, Epoch: 37, Batch: 310, Training Loss: 0.028366195783019067, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:01, Epoch: 37, Batch: 320, Training Loss: 0.040893567353487016, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:02, Epoch: 37, Batch: 330, Training Loss: 0.05058348253369331, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:02, Epoch: 37, Batch: 340, Training Loss: 0.047768260538578036, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:03, Epoch: 37, Batch: 350, Training Loss: 0.03925788030028343, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:04, Epoch: 37, Batch: 360, Training Loss: 0.05860680639743805, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:05, Epoch: 37, Batch: 370, Training Loss: 0.052317436784505844, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:05, Epoch: 37, Batch: 380, Training Loss: 0.049025430157780645, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:06, Epoch: 37, Batch: 390, Training Loss: 0.05516556389629841, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:07, Epoch: 37, Batch: 400, Training Loss: 0.062077750265598294, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:07, Epoch: 37, Batch: 410, Training Loss: 0.040426424518227574, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:08, Epoch: 37, Batch: 420, Training Loss: 0.06255393363535404, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:09, Epoch: 37, Batch: 430, Training Loss: 0.04220884405076504, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:09, Epoch: 37, Batch: 440, Training Loss: 0.060049460455775264, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:10, Epoch: 37, Batch: 450, Training Loss: 0.0522833276540041, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:11, Epoch: 37, Batch: 460, Training Loss: 0.055656233057379725, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:12, Epoch: 37, Batch: 470, Training Loss: 0.03439582847058773, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:12, Epoch: 37, Batch: 480, Training Loss: 0.04814876466989517, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:13, Epoch: 37, Batch: 490, Training Loss: 0.03330869078636169, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:14, Epoch: 37, Batch: 500, Training Loss: 0.060065476968884465, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:14, Epoch: 37, Batch: 510, Training Loss: 0.026990531012415886, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:15, Epoch: 37, Batch: 520, Training Loss: 0.044163058325648305, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:16, Epoch: 37, Batch: 530, Training Loss: 0.028986893594264984, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:16, Epoch: 37, Batch: 540, Training Loss: 0.07037063725292683, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:17, Epoch: 37, Batch: 550, Training Loss: 0.04808904565870762, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:18, Epoch: 37, Batch: 560, Training Loss: 0.05969755649566651, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:18, Epoch: 37, Batch: 570, Training Loss: 0.057294924184679984, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:19, Epoch: 37, Batch: 580, Training Loss: 0.046876140683889386, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:20, Epoch: 37, Batch: 590, Training Loss: 0.05229659676551819, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:21, Epoch: 37, Batch: 600, Training Loss: 0.04225180670619011, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:21, Epoch: 37, Batch: 610, Training Loss: 0.03502257764339447, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:22, Epoch: 37, Batch: 620, Training Loss: 0.049109605699777605, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:23, Epoch: 37, Batch: 630, Training Loss: 0.04267694354057312, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:23, Epoch: 37, Batch: 640, Training Loss: 0.040912697464227675, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:24, Epoch: 37, Batch: 650, Training Loss: 0.045743197575211524, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:25, Epoch: 37, Batch: 660, Training Loss: 0.05199468284845352, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:25, Epoch: 37, Batch: 670, Training Loss: 0.03764127343893051, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:26, Epoch: 37, Batch: 680, Training Loss: 0.05154073163866997, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:27, Epoch: 37, Batch: 690, Training Loss: 0.04626852422952652, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:28, Epoch: 37, Batch: 700, Training Loss: 0.03758531510829925, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:28, Epoch: 37, Batch: 710, Training Loss: 0.055737517029047015, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:29, Epoch: 37, Batch: 720, Training Loss: 0.025271499156951906, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:30, Epoch: 37, Batch: 730, Training Loss: 0.04680056720972061, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:30, Epoch: 37, Batch: 740, Training Loss: 0.05624583512544632, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:31, Epoch: 37, Batch: 750, Training Loss: 0.021763353049755095, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:32, Epoch: 37, Batch: 760, Training Loss: 0.03376327455043793, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:33, Epoch: 37, Batch: 770, Training Loss: 0.07050012126564979, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:33, Epoch: 37, Batch: 780, Training Loss: 0.05143975168466568, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:34, Epoch: 37, Batch: 790, Training Loss: 0.06753715947270393, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:35, Epoch: 37, Batch: 800, Training Loss: 0.054339132457971576, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:35, Epoch: 37, Batch: 810, Training Loss: 0.041013828665018084, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:36, Epoch: 37, Batch: 820, Training Loss: 0.049539586156606676, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:37, Epoch: 37, Batch: 830, Training Loss: 0.05245146080851555, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:37, Epoch: 37, Batch: 840, Training Loss: 0.05090501755475998, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:38, Epoch: 37, Batch: 850, Training Loss: 0.06902556754648685, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:39, Epoch: 37, Batch: 860, Training Loss: 0.05140087902545929, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:40, Epoch: 37, Batch: 870, Training Loss: 0.06401363983750344, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:40, Epoch: 37, Batch: 880, Training Loss: 0.04179779551923275, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:41, Epoch: 37, Batch: 890, Training Loss: 0.042273149266839026, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:42, Epoch: 37, Batch: 900, Training Loss: 0.055741669982671736, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:42, Epoch: 37, Batch: 910, Training Loss: 0.059724028781056406, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:43, Epoch: 37, Batch: 920, Training Loss: 0.03940988890826702, LR: 0.0010000000000000002
Time, 2019-01-01T17:43:44, Epoch: 37, Batch: 930, Training Loss: 0.03901445418596268, LR: 0.0010000000000000002
Epoch: 37, Validation Top 1 acc: 98.60740661621094
Epoch: 37, Validation Top 5 acc: 99.99000549316406
Epoch: 37, Validation Set Loss: 0.04808967560529709
Start training epoch 38
Time, 2019-01-01T17:44:09, Epoch: 38, Batch: 10, Training Loss: 0.03393909297883511, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:10, Epoch: 38, Batch: 20, Training Loss: 0.06631418839097022, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:11, Epoch: 38, Batch: 30, Training Loss: 0.036138007417321205, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:12, Epoch: 38, Batch: 40, Training Loss: 0.03305251374840736, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:12, Epoch: 38, Batch: 50, Training Loss: 0.052753320336341857, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:13, Epoch: 38, Batch: 60, Training Loss: 0.07659258022904396, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:14, Epoch: 38, Batch: 70, Training Loss: 0.07071352154016494, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:14, Epoch: 38, Batch: 80, Training Loss: 0.04498755037784576, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:15, Epoch: 38, Batch: 90, Training Loss: 0.05683923214673996, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:16, Epoch: 38, Batch: 100, Training Loss: 0.04747042693197727, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:16, Epoch: 38, Batch: 110, Training Loss: 0.05007241144776344, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:17, Epoch: 38, Batch: 120, Training Loss: 0.04896672517061233, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:18, Epoch: 38, Batch: 130, Training Loss: 0.05376245453953743, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:18, Epoch: 38, Batch: 140, Training Loss: 0.05414909273386002, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:19, Epoch: 38, Batch: 150, Training Loss: 0.037552598118782046, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:20, Epoch: 38, Batch: 160, Training Loss: 0.0374311488121748, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:21, Epoch: 38, Batch: 170, Training Loss: 0.04924525618553162, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:21, Epoch: 38, Batch: 180, Training Loss: 0.054325371980667114, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:22, Epoch: 38, Batch: 190, Training Loss: 0.037544529512524606, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:23, Epoch: 38, Batch: 200, Training Loss: 0.0598920650780201, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:23, Epoch: 38, Batch: 210, Training Loss: 0.08376418426632881, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:24, Epoch: 38, Batch: 220, Training Loss: 0.05264328308403492, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:25, Epoch: 38, Batch: 230, Training Loss: 0.04113083891570568, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:26, Epoch: 38, Batch: 240, Training Loss: 0.060679894685745236, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:26, Epoch: 38, Batch: 250, Training Loss: 0.05065627209842205, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:27, Epoch: 38, Batch: 260, Training Loss: 0.046673694252967836, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:28, Epoch: 38, Batch: 270, Training Loss: 0.04874942526221275, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:28, Epoch: 38, Batch: 280, Training Loss: 0.04835740774869919, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:29, Epoch: 38, Batch: 290, Training Loss: 0.05305863469839096, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:30, Epoch: 38, Batch: 300, Training Loss: 0.04593244567513466, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:30, Epoch: 38, Batch: 310, Training Loss: 0.030346116796135903, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:31, Epoch: 38, Batch: 320, Training Loss: 0.05675430335104466, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:32, Epoch: 38, Batch: 330, Training Loss: 0.04135957062244415, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:32, Epoch: 38, Batch: 340, Training Loss: 0.04381283670663834, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:33, Epoch: 38, Batch: 350, Training Loss: 0.03933469727635384, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:34, Epoch: 38, Batch: 360, Training Loss: 0.039864598959684375, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:35, Epoch: 38, Batch: 370, Training Loss: 0.04453345239162445, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:35, Epoch: 38, Batch: 380, Training Loss: 0.03844000585377216, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:36, Epoch: 38, Batch: 390, Training Loss: 0.039787546172738074, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:37, Epoch: 38, Batch: 400, Training Loss: 0.036793308705091475, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:37, Epoch: 38, Batch: 410, Training Loss: 0.05050297193229199, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:38, Epoch: 38, Batch: 420, Training Loss: 0.04387461878359318, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:39, Epoch: 38, Batch: 430, Training Loss: 0.04685229137539863, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:39, Epoch: 38, Batch: 440, Training Loss: 0.027620809525251387, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:40, Epoch: 38, Batch: 450, Training Loss: 0.031517109647393224, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:41, Epoch: 38, Batch: 460, Training Loss: 0.08751162923872471, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:42, Epoch: 38, Batch: 470, Training Loss: 0.03564013242721557, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:42, Epoch: 38, Batch: 480, Training Loss: 0.050984049960970876, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:43, Epoch: 38, Batch: 490, Training Loss: 0.0493613388389349, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:44, Epoch: 38, Batch: 500, Training Loss: 0.04467737600207329, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:44, Epoch: 38, Batch: 510, Training Loss: 0.043943770602345465, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:45, Epoch: 38, Batch: 520, Training Loss: 0.0387684803456068, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:46, Epoch: 38, Batch: 530, Training Loss: 0.0412931215018034, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:46, Epoch: 38, Batch: 540, Training Loss: 0.0681040707975626, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:47, Epoch: 38, Batch: 550, Training Loss: 0.06912225186824798, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:48, Epoch: 38, Batch: 560, Training Loss: 0.04928179644048214, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:49, Epoch: 38, Batch: 570, Training Loss: 0.06662833839654922, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:49, Epoch: 38, Batch: 580, Training Loss: 0.03706978410482407, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:50, Epoch: 38, Batch: 590, Training Loss: 0.04302682802081108, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:51, Epoch: 38, Batch: 600, Training Loss: 0.0607735637575388, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:51, Epoch: 38, Batch: 610, Training Loss: 0.046717021986842154, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:52, Epoch: 38, Batch: 620, Training Loss: 0.05840083546936512, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:53, Epoch: 38, Batch: 630, Training Loss: 0.05084108300507069, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:54, Epoch: 38, Batch: 640, Training Loss: 0.05831468775868416, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:54, Epoch: 38, Batch: 650, Training Loss: 0.04594671502709389, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:55, Epoch: 38, Batch: 660, Training Loss: 0.043316411599516866, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:56, Epoch: 38, Batch: 670, Training Loss: 0.043452856317162517, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:56, Epoch: 38, Batch: 680, Training Loss: 0.0362190842628479, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:57, Epoch: 38, Batch: 690, Training Loss: 0.04485764317214489, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:58, Epoch: 38, Batch: 700, Training Loss: 0.056943774968385694, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:58, Epoch: 38, Batch: 710, Training Loss: 0.036013801395893094, LR: 0.0010000000000000002
Time, 2019-01-01T17:44:59, Epoch: 38, Batch: 720, Training Loss: 0.02508750930428505, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:00, Epoch: 38, Batch: 730, Training Loss: 0.042728318646550176, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:01, Epoch: 38, Batch: 740, Training Loss: 0.03575252965092659, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:01, Epoch: 38, Batch: 750, Training Loss: 0.05833523795008659, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:02, Epoch: 38, Batch: 760, Training Loss: 0.059809368848800656, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:03, Epoch: 38, Batch: 770, Training Loss: 0.04145939126610756, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:03, Epoch: 38, Batch: 780, Training Loss: 0.05049010813236236, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:04, Epoch: 38, Batch: 790, Training Loss: 0.05099001117050648, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:05, Epoch: 38, Batch: 800, Training Loss: 0.02822420746088028, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:06, Epoch: 38, Batch: 810, Training Loss: 0.07024947553873062, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:06, Epoch: 38, Batch: 820, Training Loss: 0.037147189304232595, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:07, Epoch: 38, Batch: 830, Training Loss: 0.049191443249583244, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:08, Epoch: 38, Batch: 840, Training Loss: 0.036613012105226515, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:08, Epoch: 38, Batch: 850, Training Loss: 0.031451332941651346, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:09, Epoch: 38, Batch: 860, Training Loss: 0.03820545449852943, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:10, Epoch: 38, Batch: 870, Training Loss: 0.04345745332539082, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:10, Epoch: 38, Batch: 880, Training Loss: 0.05391430370509624, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:11, Epoch: 38, Batch: 890, Training Loss: 0.040558576583862305, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:12, Epoch: 38, Batch: 900, Training Loss: 0.04298614300787449, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:13, Epoch: 38, Batch: 910, Training Loss: 0.06301733329892159, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:13, Epoch: 38, Batch: 920, Training Loss: 0.030716787651181222, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:14, Epoch: 38, Batch: 930, Training Loss: 0.046959086507558825, LR: 0.0010000000000000002
Epoch: 38, Validation Top 1 acc: 98.67070770263672
Epoch: 38, Validation Top 5 acc: 99.9866714477539
Epoch: 38, Validation Set Loss: 0.04625925049185753
Start training epoch 39
Time, 2019-01-01T17:45:40, Epoch: 39, Batch: 10, Training Loss: 0.04591564685106277, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:40, Epoch: 39, Batch: 20, Training Loss: 0.04384590350091457, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:41, Epoch: 39, Batch: 30, Training Loss: 0.04878080263733864, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:42, Epoch: 39, Batch: 40, Training Loss: 0.04133895635604858, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:43, Epoch: 39, Batch: 50, Training Loss: 0.03860390782356262, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:43, Epoch: 39, Batch: 60, Training Loss: 0.03067258819937706, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:44, Epoch: 39, Batch: 70, Training Loss: 0.05118130072951317, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:45, Epoch: 39, Batch: 80, Training Loss: 0.05125207081437111, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:45, Epoch: 39, Batch: 90, Training Loss: 0.03558561652898788, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:46, Epoch: 39, Batch: 100, Training Loss: 0.027645715326070786, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:47, Epoch: 39, Batch: 110, Training Loss: 0.030135125294327735, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:48, Epoch: 39, Batch: 120, Training Loss: 0.057673501595854756, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:48, Epoch: 39, Batch: 130, Training Loss: 0.04629189930856228, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:49, Epoch: 39, Batch: 140, Training Loss: 0.04727388843894005, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:50, Epoch: 39, Batch: 150, Training Loss: 0.0435267087072134, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:50, Epoch: 39, Batch: 160, Training Loss: 0.03707449808716774, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:51, Epoch: 39, Batch: 170, Training Loss: 0.06662513092160224, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:52, Epoch: 39, Batch: 180, Training Loss: 0.06718615666031838, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:52, Epoch: 39, Batch: 190, Training Loss: 0.05338558703660965, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:53, Epoch: 39, Batch: 200, Training Loss: 0.08615848198533058, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:54, Epoch: 39, Batch: 210, Training Loss: 0.031026091054081918, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:55, Epoch: 39, Batch: 220, Training Loss: 0.06783116869628429, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:55, Epoch: 39, Batch: 230, Training Loss: 0.05037889443337917, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:56, Epoch: 39, Batch: 240, Training Loss: 0.04208754263818264, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:57, Epoch: 39, Batch: 250, Training Loss: 0.03944200128316879, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:57, Epoch: 39, Batch: 260, Training Loss: 0.04705060049891472, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:58, Epoch: 39, Batch: 270, Training Loss: 0.0699643611907959, LR: 0.0010000000000000002
Time, 2019-01-01T17:45:59, Epoch: 39, Batch: 280, Training Loss: 0.050342366099357605, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:00, Epoch: 39, Batch: 290, Training Loss: 0.03858134113252163, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:00, Epoch: 39, Batch: 300, Training Loss: 0.0525362029671669, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:01, Epoch: 39, Batch: 310, Training Loss: 0.05592978186905384, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:02, Epoch: 39, Batch: 320, Training Loss: 0.046075981110334396, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:02, Epoch: 39, Batch: 330, Training Loss: 0.06230237111449242, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:03, Epoch: 39, Batch: 340, Training Loss: 0.042341865971684454, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:04, Epoch: 39, Batch: 350, Training Loss: 0.06116696372628212, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:04, Epoch: 39, Batch: 360, Training Loss: 0.04284366853535175, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:05, Epoch: 39, Batch: 370, Training Loss: 0.02839745506644249, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:06, Epoch: 39, Batch: 380, Training Loss: 0.0687962856143713, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:07, Epoch: 39, Batch: 390, Training Loss: 0.05677604004740715, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:07, Epoch: 39, Batch: 400, Training Loss: 0.03478267230093479, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:08, Epoch: 39, Batch: 410, Training Loss: 0.04997551329433918, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:09, Epoch: 39, Batch: 420, Training Loss: 0.04836766608059406, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:09, Epoch: 39, Batch: 430, Training Loss: 0.04246514290571213, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:10, Epoch: 39, Batch: 440, Training Loss: 0.04253063127398491, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:11, Epoch: 39, Batch: 450, Training Loss: 0.03401791714131832, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:12, Epoch: 39, Batch: 460, Training Loss: 0.05289486162364483, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:12, Epoch: 39, Batch: 470, Training Loss: 0.04322741739451885, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:13, Epoch: 39, Batch: 480, Training Loss: 0.04230424053966999, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:14, Epoch: 39, Batch: 490, Training Loss: 0.06303087770938873, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:14, Epoch: 39, Batch: 500, Training Loss: 0.04777712970972061, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:15, Epoch: 39, Batch: 510, Training Loss: 0.04324419982731342, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:16, Epoch: 39, Batch: 520, Training Loss: 0.028007812052965163, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:17, Epoch: 39, Batch: 530, Training Loss: 0.05596947744488716, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:17, Epoch: 39, Batch: 540, Training Loss: 0.04279513284564018, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:18, Epoch: 39, Batch: 550, Training Loss: 0.04879901334643364, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:19, Epoch: 39, Batch: 560, Training Loss: 0.04425890147686005, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:19, Epoch: 39, Batch: 570, Training Loss: 0.03544987589120865, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:20, Epoch: 39, Batch: 580, Training Loss: 0.035443566739559174, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:21, Epoch: 39, Batch: 590, Training Loss: 0.049460092559456825, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:21, Epoch: 39, Batch: 600, Training Loss: 0.0269446287304163, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:22, Epoch: 39, Batch: 610, Training Loss: 0.052809569984674454, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:23, Epoch: 39, Batch: 620, Training Loss: 0.03806136175990105, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:24, Epoch: 39, Batch: 630, Training Loss: 0.044434796646237375, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:25, Epoch: 39, Batch: 640, Training Loss: 0.040697091817855836, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:25, Epoch: 39, Batch: 650, Training Loss: 0.04240606278181076, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:26, Epoch: 39, Batch: 660, Training Loss: 0.033324248343706134, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:27, Epoch: 39, Batch: 670, Training Loss: 0.041589947417378426, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:27, Epoch: 39, Batch: 680, Training Loss: 0.03941527493298054, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:28, Epoch: 39, Batch: 690, Training Loss: 0.07256420962512493, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:29, Epoch: 39, Batch: 700, Training Loss: 0.03511885888874531, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:30, Epoch: 39, Batch: 710, Training Loss: 0.027473608404397963, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:31, Epoch: 39, Batch: 720, Training Loss: 0.040562334656715396, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:31, Epoch: 39, Batch: 730, Training Loss: 0.0380829568952322, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:32, Epoch: 39, Batch: 740, Training Loss: 0.03854154497385025, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:33, Epoch: 39, Batch: 750, Training Loss: 0.044202814996242526, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:34, Epoch: 39, Batch: 760, Training Loss: 0.04342624954879284, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:35, Epoch: 39, Batch: 770, Training Loss: 0.0643226996064186, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:35, Epoch: 39, Batch: 780, Training Loss: 0.05209948681294918, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:36, Epoch: 39, Batch: 790, Training Loss: 0.03353531286120415, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:37, Epoch: 39, Batch: 800, Training Loss: 0.07929936647415162, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:38, Epoch: 39, Batch: 810, Training Loss: 0.05387772098183632, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:38, Epoch: 39, Batch: 820, Training Loss: 0.05175713039934635, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:39, Epoch: 39, Batch: 830, Training Loss: 0.057203761860728265, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:40, Epoch: 39, Batch: 840, Training Loss: 0.051292966678738595, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:40, Epoch: 39, Batch: 850, Training Loss: 0.042418502271175385, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:41, Epoch: 39, Batch: 860, Training Loss: 0.03527385219931602, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:42, Epoch: 39, Batch: 870, Training Loss: 0.05041831023991108, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:42, Epoch: 39, Batch: 880, Training Loss: 0.032454409450292585, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:43, Epoch: 39, Batch: 890, Training Loss: 0.05126141607761383, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:44, Epoch: 39, Batch: 900, Training Loss: 0.04383605383336544, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:45, Epoch: 39, Batch: 910, Training Loss: 0.044890503585338595, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:45, Epoch: 39, Batch: 920, Training Loss: 0.06838271096348762, LR: 0.0010000000000000002
Time, 2019-01-01T17:46:46, Epoch: 39, Batch: 930, Training Loss: 0.03609584458172321, LR: 0.0010000000000000002
Epoch: 39, Validation Top 1 acc: 98.64738464355469
Epoch: 39, Validation Top 5 acc: 99.99166870117188
Epoch: 39, Validation Set Loss: 0.04609433561563492
Start training epoch 40
Time, 2019-01-01T17:47:12, Epoch: 40, Batch: 10, Training Loss: 0.054576174914836885, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:13, Epoch: 40, Batch: 20, Training Loss: 0.03600157350301743, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:13, Epoch: 40, Batch: 30, Training Loss: 0.046738313138484956, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:14, Epoch: 40, Batch: 40, Training Loss: 0.034996901825070384, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:15, Epoch: 40, Batch: 50, Training Loss: 0.05414335355162621, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:15, Epoch: 40, Batch: 60, Training Loss: 0.05944313071668148, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:16, Epoch: 40, Batch: 70, Training Loss: 0.027626467496156694, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:17, Epoch: 40, Batch: 80, Training Loss: 0.04912087842822075, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:17, Epoch: 40, Batch: 90, Training Loss: 0.03564639650285244, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:18, Epoch: 40, Batch: 100, Training Loss: 0.03699976168572903, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:19, Epoch: 40, Batch: 110, Training Loss: 0.04705166593194008, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:20, Epoch: 40, Batch: 120, Training Loss: 0.034940894693136215, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:20, Epoch: 40, Batch: 130, Training Loss: 0.07527228966355323, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:21, Epoch: 40, Batch: 140, Training Loss: 0.05751567371189594, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:22, Epoch: 40, Batch: 150, Training Loss: 0.03718731366097927, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:22, Epoch: 40, Batch: 160, Training Loss: 0.04609759896993637, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:23, Epoch: 40, Batch: 170, Training Loss: 0.0576562374830246, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:24, Epoch: 40, Batch: 180, Training Loss: 0.05544640719890594, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:25, Epoch: 40, Batch: 190, Training Loss: 0.035531188175082204, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:25, Epoch: 40, Batch: 200, Training Loss: 0.034821616113185884, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:26, Epoch: 40, Batch: 210, Training Loss: 0.034026125073432924, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:27, Epoch: 40, Batch: 220, Training Loss: 0.03266358561813831, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:27, Epoch: 40, Batch: 230, Training Loss: 0.030679666623473167, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:28, Epoch: 40, Batch: 240, Training Loss: 0.06811745017766953, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:29, Epoch: 40, Batch: 250, Training Loss: 0.045526623353362085, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:29, Epoch: 40, Batch: 260, Training Loss: 0.04033284187316895, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:30, Epoch: 40, Batch: 270, Training Loss: 0.03858179971575737, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:31, Epoch: 40, Batch: 280, Training Loss: 0.039895329624414444, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:32, Epoch: 40, Batch: 290, Training Loss: 0.03918755874037742, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:32, Epoch: 40, Batch: 300, Training Loss: 0.04387716501951218, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:33, Epoch: 40, Batch: 310, Training Loss: 0.05009741745889187, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:34, Epoch: 40, Batch: 320, Training Loss: 0.062109680846333504, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:34, Epoch: 40, Batch: 330, Training Loss: 0.05949465334415436, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:35, Epoch: 40, Batch: 340, Training Loss: 0.042524092644453046, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:36, Epoch: 40, Batch: 350, Training Loss: 0.05246788635849953, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:37, Epoch: 40, Batch: 360, Training Loss: 0.048416049033403394, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:37, Epoch: 40, Batch: 370, Training Loss: 0.04291144087910652, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:38, Epoch: 40, Batch: 380, Training Loss: 0.06196178793907166, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:39, Epoch: 40, Batch: 390, Training Loss: 0.03908381722867489, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:39, Epoch: 40, Batch: 400, Training Loss: 0.041270564496517184, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:40, Epoch: 40, Batch: 410, Training Loss: 0.06346831694245339, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:41, Epoch: 40, Batch: 420, Training Loss: 0.07460258789360523, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:42, Epoch: 40, Batch: 430, Training Loss: 0.05534277930855751, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:42, Epoch: 40, Batch: 440, Training Loss: 0.0392203401774168, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:43, Epoch: 40, Batch: 450, Training Loss: 0.03936482556164265, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:44, Epoch: 40, Batch: 460, Training Loss: 0.028029090911149978, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:44, Epoch: 40, Batch: 470, Training Loss: 0.040903158113360406, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:45, Epoch: 40, Batch: 480, Training Loss: 0.03173922672867775, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:46, Epoch: 40, Batch: 490, Training Loss: 0.04924488067626953, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:46, Epoch: 40, Batch: 500, Training Loss: 0.04273035526275635, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:47, Epoch: 40, Batch: 510, Training Loss: 0.06066826954483986, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:48, Epoch: 40, Batch: 520, Training Loss: 0.06524667702615261, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:49, Epoch: 40, Batch: 530, Training Loss: 0.042672480642795566, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:49, Epoch: 40, Batch: 540, Training Loss: 0.043806961923837665, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:50, Epoch: 40, Batch: 550, Training Loss: 0.055150156468153, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:51, Epoch: 40, Batch: 560, Training Loss: 0.02589114345610142, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:51, Epoch: 40, Batch: 570, Training Loss: 0.05287975519895553, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:52, Epoch: 40, Batch: 580, Training Loss: 0.04766938015818596, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:53, Epoch: 40, Batch: 590, Training Loss: 0.047202061861753464, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:54, Epoch: 40, Batch: 600, Training Loss: 0.049246178939938545, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:54, Epoch: 40, Batch: 610, Training Loss: 0.043275947123765944, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:55, Epoch: 40, Batch: 620, Training Loss: 0.05011064037680626, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:56, Epoch: 40, Batch: 630, Training Loss: 0.061322415992617604, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:56, Epoch: 40, Batch: 640, Training Loss: 0.038222845643758774, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:57, Epoch: 40, Batch: 650, Training Loss: 0.05067420341074467, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:58, Epoch: 40, Batch: 660, Training Loss: 0.043856344372034076, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:59, Epoch: 40, Batch: 670, Training Loss: 0.060972707346081734, LR: 0.0010000000000000002
Time, 2019-01-01T17:47:59, Epoch: 40, Batch: 680, Training Loss: 0.03121999278664589, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:00, Epoch: 40, Batch: 690, Training Loss: 0.05498263388872147, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:01, Epoch: 40, Batch: 700, Training Loss: 0.050018205493688586, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:01, Epoch: 40, Batch: 710, Training Loss: 0.041198940202593806, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:02, Epoch: 40, Batch: 720, Training Loss: 0.04488240331411362, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:03, Epoch: 40, Batch: 730, Training Loss: 0.035902779549360275, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:04, Epoch: 40, Batch: 740, Training Loss: 0.03171528056263924, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:04, Epoch: 40, Batch: 750, Training Loss: 0.04107248485088348, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:05, Epoch: 40, Batch: 760, Training Loss: 0.06612814366817474, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:06, Epoch: 40, Batch: 770, Training Loss: 0.03420665264129639, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:06, Epoch: 40, Batch: 780, Training Loss: 0.053277221694588664, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:07, Epoch: 40, Batch: 790, Training Loss: 0.04184499569237232, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:08, Epoch: 40, Batch: 800, Training Loss: 0.05394387692213058, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:08, Epoch: 40, Batch: 810, Training Loss: 0.04842375665903091, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:09, Epoch: 40, Batch: 820, Training Loss: 0.04192065224051476, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:10, Epoch: 40, Batch: 830, Training Loss: 0.05074655339121818, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:11, Epoch: 40, Batch: 840, Training Loss: 0.04292546547949314, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:11, Epoch: 40, Batch: 850, Training Loss: 0.025410453975200652, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:12, Epoch: 40, Batch: 860, Training Loss: 0.04249342009425163, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:13, Epoch: 40, Batch: 870, Training Loss: 0.05332631319761276, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:13, Epoch: 40, Batch: 880, Training Loss: 0.04928071387112141, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:14, Epoch: 40, Batch: 890, Training Loss: 0.03387353606522083, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:15, Epoch: 40, Batch: 900, Training Loss: 0.02614366337656975, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:16, Epoch: 40, Batch: 910, Training Loss: 0.058892006427049635, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:16, Epoch: 40, Batch: 920, Training Loss: 0.04375860318541527, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:17, Epoch: 40, Batch: 930, Training Loss: 0.04946905635297298, LR: 0.0010000000000000002
Epoch: 40, Validation Top 1 acc: 98.70069122314453
Epoch: 40, Validation Top 5 acc: 99.99000549316406
Epoch: 40, Validation Set Loss: 0.045148324221372604
Start training epoch 41
Time, 2019-01-01T17:48:43, Epoch: 41, Batch: 10, Training Loss: 0.06373701952397823, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:44, Epoch: 41, Batch: 20, Training Loss: 0.05918783061206341, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:45, Epoch: 41, Batch: 30, Training Loss: 0.03825249597430229, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:45, Epoch: 41, Batch: 40, Training Loss: 0.03519056029617786, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:46, Epoch: 41, Batch: 50, Training Loss: 0.049123293906450274, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:47, Epoch: 41, Batch: 60, Training Loss: 0.04787160232663155, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:47, Epoch: 41, Batch: 70, Training Loss: 0.02944938838481903, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:48, Epoch: 41, Batch: 80, Training Loss: 0.04646034725010395, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:49, Epoch: 41, Batch: 90, Training Loss: 0.03653225377202034, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:49, Epoch: 41, Batch: 100, Training Loss: 0.06012061685323715, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:50, Epoch: 41, Batch: 110, Training Loss: 0.03658213354647159, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:51, Epoch: 41, Batch: 120, Training Loss: 0.03557264730334282, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:52, Epoch: 41, Batch: 130, Training Loss: 0.059164627268910405, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:52, Epoch: 41, Batch: 140, Training Loss: 0.04017404653131962, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:53, Epoch: 41, Batch: 150, Training Loss: 0.0483977235853672, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:54, Epoch: 41, Batch: 160, Training Loss: 0.035940487310290335, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:54, Epoch: 41, Batch: 170, Training Loss: 0.041842589527368544, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:55, Epoch: 41, Batch: 180, Training Loss: 0.03370588608086109, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:56, Epoch: 41, Batch: 190, Training Loss: 0.031168819591403007, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:57, Epoch: 41, Batch: 200, Training Loss: 0.060747331380844115, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:57, Epoch: 41, Batch: 210, Training Loss: 0.05751065090298653, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:58, Epoch: 41, Batch: 220, Training Loss: 0.028783833235502244, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:59, Epoch: 41, Batch: 230, Training Loss: 0.05496487058699131, LR: 0.0010000000000000002
Time, 2019-01-01T17:48:59, Epoch: 41, Batch: 240, Training Loss: 0.05792182721197605, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:00, Epoch: 41, Batch: 250, Training Loss: 0.04145312272012234, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:01, Epoch: 41, Batch: 260, Training Loss: 0.04711137674748898, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:02, Epoch: 41, Batch: 270, Training Loss: 0.05468854866921902, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:02, Epoch: 41, Batch: 280, Training Loss: 0.04174363575875759, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:03, Epoch: 41, Batch: 290, Training Loss: 0.03200382590293884, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:04, Epoch: 41, Batch: 300, Training Loss: 0.043726534396409986, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:04, Epoch: 41, Batch: 310, Training Loss: 0.03224896341562271, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:05, Epoch: 41, Batch: 320, Training Loss: 0.03303281031548977, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:06, Epoch: 41, Batch: 330, Training Loss: 0.044452793151140216, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:07, Epoch: 41, Batch: 340, Training Loss: 0.05357578247785568, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:07, Epoch: 41, Batch: 350, Training Loss: 0.04303502291440964, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:08, Epoch: 41, Batch: 360, Training Loss: 0.045243522897362706, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:09, Epoch: 41, Batch: 370, Training Loss: 0.05539440959692001, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:10, Epoch: 41, Batch: 380, Training Loss: 0.03423969000577927, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:10, Epoch: 41, Batch: 390, Training Loss: 0.051194992288947104, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:11, Epoch: 41, Batch: 400, Training Loss: 0.03784570433199406, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:12, Epoch: 41, Batch: 410, Training Loss: 0.04991019442677498, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:12, Epoch: 41, Batch: 420, Training Loss: 0.04110131487250328, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:13, Epoch: 41, Batch: 430, Training Loss: 0.04447090215981007, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:14, Epoch: 41, Batch: 440, Training Loss: 0.046280573680996896, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:14, Epoch: 41, Batch: 450, Training Loss: 0.02802248001098633, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:15, Epoch: 41, Batch: 460, Training Loss: 0.05024624988436699, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:16, Epoch: 41, Batch: 470, Training Loss: 0.03966817110776901, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:17, Epoch: 41, Batch: 480, Training Loss: 0.05463002398610115, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:17, Epoch: 41, Batch: 490, Training Loss: 0.05382573641836643, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:18, Epoch: 41, Batch: 500, Training Loss: 0.03419812545180321, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:19, Epoch: 41, Batch: 510, Training Loss: 0.05989550910890103, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:19, Epoch: 41, Batch: 520, Training Loss: 0.07109687142074109, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:20, Epoch: 41, Batch: 530, Training Loss: 0.0653951846063137, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:21, Epoch: 41, Batch: 540, Training Loss: 0.039261755719780925, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:22, Epoch: 41, Batch: 550, Training Loss: 0.03464580550789833, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:22, Epoch: 41, Batch: 560, Training Loss: 0.03969733603298664, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:23, Epoch: 41, Batch: 570, Training Loss: 0.04598342701792717, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:24, Epoch: 41, Batch: 580, Training Loss: 0.03824684619903564, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:24, Epoch: 41, Batch: 590, Training Loss: 0.07126232609152794, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:25, Epoch: 41, Batch: 600, Training Loss: 0.04271391332149506, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:26, Epoch: 41, Batch: 610, Training Loss: 0.03745229803025722, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:27, Epoch: 41, Batch: 620, Training Loss: 0.053876110911369325, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:27, Epoch: 41, Batch: 630, Training Loss: 0.04989911988377571, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:28, Epoch: 41, Batch: 640, Training Loss: 0.06560051143169403, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:29, Epoch: 41, Batch: 650, Training Loss: 0.04667205587029457, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:29, Epoch: 41, Batch: 660, Training Loss: 0.04411134868860245, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:30, Epoch: 41, Batch: 670, Training Loss: 0.04278882518410683, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:31, Epoch: 41, Batch: 680, Training Loss: 0.04198053441941738, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:32, Epoch: 41, Batch: 690, Training Loss: 0.053722216933965686, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:32, Epoch: 41, Batch: 700, Training Loss: 0.050270558893680574, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:33, Epoch: 41, Batch: 710, Training Loss: 0.052261119708418846, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:34, Epoch: 41, Batch: 720, Training Loss: 0.033783718943595886, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:34, Epoch: 41, Batch: 730, Training Loss: 0.04199169091880321, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:35, Epoch: 41, Batch: 740, Training Loss: 0.04353203773498535, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:36, Epoch: 41, Batch: 750, Training Loss: 0.0431212306022644, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:37, Epoch: 41, Batch: 760, Training Loss: 0.04020966589450836, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:37, Epoch: 41, Batch: 770, Training Loss: 0.0499685175716877, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:38, Epoch: 41, Batch: 780, Training Loss: 0.04292251653969288, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:39, Epoch: 41, Batch: 790, Training Loss: 0.0340090461075306, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:39, Epoch: 41, Batch: 800, Training Loss: 0.048344825953245164, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:40, Epoch: 41, Batch: 810, Training Loss: 0.034271320700645445, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:41, Epoch: 41, Batch: 820, Training Loss: 0.052258989214897154, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:42, Epoch: 41, Batch: 830, Training Loss: 0.025242822617292403, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:42, Epoch: 41, Batch: 840, Training Loss: 0.06951897740364074, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:43, Epoch: 41, Batch: 850, Training Loss: 0.036452073603868484, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:44, Epoch: 41, Batch: 860, Training Loss: 0.06740107648074627, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:44, Epoch: 41, Batch: 870, Training Loss: 0.04540675207972526, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:45, Epoch: 41, Batch: 880, Training Loss: 0.04471170715987682, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:46, Epoch: 41, Batch: 890, Training Loss: 0.033423779904842375, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:46, Epoch: 41, Batch: 900, Training Loss: 0.043863017484545706, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:47, Epoch: 41, Batch: 910, Training Loss: 0.042435362562537195, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:48, Epoch: 41, Batch: 920, Training Loss: 0.056464217230677606, LR: 0.0010000000000000002
Time, 2019-01-01T17:49:49, Epoch: 41, Batch: 930, Training Loss: 0.056542554125189784, LR: 0.0010000000000000002
Epoch: 41, Validation Top 1 acc: 98.70402526855469
Epoch: 41, Validation Top 5 acc: 99.98834228515625
Epoch: 41, Validation Set Loss: 0.045044198632240295
Start training epoch 42
Time, 2019-01-01T17:50:15, Epoch: 42, Batch: 10, Training Loss: 0.06513720527291297, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:15, Epoch: 42, Batch: 20, Training Loss: 0.04584491439163685, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:16, Epoch: 42, Batch: 30, Training Loss: 0.05400029048323631, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:17, Epoch: 42, Batch: 40, Training Loss: 0.05566878393292427, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:17, Epoch: 42, Batch: 50, Training Loss: 0.044419555366039275, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:18, Epoch: 42, Batch: 60, Training Loss: 0.061664816737174985, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:19, Epoch: 42, Batch: 70, Training Loss: 0.036154134571552275, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:19, Epoch: 42, Batch: 80, Training Loss: 0.033428629487752916, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:20, Epoch: 42, Batch: 90, Training Loss: 0.06210160031914711, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:21, Epoch: 42, Batch: 100, Training Loss: 0.04424313940107823, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:22, Epoch: 42, Batch: 110, Training Loss: 0.05244840793311596, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:22, Epoch: 42, Batch: 120, Training Loss: 0.04295061528682709, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:23, Epoch: 42, Batch: 130, Training Loss: 0.04206020496785641, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:24, Epoch: 42, Batch: 140, Training Loss: 0.04630257822573185, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:24, Epoch: 42, Batch: 150, Training Loss: 0.03515847809612751, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:25, Epoch: 42, Batch: 160, Training Loss: 0.03594507314264774, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:26, Epoch: 42, Batch: 170, Training Loss: 0.043930789455771446, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:27, Epoch: 42, Batch: 180, Training Loss: 0.03272977545857429, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:27, Epoch: 42, Batch: 190, Training Loss: 0.0656937152147293, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:28, Epoch: 42, Batch: 200, Training Loss: 0.028507525473833083, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:29, Epoch: 42, Batch: 210, Training Loss: 0.04153962507843971, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:29, Epoch: 42, Batch: 220, Training Loss: 0.0480913482606411, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:30, Epoch: 42, Batch: 230, Training Loss: 0.043380682915449144, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:31, Epoch: 42, Batch: 240, Training Loss: 0.055499807745218274, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:32, Epoch: 42, Batch: 250, Training Loss: 0.04390972703695297, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:32, Epoch: 42, Batch: 260, Training Loss: 0.03726937621831894, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:33, Epoch: 42, Batch: 270, Training Loss: 0.03311187289655208, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:34, Epoch: 42, Batch: 280, Training Loss: 0.05437105894088745, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:34, Epoch: 42, Batch: 290, Training Loss: 0.07031775191426277, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:35, Epoch: 42, Batch: 300, Training Loss: 0.05045988857746124, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:36, Epoch: 42, Batch: 310, Training Loss: 0.04499078318476677, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:37, Epoch: 42, Batch: 320, Training Loss: 0.04344784468412399, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:37, Epoch: 42, Batch: 330, Training Loss: 0.050830159708857535, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:38, Epoch: 42, Batch: 340, Training Loss: 0.02445596382021904, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:39, Epoch: 42, Batch: 350, Training Loss: 0.03572339713573456, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:39, Epoch: 42, Batch: 360, Training Loss: 0.0499028816819191, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:40, Epoch: 42, Batch: 370, Training Loss: 0.04375295639038086, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:41, Epoch: 42, Batch: 380, Training Loss: 0.05573207288980484, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:41, Epoch: 42, Batch: 390, Training Loss: 0.048550593480467794, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:42, Epoch: 42, Batch: 400, Training Loss: 0.0410576231777668, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:43, Epoch: 42, Batch: 410, Training Loss: 0.05109127908945084, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:44, Epoch: 42, Batch: 420, Training Loss: 0.02419113405048847, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:44, Epoch: 42, Batch: 430, Training Loss: 0.07345832102000713, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:45, Epoch: 42, Batch: 440, Training Loss: 0.04977784380316734, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:46, Epoch: 42, Batch: 450, Training Loss: 0.039156275987625125, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:46, Epoch: 42, Batch: 460, Training Loss: 0.03640947490930557, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:47, Epoch: 42, Batch: 470, Training Loss: 0.05373918637633324, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:48, Epoch: 42, Batch: 480, Training Loss: 0.05017867870628834, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:49, Epoch: 42, Batch: 490, Training Loss: 0.0712827168405056, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:49, Epoch: 42, Batch: 500, Training Loss: 0.05463377274572849, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:50, Epoch: 42, Batch: 510, Training Loss: 0.05326357260346413, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:51, Epoch: 42, Batch: 520, Training Loss: 0.04288513846695423, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:51, Epoch: 42, Batch: 530, Training Loss: 0.06289219819009303, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:52, Epoch: 42, Batch: 540, Training Loss: 0.04995001405477524, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:53, Epoch: 42, Batch: 550, Training Loss: 0.02801241837441921, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:54, Epoch: 42, Batch: 560, Training Loss: 0.029997418448328973, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:54, Epoch: 42, Batch: 570, Training Loss: 0.056237442046403886, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:55, Epoch: 42, Batch: 580, Training Loss: 0.039098198711872104, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:56, Epoch: 42, Batch: 590, Training Loss: 0.05180784836411476, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:56, Epoch: 42, Batch: 600, Training Loss: 0.04387963004410267, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:57, Epoch: 42, Batch: 610, Training Loss: 0.04990165904164314, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:58, Epoch: 42, Batch: 620, Training Loss: 0.034371306747198106, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:59, Epoch: 42, Batch: 630, Training Loss: 0.030494047328829765, LR: 0.0010000000000000002
Time, 2019-01-01T17:50:59, Epoch: 42, Batch: 640, Training Loss: 0.0417819295078516, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:00, Epoch: 42, Batch: 650, Training Loss: 0.033162157982587814, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:01, Epoch: 42, Batch: 660, Training Loss: 0.04020901210606098, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:01, Epoch: 42, Batch: 670, Training Loss: 0.048646746575832366, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:02, Epoch: 42, Batch: 680, Training Loss: 0.04342132285237312, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:03, Epoch: 42, Batch: 690, Training Loss: 0.0344865720719099, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:04, Epoch: 42, Batch: 700, Training Loss: 0.05952561870217323, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:04, Epoch: 42, Batch: 710, Training Loss: 0.03679901175200939, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:05, Epoch: 42, Batch: 720, Training Loss: 0.031635066121816637, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:06, Epoch: 42, Batch: 730, Training Loss: 0.041593197733163834, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:06, Epoch: 42, Batch: 740, Training Loss: 0.041936757043004036, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:07, Epoch: 42, Batch: 750, Training Loss: 0.04674005135893822, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:08, Epoch: 42, Batch: 760, Training Loss: 0.04023400396108627, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:09, Epoch: 42, Batch: 770, Training Loss: 0.030279462039470673, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:09, Epoch: 42, Batch: 780, Training Loss: 0.057121946662664416, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:10, Epoch: 42, Batch: 790, Training Loss: 0.07480352334678173, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:11, Epoch: 42, Batch: 800, Training Loss: 0.039835406467318535, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:11, Epoch: 42, Batch: 810, Training Loss: 0.04078502506017685, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:12, Epoch: 42, Batch: 820, Training Loss: 0.040176183357834815, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:13, Epoch: 42, Batch: 830, Training Loss: 0.03719221204519272, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:13, Epoch: 42, Batch: 840, Training Loss: 0.036530470475554466, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:14, Epoch: 42, Batch: 850, Training Loss: 0.04985726624727249, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:15, Epoch: 42, Batch: 860, Training Loss: 0.03933033794164657, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:16, Epoch: 42, Batch: 870, Training Loss: 0.04057129509747028, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:16, Epoch: 42, Batch: 880, Training Loss: 0.03628170266747475, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:17, Epoch: 42, Batch: 890, Training Loss: 0.0581849567592144, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:18, Epoch: 42, Batch: 900, Training Loss: 0.036605023220181464, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:18, Epoch: 42, Batch: 910, Training Loss: 0.03019874207675457, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:19, Epoch: 42, Batch: 920, Training Loss: 0.0371132668107748, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:20, Epoch: 42, Batch: 930, Training Loss: 0.0368072085082531, LR: 0.0010000000000000002
Epoch: 42, Validation Top 1 acc: 98.73567199707031
Epoch: 42, Validation Top 5 acc: 99.9866714477539
Epoch: 42, Validation Set Loss: 0.04437436908483505
Start training epoch 43
Time, 2019-01-01T17:51:46, Epoch: 43, Batch: 10, Training Loss: 0.04787085130810738, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:47, Epoch: 43, Batch: 20, Training Loss: 0.03842310458421707, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:47, Epoch: 43, Batch: 30, Training Loss: 0.046852996572852135, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:48, Epoch: 43, Batch: 40, Training Loss: 0.03993314132094383, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:49, Epoch: 43, Batch: 50, Training Loss: 0.055152393877506256, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:50, Epoch: 43, Batch: 60, Training Loss: 0.03776210993528366, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:50, Epoch: 43, Batch: 70, Training Loss: 0.06864542067050934, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:51, Epoch: 43, Batch: 80, Training Loss: 0.0468549519777298, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:52, Epoch: 43, Batch: 90, Training Loss: 0.05404621846973896, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:52, Epoch: 43, Batch: 100, Training Loss: 0.04163832664489746, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:53, Epoch: 43, Batch: 110, Training Loss: 0.032706239819526674, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:54, Epoch: 43, Batch: 120, Training Loss: 0.0528823010623455, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:54, Epoch: 43, Batch: 130, Training Loss: 0.05567116588354111, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:55, Epoch: 43, Batch: 140, Training Loss: 0.04211612045764923, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:56, Epoch: 43, Batch: 150, Training Loss: 0.04562360271811485, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:57, Epoch: 43, Batch: 160, Training Loss: 0.03565919697284699, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:57, Epoch: 43, Batch: 170, Training Loss: 0.03967651985585689, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:58, Epoch: 43, Batch: 180, Training Loss: 0.0315162293612957, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:59, Epoch: 43, Batch: 190, Training Loss: 0.027789350226521493, LR: 0.0010000000000000002
Time, 2019-01-01T17:51:59, Epoch: 43, Batch: 200, Training Loss: 0.026692365109920502, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:00, Epoch: 43, Batch: 210, Training Loss: 0.04728536307811737, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:01, Epoch: 43, Batch: 220, Training Loss: 0.04112582802772522, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:02, Epoch: 43, Batch: 230, Training Loss: 0.028567660599946976, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:02, Epoch: 43, Batch: 240, Training Loss: 0.03710362762212753, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:03, Epoch: 43, Batch: 250, Training Loss: 0.052713052928447725, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:04, Epoch: 43, Batch: 260, Training Loss: 0.03492064028978348, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:04, Epoch: 43, Batch: 270, Training Loss: 0.048279343917965886, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:05, Epoch: 43, Batch: 280, Training Loss: 0.04416799023747444, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:06, Epoch: 43, Batch: 290, Training Loss: 0.04430316351354122, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:07, Epoch: 43, Batch: 300, Training Loss: 0.05770625174045563, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:07, Epoch: 43, Batch: 310, Training Loss: 0.029008320719003677, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:08, Epoch: 43, Batch: 320, Training Loss: 0.0507346123456955, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:09, Epoch: 43, Batch: 330, Training Loss: 0.04258486106991768, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:09, Epoch: 43, Batch: 340, Training Loss: 0.05851735882461071, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:10, Epoch: 43, Batch: 350, Training Loss: 0.048349256813526156, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:11, Epoch: 43, Batch: 360, Training Loss: 0.040535785257816315, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:11, Epoch: 43, Batch: 370, Training Loss: 0.03507661372423172, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:12, Epoch: 43, Batch: 380, Training Loss: 0.04098207540810108, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:13, Epoch: 43, Batch: 390, Training Loss: 0.05099883377552032, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:14, Epoch: 43, Batch: 400, Training Loss: 0.04259711429476738, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:14, Epoch: 43, Batch: 410, Training Loss: 0.04176789112389088, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:15, Epoch: 43, Batch: 420, Training Loss: 0.048978570848703384, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:16, Epoch: 43, Batch: 430, Training Loss: 0.04748419150710106, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:16, Epoch: 43, Batch: 440, Training Loss: 0.056305715814232826, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:17, Epoch: 43, Batch: 450, Training Loss: 0.03673507571220398, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:18, Epoch: 43, Batch: 460, Training Loss: 0.038717343285679814, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:19, Epoch: 43, Batch: 470, Training Loss: 0.04873984679579735, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:19, Epoch: 43, Batch: 480, Training Loss: 0.06070177406072617, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:20, Epoch: 43, Batch: 490, Training Loss: 0.05192495808005333, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:21, Epoch: 43, Batch: 500, Training Loss: 0.0474726215004921, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:21, Epoch: 43, Batch: 510, Training Loss: 0.036779386550188066, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:22, Epoch: 43, Batch: 520, Training Loss: 0.035987050458788875, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:23, Epoch: 43, Batch: 530, Training Loss: 0.04545605666935444, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:23, Epoch: 43, Batch: 540, Training Loss: 0.04989353865385056, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:24, Epoch: 43, Batch: 550, Training Loss: 0.047905927523970604, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:25, Epoch: 43, Batch: 560, Training Loss: 0.052168498188257216, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:26, Epoch: 43, Batch: 570, Training Loss: 0.045778614655137065, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:26, Epoch: 43, Batch: 580, Training Loss: 0.03236471377313137, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:27, Epoch: 43, Batch: 590, Training Loss: 0.06551657915115357, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:28, Epoch: 43, Batch: 600, Training Loss: 0.05277523063123226, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:28, Epoch: 43, Batch: 610, Training Loss: 0.058505437523126605, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:29, Epoch: 43, Batch: 620, Training Loss: 0.028978368639945982, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:30, Epoch: 43, Batch: 630, Training Loss: 0.03433517888188362, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:31, Epoch: 43, Batch: 640, Training Loss: 0.03321358561515808, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:31, Epoch: 43, Batch: 650, Training Loss: 0.05070260614156723, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:32, Epoch: 43, Batch: 660, Training Loss: 0.06152275092899799, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:33, Epoch: 43, Batch: 670, Training Loss: 0.06479572989046574, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:33, Epoch: 43, Batch: 680, Training Loss: 0.04187922589480877, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:34, Epoch: 43, Batch: 690, Training Loss: 0.02662271112203598, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:35, Epoch: 43, Batch: 700, Training Loss: 0.05926909036934376, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:36, Epoch: 43, Batch: 710, Training Loss: 0.045423655211925505, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:36, Epoch: 43, Batch: 720, Training Loss: 0.03555664010345936, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:37, Epoch: 43, Batch: 730, Training Loss: 0.027276503294706343, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:38, Epoch: 43, Batch: 740, Training Loss: 0.05127598457038403, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:38, Epoch: 43, Batch: 750, Training Loss: 0.03706164434552193, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:39, Epoch: 43, Batch: 760, Training Loss: 0.047397328168153764, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:40, Epoch: 43, Batch: 770, Training Loss: 0.05001834556460381, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:41, Epoch: 43, Batch: 780, Training Loss: 0.05656946077942848, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:41, Epoch: 43, Batch: 790, Training Loss: 0.022861533612012864, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:42, Epoch: 43, Batch: 800, Training Loss: 0.07892284914851189, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:43, Epoch: 43, Batch: 810, Training Loss: 0.03189673013985157, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:43, Epoch: 43, Batch: 820, Training Loss: 0.053220146149396894, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:44, Epoch: 43, Batch: 830, Training Loss: 0.045078306645154956, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:45, Epoch: 43, Batch: 840, Training Loss: 0.039799068868160245, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:46, Epoch: 43, Batch: 850, Training Loss: 0.04564202316105366, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:46, Epoch: 43, Batch: 860, Training Loss: 0.04451711103320122, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:47, Epoch: 43, Batch: 870, Training Loss: 0.05530315823853016, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:48, Epoch: 43, Batch: 880, Training Loss: 0.04944874532520771, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:48, Epoch: 43, Batch: 890, Training Loss: 0.05542765334248543, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:49, Epoch: 43, Batch: 900, Training Loss: 0.029019039869308472, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:50, Epoch: 43, Batch: 910, Training Loss: 0.03824427165091038, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:51, Epoch: 43, Batch: 920, Training Loss: 0.035651343315839766, LR: 0.0010000000000000002
Time, 2019-01-01T17:52:51, Epoch: 43, Batch: 930, Training Loss: 0.051414746791124344, LR: 0.0010000000000000002
Epoch: 43, Validation Top 1 acc: 98.76399230957031
Epoch: 43, Validation Top 5 acc: 99.99000549316406
Epoch: 43, Validation Set Loss: 0.043991848826408386
Start training epoch 44
Time, 2019-01-01T17:53:18, Epoch: 44, Batch: 10, Training Loss: 0.04734952487051487, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:19, Epoch: 44, Batch: 20, Training Loss: 0.039575047045946124, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:20, Epoch: 44, Batch: 30, Training Loss: 0.049384255707263944, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:21, Epoch: 44, Batch: 40, Training Loss: 0.04596882537007332, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:21, Epoch: 44, Batch: 50, Training Loss: 0.050384441018104555, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:22, Epoch: 44, Batch: 60, Training Loss: 0.03924223408102989, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:23, Epoch: 44, Batch: 70, Training Loss: 0.050046037137508395, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:23, Epoch: 44, Batch: 80, Training Loss: 0.05845521539449692, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:24, Epoch: 44, Batch: 90, Training Loss: 0.0341701902449131, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:25, Epoch: 44, Batch: 100, Training Loss: 0.0336927842348814, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:26, Epoch: 44, Batch: 110, Training Loss: 0.048992006480693816, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:26, Epoch: 44, Batch: 120, Training Loss: 0.04277548938989639, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:27, Epoch: 44, Batch: 130, Training Loss: 0.053518135100603104, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:28, Epoch: 44, Batch: 140, Training Loss: 0.034653539583086965, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:28, Epoch: 44, Batch: 150, Training Loss: 0.05112686306238175, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:29, Epoch: 44, Batch: 160, Training Loss: 0.028534166142344476, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:30, Epoch: 44, Batch: 170, Training Loss: 0.03507060371339321, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:31, Epoch: 44, Batch: 180, Training Loss: 0.05032269135117531, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:31, Epoch: 44, Batch: 190, Training Loss: 0.04395124167203903, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:32, Epoch: 44, Batch: 200, Training Loss: 0.05250533036887646, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:33, Epoch: 44, Batch: 210, Training Loss: 0.028757420554757118, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:33, Epoch: 44, Batch: 220, Training Loss: 0.04075361341238022, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:34, Epoch: 44, Batch: 230, Training Loss: 0.028400058299303053, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:35, Epoch: 44, Batch: 240, Training Loss: 0.03491374030709267, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:36, Epoch: 44, Batch: 250, Training Loss: 0.04400551356375217, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:36, Epoch: 44, Batch: 260, Training Loss: 0.02996065393090248, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:37, Epoch: 44, Batch: 270, Training Loss: 0.041957228630781176, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:38, Epoch: 44, Batch: 280, Training Loss: 0.05533866323530674, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:38, Epoch: 44, Batch: 290, Training Loss: 0.04035402834415436, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:39, Epoch: 44, Batch: 300, Training Loss: 0.04413189440965652, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:40, Epoch: 44, Batch: 310, Training Loss: 0.04036377929151058, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:41, Epoch: 44, Batch: 320, Training Loss: 0.045967559516429904, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:41, Epoch: 44, Batch: 330, Training Loss: 0.06180385202169418, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:42, Epoch: 44, Batch: 340, Training Loss: 0.05019426718354225, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:43, Epoch: 44, Batch: 350, Training Loss: 0.04849963039159775, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:43, Epoch: 44, Batch: 360, Training Loss: 0.06792495101690292, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:44, Epoch: 44, Batch: 370, Training Loss: 0.05798191390931606, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:45, Epoch: 44, Batch: 380, Training Loss: 0.04665953144431114, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:45, Epoch: 44, Batch: 390, Training Loss: 0.043757728114724156, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:46, Epoch: 44, Batch: 400, Training Loss: 0.027982087805867195, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:47, Epoch: 44, Batch: 410, Training Loss: 0.04772740937769413, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:48, Epoch: 44, Batch: 420, Training Loss: 0.07113757319748401, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:48, Epoch: 44, Batch: 430, Training Loss: 0.04671858288347721, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:49, Epoch: 44, Batch: 440, Training Loss: 0.033759433031082156, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:50, Epoch: 44, Batch: 450, Training Loss: 0.02692817375063896, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:51, Epoch: 44, Batch: 460, Training Loss: 0.038461381942033766, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:51, Epoch: 44, Batch: 470, Training Loss: 0.07909966520965099, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:52, Epoch: 44, Batch: 480, Training Loss: 0.03671133331954479, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:53, Epoch: 44, Batch: 490, Training Loss: 0.03410949930548668, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:53, Epoch: 44, Batch: 500, Training Loss: 0.04495542086660862, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:54, Epoch: 44, Batch: 510, Training Loss: 0.05933557562530041, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:55, Epoch: 44, Batch: 520, Training Loss: 0.04101482257246971, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:55, Epoch: 44, Batch: 530, Training Loss: 0.046371424198150636, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:56, Epoch: 44, Batch: 540, Training Loss: 0.043631752207875255, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:57, Epoch: 44, Batch: 550, Training Loss: 0.03809618763625622, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:58, Epoch: 44, Batch: 560, Training Loss: 0.051473052427172664, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:58, Epoch: 44, Batch: 570, Training Loss: 0.05662322193384171, LR: 0.0010000000000000002
Time, 2019-01-01T17:53:59, Epoch: 44, Batch: 580, Training Loss: 0.025997093319892882, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:00, Epoch: 44, Batch: 590, Training Loss: 0.0459560327231884, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:00, Epoch: 44, Batch: 600, Training Loss: 0.03556890785694122, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:01, Epoch: 44, Batch: 610, Training Loss: 0.04934979453682899, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:02, Epoch: 44, Batch: 620, Training Loss: 0.038236741349101065, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:03, Epoch: 44, Batch: 630, Training Loss: 0.040825673192739484, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:03, Epoch: 44, Batch: 640, Training Loss: 0.050238491594791414, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:04, Epoch: 44, Batch: 650, Training Loss: 0.06150476336479187, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:05, Epoch: 44, Batch: 660, Training Loss: 0.04390961788594723, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:05, Epoch: 44, Batch: 670, Training Loss: 0.032125597819685936, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:06, Epoch: 44, Batch: 680, Training Loss: 0.048139221221208575, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:07, Epoch: 44, Batch: 690, Training Loss: 0.04872894287109375, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:08, Epoch: 44, Batch: 700, Training Loss: 0.03640008866786957, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:08, Epoch: 44, Batch: 710, Training Loss: 0.04154246374964714, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:09, Epoch: 44, Batch: 720, Training Loss: 0.03604878038167954, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:10, Epoch: 44, Batch: 730, Training Loss: 0.03483011908829212, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:10, Epoch: 44, Batch: 740, Training Loss: 0.04850127249956131, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:11, Epoch: 44, Batch: 750, Training Loss: 0.04865834824740887, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:12, Epoch: 44, Batch: 760, Training Loss: 0.05212586037814617, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:12, Epoch: 44, Batch: 770, Training Loss: 0.0301859512925148, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:13, Epoch: 44, Batch: 780, Training Loss: 0.04114545583724975, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:14, Epoch: 44, Batch: 790, Training Loss: 0.05429271534085274, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:15, Epoch: 44, Batch: 800, Training Loss: 0.04401840530335903, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:15, Epoch: 44, Batch: 810, Training Loss: 0.04242113083600998, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:16, Epoch: 44, Batch: 820, Training Loss: 0.04200419820845127, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:17, Epoch: 44, Batch: 830, Training Loss: 0.03443711139261722, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:17, Epoch: 44, Batch: 840, Training Loss: 0.05451839715242386, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:18, Epoch: 44, Batch: 850, Training Loss: 0.07498865723609924, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:19, Epoch: 44, Batch: 860, Training Loss: 0.0336438037455082, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:20, Epoch: 44, Batch: 870, Training Loss: 0.060875510796904564, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:20, Epoch: 44, Batch: 880, Training Loss: 0.03856616728007793, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:21, Epoch: 44, Batch: 890, Training Loss: 0.030836907029151917, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:22, Epoch: 44, Batch: 900, Training Loss: 0.044215400144457816, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:22, Epoch: 44, Batch: 910, Training Loss: 0.05898984223604202, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:23, Epoch: 44, Batch: 920, Training Loss: 0.03628302253782749, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:24, Epoch: 44, Batch: 930, Training Loss: 0.05117238350212574, LR: 0.0010000000000000002
Epoch: 44, Validation Top 1 acc: 98.73734283447266
Epoch: 44, Validation Top 5 acc: 99.99000549316406
Epoch: 44, Validation Set Loss: 0.044425494968891144
Start training epoch 45
Time, 2019-01-01T17:54:51, Epoch: 45, Batch: 10, Training Loss: 0.05795508623123169, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:52, Epoch: 45, Batch: 20, Training Loss: 0.04571076110005379, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:53, Epoch: 45, Batch: 30, Training Loss: 0.04056489542126655, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:54, Epoch: 45, Batch: 40, Training Loss: 0.02563006989657879, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:54, Epoch: 45, Batch: 50, Training Loss: 0.038920970633625984, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:55, Epoch: 45, Batch: 60, Training Loss: 0.036524205654859546, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:56, Epoch: 45, Batch: 70, Training Loss: 0.05854525119066238, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:56, Epoch: 45, Batch: 80, Training Loss: 0.05816525481641292, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:57, Epoch: 45, Batch: 90, Training Loss: 0.051093130558729175, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:58, Epoch: 45, Batch: 100, Training Loss: 0.03476982973515987, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:59, Epoch: 45, Batch: 110, Training Loss: 0.04089335985481739, LR: 0.0010000000000000002
Time, 2019-01-01T17:54:59, Epoch: 45, Batch: 120, Training Loss: 0.050590145587921145, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:00, Epoch: 45, Batch: 130, Training Loss: 0.05182577148079872, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:01, Epoch: 45, Batch: 140, Training Loss: 0.05199128016829491, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:01, Epoch: 45, Batch: 150, Training Loss: 0.0429614245891571, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:02, Epoch: 45, Batch: 160, Training Loss: 0.04205298349261284, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:03, Epoch: 45, Batch: 170, Training Loss: 0.06280463896691799, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:04, Epoch: 45, Batch: 180, Training Loss: 0.030825465545058252, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:04, Epoch: 45, Batch: 190, Training Loss: 0.04213760644197464, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:05, Epoch: 45, Batch: 200, Training Loss: 0.04420533776283264, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:06, Epoch: 45, Batch: 210, Training Loss: 0.059172336384654044, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:06, Epoch: 45, Batch: 220, Training Loss: 0.03670255951583386, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:07, Epoch: 45, Batch: 230, Training Loss: 0.06482507660984993, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:08, Epoch: 45, Batch: 240, Training Loss: 0.04511408731341362, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:08, Epoch: 45, Batch: 250, Training Loss: 0.04208686724305153, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:09, Epoch: 45, Batch: 260, Training Loss: 0.03646600320935249, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:10, Epoch: 45, Batch: 270, Training Loss: 0.034318267554044726, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:11, Epoch: 45, Batch: 280, Training Loss: 0.03607969805598259, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:11, Epoch: 45, Batch: 290, Training Loss: 0.04748307578265667, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:12, Epoch: 45, Batch: 300, Training Loss: 0.05043493136763573, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:13, Epoch: 45, Batch: 310, Training Loss: 0.04988653361797333, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:13, Epoch: 45, Batch: 320, Training Loss: 0.05414330139756203, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:14, Epoch: 45, Batch: 330, Training Loss: 0.034546329826116565, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:15, Epoch: 45, Batch: 340, Training Loss: 0.06494005098938942, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:16, Epoch: 45, Batch: 350, Training Loss: 0.04242235459387302, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:16, Epoch: 45, Batch: 360, Training Loss: 0.04029653519392014, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:17, Epoch: 45, Batch: 370, Training Loss: 0.03211793638765812, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:18, Epoch: 45, Batch: 380, Training Loss: 0.04445124119520187, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:18, Epoch: 45, Batch: 390, Training Loss: 0.041685326769948, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:19, Epoch: 45, Batch: 400, Training Loss: 0.054292450100183486, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:20, Epoch: 45, Batch: 410, Training Loss: 0.05539514608681202, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:21, Epoch: 45, Batch: 420, Training Loss: 0.052125044912099835, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:21, Epoch: 45, Batch: 430, Training Loss: 0.06722849681973457, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:22, Epoch: 45, Batch: 440, Training Loss: 0.034184093773365024, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:23, Epoch: 45, Batch: 450, Training Loss: 0.03982574418187142, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:23, Epoch: 45, Batch: 460, Training Loss: 0.06673985123634338, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:24, Epoch: 45, Batch: 470, Training Loss: 0.05594482868909836, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:25, Epoch: 45, Batch: 480, Training Loss: 0.024248625710606576, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:26, Epoch: 45, Batch: 490, Training Loss: 0.043754706531763075, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:26, Epoch: 45, Batch: 500, Training Loss: 0.04270817115902901, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:27, Epoch: 45, Batch: 510, Training Loss: 0.06247956082224846, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:28, Epoch: 45, Batch: 520, Training Loss: 0.037847715243697164, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:28, Epoch: 45, Batch: 530, Training Loss: 0.03366918042302132, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:29, Epoch: 45, Batch: 540, Training Loss: 0.02909090667963028, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:30, Epoch: 45, Batch: 550, Training Loss: 0.047692034393548965, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:31, Epoch: 45, Batch: 560, Training Loss: 0.03271579816937446, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:31, Epoch: 45, Batch: 570, Training Loss: 0.050808065384626386, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:32, Epoch: 45, Batch: 580, Training Loss: 0.03360726833343506, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:33, Epoch: 45, Batch: 590, Training Loss: 0.03253455422818661, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:33, Epoch: 45, Batch: 600, Training Loss: 0.042704254016280176, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:34, Epoch: 45, Batch: 610, Training Loss: 0.0373245008289814, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:35, Epoch: 45, Batch: 620, Training Loss: 0.025157342106103896, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:36, Epoch: 45, Batch: 630, Training Loss: 0.04122177436947823, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:36, Epoch: 45, Batch: 640, Training Loss: 0.031400871276855466, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:37, Epoch: 45, Batch: 650, Training Loss: 0.03331635408103466, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:38, Epoch: 45, Batch: 660, Training Loss: 0.03458800092339516, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:38, Epoch: 45, Batch: 670, Training Loss: 0.03646214753389358, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:39, Epoch: 45, Batch: 680, Training Loss: 0.03716419860720634, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:40, Epoch: 45, Batch: 690, Training Loss: 0.05445243120193481, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:41, Epoch: 45, Batch: 700, Training Loss: 0.045078623294830325, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:41, Epoch: 45, Batch: 710, Training Loss: 0.03905775435268879, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:42, Epoch: 45, Batch: 720, Training Loss: 0.05343886613845825, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:43, Epoch: 45, Batch: 730, Training Loss: 0.04398823827505112, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:43, Epoch: 45, Batch: 740, Training Loss: 0.02932953052222729, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:44, Epoch: 45, Batch: 750, Training Loss: 0.05618074089288712, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:45, Epoch: 45, Batch: 760, Training Loss: 0.04742245301604271, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:45, Epoch: 45, Batch: 770, Training Loss: 0.04786401093006134, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:46, Epoch: 45, Batch: 780, Training Loss: 0.0736069031059742, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:47, Epoch: 45, Batch: 790, Training Loss: 0.0484764251857996, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:48, Epoch: 45, Batch: 800, Training Loss: 0.05549466609954834, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:48, Epoch: 45, Batch: 810, Training Loss: 0.05665822401642799, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:49, Epoch: 45, Batch: 820, Training Loss: 0.04196377731859684, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:50, Epoch: 45, Batch: 830, Training Loss: 0.03672374337911606, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:50, Epoch: 45, Batch: 840, Training Loss: 0.07155380696058274, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:51, Epoch: 45, Batch: 850, Training Loss: 0.054610312730073926, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:52, Epoch: 45, Batch: 860, Training Loss: 0.03047936223447323, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:53, Epoch: 45, Batch: 870, Training Loss: 0.040012648329138756, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:53, Epoch: 45, Batch: 880, Training Loss: 0.03852278999984264, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:54, Epoch: 45, Batch: 890, Training Loss: 0.052100279927253725, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:55, Epoch: 45, Batch: 900, Training Loss: 0.034801306948065755, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:55, Epoch: 45, Batch: 910, Training Loss: 0.031246230378746985, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:56, Epoch: 45, Batch: 920, Training Loss: 0.04175322875380516, LR: 0.0010000000000000002
Time, 2019-01-01T17:55:57, Epoch: 45, Batch: 930, Training Loss: 0.046239360421895984, LR: 0.0010000000000000002
Epoch: 45, Validation Top 1 acc: 98.75399780273438
Epoch: 45, Validation Top 5 acc: 99.99000549316406
Epoch: 45, Validation Set Loss: 0.04371370002627373
Start training epoch 46
Time, 2019-01-01T17:56:24, Epoch: 46, Batch: 10, Training Loss: 0.03603019416332245, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:25, Epoch: 46, Batch: 20, Training Loss: 0.05320638045668602, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:26, Epoch: 46, Batch: 30, Training Loss: 0.05719212219119072, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:27, Epoch: 46, Batch: 40, Training Loss: 0.03760917223989964, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:27, Epoch: 46, Batch: 50, Training Loss: 0.05505307614803314, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:28, Epoch: 46, Batch: 60, Training Loss: 0.027400368452072145, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:29, Epoch: 46, Batch: 70, Training Loss: 0.03421579673886299, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:29, Epoch: 46, Batch: 80, Training Loss: 0.04309476725757122, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:30, Epoch: 46, Batch: 90, Training Loss: 0.046955662593245504, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:31, Epoch: 46, Batch: 100, Training Loss: 0.022045373916625977, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:32, Epoch: 46, Batch: 110, Training Loss: 0.04592254385352135, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:32, Epoch: 46, Batch: 120, Training Loss: 0.0476658433675766, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:33, Epoch: 46, Batch: 130, Training Loss: 0.055439168214797975, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:34, Epoch: 46, Batch: 140, Training Loss: 0.04148120954632759, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:34, Epoch: 46, Batch: 150, Training Loss: 0.023330343514680864, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:35, Epoch: 46, Batch: 160, Training Loss: 0.03685381412506104, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:36, Epoch: 46, Batch: 170, Training Loss: 0.02878260761499405, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:37, Epoch: 46, Batch: 180, Training Loss: 0.035150205716490746, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:37, Epoch: 46, Batch: 190, Training Loss: 0.043816541135311124, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:38, Epoch: 46, Batch: 200, Training Loss: 0.038209864497184755, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:39, Epoch: 46, Batch: 210, Training Loss: 0.038374380022287366, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:39, Epoch: 46, Batch: 220, Training Loss: 0.037054268270730974, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:40, Epoch: 46, Batch: 230, Training Loss: 0.03710246682167053, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:41, Epoch: 46, Batch: 240, Training Loss: 0.03113463893532753, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:42, Epoch: 46, Batch: 250, Training Loss: 0.04901595264673233, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:42, Epoch: 46, Batch: 260, Training Loss: 0.05049322843551636, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:43, Epoch: 46, Batch: 270, Training Loss: 0.035803922638297084, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:44, Epoch: 46, Batch: 280, Training Loss: 0.02854766733944416, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:44, Epoch: 46, Batch: 290, Training Loss: 0.03768361136317253, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:45, Epoch: 46, Batch: 300, Training Loss: 0.04398389607667923, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:46, Epoch: 46, Batch: 310, Training Loss: 0.026983503252267838, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:47, Epoch: 46, Batch: 320, Training Loss: 0.050083696469664575, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:47, Epoch: 46, Batch: 330, Training Loss: 0.060211385041475295, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:48, Epoch: 46, Batch: 340, Training Loss: 0.029153361916542053, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:49, Epoch: 46, Batch: 350, Training Loss: 0.04916203171014786, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:49, Epoch: 46, Batch: 360, Training Loss: 0.04291716404259205, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:50, Epoch: 46, Batch: 370, Training Loss: 0.057773060724139215, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:51, Epoch: 46, Batch: 380, Training Loss: 0.056300320848822596, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:52, Epoch: 46, Batch: 390, Training Loss: 0.06102861315011978, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:52, Epoch: 46, Batch: 400, Training Loss: 0.04360901042819023, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:53, Epoch: 46, Batch: 410, Training Loss: 0.03409261964261532, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:54, Epoch: 46, Batch: 420, Training Loss: 0.042602095380425455, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:54, Epoch: 46, Batch: 430, Training Loss: 0.0240932647138834, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:55, Epoch: 46, Batch: 440, Training Loss: 0.0451796468347311, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:56, Epoch: 46, Batch: 450, Training Loss: 0.048059338331222536, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:57, Epoch: 46, Batch: 460, Training Loss: 0.040863136574625966, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:57, Epoch: 46, Batch: 470, Training Loss: 0.03308870270848274, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:58, Epoch: 46, Batch: 480, Training Loss: 0.04504659026861191, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:59, Epoch: 46, Batch: 490, Training Loss: 0.047640680149197576, LR: 0.0010000000000000002
Time, 2019-01-01T17:56:59, Epoch: 46, Batch: 500, Training Loss: 0.0645283080637455, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:00, Epoch: 46, Batch: 510, Training Loss: 0.04857112318277359, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:01, Epoch: 46, Batch: 520, Training Loss: 0.04688514769077301, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:02, Epoch: 46, Batch: 530, Training Loss: 0.054468194022774696, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:02, Epoch: 46, Batch: 540, Training Loss: 0.053436288982629775, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:03, Epoch: 46, Batch: 550, Training Loss: 0.04911592192947865, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:04, Epoch: 46, Batch: 560, Training Loss: 0.06693303287029266, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:04, Epoch: 46, Batch: 570, Training Loss: 0.03971329405903816, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:05, Epoch: 46, Batch: 580, Training Loss: 0.031108467280864714, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:06, Epoch: 46, Batch: 590, Training Loss: 0.04157847985625267, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:07, Epoch: 46, Batch: 600, Training Loss: 0.06657743528485298, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:07, Epoch: 46, Batch: 610, Training Loss: 0.05190012864768505, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:08, Epoch: 46, Batch: 620, Training Loss: 0.052797339111566546, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:09, Epoch: 46, Batch: 630, Training Loss: 0.05207796692848206, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:09, Epoch: 46, Batch: 640, Training Loss: 0.04294399060308933, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:10, Epoch: 46, Batch: 650, Training Loss: 0.05291873961687088, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:11, Epoch: 46, Batch: 660, Training Loss: 0.058721382170915604, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:12, Epoch: 46, Batch: 670, Training Loss: 0.05714317969977856, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:12, Epoch: 46, Batch: 680, Training Loss: 0.047619884461164476, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:13, Epoch: 46, Batch: 690, Training Loss: 0.04208477064967155, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:14, Epoch: 46, Batch: 700, Training Loss: 0.03245908841490745, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:14, Epoch: 46, Batch: 710, Training Loss: 0.032571923732757566, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:15, Epoch: 46, Batch: 720, Training Loss: 0.03847193382680416, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:16, Epoch: 46, Batch: 730, Training Loss: 0.032223141938447955, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:17, Epoch: 46, Batch: 740, Training Loss: 0.04655401036143303, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:17, Epoch: 46, Batch: 750, Training Loss: 0.04083961248397827, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:18, Epoch: 46, Batch: 760, Training Loss: 0.05706333369016647, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:19, Epoch: 46, Batch: 770, Training Loss: 0.0450290497392416, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:19, Epoch: 46, Batch: 780, Training Loss: 0.06293955817818642, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:20, Epoch: 46, Batch: 790, Training Loss: 0.05010688528418541, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:21, Epoch: 46, Batch: 800, Training Loss: 0.06200363971292973, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:22, Epoch: 46, Batch: 810, Training Loss: 0.04429212361574173, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:22, Epoch: 46, Batch: 820, Training Loss: 0.06056825518608093, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:23, Epoch: 46, Batch: 830, Training Loss: 0.037536438554525375, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:24, Epoch: 46, Batch: 840, Training Loss: 0.044745372235774995, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:24, Epoch: 46, Batch: 850, Training Loss: 0.046703284978866576, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:25, Epoch: 46, Batch: 860, Training Loss: 0.05544599890708923, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:26, Epoch: 46, Batch: 870, Training Loss: 0.031751963496208194, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:27, Epoch: 46, Batch: 880, Training Loss: 0.05609586760401726, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:27, Epoch: 46, Batch: 890, Training Loss: 0.045602965354919436, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:28, Epoch: 46, Batch: 900, Training Loss: 0.040855060517787936, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:29, Epoch: 46, Batch: 910, Training Loss: 0.03400654643774033, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:29, Epoch: 46, Batch: 920, Training Loss: 0.0290089875459671, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:30, Epoch: 46, Batch: 930, Training Loss: 0.04099165126681328, LR: 0.0010000000000000002
Epoch: 46, Validation Top 1 acc: 98.79397583007812
Epoch: 46, Validation Top 5 acc: 99.9850082397461
Epoch: 46, Validation Set Loss: 0.04360228404402733
Start training epoch 47
Time, 2019-01-01T17:57:58, Epoch: 47, Batch: 10, Training Loss: 0.0464320607483387, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:58, Epoch: 47, Batch: 20, Training Loss: 0.03126810193061828, LR: 0.0010000000000000002
Time, 2019-01-01T17:57:59, Epoch: 47, Batch: 30, Training Loss: 0.04854582883417606, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:00, Epoch: 47, Batch: 40, Training Loss: 0.06611740402877331, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:01, Epoch: 47, Batch: 50, Training Loss: 0.04135565236210823, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:01, Epoch: 47, Batch: 60, Training Loss: 0.03594155833125114, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:02, Epoch: 47, Batch: 70, Training Loss: 0.03990973681211472, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:03, Epoch: 47, Batch: 80, Training Loss: 0.04386961683630943, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:03, Epoch: 47, Batch: 90, Training Loss: 0.051346958801150325, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:04, Epoch: 47, Batch: 100, Training Loss: 0.05590241402387619, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:05, Epoch: 47, Batch: 110, Training Loss: 0.04423026368021965, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:06, Epoch: 47, Batch: 120, Training Loss: 0.04655010998249054, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:06, Epoch: 47, Batch: 130, Training Loss: 0.041328664124011996, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:07, Epoch: 47, Batch: 140, Training Loss: 0.04600031785666943, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:08, Epoch: 47, Batch: 150, Training Loss: 0.053038159385323524, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:08, Epoch: 47, Batch: 160, Training Loss: 0.03850286528468132, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:09, Epoch: 47, Batch: 170, Training Loss: 0.05681069083511829, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:10, Epoch: 47, Batch: 180, Training Loss: 0.04878176040947437, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:11, Epoch: 47, Batch: 190, Training Loss: 0.042287853360176084, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:11, Epoch: 47, Batch: 200, Training Loss: 0.050221913307905194, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:12, Epoch: 47, Batch: 210, Training Loss: 0.04377907812595368, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:13, Epoch: 47, Batch: 220, Training Loss: 0.05377073548734188, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:13, Epoch: 47, Batch: 230, Training Loss: 0.039965855330228804, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:14, Epoch: 47, Batch: 240, Training Loss: 0.050666723772883414, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:15, Epoch: 47, Batch: 250, Training Loss: 0.041915863007307055, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:16, Epoch: 47, Batch: 260, Training Loss: 0.03965151160955429, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:16, Epoch: 47, Batch: 270, Training Loss: 0.04523927867412567, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:17, Epoch: 47, Batch: 280, Training Loss: 0.0505282461643219, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:18, Epoch: 47, Batch: 290, Training Loss: 0.047614121437072755, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:18, Epoch: 47, Batch: 300, Training Loss: 0.048907724767923356, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:19, Epoch: 47, Batch: 310, Training Loss: 0.06149855703115463, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:20, Epoch: 47, Batch: 320, Training Loss: 0.02448773831129074, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:21, Epoch: 47, Batch: 330, Training Loss: 0.03362885750830173, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:21, Epoch: 47, Batch: 340, Training Loss: 0.04724043793976307, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:22, Epoch: 47, Batch: 350, Training Loss: 0.03686233386397362, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:23, Epoch: 47, Batch: 360, Training Loss: 0.052942633256316186, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:23, Epoch: 47, Batch: 370, Training Loss: 0.0373751949518919, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:24, Epoch: 47, Batch: 380, Training Loss: 0.034501389414072034, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:25, Epoch: 47, Batch: 390, Training Loss: 0.04070679061114788, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:26, Epoch: 47, Batch: 400, Training Loss: 0.033505451679229734, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:26, Epoch: 47, Batch: 410, Training Loss: 0.05583206973969936, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:27, Epoch: 47, Batch: 420, Training Loss: 0.06386281363666058, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:28, Epoch: 47, Batch: 430, Training Loss: 0.03727196492254734, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:28, Epoch: 47, Batch: 440, Training Loss: 0.0329764723777771, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:29, Epoch: 47, Batch: 450, Training Loss: 0.04541076458990574, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:30, Epoch: 47, Batch: 460, Training Loss: 0.044731954112648964, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:31, Epoch: 47, Batch: 470, Training Loss: 0.05462774932384491, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:31, Epoch: 47, Batch: 480, Training Loss: 0.036242594569921495, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:32, Epoch: 47, Batch: 490, Training Loss: 0.04640397168695927, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:33, Epoch: 47, Batch: 500, Training Loss: 0.039429879188537596, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:33, Epoch: 47, Batch: 510, Training Loss: 0.05398370884358883, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:34, Epoch: 47, Batch: 520, Training Loss: 0.05248788073658943, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:35, Epoch: 47, Batch: 530, Training Loss: 0.05403517745435238, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:35, Epoch: 47, Batch: 540, Training Loss: 0.038628542050719264, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:36, Epoch: 47, Batch: 550, Training Loss: 0.04078167974948883, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:37, Epoch: 47, Batch: 560, Training Loss: 0.043785836175084115, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:38, Epoch: 47, Batch: 570, Training Loss: 0.03491444922983646, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:38, Epoch: 47, Batch: 580, Training Loss: 0.0420067735016346, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:39, Epoch: 47, Batch: 590, Training Loss: 0.039735909178853036, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:40, Epoch: 47, Batch: 600, Training Loss: 0.04872536063194275, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:41, Epoch: 47, Batch: 610, Training Loss: 0.06386708468198776, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:41, Epoch: 47, Batch: 620, Training Loss: 0.03346326947212219, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:42, Epoch: 47, Batch: 630, Training Loss: 0.03759334124624729, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:43, Epoch: 47, Batch: 640, Training Loss: 0.03845089115202427, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:43, Epoch: 47, Batch: 650, Training Loss: 0.07908856682479382, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:44, Epoch: 47, Batch: 660, Training Loss: 0.05728519484400749, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:45, Epoch: 47, Batch: 670, Training Loss: 0.0436043918132782, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:45, Epoch: 47, Batch: 680, Training Loss: 0.02019844055175781, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:46, Epoch: 47, Batch: 690, Training Loss: 0.04285895451903343, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:47, Epoch: 47, Batch: 700, Training Loss: 0.05700117126107216, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:48, Epoch: 47, Batch: 710, Training Loss: 0.039626491442322734, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:48, Epoch: 47, Batch: 720, Training Loss: 0.04806897044181824, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:49, Epoch: 47, Batch: 730, Training Loss: 0.034348563849925996, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:50, Epoch: 47, Batch: 740, Training Loss: 0.03775231763720512, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:50, Epoch: 47, Batch: 750, Training Loss: 0.04738558530807495, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:51, Epoch: 47, Batch: 760, Training Loss: 0.04787509441375733, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:52, Epoch: 47, Batch: 770, Training Loss: 0.05541553571820259, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:53, Epoch: 47, Batch: 780, Training Loss: 0.03911629654467106, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:53, Epoch: 47, Batch: 790, Training Loss: 0.05029776096343994, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:54, Epoch: 47, Batch: 800, Training Loss: 0.054067802429199216, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:55, Epoch: 47, Batch: 810, Training Loss: 0.057085764780640605, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:55, Epoch: 47, Batch: 820, Training Loss: 0.030069100856781005, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:56, Epoch: 47, Batch: 830, Training Loss: 0.03554918579757214, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:57, Epoch: 47, Batch: 840, Training Loss: 0.03622329756617546, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:58, Epoch: 47, Batch: 850, Training Loss: 0.03617963343858719, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:58, Epoch: 47, Batch: 860, Training Loss: 0.04286687523126602, LR: 0.0010000000000000002
Time, 2019-01-01T17:58:59, Epoch: 47, Batch: 870, Training Loss: 0.042956256121397016, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:00, Epoch: 47, Batch: 880, Training Loss: 0.03129613846540451, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:01, Epoch: 47, Batch: 890, Training Loss: 0.0315535344183445, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:01, Epoch: 47, Batch: 900, Training Loss: 0.038423949480056764, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:02, Epoch: 47, Batch: 910, Training Loss: 0.032588642463088034, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:03, Epoch: 47, Batch: 920, Training Loss: 0.0418328158557415, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:03, Epoch: 47, Batch: 930, Training Loss: 0.03448944911360741, LR: 0.0010000000000000002
Epoch: 47, Validation Top 1 acc: 98.78731536865234
Epoch: 47, Validation Top 5 acc: 99.98834228515625
Epoch: 47, Validation Set Loss: 0.04331686347723007
Start training epoch 48
Time, 2019-01-01T17:59:31, Epoch: 48, Batch: 10, Training Loss: 0.03628293797373772, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:32, Epoch: 48, Batch: 20, Training Loss: 0.04062314927577972, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:32, Epoch: 48, Batch: 30, Training Loss: 0.026923113316297532, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:33, Epoch: 48, Batch: 40, Training Loss: 0.03955349549651146, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:34, Epoch: 48, Batch: 50, Training Loss: 0.041997164860367774, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:34, Epoch: 48, Batch: 60, Training Loss: 0.03974941372871399, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:35, Epoch: 48, Batch: 70, Training Loss: 0.0409582931548357, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:36, Epoch: 48, Batch: 80, Training Loss: 0.05479041114449501, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:37, Epoch: 48, Batch: 90, Training Loss: 0.04739720076322555, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:37, Epoch: 48, Batch: 100, Training Loss: 0.04838927835226059, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:38, Epoch: 48, Batch: 110, Training Loss: 0.040804768726229665, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:39, Epoch: 48, Batch: 120, Training Loss: 0.04022998362779617, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:39, Epoch: 48, Batch: 130, Training Loss: 0.04234223030507565, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:40, Epoch: 48, Batch: 140, Training Loss: 0.04838343858718872, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:41, Epoch: 48, Batch: 150, Training Loss: 0.04699736051261425, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:42, Epoch: 48, Batch: 160, Training Loss: 0.04986986666917801, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:42, Epoch: 48, Batch: 170, Training Loss: 0.03816592209041118, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:43, Epoch: 48, Batch: 180, Training Loss: 0.040563632547855374, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:44, Epoch: 48, Batch: 190, Training Loss: 0.03759531080722809, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:45, Epoch: 48, Batch: 200, Training Loss: 0.03509869165718556, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:45, Epoch: 48, Batch: 210, Training Loss: 0.033208965137600896, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:46, Epoch: 48, Batch: 220, Training Loss: 0.05053030364215374, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:47, Epoch: 48, Batch: 230, Training Loss: 0.03599572628736496, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:47, Epoch: 48, Batch: 240, Training Loss: 0.03877759724855423, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:48, Epoch: 48, Batch: 250, Training Loss: 0.031169721111655235, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:49, Epoch: 48, Batch: 260, Training Loss: 0.023700107634067536, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:50, Epoch: 48, Batch: 270, Training Loss: 0.07416925430297852, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:50, Epoch: 48, Batch: 280, Training Loss: 0.039056165516376494, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:51, Epoch: 48, Batch: 290, Training Loss: 0.05074063874781132, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:52, Epoch: 48, Batch: 300, Training Loss: 0.035033281147480014, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:52, Epoch: 48, Batch: 310, Training Loss: 0.04043492302298546, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:53, Epoch: 48, Batch: 320, Training Loss: 0.06492593213915825, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:54, Epoch: 48, Batch: 330, Training Loss: 0.03761307448148728, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:55, Epoch: 48, Batch: 340, Training Loss: 0.04793607890605926, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:55, Epoch: 48, Batch: 350, Training Loss: 0.03357128985226154, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:56, Epoch: 48, Batch: 360, Training Loss: 0.06516059190034866, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:57, Epoch: 48, Batch: 370, Training Loss: 0.04086719155311584, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:57, Epoch: 48, Batch: 380, Training Loss: 0.052184031903743745, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:58, Epoch: 48, Batch: 390, Training Loss: 0.02829032838344574, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:59, Epoch: 48, Batch: 400, Training Loss: 0.045703768730163574, LR: 0.0010000000000000002
Time, 2019-01-01T17:59:59, Epoch: 48, Batch: 410, Training Loss: 0.03750026449561119, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:00, Epoch: 48, Batch: 420, Training Loss: 0.05691632255911827, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:01, Epoch: 48, Batch: 430, Training Loss: 0.04458671025931835, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:02, Epoch: 48, Batch: 440, Training Loss: 0.03930916115641594, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:02, Epoch: 48, Batch: 450, Training Loss: 0.04091303013265133, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:03, Epoch: 48, Batch: 460, Training Loss: 0.042708954960107806, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:04, Epoch: 48, Batch: 470, Training Loss: 0.043119965493679045, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:05, Epoch: 48, Batch: 480, Training Loss: 0.0353843092918396, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:05, Epoch: 48, Batch: 490, Training Loss: 0.04341392889618874, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:06, Epoch: 48, Batch: 500, Training Loss: 0.03293134793639183, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:07, Epoch: 48, Batch: 510, Training Loss: 0.030009818077087403, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:07, Epoch: 48, Batch: 520, Training Loss: 0.03169901855289936, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:08, Epoch: 48, Batch: 530, Training Loss: 0.04053182899951935, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:09, Epoch: 48, Batch: 540, Training Loss: 0.057290487736463544, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:09, Epoch: 48, Batch: 550, Training Loss: 0.03128093406558037, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:10, Epoch: 48, Batch: 560, Training Loss: 0.027485884726047516, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:11, Epoch: 48, Batch: 570, Training Loss: 0.04192416109144688, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:12, Epoch: 48, Batch: 580, Training Loss: 0.03761313259601593, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:12, Epoch: 48, Batch: 590, Training Loss: 0.04251682683825493, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:13, Epoch: 48, Batch: 600, Training Loss: 0.027217355370521546, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:14, Epoch: 48, Batch: 610, Training Loss: 0.04210142754018307, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:14, Epoch: 48, Batch: 620, Training Loss: 0.044238222762942314, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:15, Epoch: 48, Batch: 630, Training Loss: 0.046070121601223944, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:16, Epoch: 48, Batch: 640, Training Loss: 0.05652773268520832, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:17, Epoch: 48, Batch: 650, Training Loss: 0.05266415327787399, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:17, Epoch: 48, Batch: 660, Training Loss: 0.05230138786137104, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:18, Epoch: 48, Batch: 670, Training Loss: 0.03000120371580124, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:19, Epoch: 48, Batch: 680, Training Loss: 0.037475017458200456, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:19, Epoch: 48, Batch: 690, Training Loss: 0.032248389720916745, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:20, Epoch: 48, Batch: 700, Training Loss: 0.06521790735423565, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:21, Epoch: 48, Batch: 710, Training Loss: 0.05719132497906685, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:22, Epoch: 48, Batch: 720, Training Loss: 0.0222737118601799, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:22, Epoch: 48, Batch: 730, Training Loss: 0.04755244515836239, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:23, Epoch: 48, Batch: 740, Training Loss: 0.0534048069268465, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:24, Epoch: 48, Batch: 750, Training Loss: 0.03867846205830574, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:24, Epoch: 48, Batch: 760, Training Loss: 0.045520779117941855, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:25, Epoch: 48, Batch: 770, Training Loss: 0.050938048586249354, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:26, Epoch: 48, Batch: 780, Training Loss: 0.057306070998311046, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:27, Epoch: 48, Batch: 790, Training Loss: 0.037155328691005705, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:27, Epoch: 48, Batch: 800, Training Loss: 0.07323657236993313, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:28, Epoch: 48, Batch: 810, Training Loss: 0.0503926869481802, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:29, Epoch: 48, Batch: 820, Training Loss: 0.04403820559382439, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:29, Epoch: 48, Batch: 830, Training Loss: 0.053057539835572244, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:30, Epoch: 48, Batch: 840, Training Loss: 0.03224468380212784, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:31, Epoch: 48, Batch: 850, Training Loss: 0.06758730746805668, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:32, Epoch: 48, Batch: 860, Training Loss: 0.03447406589984894, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:32, Epoch: 48, Batch: 870, Training Loss: 0.048381442204117775, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:33, Epoch: 48, Batch: 880, Training Loss: 0.035021683946251866, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:34, Epoch: 48, Batch: 890, Training Loss: 0.05495650246739388, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:34, Epoch: 48, Batch: 900, Training Loss: 0.06000749692320824, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:35, Epoch: 48, Batch: 910, Training Loss: 0.0435490783303976, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:36, Epoch: 48, Batch: 920, Training Loss: 0.036527783051133154, LR: 0.0010000000000000002
Time, 2019-01-01T18:00:37, Epoch: 48, Batch: 930, Training Loss: 0.052333378419280055, LR: 0.0010000000000000002
Epoch: 48, Validation Top 1 acc: 98.80730438232422
Epoch: 48, Validation Top 5 acc: 99.99166870117188
Epoch: 48, Validation Set Loss: 0.04314345493912697
Start training epoch 49
Time, 2019-01-01T18:01:04, Epoch: 49, Batch: 10, Training Loss: 0.046681154146790506, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:05, Epoch: 49, Batch: 20, Training Loss: 0.03743017390370369, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:05, Epoch: 49, Batch: 30, Training Loss: 0.037629120796918866, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:06, Epoch: 49, Batch: 40, Training Loss: 0.036356934905052186, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:07, Epoch: 49, Batch: 50, Training Loss: 0.0439219631254673, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:08, Epoch: 49, Batch: 60, Training Loss: 0.059125357493758204, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:08, Epoch: 49, Batch: 70, Training Loss: 0.05403647273778915, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:09, Epoch: 49, Batch: 80, Training Loss: 0.048169324174523354, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:10, Epoch: 49, Batch: 90, Training Loss: 0.056760278344154355, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:10, Epoch: 49, Batch: 100, Training Loss: 0.04687140248715878, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:11, Epoch: 49, Batch: 110, Training Loss: 0.04030582383275032, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:12, Epoch: 49, Batch: 120, Training Loss: 0.04545219093561172, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:13, Epoch: 49, Batch: 130, Training Loss: 0.042186107486486435, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:13, Epoch: 49, Batch: 140, Training Loss: 0.02880108579993248, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:14, Epoch: 49, Batch: 150, Training Loss: 0.04414599649608135, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:15, Epoch: 49, Batch: 160, Training Loss: 0.051193952560424805, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:15, Epoch: 49, Batch: 170, Training Loss: 0.0400691170245409, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:16, Epoch: 49, Batch: 180, Training Loss: 0.04107504189014435, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:17, Epoch: 49, Batch: 190, Training Loss: 0.03310047686100006, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:18, Epoch: 49, Batch: 200, Training Loss: 0.0263413205742836, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:18, Epoch: 49, Batch: 210, Training Loss: 0.03397166356444359, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:19, Epoch: 49, Batch: 220, Training Loss: 0.03100682720541954, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:20, Epoch: 49, Batch: 230, Training Loss: 0.04078406915068626, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:20, Epoch: 49, Batch: 240, Training Loss: 0.04138508811593056, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:21, Epoch: 49, Batch: 250, Training Loss: 0.03686599582433701, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:22, Epoch: 49, Batch: 260, Training Loss: 0.049648809060454366, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:23, Epoch: 49, Batch: 270, Training Loss: 0.04409737437963486, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:23, Epoch: 49, Batch: 280, Training Loss: 0.04587262012064457, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:24, Epoch: 49, Batch: 290, Training Loss: 0.041653082519769666, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:25, Epoch: 49, Batch: 300, Training Loss: 0.0381320871412754, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:25, Epoch: 49, Batch: 310, Training Loss: 0.0276426974684, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:26, Epoch: 49, Batch: 320, Training Loss: 0.0351751871407032, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:27, Epoch: 49, Batch: 330, Training Loss: 0.0396619476377964, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:28, Epoch: 49, Batch: 340, Training Loss: 0.03049415573477745, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:28, Epoch: 49, Batch: 350, Training Loss: 0.05254732742905617, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:29, Epoch: 49, Batch: 360, Training Loss: 0.0800076186656952, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:30, Epoch: 49, Batch: 370, Training Loss: 0.056201818585395816, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:30, Epoch: 49, Batch: 380, Training Loss: 0.025220735743641854, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:31, Epoch: 49, Batch: 390, Training Loss: 0.035848192125558856, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:32, Epoch: 49, Batch: 400, Training Loss: 0.021100583672523498, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:33, Epoch: 49, Batch: 410, Training Loss: 0.054497905820608136, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:33, Epoch: 49, Batch: 420, Training Loss: 0.02508050873875618, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:34, Epoch: 49, Batch: 430, Training Loss: 0.0646848089993, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:35, Epoch: 49, Batch: 440, Training Loss: 0.05502138398587704, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:35, Epoch: 49, Batch: 450, Training Loss: 0.03456417843699455, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:36, Epoch: 49, Batch: 460, Training Loss: 0.03466989062726498, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:37, Epoch: 49, Batch: 470, Training Loss: 0.04642096422612667, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:38, Epoch: 49, Batch: 480, Training Loss: 0.056441345065832135, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:38, Epoch: 49, Batch: 490, Training Loss: 0.043276266753673555, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:39, Epoch: 49, Batch: 500, Training Loss: 0.03442104905843735, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:40, Epoch: 49, Batch: 510, Training Loss: 0.02716907747089863, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:40, Epoch: 49, Batch: 520, Training Loss: 0.03948019854724407, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:41, Epoch: 49, Batch: 530, Training Loss: 0.05028926469385624, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:42, Epoch: 49, Batch: 540, Training Loss: 0.042005278170108795, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:43, Epoch: 49, Batch: 550, Training Loss: 0.06669103428721428, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:43, Epoch: 49, Batch: 560, Training Loss: 0.03404729291796684, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:44, Epoch: 49, Batch: 570, Training Loss: 0.038455700129270555, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:45, Epoch: 49, Batch: 580, Training Loss: 0.025736096501350402, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:46, Epoch: 49, Batch: 590, Training Loss: 0.06655590608716011, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:46, Epoch: 49, Batch: 600, Training Loss: 0.05653369799256325, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:47, Epoch: 49, Batch: 610, Training Loss: 0.03963174857199192, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:48, Epoch: 49, Batch: 620, Training Loss: 0.04871585294604301, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:48, Epoch: 49, Batch: 630, Training Loss: 0.044421476498246196, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:49, Epoch: 49, Batch: 640, Training Loss: 0.04953761734068394, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:50, Epoch: 49, Batch: 650, Training Loss: 0.048123172298073766, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:51, Epoch: 49, Batch: 660, Training Loss: 0.04455997198820114, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:51, Epoch: 49, Batch: 670, Training Loss: 0.055056409910321236, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:52, Epoch: 49, Batch: 680, Training Loss: 0.038256021589040755, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:53, Epoch: 49, Batch: 690, Training Loss: 0.04546157158911228, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:53, Epoch: 49, Batch: 700, Training Loss: 0.0403889250010252, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:54, Epoch: 49, Batch: 710, Training Loss: 0.06441570818424225, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:55, Epoch: 49, Batch: 720, Training Loss: 0.04383838176727295, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:56, Epoch: 49, Batch: 730, Training Loss: 0.05775661841034889, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:56, Epoch: 49, Batch: 740, Training Loss: 0.04312855154275894, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:57, Epoch: 49, Batch: 750, Training Loss: 0.04695638045668602, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:58, Epoch: 49, Batch: 760, Training Loss: 0.044935202598571776, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:58, Epoch: 49, Batch: 770, Training Loss: 0.02957853116095066, LR: 0.0010000000000000002
Time, 2019-01-01T18:01:59, Epoch: 49, Batch: 780, Training Loss: 0.03218162953853607, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:00, Epoch: 49, Batch: 790, Training Loss: 0.04764669314026833, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:01, Epoch: 49, Batch: 800, Training Loss: 0.047599999234080315, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:01, Epoch: 49, Batch: 810, Training Loss: 0.036200763285160066, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:02, Epoch: 49, Batch: 820, Training Loss: 0.0365799356251955, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:03, Epoch: 49, Batch: 830, Training Loss: 0.03578917123377323, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:03, Epoch: 49, Batch: 840, Training Loss: 0.03050060272216797, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:04, Epoch: 49, Batch: 850, Training Loss: 0.025587695837020873, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:05, Epoch: 49, Batch: 860, Training Loss: 0.0360775388777256, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:06, Epoch: 49, Batch: 870, Training Loss: 0.04585632979869843, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:06, Epoch: 49, Batch: 880, Training Loss: 0.05016103237867355, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:07, Epoch: 49, Batch: 890, Training Loss: 0.08385567292571068, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:08, Epoch: 49, Batch: 900, Training Loss: 0.04815114699304104, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:08, Epoch: 49, Batch: 910, Training Loss: 0.05923510491847992, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:09, Epoch: 49, Batch: 920, Training Loss: 0.050242158398032186, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:10, Epoch: 49, Batch: 930, Training Loss: 0.03756936080753803, LR: 0.0010000000000000002
Epoch: 49, Validation Top 1 acc: 98.79563903808594
Epoch: 49, Validation Top 5 acc: 99.98834228515625
Epoch: 49, Validation Set Loss: 0.04370110109448433
Start training epoch 50
Time, 2019-01-01T18:02:39, Epoch: 50, Batch: 10, Training Loss: 0.039455560594797136, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:39, Epoch: 50, Batch: 20, Training Loss: 0.03949728161096573, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:40, Epoch: 50, Batch: 30, Training Loss: 0.04109521433711052, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:41, Epoch: 50, Batch: 40, Training Loss: 0.05812810435891151, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:42, Epoch: 50, Batch: 50, Training Loss: 0.042381568998098376, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:42, Epoch: 50, Batch: 60, Training Loss: 0.03181249424815178, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:43, Epoch: 50, Batch: 70, Training Loss: 0.0708350446075201, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:44, Epoch: 50, Batch: 80, Training Loss: 0.044854187220335004, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:44, Epoch: 50, Batch: 90, Training Loss: 0.052301134914159775, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:45, Epoch: 50, Batch: 100, Training Loss: 0.03306128457188606, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:46, Epoch: 50, Batch: 110, Training Loss: 0.04852789752185345, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:47, Epoch: 50, Batch: 120, Training Loss: 0.03918077386915684, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:47, Epoch: 50, Batch: 130, Training Loss: 0.029802283644676207, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:48, Epoch: 50, Batch: 140, Training Loss: 0.041511889547109604, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:49, Epoch: 50, Batch: 150, Training Loss: 0.03392883092164993, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:49, Epoch: 50, Batch: 160, Training Loss: 0.027866679057478905, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:50, Epoch: 50, Batch: 170, Training Loss: 0.047454755008220675, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:51, Epoch: 50, Batch: 180, Training Loss: 0.02449377439916134, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:52, Epoch: 50, Batch: 190, Training Loss: 0.05141724608838558, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:52, Epoch: 50, Batch: 200, Training Loss: 0.030064869672060013, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:53, Epoch: 50, Batch: 210, Training Loss: 0.056377772241830826, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:54, Epoch: 50, Batch: 220, Training Loss: 0.039274049922823906, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:54, Epoch: 50, Batch: 230, Training Loss: 0.03868316262960434, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:55, Epoch: 50, Batch: 240, Training Loss: 0.04910722374916077, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:56, Epoch: 50, Batch: 250, Training Loss: 0.05003428384661675, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:57, Epoch: 50, Batch: 260, Training Loss: 0.03176784217357635, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:57, Epoch: 50, Batch: 270, Training Loss: 0.03612076416611672, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:58, Epoch: 50, Batch: 280, Training Loss: 0.063417474552989, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:59, Epoch: 50, Batch: 290, Training Loss: 0.04668849110603333, LR: 0.0010000000000000002
Time, 2019-01-01T18:02:59, Epoch: 50, Batch: 300, Training Loss: 0.03976479172706604, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:00, Epoch: 50, Batch: 310, Training Loss: 0.03997531421482563, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:01, Epoch: 50, Batch: 320, Training Loss: 0.030859407037496567, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:02, Epoch: 50, Batch: 330, Training Loss: 0.046157682314515114, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:02, Epoch: 50, Batch: 340, Training Loss: 0.028074530884623528, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:03, Epoch: 50, Batch: 350, Training Loss: 0.03295101821422577, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:04, Epoch: 50, Batch: 360, Training Loss: 0.0380443025380373, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:04, Epoch: 50, Batch: 370, Training Loss: 0.045405379682779315, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:05, Epoch: 50, Batch: 380, Training Loss: 0.032536158338189125, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:06, Epoch: 50, Batch: 390, Training Loss: 0.06971409358084202, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:07, Epoch: 50, Batch: 400, Training Loss: 0.04124826900660992, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:07, Epoch: 50, Batch: 410, Training Loss: 0.0442039679735899, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:08, Epoch: 50, Batch: 420, Training Loss: 0.05922706462442875, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:09, Epoch: 50, Batch: 430, Training Loss: 0.03757194355130196, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:09, Epoch: 50, Batch: 440, Training Loss: 0.04686282351613045, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:10, Epoch: 50, Batch: 450, Training Loss: 0.03634216524660587, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:11, Epoch: 50, Batch: 460, Training Loss: 0.056900841742753984, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:12, Epoch: 50, Batch: 470, Training Loss: 0.05543202236294746, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:12, Epoch: 50, Batch: 480, Training Loss: 0.05245613008737564, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:13, Epoch: 50, Batch: 490, Training Loss: 0.04441444650292396, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:14, Epoch: 50, Batch: 500, Training Loss: 0.04119269773364067, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:14, Epoch: 50, Batch: 510, Training Loss: 0.06061375290155411, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:15, Epoch: 50, Batch: 520, Training Loss: 0.04628659337759018, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:16, Epoch: 50, Batch: 530, Training Loss: 0.04328530132770538, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:17, Epoch: 50, Batch: 540, Training Loss: 0.048845373839139936, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:17, Epoch: 50, Batch: 550, Training Loss: 0.05273761786520481, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:18, Epoch: 50, Batch: 560, Training Loss: 0.05230563059449196, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:19, Epoch: 50, Batch: 570, Training Loss: 0.04145293533802032, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:19, Epoch: 50, Batch: 580, Training Loss: 0.05484816506505012, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:20, Epoch: 50, Batch: 590, Training Loss: 0.04752323180437088, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:21, Epoch: 50, Batch: 600, Training Loss: 0.0561568945646286, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:22, Epoch: 50, Batch: 610, Training Loss: 0.030906952545046808, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:22, Epoch: 50, Batch: 620, Training Loss: 0.048961717262864114, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:23, Epoch: 50, Batch: 630, Training Loss: 0.040798331052064894, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:24, Epoch: 50, Batch: 640, Training Loss: 0.03087879978120327, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:24, Epoch: 50, Batch: 650, Training Loss: 0.045617255568504336, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:25, Epoch: 50, Batch: 660, Training Loss: 0.047162625938653946, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:26, Epoch: 50, Batch: 670, Training Loss: 0.04341888315975666, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:27, Epoch: 50, Batch: 680, Training Loss: 0.04598901495337486, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:27, Epoch: 50, Batch: 690, Training Loss: 0.05428773947060108, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:28, Epoch: 50, Batch: 700, Training Loss: 0.0505977712571621, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:29, Epoch: 50, Batch: 710, Training Loss: 0.027241919189691544, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:29, Epoch: 50, Batch: 720, Training Loss: 0.0382534846663475, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:30, Epoch: 50, Batch: 730, Training Loss: 0.05823947973549366, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:31, Epoch: 50, Batch: 740, Training Loss: 0.04077623188495636, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:32, Epoch: 50, Batch: 750, Training Loss: 0.04797825664281845, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:32, Epoch: 50, Batch: 760, Training Loss: 0.0371361967176199, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:33, Epoch: 50, Batch: 770, Training Loss: 0.03548639342188835, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:34, Epoch: 50, Batch: 780, Training Loss: 0.04552754983305931, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:34, Epoch: 50, Batch: 790, Training Loss: 0.04062497913837433, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:35, Epoch: 50, Batch: 800, Training Loss: 0.0420806922018528, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:36, Epoch: 50, Batch: 810, Training Loss: 0.0490320410579443, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:37, Epoch: 50, Batch: 820, Training Loss: 0.039616217091679576, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:37, Epoch: 50, Batch: 830, Training Loss: 0.04544231183826923, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:38, Epoch: 50, Batch: 840, Training Loss: 0.04241564199328422, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:39, Epoch: 50, Batch: 850, Training Loss: 0.034066188335418704, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:39, Epoch: 50, Batch: 860, Training Loss: 0.052614283934235576, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:40, Epoch: 50, Batch: 870, Training Loss: 0.0336343090981245, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:41, Epoch: 50, Batch: 880, Training Loss: 0.029275765269994737, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:42, Epoch: 50, Batch: 890, Training Loss: 0.03538888320326805, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:42, Epoch: 50, Batch: 900, Training Loss: 0.0361574649810791, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:43, Epoch: 50, Batch: 910, Training Loss: 0.03947651237249374, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:44, Epoch: 50, Batch: 920, Training Loss: 0.05785507597029209, LR: 0.0010000000000000002
Time, 2019-01-01T18:03:44, Epoch: 50, Batch: 930, Training Loss: 0.04353427812457085, LR: 0.0010000000000000002
Epoch: 50, Validation Top 1 acc: 98.79231262207031
Epoch: 50, Validation Top 5 acc: 99.99333953857422
Epoch: 50, Validation Set Loss: 0.04315674677491188
Start training epoch 51
Time, 2019-01-01T18:04:12, Epoch: 51, Batch: 10, Training Loss: 0.046870237216353416, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:13, Epoch: 51, Batch: 20, Training Loss: 0.0323512926697731, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:13, Epoch: 51, Batch: 30, Training Loss: 0.04072442129254341, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:14, Epoch: 51, Batch: 40, Training Loss: 0.0466802641749382, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:15, Epoch: 51, Batch: 50, Training Loss: 0.05570412389934063, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:16, Epoch: 51, Batch: 60, Training Loss: 0.04760014042258263, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:16, Epoch: 51, Batch: 70, Training Loss: 0.04575702100992203, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:17, Epoch: 51, Batch: 80, Training Loss: 0.038484399765729906, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:18, Epoch: 51, Batch: 90, Training Loss: 0.03731112033128738, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:18, Epoch: 51, Batch: 100, Training Loss: 0.0343787744641304, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:19, Epoch: 51, Batch: 110, Training Loss: 0.02942632809281349, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:20, Epoch: 51, Batch: 120, Training Loss: 0.047405853122472766, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:21, Epoch: 51, Batch: 130, Training Loss: 0.038139878585934636, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:21, Epoch: 51, Batch: 140, Training Loss: 0.033826353028416634, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:22, Epoch: 51, Batch: 150, Training Loss: 0.036192094534635545, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:23, Epoch: 51, Batch: 160, Training Loss: 0.03728145994246006, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:23, Epoch: 51, Batch: 170, Training Loss: 0.05216139517724514, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:24, Epoch: 51, Batch: 180, Training Loss: 0.039819956943392756, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:25, Epoch: 51, Batch: 190, Training Loss: 0.04358703345060348, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:26, Epoch: 51, Batch: 200, Training Loss: 0.0557000357657671, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:26, Epoch: 51, Batch: 210, Training Loss: 0.04715838953852654, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:27, Epoch: 51, Batch: 220, Training Loss: 0.03136586993932724, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:28, Epoch: 51, Batch: 230, Training Loss: 0.050521764904260635, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:28, Epoch: 51, Batch: 240, Training Loss: 0.06440356560051441, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:29, Epoch: 51, Batch: 250, Training Loss: 0.03992325849831104, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:30, Epoch: 51, Batch: 260, Training Loss: 0.05320616215467453, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:31, Epoch: 51, Batch: 270, Training Loss: 0.0497379757463932, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:31, Epoch: 51, Batch: 280, Training Loss: 0.056481218338012694, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:32, Epoch: 51, Batch: 290, Training Loss: 0.041151991486549376, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:33, Epoch: 51, Batch: 300, Training Loss: 0.040890925005078314, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:33, Epoch: 51, Batch: 310, Training Loss: 0.04302234798669815, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:34, Epoch: 51, Batch: 320, Training Loss: 0.03140460439026356, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:35, Epoch: 51, Batch: 330, Training Loss: 0.040468092635273935, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:36, Epoch: 51, Batch: 340, Training Loss: 0.0316869180649519, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:36, Epoch: 51, Batch: 350, Training Loss: 0.03586281910538673, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:37, Epoch: 51, Batch: 360, Training Loss: 0.05932568944990635, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:38, Epoch: 51, Batch: 370, Training Loss: 0.045521557331085205, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:38, Epoch: 51, Batch: 380, Training Loss: 0.03875424191355705, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:39, Epoch: 51, Batch: 390, Training Loss: 0.039752212911844255, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:40, Epoch: 51, Batch: 400, Training Loss: 0.050485865026712415, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:41, Epoch: 51, Batch: 410, Training Loss: 0.06620325334370136, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:41, Epoch: 51, Batch: 420, Training Loss: 0.046635646000504495, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:42, Epoch: 51, Batch: 430, Training Loss: 0.05063123181462288, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:43, Epoch: 51, Batch: 440, Training Loss: 0.033873438462615015, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:43, Epoch: 51, Batch: 450, Training Loss: 0.04046684466302395, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:44, Epoch: 51, Batch: 460, Training Loss: 0.02986598089337349, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:45, Epoch: 51, Batch: 470, Training Loss: 0.05017809309065342, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:46, Epoch: 51, Batch: 480, Training Loss: 0.04034230820834637, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:46, Epoch: 51, Batch: 490, Training Loss: 0.05812330991029739, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:47, Epoch: 51, Batch: 500, Training Loss: 0.04626188576221466, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:48, Epoch: 51, Batch: 510, Training Loss: 0.04373394027352333, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:48, Epoch: 51, Batch: 520, Training Loss: 0.033673370629549025, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:49, Epoch: 51, Batch: 530, Training Loss: 0.05358620509505272, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:50, Epoch: 51, Batch: 540, Training Loss: 0.04060911014676094, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:50, Epoch: 51, Batch: 550, Training Loss: 0.034664659202098845, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:51, Epoch: 51, Batch: 560, Training Loss: 0.03525728471577168, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:52, Epoch: 51, Batch: 570, Training Loss: 0.05598145797848701, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:53, Epoch: 51, Batch: 580, Training Loss: 0.04142771027982235, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:53, Epoch: 51, Batch: 590, Training Loss: 0.040970037877559665, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:54, Epoch: 51, Batch: 600, Training Loss: 0.039063500985503194, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:55, Epoch: 51, Batch: 610, Training Loss: 0.050904767215251924, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:56, Epoch: 51, Batch: 620, Training Loss: 0.06410141065716743, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:56, Epoch: 51, Batch: 630, Training Loss: 0.04141465276479721, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:57, Epoch: 51, Batch: 640, Training Loss: 0.038307614251971246, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:58, Epoch: 51, Batch: 650, Training Loss: 0.06944143623113633, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:58, Epoch: 51, Batch: 660, Training Loss: 0.03027302585542202, LR: 0.0010000000000000002
Time, 2019-01-01T18:04:59, Epoch: 51, Batch: 670, Training Loss: 0.05148514211177826, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:00, Epoch: 51, Batch: 680, Training Loss: 0.05947323180735111, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:01, Epoch: 51, Batch: 690, Training Loss: 0.0461466770619154, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:01, Epoch: 51, Batch: 700, Training Loss: 0.024790557846426965, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:02, Epoch: 51, Batch: 710, Training Loss: 0.03691353760659695, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:03, Epoch: 51, Batch: 720, Training Loss: 0.039167136698961255, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:03, Epoch: 51, Batch: 730, Training Loss: 0.037808715179562566, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:04, Epoch: 51, Batch: 740, Training Loss: 0.043799986317753795, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:05, Epoch: 51, Batch: 750, Training Loss: 0.03347923383116722, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:06, Epoch: 51, Batch: 760, Training Loss: 0.03651138991117477, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:06, Epoch: 51, Batch: 770, Training Loss: 0.05570727773010731, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:07, Epoch: 51, Batch: 780, Training Loss: 0.0552615761756897, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:08, Epoch: 51, Batch: 790, Training Loss: 0.0445032861083746, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:08, Epoch: 51, Batch: 800, Training Loss: 0.041201424598693845, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:09, Epoch: 51, Batch: 810, Training Loss: 0.05935704931616783, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:10, Epoch: 51, Batch: 820, Training Loss: 0.05918313525617123, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:11, Epoch: 51, Batch: 830, Training Loss: 0.03496256172657013, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:11, Epoch: 51, Batch: 840, Training Loss: 0.03476081825792789, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:12, Epoch: 51, Batch: 850, Training Loss: 0.06337739638984204, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:13, Epoch: 51, Batch: 860, Training Loss: 0.051308019831776616, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:13, Epoch: 51, Batch: 870, Training Loss: 0.02890666015446186, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:14, Epoch: 51, Batch: 880, Training Loss: 0.024791228026151656, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:15, Epoch: 51, Batch: 890, Training Loss: 0.041732152551412584, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:16, Epoch: 51, Batch: 900, Training Loss: 0.039256544038653376, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:16, Epoch: 51, Batch: 910, Training Loss: 0.04518215991556644, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:17, Epoch: 51, Batch: 920, Training Loss: 0.03565065041184425, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:18, Epoch: 51, Batch: 930, Training Loss: 0.03656894415616989, LR: 0.0010000000000000002
Epoch: 51, Validation Top 1 acc: 98.76898956298828
Epoch: 51, Validation Top 5 acc: 99.99166870117188
Epoch: 51, Validation Set Loss: 0.04311012849211693
Start training epoch 52
Time, 2019-01-01T18:05:45, Epoch: 52, Batch: 10, Training Loss: 0.05214797332882881, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:46, Epoch: 52, Batch: 20, Training Loss: 0.03739002272486687, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:47, Epoch: 52, Batch: 30, Training Loss: 0.04725214876234531, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:47, Epoch: 52, Batch: 40, Training Loss: 0.0338303729891777, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:48, Epoch: 52, Batch: 50, Training Loss: 0.048230066522955896, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:49, Epoch: 52, Batch: 60, Training Loss: 0.038674022257328036, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:50, Epoch: 52, Batch: 70, Training Loss: 0.023021192848682405, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:50, Epoch: 52, Batch: 80, Training Loss: 0.04766378402709961, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:51, Epoch: 52, Batch: 90, Training Loss: 0.05583451092243195, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:52, Epoch: 52, Batch: 100, Training Loss: 0.052989090979099276, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:52, Epoch: 52, Batch: 110, Training Loss: 0.053381995856761934, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:53, Epoch: 52, Batch: 120, Training Loss: 0.042232155427336694, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:54, Epoch: 52, Batch: 130, Training Loss: 0.03914113231003284, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:55, Epoch: 52, Batch: 140, Training Loss: 0.04210755899548531, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:55, Epoch: 52, Batch: 150, Training Loss: 0.042514750733971596, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:56, Epoch: 52, Batch: 160, Training Loss: 0.055157307907938956, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:57, Epoch: 52, Batch: 170, Training Loss: 0.05102255679666996, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:57, Epoch: 52, Batch: 180, Training Loss: 0.03766825646162033, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:58, Epoch: 52, Batch: 190, Training Loss: 0.04581954069435597, LR: 0.0010000000000000002
Time, 2019-01-01T18:05:59, Epoch: 52, Batch: 200, Training Loss: 0.035000017285346983, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:00, Epoch: 52, Batch: 210, Training Loss: 0.04445140734314919, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:00, Epoch: 52, Batch: 220, Training Loss: 0.036439049243927005, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:01, Epoch: 52, Batch: 230, Training Loss: 0.04555974453687668, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:02, Epoch: 52, Batch: 240, Training Loss: 0.06336083821952343, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:02, Epoch: 52, Batch: 250, Training Loss: 0.046269039064645766, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:03, Epoch: 52, Batch: 260, Training Loss: 0.06033141575753689, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:04, Epoch: 52, Batch: 270, Training Loss: 0.03669649325311184, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:05, Epoch: 52, Batch: 280, Training Loss: 0.03373037241399288, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:05, Epoch: 52, Batch: 290, Training Loss: 0.030208006501197815, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:06, Epoch: 52, Batch: 300, Training Loss: 0.03128070943057537, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:07, Epoch: 52, Batch: 310, Training Loss: 0.04886363074183464, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:07, Epoch: 52, Batch: 320, Training Loss: 0.04580218195915222, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:08, Epoch: 52, Batch: 330, Training Loss: 0.03959462381899357, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:09, Epoch: 52, Batch: 340, Training Loss: 0.0379294142127037, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:10, Epoch: 52, Batch: 350, Training Loss: 0.037152812257409094, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:10, Epoch: 52, Batch: 360, Training Loss: 0.05467603281140328, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:11, Epoch: 52, Batch: 370, Training Loss: 0.03939559385180473, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:12, Epoch: 52, Batch: 380, Training Loss: 0.04109376817941666, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:13, Epoch: 52, Batch: 390, Training Loss: 0.04703420102596283, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:13, Epoch: 52, Batch: 400, Training Loss: 0.04894656725227833, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:14, Epoch: 52, Batch: 410, Training Loss: 0.0585825003683567, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:15, Epoch: 52, Batch: 420, Training Loss: 0.037460781261324884, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:15, Epoch: 52, Batch: 430, Training Loss: 0.0654495231807232, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:16, Epoch: 52, Batch: 440, Training Loss: 0.05832068920135498, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:17, Epoch: 52, Batch: 450, Training Loss: 0.029754814878106117, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:17, Epoch: 52, Batch: 460, Training Loss: 0.048424385115504266, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:18, Epoch: 52, Batch: 470, Training Loss: 0.0416858471930027, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:19, Epoch: 52, Batch: 480, Training Loss: 0.03693950176239014, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:20, Epoch: 52, Batch: 490, Training Loss: 0.04751472063362598, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:20, Epoch: 52, Batch: 500, Training Loss: 0.04311495013535023, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:21, Epoch: 52, Batch: 510, Training Loss: 0.05256439782679081, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:22, Epoch: 52, Batch: 520, Training Loss: 0.03848237469792366, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:22, Epoch: 52, Batch: 530, Training Loss: 0.04427071064710617, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:23, Epoch: 52, Batch: 540, Training Loss: 0.044919629395008084, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:24, Epoch: 52, Batch: 550, Training Loss: 0.037543900310993195, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:25, Epoch: 52, Batch: 560, Training Loss: 0.0522400364279747, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:25, Epoch: 52, Batch: 570, Training Loss: 0.028896376863121985, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:26, Epoch: 52, Batch: 580, Training Loss: 0.028940989449620248, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:27, Epoch: 52, Batch: 590, Training Loss: 0.030200476199388503, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:27, Epoch: 52, Batch: 600, Training Loss: 0.05608021840453148, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:28, Epoch: 52, Batch: 610, Training Loss: 0.06593228876590729, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:29, Epoch: 52, Batch: 620, Training Loss: 0.058650316298007966, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:30, Epoch: 52, Batch: 630, Training Loss: 0.032285161316394806, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:30, Epoch: 52, Batch: 640, Training Loss: 0.03757883943617344, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:31, Epoch: 52, Batch: 650, Training Loss: 0.035461707040667534, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:32, Epoch: 52, Batch: 660, Training Loss: 0.030313820764422416, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:33, Epoch: 52, Batch: 670, Training Loss: 0.03308536633849144, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:33, Epoch: 52, Batch: 680, Training Loss: 0.04520992040634155, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:34, Epoch: 52, Batch: 690, Training Loss: 0.0366801206022501, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:35, Epoch: 52, Batch: 700, Training Loss: 0.05359561406075954, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:35, Epoch: 52, Batch: 710, Training Loss: 0.041546664014458654, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:36, Epoch: 52, Batch: 720, Training Loss: 0.04213972277939319, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:37, Epoch: 52, Batch: 730, Training Loss: 0.05483405515551567, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:38, Epoch: 52, Batch: 740, Training Loss: 0.055135133862495425, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:38, Epoch: 52, Batch: 750, Training Loss: 0.050405648350715634, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:39, Epoch: 52, Batch: 760, Training Loss: 0.06398779340088367, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:40, Epoch: 52, Batch: 770, Training Loss: 0.040077412873506545, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:40, Epoch: 52, Batch: 780, Training Loss: 0.05037719830870628, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:41, Epoch: 52, Batch: 790, Training Loss: 0.03672223649919033, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:42, Epoch: 52, Batch: 800, Training Loss: 0.04298589080572128, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:43, Epoch: 52, Batch: 810, Training Loss: 0.04697928912937641, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:43, Epoch: 52, Batch: 820, Training Loss: 0.054984641075134275, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:44, Epoch: 52, Batch: 830, Training Loss: 0.04896100014448166, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:45, Epoch: 52, Batch: 840, Training Loss: 0.03615769520401955, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:45, Epoch: 52, Batch: 850, Training Loss: 0.052168970555067064, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:46, Epoch: 52, Batch: 860, Training Loss: 0.04062216393649578, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:47, Epoch: 52, Batch: 870, Training Loss: 0.03915377408266067, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:47, Epoch: 52, Batch: 880, Training Loss: 0.05215769931674004, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:48, Epoch: 52, Batch: 890, Training Loss: 0.030851731449365614, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:49, Epoch: 52, Batch: 900, Training Loss: 0.02963332086801529, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:50, Epoch: 52, Batch: 910, Training Loss: 0.030125006660819055, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:50, Epoch: 52, Batch: 920, Training Loss: 0.04619908407330513, LR: 0.0010000000000000002
Time, 2019-01-01T18:06:51, Epoch: 52, Batch: 930, Training Loss: 0.031311781331896785, LR: 0.0010000000000000002
Epoch: 52, Validation Top 1 acc: 98.8006362915039
Epoch: 52, Validation Top 5 acc: 99.99166870117188
Epoch: 52, Validation Set Loss: 0.04284033924341202
Start training epoch 53
Time, 2019-01-01T18:07:19, Epoch: 53, Batch: 10, Training Loss: 0.04898718670010567, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:19, Epoch: 53, Batch: 20, Training Loss: 0.05570949837565422, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:20, Epoch: 53, Batch: 30, Training Loss: 0.026844871789216997, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:21, Epoch: 53, Batch: 40, Training Loss: 0.03228660188615322, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:22, Epoch: 53, Batch: 50, Training Loss: 0.046133846044540405, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:22, Epoch: 53, Batch: 60, Training Loss: 0.03811271339654922, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:23, Epoch: 53, Batch: 70, Training Loss: 0.0427839994430542, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:24, Epoch: 53, Batch: 80, Training Loss: 0.030146098136901854, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:24, Epoch: 53, Batch: 90, Training Loss: 0.04279631599783897, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:25, Epoch: 53, Batch: 100, Training Loss: 0.052566269785165785, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:26, Epoch: 53, Batch: 110, Training Loss: 0.0454393919557333, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:27, Epoch: 53, Batch: 120, Training Loss: 0.04808828942477703, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:27, Epoch: 53, Batch: 130, Training Loss: 0.05253095366060734, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:28, Epoch: 53, Batch: 140, Training Loss: 0.048124068230390546, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:29, Epoch: 53, Batch: 150, Training Loss: 0.027698380500078203, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:29, Epoch: 53, Batch: 160, Training Loss: 0.03972574546933174, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:30, Epoch: 53, Batch: 170, Training Loss: 0.04691286310553551, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:31, Epoch: 53, Batch: 180, Training Loss: 0.03474018573760986, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:32, Epoch: 53, Batch: 190, Training Loss: 0.04505624622106552, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:32, Epoch: 53, Batch: 200, Training Loss: 0.041987472027540204, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:33, Epoch: 53, Batch: 210, Training Loss: 0.027567665651440622, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:34, Epoch: 53, Batch: 220, Training Loss: 0.04779356271028519, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:34, Epoch: 53, Batch: 230, Training Loss: 0.04747410379350185, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:35, Epoch: 53, Batch: 240, Training Loss: 0.03194350749254227, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:36, Epoch: 53, Batch: 250, Training Loss: 0.04653533548116684, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:37, Epoch: 53, Batch: 260, Training Loss: 0.04910518787801266, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:37, Epoch: 53, Batch: 270, Training Loss: 0.0411094855517149, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:38, Epoch: 53, Batch: 280, Training Loss: 0.05211735554039478, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:39, Epoch: 53, Batch: 290, Training Loss: 0.045679963380098346, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:39, Epoch: 53, Batch: 300, Training Loss: 0.05271381810307503, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:40, Epoch: 53, Batch: 310, Training Loss: 0.03348422199487686, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:41, Epoch: 53, Batch: 320, Training Loss: 0.04892623499035835, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:42, Epoch: 53, Batch: 330, Training Loss: 0.040675468370318414, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:42, Epoch: 53, Batch: 340, Training Loss: 0.048279330134391785, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:43, Epoch: 53, Batch: 350, Training Loss: 0.04148397892713547, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:44, Epoch: 53, Batch: 360, Training Loss: 0.041009795665740964, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:44, Epoch: 53, Batch: 370, Training Loss: 0.03910696506500244, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:45, Epoch: 53, Batch: 380, Training Loss: 0.06190962195396423, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:46, Epoch: 53, Batch: 390, Training Loss: 0.059213487058877946, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:47, Epoch: 53, Batch: 400, Training Loss: 0.04092798084020614, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:47, Epoch: 53, Batch: 410, Training Loss: 0.03972799219191074, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:48, Epoch: 53, Batch: 420, Training Loss: 0.04257937781512737, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:49, Epoch: 53, Batch: 430, Training Loss: 0.05360701754689216, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:49, Epoch: 53, Batch: 440, Training Loss: 0.04301128908991814, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:50, Epoch: 53, Batch: 450, Training Loss: 0.03677310794591904, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:51, Epoch: 53, Batch: 460, Training Loss: 0.028688934817910194, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:52, Epoch: 53, Batch: 470, Training Loss: 0.037206371501088145, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:52, Epoch: 53, Batch: 480, Training Loss: 0.0365039337426424, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:53, Epoch: 53, Batch: 490, Training Loss: 0.0368687629699707, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:54, Epoch: 53, Batch: 500, Training Loss: 0.04612908288836479, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:54, Epoch: 53, Batch: 510, Training Loss: 0.0442467600107193, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:55, Epoch: 53, Batch: 520, Training Loss: 0.044057497009634974, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:56, Epoch: 53, Batch: 530, Training Loss: 0.06365562416613102, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:57, Epoch: 53, Batch: 540, Training Loss: 0.04373025558888912, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:57, Epoch: 53, Batch: 550, Training Loss: 0.04986301623284817, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:58, Epoch: 53, Batch: 560, Training Loss: 0.04087132588028908, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:59, Epoch: 53, Batch: 570, Training Loss: 0.03425774872303009, LR: 0.0010000000000000002
Time, 2019-01-01T18:07:59, Epoch: 53, Batch: 580, Training Loss: 0.04552128687500954, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:00, Epoch: 53, Batch: 590, Training Loss: 0.03751164898276329, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:01, Epoch: 53, Batch: 600, Training Loss: 0.04267537444829941, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:02, Epoch: 53, Batch: 610, Training Loss: 0.038340700790286064, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:02, Epoch: 53, Batch: 620, Training Loss: 0.040124408528208734, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:03, Epoch: 53, Batch: 630, Training Loss: 0.045751172304153445, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:04, Epoch: 53, Batch: 640, Training Loss: 0.06724949963390828, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:04, Epoch: 53, Batch: 650, Training Loss: 0.03576781637966633, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:05, Epoch: 53, Batch: 660, Training Loss: 0.03619842119514942, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:06, Epoch: 53, Batch: 670, Training Loss: 0.03660930171608925, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:07, Epoch: 53, Batch: 680, Training Loss: 0.03255317322909832, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:07, Epoch: 53, Batch: 690, Training Loss: 0.055259228497743604, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:08, Epoch: 53, Batch: 700, Training Loss: 0.044774274528026584, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:09, Epoch: 53, Batch: 710, Training Loss: 0.0413911048322916, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:09, Epoch: 53, Batch: 720, Training Loss: 0.049164082109928134, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:10, Epoch: 53, Batch: 730, Training Loss: 0.035343319177627563, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:11, Epoch: 53, Batch: 740, Training Loss: 0.052031466364860536, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:12, Epoch: 53, Batch: 750, Training Loss: 0.025532903149724007, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:12, Epoch: 53, Batch: 760, Training Loss: 0.06549161002039909, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:13, Epoch: 53, Batch: 770, Training Loss: 0.05999614857137203, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:14, Epoch: 53, Batch: 780, Training Loss: 0.04620446003973484, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:14, Epoch: 53, Batch: 790, Training Loss: 0.04470905624330044, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:15, Epoch: 53, Batch: 800, Training Loss: 0.04911570027470589, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:16, Epoch: 53, Batch: 810, Training Loss: 0.04936421476304531, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:17, Epoch: 53, Batch: 820, Training Loss: 0.04079721570014953, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:17, Epoch: 53, Batch: 830, Training Loss: 0.02529986873269081, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:18, Epoch: 53, Batch: 840, Training Loss: 0.037634531036019325, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:19, Epoch: 53, Batch: 850, Training Loss: 0.0383138969540596, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:19, Epoch: 53, Batch: 860, Training Loss: 0.05504980683326721, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:20, Epoch: 53, Batch: 870, Training Loss: 0.035240774229168895, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:21, Epoch: 53, Batch: 880, Training Loss: 0.04280148595571518, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:22, Epoch: 53, Batch: 890, Training Loss: 0.044165465980768204, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:22, Epoch: 53, Batch: 900, Training Loss: 0.05336491353809834, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:23, Epoch: 53, Batch: 910, Training Loss: 0.04565912112593651, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:24, Epoch: 53, Batch: 920, Training Loss: 0.035510895401239397, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:24, Epoch: 53, Batch: 930, Training Loss: 0.03670227192342281, LR: 0.0010000000000000002
Epoch: 53, Validation Top 1 acc: 98.81230163574219
Epoch: 53, Validation Top 5 acc: 99.99166870117188
Epoch: 53, Validation Set Loss: 0.04281831905245781
Start training epoch 54
Time, 2019-01-01T18:08:52, Epoch: 54, Batch: 10, Training Loss: 0.03865970708429813, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:53, Epoch: 54, Batch: 20, Training Loss: 0.06700181476771831, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:53, Epoch: 54, Batch: 30, Training Loss: 0.05582452490925789, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:54, Epoch: 54, Batch: 40, Training Loss: 0.04215839058160782, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:55, Epoch: 54, Batch: 50, Training Loss: 0.040448247641325, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:56, Epoch: 54, Batch: 60, Training Loss: 0.049806824326515196, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:56, Epoch: 54, Batch: 70, Training Loss: 0.04317909330129623, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:57, Epoch: 54, Batch: 80, Training Loss: 0.05132785998284817, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:58, Epoch: 54, Batch: 90, Training Loss: 0.06694880202412605, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:58, Epoch: 54, Batch: 100, Training Loss: 0.03658552356064319, LR: 0.0010000000000000002
Time, 2019-01-01T18:08:59, Epoch: 54, Batch: 110, Training Loss: 0.0257366344332695, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:00, Epoch: 54, Batch: 120, Training Loss: 0.05130870752036572, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:01, Epoch: 54, Batch: 130, Training Loss: 0.04583213925361633, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:01, Epoch: 54, Batch: 140, Training Loss: 0.04198090210556984, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:02, Epoch: 54, Batch: 150, Training Loss: 0.04278028756380081, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:03, Epoch: 54, Batch: 160, Training Loss: 0.03376068510115147, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:03, Epoch: 54, Batch: 170, Training Loss: 0.027399279549717902, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:04, Epoch: 54, Batch: 180, Training Loss: 0.02928498089313507, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:05, Epoch: 54, Batch: 190, Training Loss: 0.03830468580126763, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:06, Epoch: 54, Batch: 200, Training Loss: 0.055867607891559604, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:06, Epoch: 54, Batch: 210, Training Loss: 0.04488199688494206, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:07, Epoch: 54, Batch: 220, Training Loss: 0.04761375226080418, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:08, Epoch: 54, Batch: 230, Training Loss: 0.05486535616219044, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:08, Epoch: 54, Batch: 240, Training Loss: 0.05257600806653499, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:09, Epoch: 54, Batch: 250, Training Loss: 0.03518578559160233, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:10, Epoch: 54, Batch: 260, Training Loss: 0.04238486923277378, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:10, Epoch: 54, Batch: 270, Training Loss: 0.03044656999409199, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:11, Epoch: 54, Batch: 280, Training Loss: 0.05295865535736084, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:12, Epoch: 54, Batch: 290, Training Loss: 0.05486789904534817, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:13, Epoch: 54, Batch: 300, Training Loss: 0.03869823962450027, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:13, Epoch: 54, Batch: 310, Training Loss: 0.04198698699474335, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:14, Epoch: 54, Batch: 320, Training Loss: 0.04175055287778377, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:15, Epoch: 54, Batch: 330, Training Loss: 0.04505995474755764, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:15, Epoch: 54, Batch: 340, Training Loss: 0.03986632600426674, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:16, Epoch: 54, Batch: 350, Training Loss: 0.04838922955095768, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:17, Epoch: 54, Batch: 360, Training Loss: 0.035065971314907074, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:18, Epoch: 54, Batch: 370, Training Loss: 0.06943498402833939, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:18, Epoch: 54, Batch: 380, Training Loss: 0.02888036221265793, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:19, Epoch: 54, Batch: 390, Training Loss: 0.05577143244445324, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:20, Epoch: 54, Batch: 400, Training Loss: 0.038473017141222955, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:20, Epoch: 54, Batch: 410, Training Loss: 0.03504834026098251, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:21, Epoch: 54, Batch: 420, Training Loss: 0.03696211501955986, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:22, Epoch: 54, Batch: 430, Training Loss: 0.06395837329328061, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:23, Epoch: 54, Batch: 440, Training Loss: 0.031871866434812546, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:23, Epoch: 54, Batch: 450, Training Loss: 0.03701258823275566, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:24, Epoch: 54, Batch: 460, Training Loss: 0.05737190917134285, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:25, Epoch: 54, Batch: 470, Training Loss: 0.049305353313684464, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:25, Epoch: 54, Batch: 480, Training Loss: 0.04119874276220799, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:26, Epoch: 54, Batch: 490, Training Loss: 0.03925333470106125, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:27, Epoch: 54, Batch: 500, Training Loss: 0.04835727140307426, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:28, Epoch: 54, Batch: 510, Training Loss: 0.057325709611177444, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:28, Epoch: 54, Batch: 520, Training Loss: 0.04122318029403686, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:29, Epoch: 54, Batch: 530, Training Loss: 0.026816772669553755, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:30, Epoch: 54, Batch: 540, Training Loss: 0.04146298356354237, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:30, Epoch: 54, Batch: 550, Training Loss: 0.056618261337280276, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:31, Epoch: 54, Batch: 560, Training Loss: 0.04232203811407089, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:32, Epoch: 54, Batch: 570, Training Loss: 0.036554540321230886, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:33, Epoch: 54, Batch: 580, Training Loss: 0.03186885453760624, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:33, Epoch: 54, Batch: 590, Training Loss: 0.0473119281232357, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:34, Epoch: 54, Batch: 600, Training Loss: 0.043767181038856504, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:35, Epoch: 54, Batch: 610, Training Loss: 0.04130448512732983, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:35, Epoch: 54, Batch: 620, Training Loss: 0.04234256260097027, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:36, Epoch: 54, Batch: 630, Training Loss: 0.04296324253082275, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:37, Epoch: 54, Batch: 640, Training Loss: 0.03662592768669128, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:38, Epoch: 54, Batch: 650, Training Loss: 0.050052400305867197, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:38, Epoch: 54, Batch: 660, Training Loss: 0.02920565977692604, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:39, Epoch: 54, Batch: 670, Training Loss: 0.04766855537891388, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:40, Epoch: 54, Batch: 680, Training Loss: 0.028438151627779008, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:40, Epoch: 54, Batch: 690, Training Loss: 0.05496965274214745, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:41, Epoch: 54, Batch: 700, Training Loss: 0.029194413498044015, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:42, Epoch: 54, Batch: 710, Training Loss: 0.058937210217118265, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:43, Epoch: 54, Batch: 720, Training Loss: 0.04547026604413986, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:43, Epoch: 54, Batch: 730, Training Loss: 0.055545621737837794, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:44, Epoch: 54, Batch: 740, Training Loss: 0.033601905405521396, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:45, Epoch: 54, Batch: 750, Training Loss: 0.043052653595805165, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:45, Epoch: 54, Batch: 760, Training Loss: 0.04364146180450916, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:46, Epoch: 54, Batch: 770, Training Loss: 0.04713314361870289, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:47, Epoch: 54, Batch: 780, Training Loss: 0.039009886980056765, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:48, Epoch: 54, Batch: 790, Training Loss: 0.04249897338449955, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:48, Epoch: 54, Batch: 800, Training Loss: 0.026140305399894714, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:49, Epoch: 54, Batch: 810, Training Loss: 0.04055146351456642, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:50, Epoch: 54, Batch: 820, Training Loss: 0.028485164791345597, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:51, Epoch: 54, Batch: 830, Training Loss: 0.03575594834983349, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:51, Epoch: 54, Batch: 840, Training Loss: 0.0350874163210392, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:52, Epoch: 54, Batch: 850, Training Loss: 0.03997281268239021, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:53, Epoch: 54, Batch: 860, Training Loss: 0.026931647211313248, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:53, Epoch: 54, Batch: 870, Training Loss: 0.035708938539028165, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:54, Epoch: 54, Batch: 880, Training Loss: 0.044870850071310996, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:55, Epoch: 54, Batch: 890, Training Loss: 0.04630844220519066, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:56, Epoch: 54, Batch: 900, Training Loss: 0.05185478404164314, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:56, Epoch: 54, Batch: 910, Training Loss: 0.04556085243821144, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:57, Epoch: 54, Batch: 920, Training Loss: 0.08298877887427807, LR: 0.0010000000000000002
Time, 2019-01-01T18:09:58, Epoch: 54, Batch: 930, Training Loss: 0.04916442446410656, LR: 0.0010000000000000002
Epoch: 54, Validation Top 1 acc: 98.7773208618164
Epoch: 54, Validation Top 5 acc: 99.99000549316406
Epoch: 54, Validation Set Loss: 0.043248191475868225
Start training epoch 55
Time, 2019-01-01T18:10:26, Epoch: 55, Batch: 10, Training Loss: 0.0529952134937048, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:26, Epoch: 55, Batch: 20, Training Loss: 0.04649497233331203, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:27, Epoch: 55, Batch: 30, Training Loss: 0.03489858321845531, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:28, Epoch: 55, Batch: 40, Training Loss: 0.03098049759864807, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:29, Epoch: 55, Batch: 50, Training Loss: 0.050816909968853, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:29, Epoch: 55, Batch: 60, Training Loss: 0.04270734637975693, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:30, Epoch: 55, Batch: 70, Training Loss: 0.04309354797005653, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:31, Epoch: 55, Batch: 80, Training Loss: 0.03938308730721474, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:31, Epoch: 55, Batch: 90, Training Loss: 0.039412052184343335, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:32, Epoch: 55, Batch: 100, Training Loss: 0.03242497220635414, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:33, Epoch: 55, Batch: 110, Training Loss: 0.0479510597884655, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:34, Epoch: 55, Batch: 120, Training Loss: 0.04825277179479599, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:34, Epoch: 55, Batch: 130, Training Loss: 0.04490501061081886, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:35, Epoch: 55, Batch: 140, Training Loss: 0.05146049112081528, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:36, Epoch: 55, Batch: 150, Training Loss: 0.0318158071488142, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:36, Epoch: 55, Batch: 160, Training Loss: 0.036783397942781446, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:37, Epoch: 55, Batch: 170, Training Loss: 0.03681115098297596, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:38, Epoch: 55, Batch: 180, Training Loss: 0.03741546720266342, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:39, Epoch: 55, Batch: 190, Training Loss: 0.04500472247600555, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:39, Epoch: 55, Batch: 200, Training Loss: 0.033324084430933, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:40, Epoch: 55, Batch: 210, Training Loss: 0.040289115905761716, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:41, Epoch: 55, Batch: 220, Training Loss: 0.027315419167280197, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:42, Epoch: 55, Batch: 230, Training Loss: 0.04334306791424751, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:42, Epoch: 55, Batch: 240, Training Loss: 0.03942262604832649, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:43, Epoch: 55, Batch: 250, Training Loss: 0.020820462703704835, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:44, Epoch: 55, Batch: 260, Training Loss: 0.0481856781989336, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:44, Epoch: 55, Batch: 270, Training Loss: 0.03581405207514763, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:45, Epoch: 55, Batch: 280, Training Loss: 0.06407696083188057, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:46, Epoch: 55, Batch: 290, Training Loss: 0.05956137739121914, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:47, Epoch: 55, Batch: 300, Training Loss: 0.03236525952816009, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:47, Epoch: 55, Batch: 310, Training Loss: 0.03166796490550041, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:48, Epoch: 55, Batch: 320, Training Loss: 0.03457859717309475, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:49, Epoch: 55, Batch: 330, Training Loss: 0.03240472674369812, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:49, Epoch: 55, Batch: 340, Training Loss: 0.03627405352890491, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:50, Epoch: 55, Batch: 350, Training Loss: 0.03871184699237347, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:51, Epoch: 55, Batch: 360, Training Loss: 0.057407695800065994, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:52, Epoch: 55, Batch: 370, Training Loss: 0.03529000729322433, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:52, Epoch: 55, Batch: 380, Training Loss: 0.04308774620294571, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:53, Epoch: 55, Batch: 390, Training Loss: 0.04288245365023613, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:54, Epoch: 55, Batch: 400, Training Loss: 0.03949856944382191, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:55, Epoch: 55, Batch: 410, Training Loss: 0.032369377091526985, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:55, Epoch: 55, Batch: 420, Training Loss: 0.04462212398648262, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:56, Epoch: 55, Batch: 430, Training Loss: 0.04216034896671772, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:57, Epoch: 55, Batch: 440, Training Loss: 0.033864966779947284, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:57, Epoch: 55, Batch: 450, Training Loss: 0.062509535998106, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:58, Epoch: 55, Batch: 460, Training Loss: 0.03041986599564552, LR: 0.0010000000000000002
Time, 2019-01-01T18:10:59, Epoch: 55, Batch: 470, Training Loss: 0.02909374125301838, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:00, Epoch: 55, Batch: 480, Training Loss: 0.04361554272472858, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:00, Epoch: 55, Batch: 490, Training Loss: 0.03886147327721119, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:01, Epoch: 55, Batch: 500, Training Loss: 0.033835945278406145, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:02, Epoch: 55, Batch: 510, Training Loss: 0.0750715408474207, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:03, Epoch: 55, Batch: 520, Training Loss: 0.04300832599401474, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:03, Epoch: 55, Batch: 530, Training Loss: 0.04644838571548462, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:04, Epoch: 55, Batch: 540, Training Loss: 0.04073617607355118, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:05, Epoch: 55, Batch: 550, Training Loss: 0.02970651686191559, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:05, Epoch: 55, Batch: 560, Training Loss: 0.07000192739069462, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:06, Epoch: 55, Batch: 570, Training Loss: 0.042628294229507445, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:07, Epoch: 55, Batch: 580, Training Loss: 0.03936086967587471, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:08, Epoch: 55, Batch: 590, Training Loss: 0.03820948302745819, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:08, Epoch: 55, Batch: 600, Training Loss: 0.05673865005373955, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:09, Epoch: 55, Batch: 610, Training Loss: 0.04024547636508942, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:10, Epoch: 55, Batch: 620, Training Loss: 0.051594207808375356, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:10, Epoch: 55, Batch: 630, Training Loss: 0.0544204831123352, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:11, Epoch: 55, Batch: 640, Training Loss: 0.03517374321818352, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:12, Epoch: 55, Batch: 650, Training Loss: 0.044380712881684305, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:13, Epoch: 55, Batch: 660, Training Loss: 0.036210279911756516, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:14, Epoch: 55, Batch: 670, Training Loss: 0.037881947681307794, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:15, Epoch: 55, Batch: 680, Training Loss: 0.04190945029258728, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:16, Epoch: 55, Batch: 690, Training Loss: 0.04353897906839847, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:16, Epoch: 55, Batch: 700, Training Loss: 0.05432148650288582, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:17, Epoch: 55, Batch: 710, Training Loss: 0.027746278792619705, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:18, Epoch: 55, Batch: 720, Training Loss: 0.05014615580439567, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:19, Epoch: 55, Batch: 730, Training Loss: 0.03844004087150097, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:19, Epoch: 55, Batch: 740, Training Loss: 0.050013402476906776, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:20, Epoch: 55, Batch: 750, Training Loss: 0.04822082631289959, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:21, Epoch: 55, Batch: 760, Training Loss: 0.04455543234944344, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:22, Epoch: 55, Batch: 770, Training Loss: 0.03993297889828682, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:22, Epoch: 55, Batch: 780, Training Loss: 0.057835820317268374, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:23, Epoch: 55, Batch: 790, Training Loss: 0.04374093376100063, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:24, Epoch: 55, Batch: 800, Training Loss: 0.045623285323381425, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:25, Epoch: 55, Batch: 810, Training Loss: 0.07299759164452553, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:25, Epoch: 55, Batch: 820, Training Loss: 0.05022859871387482, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:26, Epoch: 55, Batch: 830, Training Loss: 0.051963115483522414, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:27, Epoch: 55, Batch: 840, Training Loss: 0.04984688758850098, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:28, Epoch: 55, Batch: 850, Training Loss: 0.051764659211039545, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:29, Epoch: 55, Batch: 860, Training Loss: 0.046316222101449964, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:29, Epoch: 55, Batch: 870, Training Loss: 0.03459853455424309, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:30, Epoch: 55, Batch: 880, Training Loss: 0.04519523046910763, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:31, Epoch: 55, Batch: 890, Training Loss: 0.058767886087298396, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:32, Epoch: 55, Batch: 900, Training Loss: 0.06575638316571712, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:32, Epoch: 55, Batch: 910, Training Loss: 0.048175379261374475, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:33, Epoch: 55, Batch: 920, Training Loss: 0.04705675803124905, LR: 0.0010000000000000002
Time, 2019-01-01T18:11:34, Epoch: 55, Batch: 930, Training Loss: 0.028849967941641807, LR: 0.0010000000000000002
Epoch: 55, Validation Top 1 acc: 98.80563354492188
Epoch: 55, Validation Top 5 acc: 99.99000549316406
Epoch: 55, Validation Set Loss: 0.042887087911367416
Start training epoch 56
Time, 2019-01-01T18:12:07, Epoch: 56, Batch: 10, Training Loss: 0.04771109744906425, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:08, Epoch: 56, Batch: 20, Training Loss: 0.027815253660082816, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:09, Epoch: 56, Batch: 30, Training Loss: 0.044947107508778575, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:10, Epoch: 56, Batch: 40, Training Loss: 0.017147060483694077, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:10, Epoch: 56, Batch: 50, Training Loss: 0.060162272304296494, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:11, Epoch: 56, Batch: 60, Training Loss: 0.032827083766460416, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:12, Epoch: 56, Batch: 70, Training Loss: 0.03741476312279701, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:13, Epoch: 56, Batch: 80, Training Loss: 0.046680402010679245, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:14, Epoch: 56, Batch: 90, Training Loss: 0.040450767427682874, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:15, Epoch: 56, Batch: 100, Training Loss: 0.025510407611727716, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:16, Epoch: 56, Batch: 110, Training Loss: 0.046140156686306, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:16, Epoch: 56, Batch: 120, Training Loss: 0.04900076314806938, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:17, Epoch: 56, Batch: 130, Training Loss: 0.03622957430779934, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:18, Epoch: 56, Batch: 140, Training Loss: 0.04815843477845192, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:19, Epoch: 56, Batch: 150, Training Loss: 0.04835705682635307, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:19, Epoch: 56, Batch: 160, Training Loss: 0.05215941667556763, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:20, Epoch: 56, Batch: 170, Training Loss: 0.04981781020760536, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:21, Epoch: 56, Batch: 180, Training Loss: 0.043404332175850865, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:22, Epoch: 56, Batch: 190, Training Loss: 0.06245533376932144, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:22, Epoch: 56, Batch: 200, Training Loss: 0.04958499036729336, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:23, Epoch: 56, Batch: 210, Training Loss: 0.0553812675178051, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:24, Epoch: 56, Batch: 220, Training Loss: 0.04870239086449146, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:25, Epoch: 56, Batch: 230, Training Loss: 0.04004172310233116, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:25, Epoch: 56, Batch: 240, Training Loss: 0.02792169228196144, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:26, Epoch: 56, Batch: 250, Training Loss: 0.048912898451089856, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:27, Epoch: 56, Batch: 260, Training Loss: 0.03213965781033039, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:27, Epoch: 56, Batch: 270, Training Loss: 0.04343748837709427, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:28, Epoch: 56, Batch: 280, Training Loss: 0.03241124302148819, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:29, Epoch: 56, Batch: 290, Training Loss: 0.03298887945711613, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:30, Epoch: 56, Batch: 300, Training Loss: 0.051968416944146154, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:30, Epoch: 56, Batch: 310, Training Loss: 0.028606294095516203, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:31, Epoch: 56, Batch: 320, Training Loss: 0.04019389897584915, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:32, Epoch: 56, Batch: 330, Training Loss: 0.04163036271929741, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:33, Epoch: 56, Batch: 340, Training Loss: 0.04507458582520485, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:33, Epoch: 56, Batch: 350, Training Loss: 0.05337132290005684, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:34, Epoch: 56, Batch: 360, Training Loss: 0.03807784467935562, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:35, Epoch: 56, Batch: 370, Training Loss: 0.03722457513213158, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:35, Epoch: 56, Batch: 380, Training Loss: 0.0261530764400959, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:36, Epoch: 56, Batch: 390, Training Loss: 0.03308230005204678, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:37, Epoch: 56, Batch: 400, Training Loss: 0.02557828314602375, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:38, Epoch: 56, Batch: 410, Training Loss: 0.05409507192671299, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:38, Epoch: 56, Batch: 420, Training Loss: 0.051077669858932494, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:39, Epoch: 56, Batch: 430, Training Loss: 0.051504331454634664, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:40, Epoch: 56, Batch: 440, Training Loss: 0.04008999802172184, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:41, Epoch: 56, Batch: 450, Training Loss: 0.06038457751274109, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:41, Epoch: 56, Batch: 460, Training Loss: 0.03441724367439747, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:42, Epoch: 56, Batch: 470, Training Loss: 0.04186311438679695, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:43, Epoch: 56, Batch: 480, Training Loss: 0.06602625139057636, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:44, Epoch: 56, Batch: 490, Training Loss: 0.029570092260837556, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:44, Epoch: 56, Batch: 500, Training Loss: 0.04956225603818894, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:45, Epoch: 56, Batch: 510, Training Loss: 0.0317527387291193, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:46, Epoch: 56, Batch: 520, Training Loss: 0.0592037420719862, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:46, Epoch: 56, Batch: 530, Training Loss: 0.03108164742588997, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:47, Epoch: 56, Batch: 540, Training Loss: 0.04466783404350281, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:48, Epoch: 56, Batch: 550, Training Loss: 0.04282747767865658, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:49, Epoch: 56, Batch: 560, Training Loss: 0.0492271326482296, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:49, Epoch: 56, Batch: 570, Training Loss: 0.05193260908126831, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:50, Epoch: 56, Batch: 580, Training Loss: 0.05680481493473053, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:51, Epoch: 56, Batch: 590, Training Loss: 0.0403794813901186, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:52, Epoch: 56, Batch: 600, Training Loss: 0.03893579617142677, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:52, Epoch: 56, Batch: 610, Training Loss: 0.05234825052320957, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:53, Epoch: 56, Batch: 620, Training Loss: 0.051025239005684855, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:54, Epoch: 56, Batch: 630, Training Loss: 0.033084753155708316, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:55, Epoch: 56, Batch: 640, Training Loss: 0.03483190760016441, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:55, Epoch: 56, Batch: 650, Training Loss: 0.06514869742095471, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:56, Epoch: 56, Batch: 660, Training Loss: 0.03423109576106072, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:57, Epoch: 56, Batch: 670, Training Loss: 0.04140697196125984, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:57, Epoch: 56, Batch: 680, Training Loss: 0.03752259537577629, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:58, Epoch: 56, Batch: 690, Training Loss: 0.03315442875027656, LR: 0.0010000000000000002
Time, 2019-01-01T18:12:59, Epoch: 56, Batch: 700, Training Loss: 0.06233226247131825, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:00, Epoch: 56, Batch: 710, Training Loss: 0.04216906279325485, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:00, Epoch: 56, Batch: 720, Training Loss: 0.03582884669303894, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:01, Epoch: 56, Batch: 730, Training Loss: 0.05114087462425232, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:02, Epoch: 56, Batch: 740, Training Loss: 0.04342868886888027, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:03, Epoch: 56, Batch: 750, Training Loss: 0.06480491980910301, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:03, Epoch: 56, Batch: 760, Training Loss: 0.03198955841362476, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:04, Epoch: 56, Batch: 770, Training Loss: 0.05402126275002957, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:05, Epoch: 56, Batch: 780, Training Loss: 0.040151939168572424, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:05, Epoch: 56, Batch: 790, Training Loss: 0.03483873195946217, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:06, Epoch: 56, Batch: 800, Training Loss: 0.057739420980215075, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:07, Epoch: 56, Batch: 810, Training Loss: 0.04416319243609905, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:08, Epoch: 56, Batch: 820, Training Loss: 0.034376691281795504, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:08, Epoch: 56, Batch: 830, Training Loss: 0.0408581230789423, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:09, Epoch: 56, Batch: 840, Training Loss: 0.05396907478570938, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:10, Epoch: 56, Batch: 850, Training Loss: 0.033201175183057784, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:11, Epoch: 56, Batch: 860, Training Loss: 0.04123690910637379, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:12, Epoch: 56, Batch: 870, Training Loss: 0.04224489480257034, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:12, Epoch: 56, Batch: 880, Training Loss: 0.03806709125638008, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:13, Epoch: 56, Batch: 890, Training Loss: 0.040384257584810256, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:14, Epoch: 56, Batch: 900, Training Loss: 0.03544873408973217, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:15, Epoch: 56, Batch: 910, Training Loss: 0.037929290905594826, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:15, Epoch: 56, Batch: 920, Training Loss: 0.04417249858379364, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:16, Epoch: 56, Batch: 930, Training Loss: 0.06216658391058445, LR: 0.0010000000000000002
Epoch: 56, Validation Top 1 acc: 98.81396484375
Epoch: 56, Validation Top 5 acc: 99.98834228515625
Epoch: 56, Validation Set Loss: 0.04275192692875862
Start training epoch 57
Time, 2019-01-01T18:13:45, Epoch: 57, Batch: 10, Training Loss: 0.07482276409864426, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:46, Epoch: 57, Batch: 20, Training Loss: 0.050135573372244835, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:46, Epoch: 57, Batch: 30, Training Loss: 0.059497741237282756, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:47, Epoch: 57, Batch: 40, Training Loss: 0.047846494987607, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:48, Epoch: 57, Batch: 50, Training Loss: 0.05086346939206123, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:48, Epoch: 57, Batch: 60, Training Loss: 0.04721393473446369, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:49, Epoch: 57, Batch: 70, Training Loss: 0.04372898936271667, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:50, Epoch: 57, Batch: 80, Training Loss: 0.0403885867446661, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:51, Epoch: 57, Batch: 90, Training Loss: 0.05178599990904331, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:51, Epoch: 57, Batch: 100, Training Loss: 0.06969424672424793, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:52, Epoch: 57, Batch: 110, Training Loss: 0.04095242135226727, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:53, Epoch: 57, Batch: 120, Training Loss: 0.048684948682785036, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:54, Epoch: 57, Batch: 130, Training Loss: 0.03405996896326542, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:54, Epoch: 57, Batch: 140, Training Loss: 0.060095999389886856, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:55, Epoch: 57, Batch: 150, Training Loss: 0.03262668512761593, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:56, Epoch: 57, Batch: 160, Training Loss: 0.03197683058679104, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:56, Epoch: 57, Batch: 170, Training Loss: 0.03776544705033302, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:57, Epoch: 57, Batch: 180, Training Loss: 0.03197139948606491, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:58, Epoch: 57, Batch: 190, Training Loss: 0.0429113570600748, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:59, Epoch: 57, Batch: 200, Training Loss: 0.03251220844686031, LR: 0.0010000000000000002
Time, 2019-01-01T18:13:59, Epoch: 57, Batch: 210, Training Loss: 0.04330495744943619, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:00, Epoch: 57, Batch: 220, Training Loss: 0.0634751170873642, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:01, Epoch: 57, Batch: 230, Training Loss: 0.03383873924612999, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:01, Epoch: 57, Batch: 240, Training Loss: 0.039218460023403165, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:02, Epoch: 57, Batch: 250, Training Loss: 0.04625041708350182, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:03, Epoch: 57, Batch: 260, Training Loss: 0.048465029895305635, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:04, Epoch: 57, Batch: 270, Training Loss: 0.0344206053763628, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:04, Epoch: 57, Batch: 280, Training Loss: 0.03391457498073578, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:05, Epoch: 57, Batch: 290, Training Loss: 0.04685646370053291, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:06, Epoch: 57, Batch: 300, Training Loss: 0.06212949119508267, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:07, Epoch: 57, Batch: 310, Training Loss: 0.042245029285550115, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:07, Epoch: 57, Batch: 320, Training Loss: 0.05498548150062561, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:08, Epoch: 57, Batch: 330, Training Loss: 0.04610427916049957, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:09, Epoch: 57, Batch: 340, Training Loss: 0.03180519863963127, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:10, Epoch: 57, Batch: 350, Training Loss: 0.027948106080293654, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:10, Epoch: 57, Batch: 360, Training Loss: 0.03566022887825966, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:11, Epoch: 57, Batch: 370, Training Loss: 0.051042165979743, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:12, Epoch: 57, Batch: 380, Training Loss: 0.04220970347523689, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:13, Epoch: 57, Batch: 390, Training Loss: 0.04288967177271843, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:13, Epoch: 57, Batch: 400, Training Loss: 0.03851157873868942, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:14, Epoch: 57, Batch: 410, Training Loss: 0.0409510251134634, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:15, Epoch: 57, Batch: 420, Training Loss: 0.029733624681830405, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:16, Epoch: 57, Batch: 430, Training Loss: 0.03223399892449379, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:16, Epoch: 57, Batch: 440, Training Loss: 0.03501315601170063, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:17, Epoch: 57, Batch: 450, Training Loss: 0.0717179711908102, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:18, Epoch: 57, Batch: 460, Training Loss: 0.036703994125127794, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:18, Epoch: 57, Batch: 470, Training Loss: 0.03846730813384056, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:19, Epoch: 57, Batch: 480, Training Loss: 0.03282510414719582, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:20, Epoch: 57, Batch: 490, Training Loss: 0.05461172536015511, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:21, Epoch: 57, Batch: 500, Training Loss: 0.05325878001749516, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:21, Epoch: 57, Batch: 510, Training Loss: 0.031588036194443704, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:22, Epoch: 57, Batch: 520, Training Loss: 0.04481481276452541, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:23, Epoch: 57, Batch: 530, Training Loss: 0.042163956165313723, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:24, Epoch: 57, Batch: 540, Training Loss: 0.06781558617949486, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:24, Epoch: 57, Batch: 550, Training Loss: 0.06701532676815987, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:25, Epoch: 57, Batch: 560, Training Loss: 0.037248126789927484, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:26, Epoch: 57, Batch: 570, Training Loss: 0.03195549696683884, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:26, Epoch: 57, Batch: 580, Training Loss: 0.028769323229789735, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:27, Epoch: 57, Batch: 590, Training Loss: 0.027915900200605394, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:28, Epoch: 57, Batch: 600, Training Loss: 0.044210847839713095, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:29, Epoch: 57, Batch: 610, Training Loss: 0.0519032433629036, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:29, Epoch: 57, Batch: 620, Training Loss: 0.031448859721422195, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:30, Epoch: 57, Batch: 630, Training Loss: 0.038657990470528605, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:31, Epoch: 57, Batch: 640, Training Loss: 0.04757300764322281, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:32, Epoch: 57, Batch: 650, Training Loss: 0.048737405240535735, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:32, Epoch: 57, Batch: 660, Training Loss: 0.04543783962726593, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:33, Epoch: 57, Batch: 670, Training Loss: 0.04049904681742191, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:34, Epoch: 57, Batch: 680, Training Loss: 0.06629635468125343, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:34, Epoch: 57, Batch: 690, Training Loss: 0.03778076097369194, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:35, Epoch: 57, Batch: 700, Training Loss: 0.06334460601210594, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:36, Epoch: 57, Batch: 710, Training Loss: 0.06210615187883377, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:37, Epoch: 57, Batch: 720, Training Loss: 0.043736522644758226, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:37, Epoch: 57, Batch: 730, Training Loss: 0.036638807505369186, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:38, Epoch: 57, Batch: 740, Training Loss: 0.04354376494884491, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:39, Epoch: 57, Batch: 750, Training Loss: 0.02632158324122429, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:39, Epoch: 57, Batch: 760, Training Loss: 0.04077060520648956, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:40, Epoch: 57, Batch: 770, Training Loss: 0.042609254270792006, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:41, Epoch: 57, Batch: 780, Training Loss: 0.044171535223722455, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:42, Epoch: 57, Batch: 790, Training Loss: 0.0297020073980093, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:42, Epoch: 57, Batch: 800, Training Loss: 0.049795038998126984, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:44, Epoch: 57, Batch: 810, Training Loss: 0.03837736621499062, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:44, Epoch: 57, Batch: 820, Training Loss: 0.04064379408955574, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:45, Epoch: 57, Batch: 830, Training Loss: 0.027581632882356644, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:46, Epoch: 57, Batch: 840, Training Loss: 0.03537979945540428, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:47, Epoch: 57, Batch: 850, Training Loss: 0.03879155181348324, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:48, Epoch: 57, Batch: 860, Training Loss: 0.05137329250574112, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:48, Epoch: 57, Batch: 870, Training Loss: 0.036898309364914894, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:49, Epoch: 57, Batch: 880, Training Loss: 0.04761105924844742, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:50, Epoch: 57, Batch: 890, Training Loss: 0.039544454962015155, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:51, Epoch: 57, Batch: 900, Training Loss: 0.039920200034976004, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:51, Epoch: 57, Batch: 910, Training Loss: 0.03345775976777077, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:52, Epoch: 57, Batch: 920, Training Loss: 0.03970372900366783, LR: 0.0010000000000000002
Time, 2019-01-01T18:14:53, Epoch: 57, Batch: 930, Training Loss: 0.032446162402629854, LR: 0.0010000000000000002
Epoch: 57, Validation Top 1 acc: 98.79397583007812
Epoch: 57, Validation Top 5 acc: 99.98834228515625
Epoch: 57, Validation Set Loss: 0.04247689247131348
Start training epoch 58
Time, 2019-01-01T18:15:22, Epoch: 58, Batch: 10, Training Loss: 0.03035150095820427, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:23, Epoch: 58, Batch: 20, Training Loss: 0.05446628853678703, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:24, Epoch: 58, Batch: 30, Training Loss: 0.052558381110429764, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:25, Epoch: 58, Batch: 40, Training Loss: 0.03902034908533096, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:25, Epoch: 58, Batch: 50, Training Loss: 0.05529556833207607, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:26, Epoch: 58, Batch: 60, Training Loss: 0.03726719431579113, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:27, Epoch: 58, Batch: 70, Training Loss: 0.04218243323266506, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:27, Epoch: 58, Batch: 80, Training Loss: 0.07474160008132458, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:28, Epoch: 58, Batch: 90, Training Loss: 0.05706229507923126, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:29, Epoch: 58, Batch: 100, Training Loss: 0.02685927748680115, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:30, Epoch: 58, Batch: 110, Training Loss: 0.04737334214150905, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:30, Epoch: 58, Batch: 120, Training Loss: 0.03474052436649799, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:31, Epoch: 58, Batch: 130, Training Loss: 0.041723527014255524, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:32, Epoch: 58, Batch: 140, Training Loss: 0.03515898063778877, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:33, Epoch: 58, Batch: 150, Training Loss: 0.04802514910697937, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:33, Epoch: 58, Batch: 160, Training Loss: 0.03243201859295368, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:34, Epoch: 58, Batch: 170, Training Loss: 0.058578786253929135, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:35, Epoch: 58, Batch: 180, Training Loss: 0.04948004521429539, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:35, Epoch: 58, Batch: 190, Training Loss: 0.029829569160938263, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:36, Epoch: 58, Batch: 200, Training Loss: 0.05110461823642254, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:37, Epoch: 58, Batch: 210, Training Loss: 0.0336164515465498, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:38, Epoch: 58, Batch: 220, Training Loss: 0.02310667298734188, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:38, Epoch: 58, Batch: 230, Training Loss: 0.04840692952275276, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:39, Epoch: 58, Batch: 240, Training Loss: 0.061030270904302596, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:40, Epoch: 58, Batch: 250, Training Loss: 0.03749222345650196, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:41, Epoch: 58, Batch: 260, Training Loss: 0.04464393556118011, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:41, Epoch: 58, Batch: 270, Training Loss: 0.03490469083189964, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:42, Epoch: 58, Batch: 280, Training Loss: 0.04209006950259209, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:43, Epoch: 58, Batch: 290, Training Loss: 0.04263192191720009, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:44, Epoch: 58, Batch: 300, Training Loss: 0.04574669152498245, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:44, Epoch: 58, Batch: 310, Training Loss: 0.04729088693857193, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:45, Epoch: 58, Batch: 320, Training Loss: 0.05654858313500881, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:46, Epoch: 58, Batch: 330, Training Loss: 0.04700086191296578, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:47, Epoch: 58, Batch: 340, Training Loss: 0.03451398387551308, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:48, Epoch: 58, Batch: 350, Training Loss: 0.048251505196094516, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:48, Epoch: 58, Batch: 360, Training Loss: 0.04248441010713577, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:49, Epoch: 58, Batch: 370, Training Loss: 0.03966135531663895, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:50, Epoch: 58, Batch: 380, Training Loss: 0.039383499324321745, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:51, Epoch: 58, Batch: 390, Training Loss: 0.05385627336800099, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:52, Epoch: 58, Batch: 400, Training Loss: 0.03400067687034607, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:53, Epoch: 58, Batch: 410, Training Loss: 0.04510299079120159, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:53, Epoch: 58, Batch: 420, Training Loss: 0.048741009831428525, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:54, Epoch: 58, Batch: 430, Training Loss: 0.057488593831658365, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:55, Epoch: 58, Batch: 440, Training Loss: 0.048113863170146945, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:56, Epoch: 58, Batch: 450, Training Loss: 0.04836254045367241, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:56, Epoch: 58, Batch: 460, Training Loss: 0.03470097854733467, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:57, Epoch: 58, Batch: 470, Training Loss: 0.03830107375979423, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:58, Epoch: 58, Batch: 480, Training Loss: 0.05437692813575268, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:58, Epoch: 58, Batch: 490, Training Loss: 0.04353000931441784, LR: 0.0010000000000000002
Time, 2019-01-01T18:15:59, Epoch: 58, Batch: 500, Training Loss: 0.024620360136032103, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:00, Epoch: 58, Batch: 510, Training Loss: 0.04976294338703156, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:01, Epoch: 58, Batch: 520, Training Loss: 0.039445622637867925, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:01, Epoch: 58, Batch: 530, Training Loss: 0.03463245406746864, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:02, Epoch: 58, Batch: 540, Training Loss: 0.05067312568426132, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:03, Epoch: 58, Batch: 550, Training Loss: 0.027801461145281793, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:04, Epoch: 58, Batch: 560, Training Loss: 0.04935748353600502, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:05, Epoch: 58, Batch: 570, Training Loss: 0.05198509693145752, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:05, Epoch: 58, Batch: 580, Training Loss: 0.05593457147479057, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:06, Epoch: 58, Batch: 590, Training Loss: 0.029156192392110824, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:07, Epoch: 58, Batch: 600, Training Loss: 0.03018319234251976, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:08, Epoch: 58, Batch: 610, Training Loss: 0.03466951660811901, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:08, Epoch: 58, Batch: 620, Training Loss: 0.05672786980867386, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:09, Epoch: 58, Batch: 630, Training Loss: 0.046630892902612686, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:10, Epoch: 58, Batch: 640, Training Loss: 0.04986196644604206, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:11, Epoch: 58, Batch: 650, Training Loss: 0.03742766156792641, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:11, Epoch: 58, Batch: 660, Training Loss: 0.03615045100450516, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:12, Epoch: 58, Batch: 670, Training Loss: 0.054322000965476035, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:13, Epoch: 58, Batch: 680, Training Loss: 0.03095315247774124, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:14, Epoch: 58, Batch: 690, Training Loss: 0.0372813418507576, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:14, Epoch: 58, Batch: 700, Training Loss: 0.03650994449853897, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:15, Epoch: 58, Batch: 710, Training Loss: 0.06450512036681175, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:16, Epoch: 58, Batch: 720, Training Loss: 0.03348971232771873, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:17, Epoch: 58, Batch: 730, Training Loss: 0.02835860848426819, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:17, Epoch: 58, Batch: 740, Training Loss: 0.03269098587334156, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:18, Epoch: 58, Batch: 750, Training Loss: 0.03505057319998741, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:19, Epoch: 58, Batch: 760, Training Loss: 0.041409043967723845, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:20, Epoch: 58, Batch: 770, Training Loss: 0.0248009517788887, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:20, Epoch: 58, Batch: 780, Training Loss: 0.03739637918770313, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:21, Epoch: 58, Batch: 790, Training Loss: 0.045863700285553935, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:22, Epoch: 58, Batch: 800, Training Loss: 0.05148925334215164, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:22, Epoch: 58, Batch: 810, Training Loss: 0.06014816612005234, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:23, Epoch: 58, Batch: 820, Training Loss: 0.049958803504705426, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:24, Epoch: 58, Batch: 830, Training Loss: 0.03637233823537826, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:25, Epoch: 58, Batch: 840, Training Loss: 0.033893735706806184, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:25, Epoch: 58, Batch: 850, Training Loss: 0.0462671373039484, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:26, Epoch: 58, Batch: 860, Training Loss: 0.03967709578573704, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:27, Epoch: 58, Batch: 870, Training Loss: 0.05472402758896351, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:28, Epoch: 58, Batch: 880, Training Loss: 0.04199601002037525, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:28, Epoch: 58, Batch: 890, Training Loss: 0.04323580861091614, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:29, Epoch: 58, Batch: 900, Training Loss: 0.02605528011918068, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:30, Epoch: 58, Batch: 910, Training Loss: 0.059453700482845304, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:31, Epoch: 58, Batch: 920, Training Loss: 0.027325164899230002, LR: 0.0010000000000000002
Time, 2019-01-01T18:16:31, Epoch: 58, Batch: 930, Training Loss: 0.07150812670588494, LR: 0.0010000000000000002
Epoch: 58, Validation Top 1 acc: 98.79563903808594
Epoch: 58, Validation Top 5 acc: 99.99166870117188
Epoch: 58, Validation Set Loss: 0.04289329797029495
Start training epoch 59
Time, 2019-01-01T18:17:00, Epoch: 59, Batch: 10, Training Loss: 0.02876824028789997, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:00, Epoch: 59, Batch: 20, Training Loss: 0.05177352279424667, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:01, Epoch: 59, Batch: 30, Training Loss: 0.03972376361489296, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:02, Epoch: 59, Batch: 40, Training Loss: 0.03747902438044548, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:03, Epoch: 59, Batch: 50, Training Loss: 0.0417102824896574, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:03, Epoch: 59, Batch: 60, Training Loss: 0.0419376228004694, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:04, Epoch: 59, Batch: 70, Training Loss: 0.028633880987763406, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:05, Epoch: 59, Batch: 80, Training Loss: 0.05706087648868561, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:05, Epoch: 59, Batch: 90, Training Loss: 0.047816759720444676, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:06, Epoch: 59, Batch: 100, Training Loss: 0.04547845870256424, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:07, Epoch: 59, Batch: 110, Training Loss: 0.03756849095225334, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:08, Epoch: 59, Batch: 120, Training Loss: 0.05868977680802345, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:08, Epoch: 59, Batch: 130, Training Loss: 0.04559852220118046, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:09, Epoch: 59, Batch: 140, Training Loss: 0.03546894416213035, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:10, Epoch: 59, Batch: 150, Training Loss: 0.042456377670168875, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:10, Epoch: 59, Batch: 160, Training Loss: 0.05194609984755516, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:11, Epoch: 59, Batch: 170, Training Loss: 0.035879985243082044, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:12, Epoch: 59, Batch: 180, Training Loss: 0.04021021798253059, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:13, Epoch: 59, Batch: 190, Training Loss: 0.043684685975313185, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:13, Epoch: 59, Batch: 200, Training Loss: 0.03961833342909813, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:14, Epoch: 59, Batch: 210, Training Loss: 0.03242098465561867, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:15, Epoch: 59, Batch: 220, Training Loss: 0.043443360924720766, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:16, Epoch: 59, Batch: 230, Training Loss: 0.048162488639354704, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:16, Epoch: 59, Batch: 240, Training Loss: 0.03932269141077995, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:17, Epoch: 59, Batch: 250, Training Loss: 0.04449244439601898, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:18, Epoch: 59, Batch: 260, Training Loss: 0.050346051901578905, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:18, Epoch: 59, Batch: 270, Training Loss: 0.05038592629134655, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:19, Epoch: 59, Batch: 280, Training Loss: 0.040540005639195444, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:20, Epoch: 59, Batch: 290, Training Loss: 0.05453521274030208, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:21, Epoch: 59, Batch: 300, Training Loss: 0.04730972945690155, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:21, Epoch: 59, Batch: 310, Training Loss: 0.04749072380363941, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:22, Epoch: 59, Batch: 320, Training Loss: 0.02791106253862381, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:23, Epoch: 59, Batch: 330, Training Loss: 0.03651449531316757, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:23, Epoch: 59, Batch: 340, Training Loss: 0.05368269197642803, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:24, Epoch: 59, Batch: 350, Training Loss: 0.042954251170158386, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:25, Epoch: 59, Batch: 360, Training Loss: 0.03193533830344677, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:26, Epoch: 59, Batch: 370, Training Loss: 0.03197607658803463, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:26, Epoch: 59, Batch: 380, Training Loss: 0.030966514348983766, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:27, Epoch: 59, Batch: 390, Training Loss: 0.04760800488293171, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:28, Epoch: 59, Batch: 400, Training Loss: 0.05361099094152451, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:29, Epoch: 59, Batch: 410, Training Loss: 0.055972423404455185, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:29, Epoch: 59, Batch: 420, Training Loss: 0.03696125522255898, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:30, Epoch: 59, Batch: 430, Training Loss: 0.03520080223679543, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:31, Epoch: 59, Batch: 440, Training Loss: 0.04566625207662582, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:31, Epoch: 59, Batch: 450, Training Loss: 0.037548846751451495, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:32, Epoch: 59, Batch: 460, Training Loss: 0.05046146437525749, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:33, Epoch: 59, Batch: 470, Training Loss: 0.044694050773978235, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:34, Epoch: 59, Batch: 480, Training Loss: 0.06195104606449604, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:34, Epoch: 59, Batch: 490, Training Loss: 0.05764777027070522, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:35, Epoch: 59, Batch: 500, Training Loss: 0.04230303131043911, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:36, Epoch: 59, Batch: 510, Training Loss: 0.04985028728842735, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:36, Epoch: 59, Batch: 520, Training Loss: 0.037648939341306684, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:37, Epoch: 59, Batch: 530, Training Loss: 0.04421220161020756, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:38, Epoch: 59, Batch: 540, Training Loss: 0.03324755094945431, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:39, Epoch: 59, Batch: 550, Training Loss: 0.027991927415132522, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:39, Epoch: 59, Batch: 560, Training Loss: 0.05162931829690933, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:40, Epoch: 59, Batch: 570, Training Loss: 0.03613606616854668, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:41, Epoch: 59, Batch: 580, Training Loss: 0.06150983422994614, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:42, Epoch: 59, Batch: 590, Training Loss: 0.024278710409998894, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:42, Epoch: 59, Batch: 600, Training Loss: 0.051958038657903674, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:43, Epoch: 59, Batch: 610, Training Loss: 0.03201957046985626, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:44, Epoch: 59, Batch: 620, Training Loss: 0.034840039908885956, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:44, Epoch: 59, Batch: 630, Training Loss: 0.04234054535627365, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:45, Epoch: 59, Batch: 640, Training Loss: 0.04563109092414379, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:46, Epoch: 59, Batch: 650, Training Loss: 0.04248157255351544, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:47, Epoch: 59, Batch: 660, Training Loss: 0.05385291762650013, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:47, Epoch: 59, Batch: 670, Training Loss: 0.048188816010952, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:48, Epoch: 59, Batch: 680, Training Loss: 0.03691880106925964, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:49, Epoch: 59, Batch: 690, Training Loss: 0.049336128681898114, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:49, Epoch: 59, Batch: 700, Training Loss: 0.05991850830614567, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:50, Epoch: 59, Batch: 710, Training Loss: 0.047078431397676465, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:51, Epoch: 59, Batch: 720, Training Loss: 0.04183522388339043, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:52, Epoch: 59, Batch: 730, Training Loss: 0.045203198492527005, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:52, Epoch: 59, Batch: 740, Training Loss: 0.040212303027510644, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:53, Epoch: 59, Batch: 750, Training Loss: 0.06482415609061717, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:54, Epoch: 59, Batch: 760, Training Loss: 0.06818746402859688, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:55, Epoch: 59, Batch: 770, Training Loss: 0.03900276012718677, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:55, Epoch: 59, Batch: 780, Training Loss: 0.05243631303310394, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:56, Epoch: 59, Batch: 790, Training Loss: 0.05465423911809921, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:57, Epoch: 59, Batch: 800, Training Loss: 0.0449327141046524, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:57, Epoch: 59, Batch: 810, Training Loss: 0.028181317448616027, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:58, Epoch: 59, Batch: 820, Training Loss: 0.04606655463576317, LR: 0.0010000000000000002
Time, 2019-01-01T18:17:59, Epoch: 59, Batch: 830, Training Loss: 0.041160864382982255, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:00, Epoch: 59, Batch: 840, Training Loss: 0.0493023507297039, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:00, Epoch: 59, Batch: 850, Training Loss: 0.04426769316196442, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:01, Epoch: 59, Batch: 860, Training Loss: 0.055692434310913086, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:02, Epoch: 59, Batch: 870, Training Loss: 0.035678330808877945, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:02, Epoch: 59, Batch: 880, Training Loss: 0.04050329178571701, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:03, Epoch: 59, Batch: 890, Training Loss: 0.03593610823154449, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:04, Epoch: 59, Batch: 900, Training Loss: 0.03337446413934231, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:05, Epoch: 59, Batch: 910, Training Loss: 0.035324099287390706, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:05, Epoch: 59, Batch: 920, Training Loss: 0.04029238782823086, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:06, Epoch: 59, Batch: 930, Training Loss: 0.03656894899904728, LR: 0.0010000000000000002
Epoch: 59, Validation Top 1 acc: 98.78064727783203
Epoch: 59, Validation Top 5 acc: 99.99000549316406
Epoch: 59, Validation Set Loss: 0.04296845942735672
Start training epoch 60
Time, 2019-01-01T18:18:34, Epoch: 60, Batch: 10, Training Loss: 0.04321170374751091, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:35, Epoch: 60, Batch: 20, Training Loss: 0.0562721386551857, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:36, Epoch: 60, Batch: 30, Training Loss: 0.04622734785079956, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:36, Epoch: 60, Batch: 40, Training Loss: 0.046233910322189334, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:37, Epoch: 60, Batch: 50, Training Loss: 0.039076749607920645, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:38, Epoch: 60, Batch: 60, Training Loss: 0.026529832184314726, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:38, Epoch: 60, Batch: 70, Training Loss: 0.032882393896579744, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:39, Epoch: 60, Batch: 80, Training Loss: 0.0431598961353302, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:40, Epoch: 60, Batch: 90, Training Loss: 0.03935912325978279, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:41, Epoch: 60, Batch: 100, Training Loss: 0.06118008866906166, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:41, Epoch: 60, Batch: 110, Training Loss: 0.046011080592870714, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:42, Epoch: 60, Batch: 120, Training Loss: 0.03615723997354507, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:43, Epoch: 60, Batch: 130, Training Loss: 0.060983195155858996, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:44, Epoch: 60, Batch: 140, Training Loss: 0.041541813686490056, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:44, Epoch: 60, Batch: 150, Training Loss: 0.03784261643886566, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:45, Epoch: 60, Batch: 160, Training Loss: 0.05211502723395824, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:46, Epoch: 60, Batch: 170, Training Loss: 0.055545884743332864, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:46, Epoch: 60, Batch: 180, Training Loss: 0.02687997743487358, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:47, Epoch: 60, Batch: 190, Training Loss: 0.04370473027229309, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:48, Epoch: 60, Batch: 200, Training Loss: 0.030319851264357565, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:49, Epoch: 60, Batch: 210, Training Loss: 0.03866662420332432, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:49, Epoch: 60, Batch: 220, Training Loss: 0.048797405511140826, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:50, Epoch: 60, Batch: 230, Training Loss: 0.03642721772193909, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:51, Epoch: 60, Batch: 240, Training Loss: 0.027512957528233527, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:51, Epoch: 60, Batch: 250, Training Loss: 0.05848640650510788, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:52, Epoch: 60, Batch: 260, Training Loss: 0.058323025703430176, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:53, Epoch: 60, Batch: 270, Training Loss: 0.034281987324357034, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:54, Epoch: 60, Batch: 280, Training Loss: 0.04909416511654854, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:54, Epoch: 60, Batch: 290, Training Loss: 0.04801476188004017, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:55, Epoch: 60, Batch: 300, Training Loss: 0.039299574494361875, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:56, Epoch: 60, Batch: 310, Training Loss: 0.05605813227593899, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:57, Epoch: 60, Batch: 320, Training Loss: 0.04763134382665157, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:57, Epoch: 60, Batch: 330, Training Loss: 0.04677030593156815, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:58, Epoch: 60, Batch: 340, Training Loss: 0.047191346436738967, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:59, Epoch: 60, Batch: 350, Training Loss: 0.04330783747136593, LR: 0.0010000000000000002
Time, 2019-01-01T18:18:59, Epoch: 60, Batch: 360, Training Loss: 0.04182824976742268, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:00, Epoch: 60, Batch: 370, Training Loss: 0.03600054606795311, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:01, Epoch: 60, Batch: 380, Training Loss: 0.05824953317642212, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:02, Epoch: 60, Batch: 390, Training Loss: 0.05808786153793335, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:02, Epoch: 60, Batch: 400, Training Loss: 0.028784146532416344, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:03, Epoch: 60, Batch: 410, Training Loss: 0.03831387124955654, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:04, Epoch: 60, Batch: 420, Training Loss: 0.04250252321362495, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:05, Epoch: 60, Batch: 430, Training Loss: 0.029460806772112845, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:05, Epoch: 60, Batch: 440, Training Loss: 0.034287247061729434, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:06, Epoch: 60, Batch: 450, Training Loss: 0.04795030951499939, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:07, Epoch: 60, Batch: 460, Training Loss: 0.04392907842993736, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:07, Epoch: 60, Batch: 470, Training Loss: 0.055128566548228265, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:08, Epoch: 60, Batch: 480, Training Loss: 0.08166058100759983, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:09, Epoch: 60, Batch: 490, Training Loss: 0.04915829487144947, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:10, Epoch: 60, Batch: 500, Training Loss: 0.03821600191295147, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:10, Epoch: 60, Batch: 510, Training Loss: 0.029093595594167708, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:11, Epoch: 60, Batch: 520, Training Loss: 0.03127015605568886, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:12, Epoch: 60, Batch: 530, Training Loss: 0.050506194680929185, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:12, Epoch: 60, Batch: 540, Training Loss: 0.03313918523490429, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:13, Epoch: 60, Batch: 550, Training Loss: 0.03497603796422481, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:14, Epoch: 60, Batch: 560, Training Loss: 0.0634748987853527, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:15, Epoch: 60, Batch: 570, Training Loss: 0.04259136542677879, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:15, Epoch: 60, Batch: 580, Training Loss: 0.041056849807500836, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:16, Epoch: 60, Batch: 590, Training Loss: 0.04505553767085076, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:17, Epoch: 60, Batch: 600, Training Loss: 0.03842129521071911, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:18, Epoch: 60, Batch: 610, Training Loss: 0.034563615918159485, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:18, Epoch: 60, Batch: 620, Training Loss: 0.036773770302534106, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:19, Epoch: 60, Batch: 630, Training Loss: 0.03410426564514637, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:20, Epoch: 60, Batch: 640, Training Loss: 0.05699833482503891, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:20, Epoch: 60, Batch: 650, Training Loss: 0.03912443108856678, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:21, Epoch: 60, Batch: 660, Training Loss: 0.04148859828710556, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:22, Epoch: 60, Batch: 670, Training Loss: 0.039128818362951276, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:23, Epoch: 60, Batch: 680, Training Loss: 0.025789427012205123, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:23, Epoch: 60, Batch: 690, Training Loss: 0.05626799017190933, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:24, Epoch: 60, Batch: 700, Training Loss: 0.027083174884319307, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:25, Epoch: 60, Batch: 710, Training Loss: 0.049214483425021174, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:25, Epoch: 60, Batch: 720, Training Loss: 0.055216753482818605, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:26, Epoch: 60, Batch: 730, Training Loss: 0.05315843895077706, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:27, Epoch: 60, Batch: 740, Training Loss: 0.03753154501318932, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:28, Epoch: 60, Batch: 750, Training Loss: 0.03302222862839699, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:28, Epoch: 60, Batch: 760, Training Loss: 0.045751890540122984, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:29, Epoch: 60, Batch: 770, Training Loss: 0.03669562079012394, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:30, Epoch: 60, Batch: 780, Training Loss: 0.03151270151138306, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:31, Epoch: 60, Batch: 790, Training Loss: 0.03710966557264328, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:31, Epoch: 60, Batch: 800, Training Loss: 0.03673991523683071, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:32, Epoch: 60, Batch: 810, Training Loss: 0.03026241473853588, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:33, Epoch: 60, Batch: 820, Training Loss: 0.0404309906065464, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:33, Epoch: 60, Batch: 830, Training Loss: 0.04464502707123756, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:34, Epoch: 60, Batch: 840, Training Loss: 0.05242741815745831, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:35, Epoch: 60, Batch: 850, Training Loss: 0.07827220559120178, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:36, Epoch: 60, Batch: 860, Training Loss: 0.04695190116763115, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:36, Epoch: 60, Batch: 870, Training Loss: 0.033607688173651694, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:37, Epoch: 60, Batch: 880, Training Loss: 0.03592485189437866, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:38, Epoch: 60, Batch: 890, Training Loss: 0.049238051474094394, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:38, Epoch: 60, Batch: 900, Training Loss: 0.04271477684378624, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:39, Epoch: 60, Batch: 910, Training Loss: 0.03301207050681114, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:40, Epoch: 60, Batch: 920, Training Loss: 0.046933000162243846, LR: 0.0010000000000000002
Time, 2019-01-01T18:19:41, Epoch: 60, Batch: 930, Training Loss: 0.03770451061427593, LR: 0.0010000000000000002
Epoch: 60, Validation Top 1 acc: 98.83562469482422
Epoch: 60, Validation Top 5 acc: 99.99166870117188
Epoch: 60, Validation Set Loss: 0.042295534163713455
Start training epoch 61
Time, 2019-01-01T18:20:08, Epoch: 61, Batch: 10, Training Loss: 0.05097922533750534, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:09, Epoch: 61, Batch: 20, Training Loss: 0.03512720540165901, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:10, Epoch: 61, Batch: 30, Training Loss: 0.032314648106694224, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:11, Epoch: 61, Batch: 40, Training Loss: 0.024420810490846635, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:11, Epoch: 61, Batch: 50, Training Loss: 0.03563370481133461, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:12, Epoch: 61, Batch: 60, Training Loss: 0.04279189929366112, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:13, Epoch: 61, Batch: 70, Training Loss: 0.032872239500284194, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:13, Epoch: 61, Batch: 80, Training Loss: 0.04173719882965088, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:14, Epoch: 61, Batch: 90, Training Loss: 0.041668199747800824, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:15, Epoch: 61, Batch: 100, Training Loss: 0.029761425405740737, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:16, Epoch: 61, Batch: 110, Training Loss: 0.041585564613342285, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:16, Epoch: 61, Batch: 120, Training Loss: 0.05524768903851509, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:17, Epoch: 61, Batch: 130, Training Loss: 0.06371791549026966, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:18, Epoch: 61, Batch: 140, Training Loss: 0.03277460411190987, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:18, Epoch: 61, Batch: 150, Training Loss: 0.06540053375065327, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:19, Epoch: 61, Batch: 160, Training Loss: 0.039014137163758275, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:20, Epoch: 61, Batch: 170, Training Loss: 0.03102184385061264, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:21, Epoch: 61, Batch: 180, Training Loss: 0.038561828434467316, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:21, Epoch: 61, Batch: 190, Training Loss: 0.050805724784731866, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:22, Epoch: 61, Batch: 200, Training Loss: 0.042613666504621506, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:23, Epoch: 61, Batch: 210, Training Loss: 0.043276799097657206, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:23, Epoch: 61, Batch: 220, Training Loss: 0.03170410804450512, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:24, Epoch: 61, Batch: 230, Training Loss: 0.031211039051413536, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:25, Epoch: 61, Batch: 240, Training Loss: 0.03408765085041523, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:26, Epoch: 61, Batch: 250, Training Loss: 0.06262538656592369, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:26, Epoch: 61, Batch: 260, Training Loss: 0.05775472596287727, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:27, Epoch: 61, Batch: 270, Training Loss: 0.027087774127721786, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:28, Epoch: 61, Batch: 280, Training Loss: 0.03632588721811771, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:28, Epoch: 61, Batch: 290, Training Loss: 0.02881646603345871, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:29, Epoch: 61, Batch: 300, Training Loss: 0.03924648799002171, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:30, Epoch: 61, Batch: 310, Training Loss: 0.050568345189094546, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:31, Epoch: 61, Batch: 320, Training Loss: 0.06562194451689721, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:31, Epoch: 61, Batch: 330, Training Loss: 0.028665997460484503, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:32, Epoch: 61, Batch: 340, Training Loss: 0.04996550902724266, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:33, Epoch: 61, Batch: 350, Training Loss: 0.0412692055106163, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:33, Epoch: 61, Batch: 360, Training Loss: 0.052744091302156446, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:34, Epoch: 61, Batch: 370, Training Loss: 0.02084053009748459, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:35, Epoch: 61, Batch: 380, Training Loss: 0.03892408683896065, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:36, Epoch: 61, Batch: 390, Training Loss: 0.03764859959483147, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:36, Epoch: 61, Batch: 400, Training Loss: 0.047446203604340556, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:37, Epoch: 61, Batch: 410, Training Loss: 0.04061883464455605, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:38, Epoch: 61, Batch: 420, Training Loss: 0.05789207592606545, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:39, Epoch: 61, Batch: 430, Training Loss: 0.048740992695093154, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:39, Epoch: 61, Batch: 440, Training Loss: 0.05711302012205124, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:40, Epoch: 61, Batch: 450, Training Loss: 0.03127896450459957, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:41, Epoch: 61, Batch: 460, Training Loss: 0.03113378807902336, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:41, Epoch: 61, Batch: 470, Training Loss: 0.04670540690422058, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:42, Epoch: 61, Batch: 480, Training Loss: 0.04636108763515949, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:43, Epoch: 61, Batch: 490, Training Loss: 0.039269663766026495, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:44, Epoch: 61, Batch: 500, Training Loss: 0.06513579376041889, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:44, Epoch: 61, Batch: 510, Training Loss: 0.0433298584073782, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:45, Epoch: 61, Batch: 520, Training Loss: 0.03681667856872082, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:46, Epoch: 61, Batch: 530, Training Loss: 0.056670534610748294, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:46, Epoch: 61, Batch: 540, Training Loss: 0.037022604048252104, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:47, Epoch: 61, Batch: 550, Training Loss: 0.03462143465876579, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:48, Epoch: 61, Batch: 560, Training Loss: 0.03154296092689037, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:49, Epoch: 61, Batch: 570, Training Loss: 0.06452102400362492, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:49, Epoch: 61, Batch: 580, Training Loss: 0.037188298627734186, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:50, Epoch: 61, Batch: 590, Training Loss: 0.036061954125761984, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:51, Epoch: 61, Batch: 600, Training Loss: 0.04598208963871002, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:51, Epoch: 61, Batch: 610, Training Loss: 0.024837907403707504, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:52, Epoch: 61, Batch: 620, Training Loss: 0.04448093995451927, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:53, Epoch: 61, Batch: 630, Training Loss: 0.03777152225375176, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:54, Epoch: 61, Batch: 640, Training Loss: 0.04611595943570137, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:54, Epoch: 61, Batch: 650, Training Loss: 0.0414159681648016, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:55, Epoch: 61, Batch: 660, Training Loss: 0.044122079759836196, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:56, Epoch: 61, Batch: 670, Training Loss: 0.06143708899617195, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:57, Epoch: 61, Batch: 680, Training Loss: 0.040845555067062375, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:57, Epoch: 61, Batch: 690, Training Loss: 0.07621015347540379, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:58, Epoch: 61, Batch: 700, Training Loss: 0.06052288487553596, LR: 0.0010000000000000002
Time, 2019-01-01T18:20:59, Epoch: 61, Batch: 710, Training Loss: 0.03000738099217415, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:00, Epoch: 61, Batch: 720, Training Loss: 0.02681554891169071, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:01, Epoch: 61, Batch: 730, Training Loss: 0.06580431498587132, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:01, Epoch: 61, Batch: 740, Training Loss: 0.06370212621986866, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:02, Epoch: 61, Batch: 750, Training Loss: 0.03445717021822929, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:03, Epoch: 61, Batch: 760, Training Loss: 0.044773365184664723, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:04, Epoch: 61, Batch: 770, Training Loss: 0.03916565030813217, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:05, Epoch: 61, Batch: 780, Training Loss: 0.0343529187142849, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:05, Epoch: 61, Batch: 790, Training Loss: 0.051174509897828103, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:06, Epoch: 61, Batch: 800, Training Loss: 0.030290549248456956, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:07, Epoch: 61, Batch: 810, Training Loss: 0.044554708525538445, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:08, Epoch: 61, Batch: 820, Training Loss: 0.061316465958952904, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:09, Epoch: 61, Batch: 830, Training Loss: 0.033527592569589613, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:09, Epoch: 61, Batch: 840, Training Loss: 0.03790340460836887, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:10, Epoch: 61, Batch: 850, Training Loss: 0.04093055687844753, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:11, Epoch: 61, Batch: 860, Training Loss: 0.045175904780626296, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:12, Epoch: 61, Batch: 870, Training Loss: 0.06446113735437393, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:12, Epoch: 61, Batch: 880, Training Loss: 0.03822919353842735, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:13, Epoch: 61, Batch: 890, Training Loss: 0.06033108457922935, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:14, Epoch: 61, Batch: 900, Training Loss: 0.033358968794345856, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:14, Epoch: 61, Batch: 910, Training Loss: 0.029637382179498673, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:15, Epoch: 61, Batch: 920, Training Loss: 0.04434635117650032, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:16, Epoch: 61, Batch: 930, Training Loss: 0.04267228841781616, LR: 0.0010000000000000002
Epoch: 61, Validation Top 1 acc: 98.80230712890625
Epoch: 61, Validation Top 5 acc: 99.99166870117188
Epoch: 61, Validation Set Loss: 0.04305964708328247
Start training epoch 62
Time, 2019-01-01T18:21:44, Epoch: 62, Batch: 10, Training Loss: 0.03454243801534176, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:44, Epoch: 62, Batch: 20, Training Loss: 0.04893416501581669, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:45, Epoch: 62, Batch: 30, Training Loss: 0.04402319267392159, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:46, Epoch: 62, Batch: 40, Training Loss: 0.046615950763225555, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:47, Epoch: 62, Batch: 50, Training Loss: 0.024903318285942076, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:47, Epoch: 62, Batch: 60, Training Loss: 0.06619499549269676, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:48, Epoch: 62, Batch: 70, Training Loss: 0.027823532372713088, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:49, Epoch: 62, Batch: 80, Training Loss: 0.03576719984412193, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:49, Epoch: 62, Batch: 90, Training Loss: 0.05527001097798347, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:50, Epoch: 62, Batch: 100, Training Loss: 0.04992211684584617, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:51, Epoch: 62, Batch: 110, Training Loss: 0.06584727428853512, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:52, Epoch: 62, Batch: 120, Training Loss: 0.03310599997639656, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:52, Epoch: 62, Batch: 130, Training Loss: 0.03919765464961529, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:53, Epoch: 62, Batch: 140, Training Loss: 0.04333348460495472, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:54, Epoch: 62, Batch: 150, Training Loss: 0.03553539589047432, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:54, Epoch: 62, Batch: 160, Training Loss: 0.035798973962664606, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:55, Epoch: 62, Batch: 170, Training Loss: 0.04404888600111008, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:56, Epoch: 62, Batch: 180, Training Loss: 0.0542324248701334, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:57, Epoch: 62, Batch: 190, Training Loss: 0.03799036219716072, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:57, Epoch: 62, Batch: 200, Training Loss: 0.05774718523025513, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:58, Epoch: 62, Batch: 210, Training Loss: 0.04492113813757896, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:59, Epoch: 62, Batch: 220, Training Loss: 0.05116179287433624, LR: 0.0010000000000000002
Time, 2019-01-01T18:21:59, Epoch: 62, Batch: 230, Training Loss: 0.028392090648412704, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:00, Epoch: 62, Batch: 240, Training Loss: 0.03443510830402374, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:01, Epoch: 62, Batch: 250, Training Loss: 0.06383992210030556, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:02, Epoch: 62, Batch: 260, Training Loss: 0.06495965160429477, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:02, Epoch: 62, Batch: 270, Training Loss: 0.02847721502184868, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:03, Epoch: 62, Batch: 280, Training Loss: 0.03737662695348263, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:04, Epoch: 62, Batch: 290, Training Loss: 0.03535926043987274, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:05, Epoch: 62, Batch: 300, Training Loss: 0.03885864950716496, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:05, Epoch: 62, Batch: 310, Training Loss: 0.02645401954650879, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:06, Epoch: 62, Batch: 320, Training Loss: 0.040992938727140424, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:07, Epoch: 62, Batch: 330, Training Loss: 0.0506050318479538, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:07, Epoch: 62, Batch: 340, Training Loss: 0.04857850596308708, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:08, Epoch: 62, Batch: 350, Training Loss: 0.05461175888776779, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:09, Epoch: 62, Batch: 360, Training Loss: 0.038649027794599534, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:10, Epoch: 62, Batch: 370, Training Loss: 0.05421769693493843, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:10, Epoch: 62, Batch: 380, Training Loss: 0.03826861456036568, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:11, Epoch: 62, Batch: 390, Training Loss: 0.0473272368311882, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:12, Epoch: 62, Batch: 400, Training Loss: 0.04130999706685543, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:12, Epoch: 62, Batch: 410, Training Loss: 0.03759350702166557, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:13, Epoch: 62, Batch: 420, Training Loss: 0.03360037952661514, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:14, Epoch: 62, Batch: 430, Training Loss: 0.03336197547614574, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:15, Epoch: 62, Batch: 440, Training Loss: 0.05519835464656353, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:15, Epoch: 62, Batch: 450, Training Loss: 0.041204272955656054, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:16, Epoch: 62, Batch: 460, Training Loss: 0.04392649382352829, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:17, Epoch: 62, Batch: 470, Training Loss: 0.04975346848368645, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:17, Epoch: 62, Batch: 480, Training Loss: 0.04621234089136124, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:18, Epoch: 62, Batch: 490, Training Loss: 0.057872699573636055, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:19, Epoch: 62, Batch: 500, Training Loss: 0.04902896471321583, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:20, Epoch: 62, Batch: 510, Training Loss: 0.043461482599377634, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:20, Epoch: 62, Batch: 520, Training Loss: 0.040681831538677216, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:21, Epoch: 62, Batch: 530, Training Loss: 0.05576310642063618, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:22, Epoch: 62, Batch: 540, Training Loss: 0.03349471986293793, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:23, Epoch: 62, Batch: 550, Training Loss: 0.05027527809143066, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:23, Epoch: 62, Batch: 560, Training Loss: 0.06281425207853317, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:24, Epoch: 62, Batch: 570, Training Loss: 0.059760022163391116, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:25, Epoch: 62, Batch: 580, Training Loss: 0.05304991267621517, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:25, Epoch: 62, Batch: 590, Training Loss: 0.03610955253243446, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:26, Epoch: 62, Batch: 600, Training Loss: 0.027126408740878107, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:27, Epoch: 62, Batch: 610, Training Loss: 0.051140809431672096, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:28, Epoch: 62, Batch: 620, Training Loss: 0.04955870807170868, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:28, Epoch: 62, Batch: 630, Training Loss: 0.03060237355530262, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:29, Epoch: 62, Batch: 640, Training Loss: 0.03561715073883533, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:30, Epoch: 62, Batch: 650, Training Loss: 0.04248413853347301, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:30, Epoch: 62, Batch: 660, Training Loss: 0.06120328642427921, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:31, Epoch: 62, Batch: 670, Training Loss: 0.032883806899189946, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:32, Epoch: 62, Batch: 680, Training Loss: 0.03424759246408939, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:33, Epoch: 62, Batch: 690, Training Loss: 0.036438196524977685, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:33, Epoch: 62, Batch: 700, Training Loss: 0.045260163769125936, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:34, Epoch: 62, Batch: 710, Training Loss: 0.0509107768535614, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:35, Epoch: 62, Batch: 720, Training Loss: 0.04736669920384884, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:36, Epoch: 62, Batch: 730, Training Loss: 0.022503431513905527, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:36, Epoch: 62, Batch: 740, Training Loss: 0.03141454942524433, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:37, Epoch: 62, Batch: 750, Training Loss: 0.0514690101146698, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:38, Epoch: 62, Batch: 760, Training Loss: 0.037990083917975426, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:38, Epoch: 62, Batch: 770, Training Loss: 0.046894628554582596, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:39, Epoch: 62, Batch: 780, Training Loss: 0.0288302194327116, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:40, Epoch: 62, Batch: 790, Training Loss: 0.03237503245472908, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:41, Epoch: 62, Batch: 800, Training Loss: 0.04191555604338646, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:41, Epoch: 62, Batch: 810, Training Loss: 0.0360600970685482, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:42, Epoch: 62, Batch: 820, Training Loss: 0.031207945570349692, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:43, Epoch: 62, Batch: 830, Training Loss: 0.05827985741198063, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:43, Epoch: 62, Batch: 840, Training Loss: 0.04531102366745472, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:44, Epoch: 62, Batch: 850, Training Loss: 0.047604383528232576, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:45, Epoch: 62, Batch: 860, Training Loss: 0.053727886825799945, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:46, Epoch: 62, Batch: 870, Training Loss: 0.05403913669288159, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:46, Epoch: 62, Batch: 880, Training Loss: 0.023124026507139205, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:47, Epoch: 62, Batch: 890, Training Loss: 0.03518268540501594, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:48, Epoch: 62, Batch: 900, Training Loss: 0.05306457467377186, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:48, Epoch: 62, Batch: 910, Training Loss: 0.03479004316031933, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:49, Epoch: 62, Batch: 920, Training Loss: 0.047897076979279515, LR: 0.0010000000000000002
Time, 2019-01-01T18:22:50, Epoch: 62, Batch: 930, Training Loss: 0.04737790375947952, LR: 0.0010000000000000002
Epoch: 62, Validation Top 1 acc: 98.83229064941406
Epoch: 62, Validation Top 5 acc: 99.99166870117188
Epoch: 62, Validation Set Loss: 0.04237932339310646
Start training epoch 63
Time, 2019-01-01T18:23:18, Epoch: 63, Batch: 10, Training Loss: 0.03756825402379036, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:19, Epoch: 63, Batch: 20, Training Loss: 0.030021338909864425, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:19, Epoch: 63, Batch: 30, Training Loss: 0.03139016628265381, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:20, Epoch: 63, Batch: 40, Training Loss: 0.030861088633537294, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:21, Epoch: 63, Batch: 50, Training Loss: 0.02802576906979084, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:21, Epoch: 63, Batch: 60, Training Loss: 0.044991083443164825, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:22, Epoch: 63, Batch: 70, Training Loss: 0.04743970483541489, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:23, Epoch: 63, Batch: 80, Training Loss: 0.05115841180086136, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:24, Epoch: 63, Batch: 90, Training Loss: 0.052456334978342053, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:24, Epoch: 63, Batch: 100, Training Loss: 0.06725614964962005, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:25, Epoch: 63, Batch: 110, Training Loss: 0.04589179195463657, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:26, Epoch: 63, Batch: 120, Training Loss: 0.038047550991177556, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:26, Epoch: 63, Batch: 130, Training Loss: 0.044398697838187215, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:27, Epoch: 63, Batch: 140, Training Loss: 0.037031234800815584, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:28, Epoch: 63, Batch: 150, Training Loss: 0.03975049741566181, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:29, Epoch: 63, Batch: 160, Training Loss: 0.05103435106575489, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:29, Epoch: 63, Batch: 170, Training Loss: 0.038270478323102, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:30, Epoch: 63, Batch: 180, Training Loss: 0.039469433948397636, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:31, Epoch: 63, Batch: 190, Training Loss: 0.05962955057621002, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:32, Epoch: 63, Batch: 200, Training Loss: 0.038306576386094096, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:32, Epoch: 63, Batch: 210, Training Loss: 0.04248945973813534, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:33, Epoch: 63, Batch: 220, Training Loss: 0.02776629328727722, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:34, Epoch: 63, Batch: 230, Training Loss: 0.04204069972038269, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:34, Epoch: 63, Batch: 240, Training Loss: 0.035436595976352694, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:35, Epoch: 63, Batch: 250, Training Loss: 0.03695351108908653, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:36, Epoch: 63, Batch: 260, Training Loss: 0.034381506219506267, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:37, Epoch: 63, Batch: 270, Training Loss: 0.050486649572849276, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:37, Epoch: 63, Batch: 280, Training Loss: 0.04188434332609177, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:38, Epoch: 63, Batch: 290, Training Loss: 0.04751271978020668, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:39, Epoch: 63, Batch: 300, Training Loss: 0.05413698479533195, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:40, Epoch: 63, Batch: 310, Training Loss: 0.03502477370202541, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:40, Epoch: 63, Batch: 320, Training Loss: 0.03165295124053955, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:41, Epoch: 63, Batch: 330, Training Loss: 0.058703846856951715, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:42, Epoch: 63, Batch: 340, Training Loss: 0.0419753547757864, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:42, Epoch: 63, Batch: 350, Training Loss: 0.04684145748615265, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:43, Epoch: 63, Batch: 360, Training Loss: 0.04832102619111538, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:44, Epoch: 63, Batch: 370, Training Loss: 0.04238996468484402, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:45, Epoch: 63, Batch: 380, Training Loss: 0.03978848084807396, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:45, Epoch: 63, Batch: 390, Training Loss: 0.05232423134148121, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:46, Epoch: 63, Batch: 400, Training Loss: 0.036288325861096385, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:47, Epoch: 63, Batch: 410, Training Loss: 0.04415772967040539, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:47, Epoch: 63, Batch: 420, Training Loss: 0.038146819174289706, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:48, Epoch: 63, Batch: 430, Training Loss: 0.03772546723484993, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:49, Epoch: 63, Batch: 440, Training Loss: 0.040398924797773364, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:50, Epoch: 63, Batch: 450, Training Loss: 0.0350612822920084, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:50, Epoch: 63, Batch: 460, Training Loss: 0.05770072564482689, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:51, Epoch: 63, Batch: 470, Training Loss: 0.05063049755990505, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:52, Epoch: 63, Batch: 480, Training Loss: 0.04099993444979191, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:53, Epoch: 63, Batch: 490, Training Loss: 0.04355011805891991, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:53, Epoch: 63, Batch: 500, Training Loss: 0.0317703016102314, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:54, Epoch: 63, Batch: 510, Training Loss: 0.05486915931105614, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:55, Epoch: 63, Batch: 520, Training Loss: 0.05414226874709129, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:55, Epoch: 63, Batch: 530, Training Loss: 0.04419991075992584, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:56, Epoch: 63, Batch: 540, Training Loss: 0.034720030054450034, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:57, Epoch: 63, Batch: 550, Training Loss: 0.05174060128629208, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:58, Epoch: 63, Batch: 560, Training Loss: 0.025800270587205888, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:58, Epoch: 63, Batch: 570, Training Loss: 0.032285846024751666, LR: 0.0010000000000000002
Time, 2019-01-01T18:23:59, Epoch: 63, Batch: 580, Training Loss: 0.05832483358681202, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:00, Epoch: 63, Batch: 590, Training Loss: 0.03609915003180504, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:01, Epoch: 63, Batch: 600, Training Loss: 0.032841674610972406, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:01, Epoch: 63, Batch: 610, Training Loss: 0.05146011970937252, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:02, Epoch: 63, Batch: 620, Training Loss: 0.051565273106098174, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:03, Epoch: 63, Batch: 630, Training Loss: 0.047981948032975195, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:03, Epoch: 63, Batch: 640, Training Loss: 0.043280134350061415, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:04, Epoch: 63, Batch: 650, Training Loss: 0.061276743188500404, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:05, Epoch: 63, Batch: 660, Training Loss: 0.03337128721177578, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:06, Epoch: 63, Batch: 670, Training Loss: 0.041445909067988396, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:07, Epoch: 63, Batch: 680, Training Loss: 0.04204797372221947, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:07, Epoch: 63, Batch: 690, Training Loss: 0.04140096381306648, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:08, Epoch: 63, Batch: 700, Training Loss: 0.03698016181588173, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:09, Epoch: 63, Batch: 710, Training Loss: 0.05399886667728424, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:10, Epoch: 63, Batch: 720, Training Loss: 0.06146918013691902, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:10, Epoch: 63, Batch: 730, Training Loss: 0.035384610295295715, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:11, Epoch: 63, Batch: 740, Training Loss: 0.0498921275138855, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:12, Epoch: 63, Batch: 750, Training Loss: 0.040034715086221695, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:13, Epoch: 63, Batch: 760, Training Loss: 0.038540737703442574, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:14, Epoch: 63, Batch: 770, Training Loss: 0.044053131341934205, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:15, Epoch: 63, Batch: 780, Training Loss: 0.043197933956980704, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:15, Epoch: 63, Batch: 790, Training Loss: 0.03435568213462829, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:16, Epoch: 63, Batch: 800, Training Loss: 0.06657075025141239, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:17, Epoch: 63, Batch: 810, Training Loss: 0.041683056950569154, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:18, Epoch: 63, Batch: 820, Training Loss: 0.03882790580391884, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:19, Epoch: 63, Batch: 830, Training Loss: 0.03478548340499401, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:19, Epoch: 63, Batch: 840, Training Loss: 0.03375403918325901, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:20, Epoch: 63, Batch: 850, Training Loss: 0.05688734576106071, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:21, Epoch: 63, Batch: 860, Training Loss: 0.051404782384634015, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:22, Epoch: 63, Batch: 870, Training Loss: 0.035853388160467146, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:23, Epoch: 63, Batch: 880, Training Loss: 0.05465045124292374, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:23, Epoch: 63, Batch: 890, Training Loss: 0.034216276556253436, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:24, Epoch: 63, Batch: 900, Training Loss: 0.032029998302459714, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:25, Epoch: 63, Batch: 910, Training Loss: 0.03620733954012394, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:26, Epoch: 63, Batch: 920, Training Loss: 0.04792438969016075, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:27, Epoch: 63, Batch: 930, Training Loss: 0.05072381533682346, LR: 0.0010000000000000002
Epoch: 63, Validation Top 1 acc: 98.82229614257812
Epoch: 63, Validation Top 5 acc: 99.99000549316406
Epoch: 63, Validation Set Loss: 0.04251696541905403
Start training epoch 64
Time, 2019-01-01T18:24:57, Epoch: 64, Batch: 10, Training Loss: 0.025797807425260545, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:58, Epoch: 64, Batch: 20, Training Loss: 0.0677045926451683, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:59, Epoch: 64, Batch: 30, Training Loss: 0.027556808292865755, LR: 0.0010000000000000002
Time, 2019-01-01T18:24:59, Epoch: 64, Batch: 40, Training Loss: 0.03614940196275711, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:00, Epoch: 64, Batch: 50, Training Loss: 0.05172378346323967, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:01, Epoch: 64, Batch: 60, Training Loss: 0.03637770414352417, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:02, Epoch: 64, Batch: 70, Training Loss: 0.06373148187994956, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:03, Epoch: 64, Batch: 80, Training Loss: 0.03208588883280754, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:03, Epoch: 64, Batch: 90, Training Loss: 0.028928318992257118, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:04, Epoch: 64, Batch: 100, Training Loss: 0.049251805618405343, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:05, Epoch: 64, Batch: 110, Training Loss: 0.05090927742421627, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:06, Epoch: 64, Batch: 120, Training Loss: 0.03424182794988155, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:07, Epoch: 64, Batch: 130, Training Loss: 0.049619541689753535, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:08, Epoch: 64, Batch: 140, Training Loss: 0.03702499866485596, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:08, Epoch: 64, Batch: 150, Training Loss: 0.05701812356710434, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:09, Epoch: 64, Batch: 160, Training Loss: 0.027662284672260284, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:10, Epoch: 64, Batch: 170, Training Loss: 0.048026963323354724, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:11, Epoch: 64, Batch: 180, Training Loss: 0.03223554119467735, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:11, Epoch: 64, Batch: 190, Training Loss: 0.05604268237948418, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:12, Epoch: 64, Batch: 200, Training Loss: 0.031129785627126694, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:13, Epoch: 64, Batch: 210, Training Loss: 0.044286889210343364, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:13, Epoch: 64, Batch: 220, Training Loss: 0.03380778133869171, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:14, Epoch: 64, Batch: 230, Training Loss: 0.05260801799595356, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:15, Epoch: 64, Batch: 240, Training Loss: 0.03165435679256916, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:16, Epoch: 64, Batch: 250, Training Loss: 0.04707476198673248, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:17, Epoch: 64, Batch: 260, Training Loss: 0.04335282817482948, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:17, Epoch: 64, Batch: 270, Training Loss: 0.046750062704086305, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:18, Epoch: 64, Batch: 280, Training Loss: 0.02239942327141762, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:19, Epoch: 64, Batch: 290, Training Loss: 0.038692606985569, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:20, Epoch: 64, Batch: 300, Training Loss: 0.06713554263114929, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:21, Epoch: 64, Batch: 310, Training Loss: 0.04417469911277294, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:22, Epoch: 64, Batch: 320, Training Loss: 0.05453830733895302, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:23, Epoch: 64, Batch: 330, Training Loss: 0.05035403184592724, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:23, Epoch: 64, Batch: 340, Training Loss: 0.05125303603708744, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:24, Epoch: 64, Batch: 350, Training Loss: 0.04659562334418297, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:25, Epoch: 64, Batch: 360, Training Loss: 0.041125711798667905, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:26, Epoch: 64, Batch: 370, Training Loss: 0.04113598838448525, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:26, Epoch: 64, Batch: 380, Training Loss: 0.03202169984579086, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:27, Epoch: 64, Batch: 390, Training Loss: 0.027782368659973144, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:28, Epoch: 64, Batch: 400, Training Loss: 0.06840741783380508, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:29, Epoch: 64, Batch: 410, Training Loss: 0.04553597494959831, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:30, Epoch: 64, Batch: 420, Training Loss: 0.04540952369570732, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:30, Epoch: 64, Batch: 430, Training Loss: 0.03209024481475353, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:31, Epoch: 64, Batch: 440, Training Loss: 0.03947581574320793, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:32, Epoch: 64, Batch: 450, Training Loss: 0.07208009585738182, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:33, Epoch: 64, Batch: 460, Training Loss: 0.04022638015449047, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:34, Epoch: 64, Batch: 470, Training Loss: 0.05913201868534088, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:34, Epoch: 64, Batch: 480, Training Loss: 0.025753138214349748, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:35, Epoch: 64, Batch: 490, Training Loss: 0.046082309633493426, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:36, Epoch: 64, Batch: 500, Training Loss: 0.06094727031886578, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:37, Epoch: 64, Batch: 510, Training Loss: 0.03176959715783596, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:38, Epoch: 64, Batch: 520, Training Loss: 0.04607894495129585, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:38, Epoch: 64, Batch: 530, Training Loss: 0.05237590707838535, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:39, Epoch: 64, Batch: 540, Training Loss: 0.037248597294092176, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:40, Epoch: 64, Batch: 550, Training Loss: 0.05211291760206223, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:41, Epoch: 64, Batch: 560, Training Loss: 0.05042767226696014, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:42, Epoch: 64, Batch: 570, Training Loss: 0.03699910417199135, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:43, Epoch: 64, Batch: 580, Training Loss: 0.03885162547230721, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:43, Epoch: 64, Batch: 590, Training Loss: 0.04741600900888443, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:44, Epoch: 64, Batch: 600, Training Loss: 0.03331759534776211, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:45, Epoch: 64, Batch: 610, Training Loss: 0.05190000683069229, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:46, Epoch: 64, Batch: 620, Training Loss: 0.04877370893955231, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:46, Epoch: 64, Batch: 630, Training Loss: 0.061671677604317666, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:47, Epoch: 64, Batch: 640, Training Loss: 0.046844401583075526, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:48, Epoch: 64, Batch: 650, Training Loss: 0.036602812632918355, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:49, Epoch: 64, Batch: 660, Training Loss: 0.03263282924890518, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:50, Epoch: 64, Batch: 670, Training Loss: 0.035656923055648805, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:51, Epoch: 64, Batch: 680, Training Loss: 0.04855593852698803, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:51, Epoch: 64, Batch: 690, Training Loss: 0.043088565766811374, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:52, Epoch: 64, Batch: 700, Training Loss: 0.03543916307389736, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:53, Epoch: 64, Batch: 710, Training Loss: 0.03930090367794037, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:54, Epoch: 64, Batch: 720, Training Loss: 0.03337761014699936, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:54, Epoch: 64, Batch: 730, Training Loss: 0.04399530924856663, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:55, Epoch: 64, Batch: 740, Training Loss: 0.03347374424338341, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:56, Epoch: 64, Batch: 750, Training Loss: 0.037962370365858075, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:57, Epoch: 64, Batch: 760, Training Loss: 0.03914618603885174, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:57, Epoch: 64, Batch: 770, Training Loss: 0.05578794814646244, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:58, Epoch: 64, Batch: 780, Training Loss: 0.04508376158773899, LR: 0.0010000000000000002
Time, 2019-01-01T18:25:59, Epoch: 64, Batch: 790, Training Loss: 0.03684687428176403, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:00, Epoch: 64, Batch: 800, Training Loss: 0.056019748002290724, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:00, Epoch: 64, Batch: 810, Training Loss: 0.045046084374189374, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:01, Epoch: 64, Batch: 820, Training Loss: 0.03576204963028431, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:02, Epoch: 64, Batch: 830, Training Loss: 0.06397343054413795, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:03, Epoch: 64, Batch: 840, Training Loss: 0.03415696695446968, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:03, Epoch: 64, Batch: 850, Training Loss: 0.02679194137454033, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:04, Epoch: 64, Batch: 860, Training Loss: 0.05891285389661789, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:05, Epoch: 64, Batch: 870, Training Loss: 0.039422353357076646, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:06, Epoch: 64, Batch: 880, Training Loss: 0.030038122460246085, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:06, Epoch: 64, Batch: 890, Training Loss: 0.03459374494850635, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:07, Epoch: 64, Batch: 900, Training Loss: 0.04109152927994728, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:08, Epoch: 64, Batch: 910, Training Loss: 0.04268223717808724, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:08, Epoch: 64, Batch: 920, Training Loss: 0.045556437224149704, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:09, Epoch: 64, Batch: 930, Training Loss: 0.036136454343795775, LR: 0.0010000000000000002
Epoch: 64, Validation Top 1 acc: 98.80563354492188
Epoch: 64, Validation Top 5 acc: 99.99000549316406
Epoch: 64, Validation Set Loss: 0.042772695422172546
Start training epoch 65
Time, 2019-01-01T18:26:38, Epoch: 65, Batch: 10, Training Loss: 0.03664243146777153, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:39, Epoch: 65, Batch: 20, Training Loss: 0.0463110938668251, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:40, Epoch: 65, Batch: 30, Training Loss: 0.04062413536012173, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:40, Epoch: 65, Batch: 40, Training Loss: 0.051629526540637016, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:41, Epoch: 65, Batch: 50, Training Loss: 0.04367826171219349, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:42, Epoch: 65, Batch: 60, Training Loss: 0.045168033614754674, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:42, Epoch: 65, Batch: 70, Training Loss: 0.04538226090371609, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:43, Epoch: 65, Batch: 80, Training Loss: 0.04376668818295002, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:44, Epoch: 65, Batch: 90, Training Loss: 0.042872346192598346, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:45, Epoch: 65, Batch: 100, Training Loss: 0.047396214306354524, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:45, Epoch: 65, Batch: 110, Training Loss: 0.027466391772031785, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:46, Epoch: 65, Batch: 120, Training Loss: 0.038046595826745036, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:47, Epoch: 65, Batch: 130, Training Loss: 0.06087489426136017, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:48, Epoch: 65, Batch: 140, Training Loss: 0.07754987552762031, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:48, Epoch: 65, Batch: 150, Training Loss: 0.058472587168216704, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:49, Epoch: 65, Batch: 160, Training Loss: 0.0397426825016737, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:50, Epoch: 65, Batch: 170, Training Loss: 0.05613971091806889, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:51, Epoch: 65, Batch: 180, Training Loss: 0.05483882874250412, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:51, Epoch: 65, Batch: 190, Training Loss: 0.03944797702133655, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:52, Epoch: 65, Batch: 200, Training Loss: 0.04950965195894241, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:53, Epoch: 65, Batch: 210, Training Loss: 0.0394134983420372, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:53, Epoch: 65, Batch: 220, Training Loss: 0.032783853262662886, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:54, Epoch: 65, Batch: 230, Training Loss: 0.03614936992526054, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:55, Epoch: 65, Batch: 240, Training Loss: 0.046272970363497735, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:56, Epoch: 65, Batch: 250, Training Loss: 0.05067557729780674, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:56, Epoch: 65, Batch: 260, Training Loss: 0.04183139465749264, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:57, Epoch: 65, Batch: 270, Training Loss: 0.03447163142263889, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:58, Epoch: 65, Batch: 280, Training Loss: 0.02533699609339237, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:59, Epoch: 65, Batch: 290, Training Loss: 0.047629281505942346, LR: 0.0010000000000000002
Time, 2019-01-01T18:26:59, Epoch: 65, Batch: 300, Training Loss: 0.03171985074877739, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:00, Epoch: 65, Batch: 310, Training Loss: 0.058696343749761584, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:01, Epoch: 65, Batch: 320, Training Loss: 0.03370600119233132, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:02, Epoch: 65, Batch: 330, Training Loss: 0.03892656341195107, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:02, Epoch: 65, Batch: 340, Training Loss: 0.02772146835923195, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:03, Epoch: 65, Batch: 350, Training Loss: 0.04588071368634701, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:04, Epoch: 65, Batch: 360, Training Loss: 0.05189235173165798, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:05, Epoch: 65, Batch: 370, Training Loss: 0.031049058958888053, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:05, Epoch: 65, Batch: 380, Training Loss: 0.046225402504205704, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:06, Epoch: 65, Batch: 390, Training Loss: 0.037817738205194476, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:07, Epoch: 65, Batch: 400, Training Loss: 0.0681358352303505, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:08, Epoch: 65, Batch: 410, Training Loss: 0.04361507073044777, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:08, Epoch: 65, Batch: 420, Training Loss: 0.030607040598988534, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:09, Epoch: 65, Batch: 430, Training Loss: 0.03818999417126179, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:10, Epoch: 65, Batch: 440, Training Loss: 0.035187271609902385, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:11, Epoch: 65, Batch: 450, Training Loss: 0.04282615259289742, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:11, Epoch: 65, Batch: 460, Training Loss: 0.033834435790777204, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:12, Epoch: 65, Batch: 470, Training Loss: 0.04700946658849716, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:13, Epoch: 65, Batch: 480, Training Loss: 0.03554306589066982, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:14, Epoch: 65, Batch: 490, Training Loss: 0.0289738304913044, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:14, Epoch: 65, Batch: 500, Training Loss: 0.04056627079844475, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:15, Epoch: 65, Batch: 510, Training Loss: 0.037049376592040065, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:16, Epoch: 65, Batch: 520, Training Loss: 0.039492813125252726, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:17, Epoch: 65, Batch: 530, Training Loss: 0.03830261901021004, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:17, Epoch: 65, Batch: 540, Training Loss: 0.02926592007279396, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:18, Epoch: 65, Batch: 550, Training Loss: 0.06553571373224258, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:19, Epoch: 65, Batch: 560, Training Loss: 0.058056009188294413, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:20, Epoch: 65, Batch: 570, Training Loss: 0.029873250424861907, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:20, Epoch: 65, Batch: 580, Training Loss: 0.05395866185426712, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:21, Epoch: 65, Batch: 590, Training Loss: 0.03880588039755821, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:22, Epoch: 65, Batch: 600, Training Loss: 0.04121659770607948, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:22, Epoch: 65, Batch: 610, Training Loss: 0.04327849224209786, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:23, Epoch: 65, Batch: 620, Training Loss: 0.05362717807292938, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:24, Epoch: 65, Batch: 630, Training Loss: 0.03519911728799343, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:25, Epoch: 65, Batch: 640, Training Loss: 0.03884233683347702, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:25, Epoch: 65, Batch: 650, Training Loss: 0.04701721221208573, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:26, Epoch: 65, Batch: 660, Training Loss: 0.03782827742397785, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:27, Epoch: 65, Batch: 670, Training Loss: 0.022033362835645675, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:28, Epoch: 65, Batch: 680, Training Loss: 0.05600544065237045, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:28, Epoch: 65, Batch: 690, Training Loss: 0.05376644916832447, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:29, Epoch: 65, Batch: 700, Training Loss: 0.08048266842961312, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:30, Epoch: 65, Batch: 710, Training Loss: 0.029841240495443344, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:30, Epoch: 65, Batch: 720, Training Loss: 0.034504085406661035, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:31, Epoch: 65, Batch: 730, Training Loss: 0.04402088671922684, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:32, Epoch: 65, Batch: 740, Training Loss: 0.0442083939909935, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:33, Epoch: 65, Batch: 750, Training Loss: 0.05272490009665489, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:33, Epoch: 65, Batch: 760, Training Loss: 0.026621108874678612, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:34, Epoch: 65, Batch: 770, Training Loss: 0.02654605209827423, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:35, Epoch: 65, Batch: 780, Training Loss: 0.04108107797801495, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:35, Epoch: 65, Batch: 790, Training Loss: 0.04606042429804802, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:36, Epoch: 65, Batch: 800, Training Loss: 0.041152387857437134, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:37, Epoch: 65, Batch: 810, Training Loss: 0.057265841960906984, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:38, Epoch: 65, Batch: 820, Training Loss: 0.06033533923327923, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:38, Epoch: 65, Batch: 830, Training Loss: 0.026918582618236542, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:39, Epoch: 65, Batch: 840, Training Loss: 0.04541462361812591, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:40, Epoch: 65, Batch: 850, Training Loss: 0.04443996772170067, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:41, Epoch: 65, Batch: 860, Training Loss: 0.04171870052814484, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:41, Epoch: 65, Batch: 870, Training Loss: 0.0287701815366745, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:42, Epoch: 65, Batch: 880, Training Loss: 0.08318782672286033, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:43, Epoch: 65, Batch: 890, Training Loss: 0.043733051791787145, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:44, Epoch: 65, Batch: 900, Training Loss: 0.043629178777337074, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:44, Epoch: 65, Batch: 910, Training Loss: 0.035541238635778426, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:45, Epoch: 65, Batch: 920, Training Loss: 0.03453446142375469, LR: 0.0010000000000000002
Time, 2019-01-01T18:27:46, Epoch: 65, Batch: 930, Training Loss: 0.04534209296107292, LR: 0.0010000000000000002
Epoch: 65, Validation Top 1 acc: 98.81063079833984
Epoch: 65, Validation Top 5 acc: 99.99000549316406
Epoch: 65, Validation Set Loss: 0.04270919784903526
Start training epoch 66
Time, 2019-01-01T18:28:14, Epoch: 66, Batch: 10, Training Loss: 0.05947484225034714, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:15, Epoch: 66, Batch: 20, Training Loss: 0.03333015814423561, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:16, Epoch: 66, Batch: 30, Training Loss: 0.04545835517346859, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:16, Epoch: 66, Batch: 40, Training Loss: 0.039180629700422284, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:17, Epoch: 66, Batch: 50, Training Loss: 0.03404162675142288, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:18, Epoch: 66, Batch: 60, Training Loss: 0.040483302623033526, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:18, Epoch: 66, Batch: 70, Training Loss: 0.03200911656022072, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:19, Epoch: 66, Batch: 80, Training Loss: 0.041956523060798646, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:20, Epoch: 66, Batch: 90, Training Loss: 0.04450091272592545, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:21, Epoch: 66, Batch: 100, Training Loss: 0.04094522222876549, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:21, Epoch: 66, Batch: 110, Training Loss: 0.04317015632987022, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:22, Epoch: 66, Batch: 120, Training Loss: 0.04837424904108047, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:23, Epoch: 66, Batch: 130, Training Loss: 0.030980563163757323, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:23, Epoch: 66, Batch: 140, Training Loss: 0.04524970911443234, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:24, Epoch: 66, Batch: 150, Training Loss: 0.04851076230406761, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:25, Epoch: 66, Batch: 160, Training Loss: 0.04924709349870682, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:26, Epoch: 66, Batch: 170, Training Loss: 0.053813967108726504, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:27, Epoch: 66, Batch: 180, Training Loss: 0.041575445979833606, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:28, Epoch: 66, Batch: 190, Training Loss: 0.02655491270124912, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:28, Epoch: 66, Batch: 200, Training Loss: 0.03257060125470161, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:29, Epoch: 66, Batch: 210, Training Loss: 0.0329266406595707, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:30, Epoch: 66, Batch: 220, Training Loss: 0.030643533170223235, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:31, Epoch: 66, Batch: 230, Training Loss: 0.04955819807946682, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:32, Epoch: 66, Batch: 240, Training Loss: 0.06122998669743538, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:32, Epoch: 66, Batch: 250, Training Loss: 0.0419898197054863, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:33, Epoch: 66, Batch: 260, Training Loss: 0.04243123829364777, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:34, Epoch: 66, Batch: 270, Training Loss: 0.04506932273507118, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:35, Epoch: 66, Batch: 280, Training Loss: 0.04525018334388733, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:35, Epoch: 66, Batch: 290, Training Loss: 0.031435827165842055, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:36, Epoch: 66, Batch: 300, Training Loss: 0.030969679728150366, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:37, Epoch: 66, Batch: 310, Training Loss: 0.029816725105047227, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:38, Epoch: 66, Batch: 320, Training Loss: 0.04540367387235165, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:38, Epoch: 66, Batch: 330, Training Loss: 0.059204211458563805, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:39, Epoch: 66, Batch: 340, Training Loss: 0.025989806279540062, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:40, Epoch: 66, Batch: 350, Training Loss: 0.057307721301913264, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:41, Epoch: 66, Batch: 360, Training Loss: 0.03650902770459652, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:41, Epoch: 66, Batch: 370, Training Loss: 0.045130998641252515, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:42, Epoch: 66, Batch: 380, Training Loss: 0.061067217588424684, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:43, Epoch: 66, Batch: 390, Training Loss: 0.07034544125199318, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:43, Epoch: 66, Batch: 400, Training Loss: 0.04681476950645447, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:44, Epoch: 66, Batch: 410, Training Loss: 0.03767685443162918, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:45, Epoch: 66, Batch: 420, Training Loss: 0.058617061376571654, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:46, Epoch: 66, Batch: 430, Training Loss: 0.040871485322713855, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:46, Epoch: 66, Batch: 440, Training Loss: 0.044576119631528854, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:47, Epoch: 66, Batch: 450, Training Loss: 0.046675705909729005, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:48, Epoch: 66, Batch: 460, Training Loss: 0.04452890530228615, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:49, Epoch: 66, Batch: 470, Training Loss: 0.05460676923394203, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:49, Epoch: 66, Batch: 480, Training Loss: 0.035736285895109174, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:50, Epoch: 66, Batch: 490, Training Loss: 0.05328075960278511, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:51, Epoch: 66, Batch: 500, Training Loss: 0.040328753739595415, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:52, Epoch: 66, Batch: 510, Training Loss: 0.05698584318161011, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:52, Epoch: 66, Batch: 520, Training Loss: 0.038665884733200075, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:53, Epoch: 66, Batch: 530, Training Loss: 0.04484708569943905, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:54, Epoch: 66, Batch: 540, Training Loss: 0.05032690465450287, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:54, Epoch: 66, Batch: 550, Training Loss: 0.04976057708263397, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:55, Epoch: 66, Batch: 560, Training Loss: 0.048652383685112, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:56, Epoch: 66, Batch: 570, Training Loss: 0.0390940897166729, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:57, Epoch: 66, Batch: 580, Training Loss: 0.04154805168509483, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:58, Epoch: 66, Batch: 590, Training Loss: 0.041708632558584216, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:58, Epoch: 66, Batch: 600, Training Loss: 0.03735248483717442, LR: 0.0010000000000000002
Time, 2019-01-01T18:28:59, Epoch: 66, Batch: 610, Training Loss: 0.02647615224123001, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:00, Epoch: 66, Batch: 620, Training Loss: 0.04748506173491478, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:01, Epoch: 66, Batch: 630, Training Loss: 0.03954143635928631, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:02, Epoch: 66, Batch: 640, Training Loss: 0.05264115333557129, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:03, Epoch: 66, Batch: 650, Training Loss: 0.04050863645970822, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:03, Epoch: 66, Batch: 660, Training Loss: 0.04612119346857071, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:04, Epoch: 66, Batch: 670, Training Loss: 0.03435954861342907, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:05, Epoch: 66, Batch: 680, Training Loss: 0.04440436996519566, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:06, Epoch: 66, Batch: 690, Training Loss: 0.03369378447532654, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:06, Epoch: 66, Batch: 700, Training Loss: 0.04135706052184105, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:07, Epoch: 66, Batch: 710, Training Loss: 0.04954032301902771, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:08, Epoch: 66, Batch: 720, Training Loss: 0.045092002302408216, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:09, Epoch: 66, Batch: 730, Training Loss: 0.04919668287038803, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:09, Epoch: 66, Batch: 740, Training Loss: 0.038506873324513434, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:10, Epoch: 66, Batch: 750, Training Loss: 0.04731487408280373, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:11, Epoch: 66, Batch: 760, Training Loss: 0.04396487660706043, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:12, Epoch: 66, Batch: 770, Training Loss: 0.06766352206468582, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:13, Epoch: 66, Batch: 780, Training Loss: 0.04522462412714958, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:13, Epoch: 66, Batch: 790, Training Loss: 0.049965353682637215, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:14, Epoch: 66, Batch: 800, Training Loss: 0.031094475463032723, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:15, Epoch: 66, Batch: 810, Training Loss: 0.043620289862155916, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:15, Epoch: 66, Batch: 820, Training Loss: 0.03085288256406784, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:16, Epoch: 66, Batch: 830, Training Loss: 0.04435063675045967, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:17, Epoch: 66, Batch: 840, Training Loss: 0.051901400461792944, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:18, Epoch: 66, Batch: 850, Training Loss: 0.04048164486885071, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:18, Epoch: 66, Batch: 860, Training Loss: 0.04510613605380058, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:19, Epoch: 66, Batch: 870, Training Loss: 0.037310344725847246, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:20, Epoch: 66, Batch: 880, Training Loss: 0.041603327915072444, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:21, Epoch: 66, Batch: 890, Training Loss: 0.03403760045766831, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:21, Epoch: 66, Batch: 900, Training Loss: 0.031165367737412453, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:22, Epoch: 66, Batch: 910, Training Loss: 0.035686340928077695, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:23, Epoch: 66, Batch: 920, Training Loss: 0.07069666385650634, LR: 0.0010000000000000002
Time, 2019-01-01T18:29:23, Epoch: 66, Batch: 930, Training Loss: 0.03191075399518013, LR: 0.0010000000000000002
Epoch: 66, Validation Top 1 acc: 98.80563354492188
Epoch: 66, Validation Top 5 acc: 99.99166870117188
Epoch: 66, Validation Set Loss: 0.04266766086220741
Start training epoch 67
Time, 2019-01-01T18:29:54, Epoch: 67, Batch: 10, Training Loss: 0.0472470298409462, LR: 0.00010000000000000003
Time, 2019-01-01T18:29:55, Epoch: 67, Batch: 20, Training Loss: 0.03155151717364788, LR: 0.00010000000000000003
Time, 2019-01-01T18:29:55, Epoch: 67, Batch: 30, Training Loss: 0.04852704107761383, LR: 0.00010000000000000003
Time, 2019-01-01T18:29:56, Epoch: 67, Batch: 40, Training Loss: 0.04004998281598091, LR: 0.00010000000000000003
Time, 2019-01-01T18:29:57, Epoch: 67, Batch: 50, Training Loss: 0.028768089413642884, LR: 0.00010000000000000003
Time, 2019-01-01T18:29:58, Epoch: 67, Batch: 60, Training Loss: 0.05380629114806652, LR: 0.00010000000000000003
Time, 2019-01-01T18:29:59, Epoch: 67, Batch: 70, Training Loss: 0.04407023787498474, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:00, Epoch: 67, Batch: 80, Training Loss: 0.04485471472144127, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:01, Epoch: 67, Batch: 90, Training Loss: 0.04779566675424576, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:01, Epoch: 67, Batch: 100, Training Loss: 0.03687397688627243, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:02, Epoch: 67, Batch: 110, Training Loss: 0.052584344521164894, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:03, Epoch: 67, Batch: 120, Training Loss: 0.05526391491293907, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:04, Epoch: 67, Batch: 130, Training Loss: 0.04272046610713005, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:05, Epoch: 67, Batch: 140, Training Loss: 0.053891146555542946, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:06, Epoch: 67, Batch: 150, Training Loss: 0.03363787941634655, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:06, Epoch: 67, Batch: 160, Training Loss: 0.0370783094316721, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:07, Epoch: 67, Batch: 170, Training Loss: 0.04234258346259594, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:08, Epoch: 67, Batch: 180, Training Loss: 0.04353959746658802, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:09, Epoch: 67, Batch: 190, Training Loss: 0.06948783621191978, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:10, Epoch: 67, Batch: 200, Training Loss: 0.028639528527855874, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:10, Epoch: 67, Batch: 210, Training Loss: 0.05820256732404232, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:11, Epoch: 67, Batch: 220, Training Loss: 0.04244631417095661, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:12, Epoch: 67, Batch: 230, Training Loss: 0.040844646841287614, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:13, Epoch: 67, Batch: 240, Training Loss: 0.03568908609449863, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:13, Epoch: 67, Batch: 250, Training Loss: 0.037268612533807755, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:14, Epoch: 67, Batch: 260, Training Loss: 0.048547723516821864, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:15, Epoch: 67, Batch: 270, Training Loss: 0.05254887416958809, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:16, Epoch: 67, Batch: 280, Training Loss: 0.02932027280330658, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:16, Epoch: 67, Batch: 290, Training Loss: 0.02718368135392666, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:17, Epoch: 67, Batch: 300, Training Loss: 0.029994513466954233, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:18, Epoch: 67, Batch: 310, Training Loss: 0.03572615906596184, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:19, Epoch: 67, Batch: 320, Training Loss: 0.039812258630990985, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:19, Epoch: 67, Batch: 330, Training Loss: 0.07601810358464718, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:20, Epoch: 67, Batch: 340, Training Loss: 0.028351005539298056, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:21, Epoch: 67, Batch: 350, Training Loss: 0.04544651210308075, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:22, Epoch: 67, Batch: 360, Training Loss: 0.04294812902808189, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:23, Epoch: 67, Batch: 370, Training Loss: 0.04773623310029507, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:24, Epoch: 67, Batch: 380, Training Loss: 0.02661173269152641, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:24, Epoch: 67, Batch: 390, Training Loss: 0.04159720353782177, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:25, Epoch: 67, Batch: 400, Training Loss: 0.05239746868610382, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:26, Epoch: 67, Batch: 410, Training Loss: 0.04928184226155281, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:27, Epoch: 67, Batch: 420, Training Loss: 0.05397490188479424, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:27, Epoch: 67, Batch: 430, Training Loss: 0.05026403926312924, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:28, Epoch: 67, Batch: 440, Training Loss: 0.03882558792829514, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:29, Epoch: 67, Batch: 450, Training Loss: 0.0541002731770277, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:30, Epoch: 67, Batch: 460, Training Loss: 0.04769325479865074, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:30, Epoch: 67, Batch: 470, Training Loss: 0.042336123436689375, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:31, Epoch: 67, Batch: 480, Training Loss: 0.04422765746712685, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:32, Epoch: 67, Batch: 490, Training Loss: 0.04407304003834724, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:32, Epoch: 67, Batch: 500, Training Loss: 0.05716732256114483, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:33, Epoch: 67, Batch: 510, Training Loss: 0.024995508044958113, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:34, Epoch: 67, Batch: 520, Training Loss: 0.03284538388252258, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:35, Epoch: 67, Batch: 530, Training Loss: 0.033276218175888064, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:35, Epoch: 67, Batch: 540, Training Loss: 0.03616573438048363, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:36, Epoch: 67, Batch: 550, Training Loss: 0.0380316860973835, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:37, Epoch: 67, Batch: 560, Training Loss: 0.05319779291749001, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:37, Epoch: 67, Batch: 570, Training Loss: 0.0558978408575058, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:38, Epoch: 67, Batch: 580, Training Loss: 0.030411072075366974, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:39, Epoch: 67, Batch: 590, Training Loss: 0.03678338006138802, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:40, Epoch: 67, Batch: 600, Training Loss: 0.026236129552125932, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:40, Epoch: 67, Batch: 610, Training Loss: 0.04341501221060753, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:41, Epoch: 67, Batch: 620, Training Loss: 0.042281411588191986, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:42, Epoch: 67, Batch: 630, Training Loss: 0.03409164696931839, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:43, Epoch: 67, Batch: 640, Training Loss: 0.03320280462503433, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:44, Epoch: 67, Batch: 650, Training Loss: 0.03701145760715008, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:45, Epoch: 67, Batch: 660, Training Loss: 0.041460714489221576, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:45, Epoch: 67, Batch: 670, Training Loss: 0.04478784576058388, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:46, Epoch: 67, Batch: 680, Training Loss: 0.03290230892598629, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:47, Epoch: 67, Batch: 690, Training Loss: 0.038499603420495986, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:48, Epoch: 67, Batch: 700, Training Loss: 0.03207039125263691, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:49, Epoch: 67, Batch: 710, Training Loss: 0.029388581961393358, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:49, Epoch: 67, Batch: 720, Training Loss: 0.04886358380317688, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:50, Epoch: 67, Batch: 730, Training Loss: 0.0417783971875906, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:51, Epoch: 67, Batch: 740, Training Loss: 0.038567525520920756, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:52, Epoch: 67, Batch: 750, Training Loss: 0.0450704038143158, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:53, Epoch: 67, Batch: 760, Training Loss: 0.049312785640358923, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:53, Epoch: 67, Batch: 770, Training Loss: 0.05961608476936817, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:54, Epoch: 67, Batch: 780, Training Loss: 0.04955938756465912, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:55, Epoch: 67, Batch: 790, Training Loss: 0.031181027367711067, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:55, Epoch: 67, Batch: 800, Training Loss: 0.029219265282154083, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:56, Epoch: 67, Batch: 810, Training Loss: 0.047238612174987794, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:57, Epoch: 67, Batch: 820, Training Loss: 0.04352494701743126, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:58, Epoch: 67, Batch: 830, Training Loss: 0.03742182776331902, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:58, Epoch: 67, Batch: 840, Training Loss: 0.04874020032584667, LR: 0.00010000000000000003
Time, 2019-01-01T18:30:59, Epoch: 67, Batch: 850, Training Loss: 0.051041390374302865, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:00, Epoch: 67, Batch: 860, Training Loss: 0.04004887491464615, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:00, Epoch: 67, Batch: 870, Training Loss: 0.030685963481664656, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:01, Epoch: 67, Batch: 880, Training Loss: 0.04738799408078194, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:02, Epoch: 67, Batch: 890, Training Loss: 0.039832311868667605, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:03, Epoch: 67, Batch: 900, Training Loss: 0.0511609110981226, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:03, Epoch: 67, Batch: 910, Training Loss: 0.049552305787801745, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:04, Epoch: 67, Batch: 920, Training Loss: 0.0402342539280653, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:05, Epoch: 67, Batch: 930, Training Loss: 0.04062487259507179, LR: 0.00010000000000000003
Epoch: 67, Validation Top 1 acc: 98.85061645507812
Epoch: 67, Validation Top 5 acc: 99.99000549316406
Epoch: 67, Validation Set Loss: 0.04181502386927605
Start training epoch 68
Time, 2019-01-01T18:31:35, Epoch: 68, Batch: 10, Training Loss: 0.04614381343126297, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:36, Epoch: 68, Batch: 20, Training Loss: 0.0556348592042923, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:37, Epoch: 68, Batch: 30, Training Loss: 0.03296106979250908, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:38, Epoch: 68, Batch: 40, Training Loss: 0.04301729761064053, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:38, Epoch: 68, Batch: 50, Training Loss: 0.034314960986375806, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:39, Epoch: 68, Batch: 60, Training Loss: 0.029914643615484238, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:40, Epoch: 68, Batch: 70, Training Loss: 0.03840986639261246, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:40, Epoch: 68, Batch: 80, Training Loss: 0.04690932929515838, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:41, Epoch: 68, Batch: 90, Training Loss: 0.040091638639569285, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:42, Epoch: 68, Batch: 100, Training Loss: 0.04894096441566944, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:43, Epoch: 68, Batch: 110, Training Loss: 0.04882825687527657, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:44, Epoch: 68, Batch: 120, Training Loss: 0.03948040306568146, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:45, Epoch: 68, Batch: 130, Training Loss: 0.0662212036550045, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:46, Epoch: 68, Batch: 140, Training Loss: 0.04178397655487061, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:47, Epoch: 68, Batch: 150, Training Loss: 0.04853740558028221, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:47, Epoch: 68, Batch: 160, Training Loss: 0.04084523729979992, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:48, Epoch: 68, Batch: 170, Training Loss: 0.04944804385304451, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:49, Epoch: 68, Batch: 180, Training Loss: 0.04303451925516129, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:49, Epoch: 68, Batch: 190, Training Loss: 0.036656961962580684, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:50, Epoch: 68, Batch: 200, Training Loss: 0.05478617027401924, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:51, Epoch: 68, Batch: 210, Training Loss: 0.045912386104464534, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:52, Epoch: 68, Batch: 220, Training Loss: 0.024236894398927688, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:52, Epoch: 68, Batch: 230, Training Loss: 0.038298399746418, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:53, Epoch: 68, Batch: 240, Training Loss: 0.050752952322363855, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:54, Epoch: 68, Batch: 250, Training Loss: 0.027165983989834785, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:55, Epoch: 68, Batch: 260, Training Loss: 0.039102021232247354, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:55, Epoch: 68, Batch: 270, Training Loss: 0.029481042921543122, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:56, Epoch: 68, Batch: 280, Training Loss: 0.05034325197339058, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:57, Epoch: 68, Batch: 290, Training Loss: 0.05399903282523155, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:58, Epoch: 68, Batch: 300, Training Loss: 0.04571856819093227, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:58, Epoch: 68, Batch: 310, Training Loss: 0.04069729149341583, LR: 0.00010000000000000003
Time, 2019-01-01T18:31:59, Epoch: 68, Batch: 320, Training Loss: 0.041899987310171125, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:00, Epoch: 68, Batch: 330, Training Loss: 0.03447916992008686, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:00, Epoch: 68, Batch: 340, Training Loss: 0.03754919730126858, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:01, Epoch: 68, Batch: 350, Training Loss: 0.039504324644804, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:02, Epoch: 68, Batch: 360, Training Loss: 0.04606897160410881, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:03, Epoch: 68, Batch: 370, Training Loss: 0.05359475016593933, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:03, Epoch: 68, Batch: 380, Training Loss: 0.02997676730155945, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:04, Epoch: 68, Batch: 390, Training Loss: 0.041605960577726364, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:05, Epoch: 68, Batch: 400, Training Loss: 0.037204650789499284, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:06, Epoch: 68, Batch: 410, Training Loss: 0.06955996304750442, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:06, Epoch: 68, Batch: 420, Training Loss: 0.04087485373020172, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:07, Epoch: 68, Batch: 430, Training Loss: 0.0459464967250824, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:08, Epoch: 68, Batch: 440, Training Loss: 0.03878283798694611, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:09, Epoch: 68, Batch: 450, Training Loss: 0.04090767726302147, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:09, Epoch: 68, Batch: 460, Training Loss: 0.05178100503981113, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:10, Epoch: 68, Batch: 470, Training Loss: 0.04257469698786735, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:11, Epoch: 68, Batch: 480, Training Loss: 0.034730945527553556, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:12, Epoch: 68, Batch: 490, Training Loss: 0.04468775019049644, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:12, Epoch: 68, Batch: 500, Training Loss: 0.047252466529607774, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:13, Epoch: 68, Batch: 510, Training Loss: 0.033352640271186826, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:14, Epoch: 68, Batch: 520, Training Loss: 0.04100154191255569, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:14, Epoch: 68, Batch: 530, Training Loss: 0.0355356439948082, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:15, Epoch: 68, Batch: 540, Training Loss: 0.03601401448249817, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:16, Epoch: 68, Batch: 550, Training Loss: 0.031896889209747314, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:17, Epoch: 68, Batch: 560, Training Loss: 0.03998843133449555, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:17, Epoch: 68, Batch: 570, Training Loss: 0.0448496337980032, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:18, Epoch: 68, Batch: 580, Training Loss: 0.05085807628929615, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:19, Epoch: 68, Batch: 590, Training Loss: 0.0420747771859169, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:20, Epoch: 68, Batch: 600, Training Loss: 0.04524216279387474, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:20, Epoch: 68, Batch: 610, Training Loss: 0.04948635064065456, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:21, Epoch: 68, Batch: 620, Training Loss: 0.05075477957725525, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:22, Epoch: 68, Batch: 630, Training Loss: 0.053490253165364265, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:23, Epoch: 68, Batch: 640, Training Loss: 0.02696395181119442, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:23, Epoch: 68, Batch: 650, Training Loss: 0.05192162357270717, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:24, Epoch: 68, Batch: 660, Training Loss: 0.032655971497297286, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:25, Epoch: 68, Batch: 670, Training Loss: 0.03652007207274437, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:25, Epoch: 68, Batch: 680, Training Loss: 0.055097862333059314, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:26, Epoch: 68, Batch: 690, Training Loss: 0.036590910702943805, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:27, Epoch: 68, Batch: 700, Training Loss: 0.0368651669472456, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:28, Epoch: 68, Batch: 710, Training Loss: 0.037034448608756064, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:28, Epoch: 68, Batch: 720, Training Loss: 0.023600328341126443, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:29, Epoch: 68, Batch: 730, Training Loss: 0.04280504882335663, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:30, Epoch: 68, Batch: 740, Training Loss: 0.04591231569647789, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:31, Epoch: 68, Batch: 750, Training Loss: 0.04501896612346172, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:31, Epoch: 68, Batch: 760, Training Loss: 0.03699644058942795, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:32, Epoch: 68, Batch: 770, Training Loss: 0.05498142316937447, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:33, Epoch: 68, Batch: 780, Training Loss: 0.04246029183268547, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:33, Epoch: 68, Batch: 790, Training Loss: 0.04198564849793911, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:34, Epoch: 68, Batch: 800, Training Loss: 0.02951369769871235, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:35, Epoch: 68, Batch: 810, Training Loss: 0.05050390213727951, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:36, Epoch: 68, Batch: 820, Training Loss: 0.03925830945372581, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:36, Epoch: 68, Batch: 830, Training Loss: 0.037885015830397606, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:37, Epoch: 68, Batch: 840, Training Loss: 0.026159950345754624, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:38, Epoch: 68, Batch: 850, Training Loss: 0.05030021592974663, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:38, Epoch: 68, Batch: 860, Training Loss: 0.05470089539885521, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:39, Epoch: 68, Batch: 870, Training Loss: 0.033997535705566406, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:40, Epoch: 68, Batch: 880, Training Loss: 0.04907961487770081, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:41, Epoch: 68, Batch: 890, Training Loss: 0.04014570973813534, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:41, Epoch: 68, Batch: 900, Training Loss: 0.03846981450915336, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:42, Epoch: 68, Batch: 910, Training Loss: 0.02483694292604923, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:43, Epoch: 68, Batch: 920, Training Loss: 0.03840468898415565, LR: 0.00010000000000000003
Time, 2019-01-01T18:32:44, Epoch: 68, Batch: 930, Training Loss: 0.0362201876938343, LR: 0.00010000000000000003
Epoch: 68, Validation Top 1 acc: 98.84728240966797
Epoch: 68, Validation Top 5 acc: 99.99000549316406
Epoch: 68, Validation Set Loss: 0.04167822748422623
Start training epoch 69
Time, 2019-01-01T18:33:12, Epoch: 69, Batch: 10, Training Loss: 0.04526405297219753, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:13, Epoch: 69, Batch: 20, Training Loss: 0.039687075465917585, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:13, Epoch: 69, Batch: 30, Training Loss: 0.0346424862742424, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:14, Epoch: 69, Batch: 40, Training Loss: 0.05301959402859211, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:15, Epoch: 69, Batch: 50, Training Loss: 0.026963295042514802, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:16, Epoch: 69, Batch: 60, Training Loss: 0.05292443744838238, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:16, Epoch: 69, Batch: 70, Training Loss: 0.03705996870994568, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:17, Epoch: 69, Batch: 80, Training Loss: 0.03132214583456516, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:18, Epoch: 69, Batch: 90, Training Loss: 0.05598288178443909, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:19, Epoch: 69, Batch: 100, Training Loss: 0.058621896058321, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:19, Epoch: 69, Batch: 110, Training Loss: 0.026156050339341163, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:20, Epoch: 69, Batch: 120, Training Loss: 0.045973803475499155, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:21, Epoch: 69, Batch: 130, Training Loss: 0.05688934214413166, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:21, Epoch: 69, Batch: 140, Training Loss: 0.05666414350271225, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:22, Epoch: 69, Batch: 150, Training Loss: 0.041140320152044295, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:23, Epoch: 69, Batch: 160, Training Loss: 0.046229133009910585, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:24, Epoch: 69, Batch: 170, Training Loss: 0.03129062205553055, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:24, Epoch: 69, Batch: 180, Training Loss: 0.033867552131414416, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:25, Epoch: 69, Batch: 190, Training Loss: 0.04190533794462681, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:26, Epoch: 69, Batch: 200, Training Loss: 0.04241323694586754, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:27, Epoch: 69, Batch: 210, Training Loss: 0.04854302853345871, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:27, Epoch: 69, Batch: 220, Training Loss: 0.05104351863265037, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:28, Epoch: 69, Batch: 230, Training Loss: 0.04772966206073761, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:29, Epoch: 69, Batch: 240, Training Loss: 0.032559479400515554, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:30, Epoch: 69, Batch: 250, Training Loss: 0.05048593133687973, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:31, Epoch: 69, Batch: 260, Training Loss: 0.04442211128771305, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:31, Epoch: 69, Batch: 270, Training Loss: 0.02820838801562786, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:32, Epoch: 69, Batch: 280, Training Loss: 0.06664283201098442, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:33, Epoch: 69, Batch: 290, Training Loss: 0.043568062409758565, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:34, Epoch: 69, Batch: 300, Training Loss: 0.03590922839939594, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:35, Epoch: 69, Batch: 310, Training Loss: 0.03495742008090019, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:36, Epoch: 69, Batch: 320, Training Loss: 0.032183165475726125, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:37, Epoch: 69, Batch: 330, Training Loss: 0.04186098128557205, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:37, Epoch: 69, Batch: 340, Training Loss: 0.024933700263500214, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:38, Epoch: 69, Batch: 350, Training Loss: 0.04514372795820236, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:39, Epoch: 69, Batch: 360, Training Loss: 0.04168424047529697, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:40, Epoch: 69, Batch: 370, Training Loss: 0.05414301753044128, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:41, Epoch: 69, Batch: 380, Training Loss: 0.04016820229589939, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:42, Epoch: 69, Batch: 390, Training Loss: 0.0295884620398283, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:43, Epoch: 69, Batch: 400, Training Loss: 0.04424030557274818, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:44, Epoch: 69, Batch: 410, Training Loss: 0.054882682487368585, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:44, Epoch: 69, Batch: 420, Training Loss: 0.042697202414274216, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:45, Epoch: 69, Batch: 430, Training Loss: 0.05269740968942642, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:46, Epoch: 69, Batch: 440, Training Loss: 0.04398595467209816, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:47, Epoch: 69, Batch: 450, Training Loss: 0.04733058735728264, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:47, Epoch: 69, Batch: 460, Training Loss: 0.05548459477722645, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:48, Epoch: 69, Batch: 470, Training Loss: 0.03588005192577839, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:49, Epoch: 69, Batch: 480, Training Loss: 0.053044723346829414, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:50, Epoch: 69, Batch: 490, Training Loss: 0.042379972711205485, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:50, Epoch: 69, Batch: 500, Training Loss: 0.026820892095565797, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:51, Epoch: 69, Batch: 510, Training Loss: 0.03000335469841957, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:52, Epoch: 69, Batch: 520, Training Loss: 0.04243223816156387, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:53, Epoch: 69, Batch: 530, Training Loss: 0.047585241869091986, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:53, Epoch: 69, Batch: 540, Training Loss: 0.039529448747634886, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:54, Epoch: 69, Batch: 550, Training Loss: 0.038183610513806344, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:55, Epoch: 69, Batch: 560, Training Loss: 0.026268043369054795, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:56, Epoch: 69, Batch: 570, Training Loss: 0.04459447860717773, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:56, Epoch: 69, Batch: 580, Training Loss: 0.052857603132724765, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:57, Epoch: 69, Batch: 590, Training Loss: 0.04083906225860119, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:58, Epoch: 69, Batch: 600, Training Loss: 0.05187042355537415, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:58, Epoch: 69, Batch: 610, Training Loss: 0.026828264817595482, LR: 0.00010000000000000003
Time, 2019-01-01T18:33:59, Epoch: 69, Batch: 620, Training Loss: 0.0401074230670929, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:00, Epoch: 69, Batch: 630, Training Loss: 0.039130236580967905, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:01, Epoch: 69, Batch: 640, Training Loss: 0.030170845240354537, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:01, Epoch: 69, Batch: 650, Training Loss: 0.0422949243336916, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:02, Epoch: 69, Batch: 660, Training Loss: 0.04470569081604481, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:03, Epoch: 69, Batch: 670, Training Loss: 0.055695815756917, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:04, Epoch: 69, Batch: 680, Training Loss: 0.03526162430644035, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:04, Epoch: 69, Batch: 690, Training Loss: 0.04050546213984489, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:05, Epoch: 69, Batch: 700, Training Loss: 0.05183220095932484, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:06, Epoch: 69, Batch: 710, Training Loss: 0.050784462690353395, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:07, Epoch: 69, Batch: 720, Training Loss: 0.04442003145813942, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:07, Epoch: 69, Batch: 730, Training Loss: 0.028157319501042367, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:08, Epoch: 69, Batch: 740, Training Loss: 0.041710886731743814, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:09, Epoch: 69, Batch: 750, Training Loss: 0.042108021676540375, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:09, Epoch: 69, Batch: 760, Training Loss: 0.04153518304228783, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:10, Epoch: 69, Batch: 770, Training Loss: 0.03219130784273148, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:11, Epoch: 69, Batch: 780, Training Loss: 0.033402790874242784, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:12, Epoch: 69, Batch: 790, Training Loss: 0.04207972176373005, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:12, Epoch: 69, Batch: 800, Training Loss: 0.029218356311321258, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:13, Epoch: 69, Batch: 810, Training Loss: 0.02749035060405731, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:14, Epoch: 69, Batch: 820, Training Loss: 0.02982753962278366, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:15, Epoch: 69, Batch: 830, Training Loss: 0.04254744127392769, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:15, Epoch: 69, Batch: 840, Training Loss: 0.048349319398403166, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:16, Epoch: 69, Batch: 850, Training Loss: 0.05387952476739884, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:17, Epoch: 69, Batch: 860, Training Loss: 0.03841151222586632, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:18, Epoch: 69, Batch: 870, Training Loss: 0.04348463974893093, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:18, Epoch: 69, Batch: 880, Training Loss: 0.04124587699770928, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:19, Epoch: 69, Batch: 890, Training Loss: 0.02964377626776695, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:20, Epoch: 69, Batch: 900, Training Loss: 0.03518107011914253, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:20, Epoch: 69, Batch: 910, Training Loss: 0.04585571363568306, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:21, Epoch: 69, Batch: 920, Training Loss: 0.046574711427092554, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:22, Epoch: 69, Batch: 930, Training Loss: 0.03172955214977265, LR: 0.00010000000000000003
Epoch: 69, Validation Top 1 acc: 98.85227966308594
Epoch: 69, Validation Top 5 acc: 99.99000549316406
Epoch: 69, Validation Set Loss: 0.04163861647248268
Start training epoch 70
Time, 2019-01-01T18:34:50, Epoch: 70, Batch: 10, Training Loss: 0.045780687034130095, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:50, Epoch: 70, Batch: 20, Training Loss: 0.04176952689886093, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:51, Epoch: 70, Batch: 30, Training Loss: 0.0392055731266737, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:52, Epoch: 70, Batch: 40, Training Loss: 0.046950599551200865, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:52, Epoch: 70, Batch: 50, Training Loss: 0.031533388420939445, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:53, Epoch: 70, Batch: 60, Training Loss: 0.034560125321149826, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:54, Epoch: 70, Batch: 70, Training Loss: 0.05585209541022777, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:55, Epoch: 70, Batch: 80, Training Loss: 0.06033290922641754, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:55, Epoch: 70, Batch: 90, Training Loss: 0.03565445803105831, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:56, Epoch: 70, Batch: 100, Training Loss: 0.04015401750802994, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:57, Epoch: 70, Batch: 110, Training Loss: 0.024668273329734803, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:58, Epoch: 70, Batch: 120, Training Loss: 0.05005474872887135, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:58, Epoch: 70, Batch: 130, Training Loss: 0.03274257890880108, LR: 0.00010000000000000003
Time, 2019-01-01T18:34:59, Epoch: 70, Batch: 140, Training Loss: 0.061697880178689955, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:00, Epoch: 70, Batch: 150, Training Loss: 0.03997428119182587, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:00, Epoch: 70, Batch: 160, Training Loss: 0.030362721532583237, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:01, Epoch: 70, Batch: 170, Training Loss: 0.04310448616743088, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:02, Epoch: 70, Batch: 180, Training Loss: 0.0408296275883913, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:02, Epoch: 70, Batch: 190, Training Loss: 0.0322343286126852, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:03, Epoch: 70, Batch: 200, Training Loss: 0.03987762108445168, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:04, Epoch: 70, Batch: 210, Training Loss: 0.040424656495451926, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:05, Epoch: 70, Batch: 220, Training Loss: 0.05258308053016662, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:05, Epoch: 70, Batch: 230, Training Loss: 0.05335719361901283, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:06, Epoch: 70, Batch: 240, Training Loss: 0.043618359789252284, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:07, Epoch: 70, Batch: 250, Training Loss: 0.03946828581392765, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:07, Epoch: 70, Batch: 260, Training Loss: 0.03469621315598488, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:08, Epoch: 70, Batch: 270, Training Loss: 0.030309366062283516, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:09, Epoch: 70, Batch: 280, Training Loss: 0.03393049314618111, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:10, Epoch: 70, Batch: 290, Training Loss: 0.07481586523354053, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:10, Epoch: 70, Batch: 300, Training Loss: 0.029971908405423166, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:11, Epoch: 70, Batch: 310, Training Loss: 0.02161787562072277, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:12, Epoch: 70, Batch: 320, Training Loss: 0.028536304831504822, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:12, Epoch: 70, Batch: 330, Training Loss: 0.03468837663531303, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:13, Epoch: 70, Batch: 340, Training Loss: 0.036240730062127115, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:14, Epoch: 70, Batch: 350, Training Loss: 0.04089131616055965, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:15, Epoch: 70, Batch: 360, Training Loss: 0.04606901183724403, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:15, Epoch: 70, Batch: 370, Training Loss: 0.04159155301749706, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:16, Epoch: 70, Batch: 380, Training Loss: 0.059730909019708636, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:17, Epoch: 70, Batch: 390, Training Loss: 0.05145340859889984, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:17, Epoch: 70, Batch: 400, Training Loss: 0.037587692588567735, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:18, Epoch: 70, Batch: 410, Training Loss: 0.03988591805100441, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:19, Epoch: 70, Batch: 420, Training Loss: 0.04250445142388344, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:20, Epoch: 70, Batch: 430, Training Loss: 0.031435102224349976, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:20, Epoch: 70, Batch: 440, Training Loss: 0.04141317866742611, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:21, Epoch: 70, Batch: 450, Training Loss: 0.05966305695474148, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:22, Epoch: 70, Batch: 460, Training Loss: 0.03925906345248222, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:22, Epoch: 70, Batch: 470, Training Loss: 0.02165328189730644, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:23, Epoch: 70, Batch: 480, Training Loss: 0.05032595694065094, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:24, Epoch: 70, Batch: 490, Training Loss: 0.028889814764261244, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:25, Epoch: 70, Batch: 500, Training Loss: 0.04901802241802215, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:25, Epoch: 70, Batch: 510, Training Loss: 0.04964544363319874, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:26, Epoch: 70, Batch: 520, Training Loss: 0.030461541935801507, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:27, Epoch: 70, Batch: 530, Training Loss: 0.04927445165812969, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:27, Epoch: 70, Batch: 540, Training Loss: 0.05470095351338387, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:28, Epoch: 70, Batch: 550, Training Loss: 0.023853032290935515, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:29, Epoch: 70, Batch: 560, Training Loss: 0.047949868440628055, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:30, Epoch: 70, Batch: 570, Training Loss: 0.03642972782254219, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:30, Epoch: 70, Batch: 580, Training Loss: 0.039750651270151136, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:31, Epoch: 70, Batch: 590, Training Loss: 0.04359045512974262, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:32, Epoch: 70, Batch: 600, Training Loss: 0.045577403903007505, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:32, Epoch: 70, Batch: 610, Training Loss: 0.06790267825126647, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:33, Epoch: 70, Batch: 620, Training Loss: 0.041404899954795835, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:34, Epoch: 70, Batch: 630, Training Loss: 0.0361970953643322, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:34, Epoch: 70, Batch: 640, Training Loss: 0.05037209503352642, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:35, Epoch: 70, Batch: 650, Training Loss: 0.03981536068022251, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:36, Epoch: 70, Batch: 660, Training Loss: 0.04952097460627556, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:37, Epoch: 70, Batch: 670, Training Loss: 0.050522264838218686, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:37, Epoch: 70, Batch: 680, Training Loss: 0.04431377053260803, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:38, Epoch: 70, Batch: 690, Training Loss: 0.058676980435848236, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:39, Epoch: 70, Batch: 700, Training Loss: 0.031574889272451404, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:39, Epoch: 70, Batch: 710, Training Loss: 0.06537528820335865, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:40, Epoch: 70, Batch: 720, Training Loss: 0.046057290583848956, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:41, Epoch: 70, Batch: 730, Training Loss: 0.04356999807059765, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:42, Epoch: 70, Batch: 740, Training Loss: 0.03551013320684433, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:42, Epoch: 70, Batch: 750, Training Loss: 0.030685002729296685, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:43, Epoch: 70, Batch: 760, Training Loss: 0.034994589164853096, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:44, Epoch: 70, Batch: 770, Training Loss: 0.03957774117588997, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:44, Epoch: 70, Batch: 780, Training Loss: 0.02839592471718788, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:45, Epoch: 70, Batch: 790, Training Loss: 0.03415122739970684, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:46, Epoch: 70, Batch: 800, Training Loss: 0.04376838877797127, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:47, Epoch: 70, Batch: 810, Training Loss: 0.039765361696481705, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:47, Epoch: 70, Batch: 820, Training Loss: 0.03559125326573849, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:48, Epoch: 70, Batch: 830, Training Loss: 0.02561766132712364, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:49, Epoch: 70, Batch: 840, Training Loss: 0.01962100937962532, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:49, Epoch: 70, Batch: 850, Training Loss: 0.04451516419649124, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:50, Epoch: 70, Batch: 860, Training Loss: 0.031153282895684243, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:51, Epoch: 70, Batch: 870, Training Loss: 0.04224014729261398, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:52, Epoch: 70, Batch: 880, Training Loss: 0.03446500897407532, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:52, Epoch: 70, Batch: 890, Training Loss: 0.04602589532732963, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:53, Epoch: 70, Batch: 900, Training Loss: 0.0313840389251709, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:54, Epoch: 70, Batch: 910, Training Loss: 0.0594347957521677, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:54, Epoch: 70, Batch: 920, Training Loss: 0.0520896952599287, LR: 0.00010000000000000003
Time, 2019-01-01T18:35:55, Epoch: 70, Batch: 930, Training Loss: 0.04873023144900799, LR: 0.00010000000000000003
Epoch: 70, Validation Top 1 acc: 98.84062194824219
Epoch: 70, Validation Top 5 acc: 99.99000549316406
Epoch: 70, Validation Set Loss: 0.04163406789302826
Start training epoch 71
Time, 2019-01-01T18:36:23, Epoch: 71, Batch: 10, Training Loss: 0.053904344141483304, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:23, Epoch: 71, Batch: 20, Training Loss: 0.03481179997324944, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:24, Epoch: 71, Batch: 30, Training Loss: 0.05096644088625908, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:25, Epoch: 71, Batch: 40, Training Loss: 0.042867114394903184, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:26, Epoch: 71, Batch: 50, Training Loss: 0.051137424632906915, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:26, Epoch: 71, Batch: 60, Training Loss: 0.03853303268551826, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:27, Epoch: 71, Batch: 70, Training Loss: 0.03032902702689171, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:28, Epoch: 71, Batch: 80, Training Loss: 0.032502095028758046, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:28, Epoch: 71, Batch: 90, Training Loss: 0.027376586198806764, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:29, Epoch: 71, Batch: 100, Training Loss: 0.024426383152604102, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:30, Epoch: 71, Batch: 110, Training Loss: 0.048994073644280434, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:30, Epoch: 71, Batch: 120, Training Loss: 0.04069049805402756, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:31, Epoch: 71, Batch: 130, Training Loss: 0.053457411378622054, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:32, Epoch: 71, Batch: 140, Training Loss: 0.05337666198611259, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:33, Epoch: 71, Batch: 150, Training Loss: 0.03641512021422386, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:33, Epoch: 71, Batch: 160, Training Loss: 0.04389865770936012, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:34, Epoch: 71, Batch: 170, Training Loss: 0.0415202647447586, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:35, Epoch: 71, Batch: 180, Training Loss: 0.040178796648979186, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:36, Epoch: 71, Batch: 190, Training Loss: 0.03992677628993988, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:36, Epoch: 71, Batch: 200, Training Loss: 0.035315126180648804, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:37, Epoch: 71, Batch: 210, Training Loss: 0.0514410100877285, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:38, Epoch: 71, Batch: 220, Training Loss: 0.0326700434088707, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:39, Epoch: 71, Batch: 230, Training Loss: 0.03271735161542892, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:39, Epoch: 71, Batch: 240, Training Loss: 0.04845056757330894, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:40, Epoch: 71, Batch: 250, Training Loss: 0.042552023008465764, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:41, Epoch: 71, Batch: 260, Training Loss: 0.06094885319471359, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:41, Epoch: 71, Batch: 270, Training Loss: 0.0400222297757864, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:42, Epoch: 71, Batch: 280, Training Loss: 0.03828469775617123, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:43, Epoch: 71, Batch: 290, Training Loss: 0.04261740036308766, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:44, Epoch: 71, Batch: 300, Training Loss: 0.0439811073243618, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:44, Epoch: 71, Batch: 310, Training Loss: 0.04641212671995163, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:45, Epoch: 71, Batch: 320, Training Loss: 0.0426044937223196, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:46, Epoch: 71, Batch: 330, Training Loss: 0.03062390685081482, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:46, Epoch: 71, Batch: 340, Training Loss: 0.030423884466290473, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:47, Epoch: 71, Batch: 350, Training Loss: 0.045394963771104815, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:48, Epoch: 71, Batch: 360, Training Loss: 0.042394102364778516, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:49, Epoch: 71, Batch: 370, Training Loss: 0.034650960564613344, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:49, Epoch: 71, Batch: 380, Training Loss: 0.05611884742975235, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:50, Epoch: 71, Batch: 390, Training Loss: 0.03549657091498375, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:51, Epoch: 71, Batch: 400, Training Loss: 0.04936455003917217, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:52, Epoch: 71, Batch: 410, Training Loss: 0.039821312949061397, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:52, Epoch: 71, Batch: 420, Training Loss: 0.04110532216727734, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:53, Epoch: 71, Batch: 430, Training Loss: 0.035335364565253255, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:54, Epoch: 71, Batch: 440, Training Loss: 0.04495483413338661, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:54, Epoch: 71, Batch: 450, Training Loss: 0.02240280881524086, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:55, Epoch: 71, Batch: 460, Training Loss: 0.03252945803105831, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:56, Epoch: 71, Batch: 470, Training Loss: 0.051431935653090476, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:57, Epoch: 71, Batch: 480, Training Loss: 0.03169072866439819, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:57, Epoch: 71, Batch: 490, Training Loss: 0.054059648886322975, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:58, Epoch: 71, Batch: 500, Training Loss: 0.03776348605751991, LR: 0.00010000000000000003
Time, 2019-01-01T18:36:59, Epoch: 71, Batch: 510, Training Loss: 0.05744906030595302, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:00, Epoch: 71, Batch: 520, Training Loss: 0.0478525523096323, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:00, Epoch: 71, Batch: 530, Training Loss: 0.046241195499897005, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:01, Epoch: 71, Batch: 540, Training Loss: 0.03522597923874855, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:02, Epoch: 71, Batch: 550, Training Loss: 0.03875038810074329, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:02, Epoch: 71, Batch: 560, Training Loss: 0.03158477693796158, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:03, Epoch: 71, Batch: 570, Training Loss: 0.031067100539803504, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:04, Epoch: 71, Batch: 580, Training Loss: 0.04374568201601505, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:05, Epoch: 71, Batch: 590, Training Loss: 0.03975650817155838, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:05, Epoch: 71, Batch: 600, Training Loss: 0.05302052535116673, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:06, Epoch: 71, Batch: 610, Training Loss: 0.04182237833738327, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:07, Epoch: 71, Batch: 620, Training Loss: 0.05497359037399292, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:07, Epoch: 71, Batch: 630, Training Loss: 0.04794745519757271, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:08, Epoch: 71, Batch: 640, Training Loss: 0.042837508395314215, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:09, Epoch: 71, Batch: 650, Training Loss: 0.03358645252883434, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:10, Epoch: 71, Batch: 660, Training Loss: 0.03972951881587505, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:10, Epoch: 71, Batch: 670, Training Loss: 0.04474782235920429, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:11, Epoch: 71, Batch: 680, Training Loss: 0.049168350920081136, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:12, Epoch: 71, Batch: 690, Training Loss: 0.04068281352519989, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:12, Epoch: 71, Batch: 700, Training Loss: 0.03512083180248737, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:13, Epoch: 71, Batch: 710, Training Loss: 0.037412232533097264, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:14, Epoch: 71, Batch: 720, Training Loss: 0.05914740078151226, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:15, Epoch: 71, Batch: 730, Training Loss: 0.04088446572422981, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:15, Epoch: 71, Batch: 740, Training Loss: 0.03317756801843643, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:16, Epoch: 71, Batch: 750, Training Loss: 0.04098953902721405, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:17, Epoch: 71, Batch: 760, Training Loss: 0.047757588326931, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:17, Epoch: 71, Batch: 770, Training Loss: 0.03463576845824719, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:18, Epoch: 71, Batch: 780, Training Loss: 0.043145766109228136, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:19, Epoch: 71, Batch: 790, Training Loss: 0.04359341748058796, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:20, Epoch: 71, Batch: 800, Training Loss: 0.04191152453422546, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:20, Epoch: 71, Batch: 810, Training Loss: 0.033749517798423764, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:21, Epoch: 71, Batch: 820, Training Loss: 0.027579206973314285, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:22, Epoch: 71, Batch: 830, Training Loss: 0.04021873064339161, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:22, Epoch: 71, Batch: 840, Training Loss: 0.03636154569685459, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:23, Epoch: 71, Batch: 850, Training Loss: 0.0666335579007864, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:24, Epoch: 71, Batch: 860, Training Loss: 0.04189130999147892, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:25, Epoch: 71, Batch: 870, Training Loss: 0.0533070832490921, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:25, Epoch: 71, Batch: 880, Training Loss: 0.04093126058578491, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:26, Epoch: 71, Batch: 890, Training Loss: 0.05044996403157711, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:27, Epoch: 71, Batch: 900, Training Loss: 0.034852666035294536, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:27, Epoch: 71, Batch: 910, Training Loss: 0.035634567588567735, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:28, Epoch: 71, Batch: 920, Training Loss: 0.036617114394903186, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:29, Epoch: 71, Batch: 930, Training Loss: 0.04389394409954548, LR: 0.00010000000000000003
Epoch: 71, Validation Top 1 acc: 98.84728240966797
Epoch: 71, Validation Top 5 acc: 99.99000549316406
Epoch: 71, Validation Set Loss: 0.041525699198246
Start training epoch 72
Time, 2019-01-01T18:37:58, Epoch: 72, Batch: 10, Training Loss: 0.04508489444851875, LR: 0.00010000000000000003
Time, 2019-01-01T18:37:59, Epoch: 72, Batch: 20, Training Loss: 0.03291362375020981, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:00, Epoch: 72, Batch: 30, Training Loss: 0.033268707990646365, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:00, Epoch: 72, Batch: 40, Training Loss: 0.03322109580039978, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:01, Epoch: 72, Batch: 50, Training Loss: 0.04399715960025787, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:02, Epoch: 72, Batch: 60, Training Loss: 0.05237107388675213, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:03, Epoch: 72, Batch: 70, Training Loss: 0.044350947812199594, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:04, Epoch: 72, Batch: 80, Training Loss: 0.03772635236382484, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:04, Epoch: 72, Batch: 90, Training Loss: 0.03463241159915924, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:05, Epoch: 72, Batch: 100, Training Loss: 0.057652272284030914, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:06, Epoch: 72, Batch: 110, Training Loss: 0.046593304723501205, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:06, Epoch: 72, Batch: 120, Training Loss: 0.033574916794896124, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:07, Epoch: 72, Batch: 130, Training Loss: 0.054716476052999494, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:08, Epoch: 72, Batch: 140, Training Loss: 0.029048599675297736, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:09, Epoch: 72, Batch: 150, Training Loss: 0.025439410656690597, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:09, Epoch: 72, Batch: 160, Training Loss: 0.03471378870308399, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:10, Epoch: 72, Batch: 170, Training Loss: 0.05704171545803547, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:11, Epoch: 72, Batch: 180, Training Loss: 0.051929716020822525, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:11, Epoch: 72, Batch: 190, Training Loss: 0.08220374621450902, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:12, Epoch: 72, Batch: 200, Training Loss: 0.04135107509791851, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:13, Epoch: 72, Batch: 210, Training Loss: 0.040918773785233495, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:14, Epoch: 72, Batch: 220, Training Loss: 0.04291499778628349, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:14, Epoch: 72, Batch: 230, Training Loss: 0.03878663294017315, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:15, Epoch: 72, Batch: 240, Training Loss: 0.047560686618089675, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:16, Epoch: 72, Batch: 250, Training Loss: 0.03888481035828591, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:16, Epoch: 72, Batch: 260, Training Loss: 0.049887868016958235, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:17, Epoch: 72, Batch: 270, Training Loss: 0.030071474611759186, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:18, Epoch: 72, Batch: 280, Training Loss: 0.040015249699354175, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:19, Epoch: 72, Batch: 290, Training Loss: 0.04483963623642921, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:19, Epoch: 72, Batch: 300, Training Loss: 0.03500784635543823, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:20, Epoch: 72, Batch: 310, Training Loss: 0.03832046017050743, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:21, Epoch: 72, Batch: 320, Training Loss: 0.029630905389785765, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:21, Epoch: 72, Batch: 330, Training Loss: 0.0458696361631155, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:22, Epoch: 72, Batch: 340, Training Loss: 0.02373778074979782, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:23, Epoch: 72, Batch: 350, Training Loss: 0.04971008412539959, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:24, Epoch: 72, Batch: 360, Training Loss: 0.06959360390901566, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:24, Epoch: 72, Batch: 370, Training Loss: 0.03515486270189285, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:25, Epoch: 72, Batch: 380, Training Loss: 0.026371678709983824, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:26, Epoch: 72, Batch: 390, Training Loss: 0.033929220587015155, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:26, Epoch: 72, Batch: 400, Training Loss: 0.027649445831775664, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:27, Epoch: 72, Batch: 410, Training Loss: 0.03506142757833004, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:28, Epoch: 72, Batch: 420, Training Loss: 0.03509043976664543, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:29, Epoch: 72, Batch: 430, Training Loss: 0.03843560516834259, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:29, Epoch: 72, Batch: 440, Training Loss: 0.034628909826278684, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:30, Epoch: 72, Batch: 450, Training Loss: 0.035380758345127106, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:31, Epoch: 72, Batch: 460, Training Loss: 0.04369097426533699, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:31, Epoch: 72, Batch: 470, Training Loss: 0.035242076963186264, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:32, Epoch: 72, Batch: 480, Training Loss: 0.0590580552816391, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:33, Epoch: 72, Batch: 490, Training Loss: 0.05997466817498207, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:34, Epoch: 72, Batch: 500, Training Loss: 0.03624178804457188, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:34, Epoch: 72, Batch: 510, Training Loss: 0.04176014065742493, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:35, Epoch: 72, Batch: 520, Training Loss: 0.04761533439159393, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:36, Epoch: 72, Batch: 530, Training Loss: 0.03945180289447307, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:36, Epoch: 72, Batch: 540, Training Loss: 0.06438074633479118, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:37, Epoch: 72, Batch: 550, Training Loss: 0.053077568858861925, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:38, Epoch: 72, Batch: 560, Training Loss: 0.04208147600293159, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:39, Epoch: 72, Batch: 570, Training Loss: 0.043920467421412465, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:39, Epoch: 72, Batch: 580, Training Loss: 0.021960961073637007, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:40, Epoch: 72, Batch: 590, Training Loss: 0.046267016977071765, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:41, Epoch: 72, Batch: 600, Training Loss: 0.028908249735832215, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:41, Epoch: 72, Batch: 610, Training Loss: 0.0222578227519989, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:42, Epoch: 72, Batch: 620, Training Loss: 0.04058028422296047, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:43, Epoch: 72, Batch: 630, Training Loss: 0.0215992346405983, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:43, Epoch: 72, Batch: 640, Training Loss: 0.045769283547997475, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:44, Epoch: 72, Batch: 650, Training Loss: 0.03249584436416626, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:45, Epoch: 72, Batch: 660, Training Loss: 0.034918344020843504, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:46, Epoch: 72, Batch: 670, Training Loss: 0.03741450086236, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:46, Epoch: 72, Batch: 680, Training Loss: 0.04300849325954914, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:47, Epoch: 72, Batch: 690, Training Loss: 0.06101944744586944, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:48, Epoch: 72, Batch: 700, Training Loss: 0.053399088233709334, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:49, Epoch: 72, Batch: 710, Training Loss: 0.035439105704426765, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:49, Epoch: 72, Batch: 720, Training Loss: 0.06016914136707783, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:50, Epoch: 72, Batch: 730, Training Loss: 0.03977258652448654, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:51, Epoch: 72, Batch: 740, Training Loss: 0.03314831294119358, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:52, Epoch: 72, Batch: 750, Training Loss: 0.04908684678375721, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:52, Epoch: 72, Batch: 760, Training Loss: 0.02766669765114784, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:53, Epoch: 72, Batch: 770, Training Loss: 0.0432120569050312, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:54, Epoch: 72, Batch: 780, Training Loss: 0.04304116480052471, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:55, Epoch: 72, Batch: 790, Training Loss: 0.04575553573668003, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:55, Epoch: 72, Batch: 800, Training Loss: 0.04321297705173492, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:56, Epoch: 72, Batch: 810, Training Loss: 0.039752197265625, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:57, Epoch: 72, Batch: 820, Training Loss: 0.04415354393422603, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:57, Epoch: 72, Batch: 830, Training Loss: 0.04454149417579174, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:58, Epoch: 72, Batch: 840, Training Loss: 0.04225152432918548, LR: 0.00010000000000000003
Time, 2019-01-01T18:38:59, Epoch: 72, Batch: 850, Training Loss: 0.03616448193788528, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:00, Epoch: 72, Batch: 860, Training Loss: 0.049772007018327714, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:00, Epoch: 72, Batch: 870, Training Loss: 0.044343090429902074, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:01, Epoch: 72, Batch: 880, Training Loss: 0.030498625338077547, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:02, Epoch: 72, Batch: 890, Training Loss: 0.04597189873456955, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:03, Epoch: 72, Batch: 900, Training Loss: 0.04967409484088421, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:03, Epoch: 72, Batch: 910, Training Loss: 0.03806886747479439, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:04, Epoch: 72, Batch: 920, Training Loss: 0.027954428642988204, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:05, Epoch: 72, Batch: 930, Training Loss: 0.04852249771356583, LR: 0.00010000000000000003
Epoch: 72, Validation Top 1 acc: 98.8572769165039
Epoch: 72, Validation Top 5 acc: 99.99000549316406
Epoch: 72, Validation Set Loss: 0.041507065296173096
Start training epoch 73
Time, 2019-01-01T18:39:34, Epoch: 73, Batch: 10, Training Loss: 0.0436224315315485, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:35, Epoch: 73, Batch: 20, Training Loss: 0.03292469345033169, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:36, Epoch: 73, Batch: 30, Training Loss: 0.04183636382222176, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:36, Epoch: 73, Batch: 40, Training Loss: 0.03997852243483067, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:37, Epoch: 73, Batch: 50, Training Loss: 0.027514046430587767, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:38, Epoch: 73, Batch: 60, Training Loss: 0.04874179102480412, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:38, Epoch: 73, Batch: 70, Training Loss: 0.04056614227592945, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:39, Epoch: 73, Batch: 80, Training Loss: 0.043831156194210054, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:40, Epoch: 73, Batch: 90, Training Loss: 0.03948100432753563, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:41, Epoch: 73, Batch: 100, Training Loss: 0.04270072653889656, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:41, Epoch: 73, Batch: 110, Training Loss: 0.03454054296016693, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:42, Epoch: 73, Batch: 120, Training Loss: 0.033627907559275624, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:43, Epoch: 73, Batch: 130, Training Loss: 0.02819724902510643, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:44, Epoch: 73, Batch: 140, Training Loss: 0.03838458582758904, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:44, Epoch: 73, Batch: 150, Training Loss: 0.04947348944842815, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:45, Epoch: 73, Batch: 160, Training Loss: 0.051798115670681, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:46, Epoch: 73, Batch: 170, Training Loss: 0.06103512197732926, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:46, Epoch: 73, Batch: 180, Training Loss: 0.038120556995272635, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:47, Epoch: 73, Batch: 190, Training Loss: 0.041386998072266576, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:48, Epoch: 73, Batch: 200, Training Loss: 0.04685369431972504, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:49, Epoch: 73, Batch: 210, Training Loss: 0.04579008296132088, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:49, Epoch: 73, Batch: 220, Training Loss: 0.0393728107213974, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:50, Epoch: 73, Batch: 230, Training Loss: 0.025328420847654343, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:51, Epoch: 73, Batch: 240, Training Loss: 0.034226608276367185, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:51, Epoch: 73, Batch: 250, Training Loss: 0.03995253145694733, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:52, Epoch: 73, Batch: 260, Training Loss: 0.06748222708702087, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:53, Epoch: 73, Batch: 270, Training Loss: 0.033837250992655755, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:54, Epoch: 73, Batch: 280, Training Loss: 0.06475544795393944, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:54, Epoch: 73, Batch: 290, Training Loss: 0.04307267591357231, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:55, Epoch: 73, Batch: 300, Training Loss: 0.04701875671744347, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:56, Epoch: 73, Batch: 310, Training Loss: 0.03623369820415974, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:57, Epoch: 73, Batch: 320, Training Loss: 0.041950471326708795, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:57, Epoch: 73, Batch: 330, Training Loss: 0.035584858059883116, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:58, Epoch: 73, Batch: 340, Training Loss: 0.046651731804013254, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:59, Epoch: 73, Batch: 350, Training Loss: 0.03926607258617878, LR: 0.00010000000000000003
Time, 2019-01-01T18:39:59, Epoch: 73, Batch: 360, Training Loss: 0.05818561762571335, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:00, Epoch: 73, Batch: 370, Training Loss: 0.04000940658152104, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:01, Epoch: 73, Batch: 380, Training Loss: 0.06161712519824505, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:02, Epoch: 73, Batch: 390, Training Loss: 0.03309055268764496, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:02, Epoch: 73, Batch: 400, Training Loss: 0.06084194667637348, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:03, Epoch: 73, Batch: 410, Training Loss: 0.031107012182474136, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:04, Epoch: 73, Batch: 420, Training Loss: 0.04102017395198345, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:04, Epoch: 73, Batch: 430, Training Loss: 0.04966762885451317, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:05, Epoch: 73, Batch: 440, Training Loss: 0.032427598163485526, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:06, Epoch: 73, Batch: 450, Training Loss: 0.033695855736732484, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:07, Epoch: 73, Batch: 460, Training Loss: 0.035747747868299484, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:07, Epoch: 73, Batch: 470, Training Loss: 0.05264512337744236, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:08, Epoch: 73, Batch: 480, Training Loss: 0.04312418475747108, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:09, Epoch: 73, Batch: 490, Training Loss: 0.03631705455482006, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:09, Epoch: 73, Batch: 500, Training Loss: 0.035979319363832474, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:10, Epoch: 73, Batch: 510, Training Loss: 0.042231077700853346, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:11, Epoch: 73, Batch: 520, Training Loss: 0.04303485304117203, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:12, Epoch: 73, Batch: 530, Training Loss: 0.029300326853990553, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:12, Epoch: 73, Batch: 540, Training Loss: 0.03816238418221474, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:13, Epoch: 73, Batch: 550, Training Loss: 0.07524545788764954, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:14, Epoch: 73, Batch: 560, Training Loss: 0.03914353661239147, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:15, Epoch: 73, Batch: 570, Training Loss: 0.06124228276312351, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:15, Epoch: 73, Batch: 580, Training Loss: 0.07310290783643722, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:16, Epoch: 73, Batch: 590, Training Loss: 0.025312507525086403, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:17, Epoch: 73, Batch: 600, Training Loss: 0.03391711935400963, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:17, Epoch: 73, Batch: 610, Training Loss: 0.03152402676641941, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:18, Epoch: 73, Batch: 620, Training Loss: 0.047129028290510175, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:19, Epoch: 73, Batch: 630, Training Loss: 0.02856242060661316, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:20, Epoch: 73, Batch: 640, Training Loss: 0.03833687715232372, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:20, Epoch: 73, Batch: 650, Training Loss: 0.04363154210150242, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:21, Epoch: 73, Batch: 660, Training Loss: 0.06712674722075462, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:22, Epoch: 73, Batch: 670, Training Loss: 0.046317584812641144, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:22, Epoch: 73, Batch: 680, Training Loss: 0.03189594894647598, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:23, Epoch: 73, Batch: 690, Training Loss: 0.05660482719540596, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:24, Epoch: 73, Batch: 700, Training Loss: 0.03134872168302536, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:25, Epoch: 73, Batch: 710, Training Loss: 0.029038067907094955, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:25, Epoch: 73, Batch: 720, Training Loss: 0.03685778453946113, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:26, Epoch: 73, Batch: 730, Training Loss: 0.029026227444410323, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:27, Epoch: 73, Batch: 740, Training Loss: 0.03635244928300381, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:28, Epoch: 73, Batch: 750, Training Loss: 0.04102788455784321, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:28, Epoch: 73, Batch: 760, Training Loss: 0.051149907335639, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:29, Epoch: 73, Batch: 770, Training Loss: 0.027261769771575926, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:30, Epoch: 73, Batch: 780, Training Loss: 0.031397046893835066, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:30, Epoch: 73, Batch: 790, Training Loss: 0.026901707425713538, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:31, Epoch: 73, Batch: 800, Training Loss: 0.0342137161642313, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:32, Epoch: 73, Batch: 810, Training Loss: 0.05695471540093422, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:33, Epoch: 73, Batch: 820, Training Loss: 0.028115732967853545, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:33, Epoch: 73, Batch: 830, Training Loss: 0.045575867593288424, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:34, Epoch: 73, Batch: 840, Training Loss: 0.05375569015741348, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:35, Epoch: 73, Batch: 850, Training Loss: 0.027802995592355727, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:35, Epoch: 73, Batch: 860, Training Loss: 0.040441961586475374, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:36, Epoch: 73, Batch: 870, Training Loss: 0.04564272947609425, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:37, Epoch: 73, Batch: 880, Training Loss: 0.05431403294205665, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:38, Epoch: 73, Batch: 890, Training Loss: 0.03377297073602677, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:38, Epoch: 73, Batch: 900, Training Loss: 0.02755958288908005, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:39, Epoch: 73, Batch: 910, Training Loss: 0.03407635428011417, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:40, Epoch: 73, Batch: 920, Training Loss: 0.031655048578977586, LR: 0.00010000000000000003
Time, 2019-01-01T18:40:41, Epoch: 73, Batch: 930, Training Loss: 0.058118248358368874, LR: 0.00010000000000000003
Epoch: 73, Validation Top 1 acc: 98.85061645507812
Epoch: 73, Validation Top 5 acc: 99.99000549316406
Epoch: 73, Validation Set Loss: 0.041501689702272415
Start training epoch 74
Time, 2019-01-01T18:41:08, Epoch: 74, Batch: 10, Training Loss: 0.035771607607603076, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:09, Epoch: 74, Batch: 20, Training Loss: 0.06548110842704773, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:10, Epoch: 74, Batch: 30, Training Loss: 0.045455801859498025, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:11, Epoch: 74, Batch: 40, Training Loss: 0.041381850838661194, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:11, Epoch: 74, Batch: 50, Training Loss: 0.047445277124643324, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:12, Epoch: 74, Batch: 60, Training Loss: 0.047955960780382154, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:13, Epoch: 74, Batch: 70, Training Loss: 0.036758580803871156, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:13, Epoch: 74, Batch: 80, Training Loss: 0.02140056937932968, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:14, Epoch: 74, Batch: 90, Training Loss: 0.04712018892168999, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:15, Epoch: 74, Batch: 100, Training Loss: 0.03676397651433945, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:16, Epoch: 74, Batch: 110, Training Loss: 0.03997162543237209, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:16, Epoch: 74, Batch: 120, Training Loss: 0.036790435761213304, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:17, Epoch: 74, Batch: 130, Training Loss: 0.040680833905935285, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:18, Epoch: 74, Batch: 140, Training Loss: 0.04882119558751583, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:18, Epoch: 74, Batch: 150, Training Loss: 0.03396328687667847, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:19, Epoch: 74, Batch: 160, Training Loss: 0.04933790862560272, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:20, Epoch: 74, Batch: 170, Training Loss: 0.044344387203454974, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:21, Epoch: 74, Batch: 180, Training Loss: 0.030264610052108766, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:21, Epoch: 74, Batch: 190, Training Loss: 0.05158577412366867, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:22, Epoch: 74, Batch: 200, Training Loss: 0.04445907771587372, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:23, Epoch: 74, Batch: 210, Training Loss: 0.03394379019737244, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:24, Epoch: 74, Batch: 220, Training Loss: 0.04205899052321911, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:24, Epoch: 74, Batch: 230, Training Loss: 0.02844378799200058, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:25, Epoch: 74, Batch: 240, Training Loss: 0.04739911109209061, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:26, Epoch: 74, Batch: 250, Training Loss: 0.03537065275013447, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:26, Epoch: 74, Batch: 260, Training Loss: 0.030720413476228715, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:27, Epoch: 74, Batch: 270, Training Loss: 0.03059626966714859, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:28, Epoch: 74, Batch: 280, Training Loss: 0.05372901856899261, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:29, Epoch: 74, Batch: 290, Training Loss: 0.03820942565798759, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:29, Epoch: 74, Batch: 300, Training Loss: 0.030637139827013014, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:30, Epoch: 74, Batch: 310, Training Loss: 0.052125058323144915, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:31, Epoch: 74, Batch: 320, Training Loss: 0.04804839938879013, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:31, Epoch: 74, Batch: 330, Training Loss: 0.03955610617995262, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:32, Epoch: 74, Batch: 340, Training Loss: 0.06621993035078048, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:33, Epoch: 74, Batch: 350, Training Loss: 0.030834553763270378, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:34, Epoch: 74, Batch: 360, Training Loss: 0.04525075554847717, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:34, Epoch: 74, Batch: 370, Training Loss: 0.03927916064858437, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:35, Epoch: 74, Batch: 380, Training Loss: 0.05573227107524872, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:36, Epoch: 74, Batch: 390, Training Loss: 0.03519310653209686, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:36, Epoch: 74, Batch: 400, Training Loss: 0.04479425102472305, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:37, Epoch: 74, Batch: 410, Training Loss: 0.03949962817132473, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:38, Epoch: 74, Batch: 420, Training Loss: 0.033754131942987445, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:39, Epoch: 74, Batch: 430, Training Loss: 0.04729938507080078, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:39, Epoch: 74, Batch: 440, Training Loss: 0.05565477237105369, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:40, Epoch: 74, Batch: 450, Training Loss: 0.03028268553316593, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:41, Epoch: 74, Batch: 460, Training Loss: 0.039425287395715714, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:41, Epoch: 74, Batch: 470, Training Loss: 0.040144511684775355, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:42, Epoch: 74, Batch: 480, Training Loss: 0.04659509956836701, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:43, Epoch: 74, Batch: 490, Training Loss: 0.04612045921385288, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:44, Epoch: 74, Batch: 500, Training Loss: 0.030225191637873648, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:44, Epoch: 74, Batch: 510, Training Loss: 0.050647220015525816, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:45, Epoch: 74, Batch: 520, Training Loss: 0.0378445353358984, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:46, Epoch: 74, Batch: 530, Training Loss: 0.028311090543866158, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:47, Epoch: 74, Batch: 540, Training Loss: 0.03937517069280148, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:47, Epoch: 74, Batch: 550, Training Loss: 0.02856140285730362, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:48, Epoch: 74, Batch: 560, Training Loss: 0.03484509326517582, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:49, Epoch: 74, Batch: 570, Training Loss: 0.023355475813150405, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:49, Epoch: 74, Batch: 580, Training Loss: 0.041879740357398984, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:50, Epoch: 74, Batch: 590, Training Loss: 0.02241923287510872, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:51, Epoch: 74, Batch: 600, Training Loss: 0.028351863473653795, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:52, Epoch: 74, Batch: 610, Training Loss: 0.03686833530664444, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:52, Epoch: 74, Batch: 620, Training Loss: 0.06014048792421818, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:53, Epoch: 74, Batch: 630, Training Loss: 0.0401529960334301, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:54, Epoch: 74, Batch: 640, Training Loss: 0.036596082523465155, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:54, Epoch: 74, Batch: 650, Training Loss: 0.044991561025381085, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:55, Epoch: 74, Batch: 660, Training Loss: 0.02608916088938713, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:56, Epoch: 74, Batch: 670, Training Loss: 0.04883781112730503, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:57, Epoch: 74, Batch: 680, Training Loss: 0.047130707278847696, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:57, Epoch: 74, Batch: 690, Training Loss: 0.04162740111351013, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:58, Epoch: 74, Batch: 700, Training Loss: 0.04518393129110336, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:59, Epoch: 74, Batch: 710, Training Loss: 0.0456388495862484, LR: 0.00010000000000000003
Time, 2019-01-01T18:41:59, Epoch: 74, Batch: 720, Training Loss: 0.042612961307168004, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:00, Epoch: 74, Batch: 730, Training Loss: 0.041364483535289764, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:01, Epoch: 74, Batch: 740, Training Loss: 0.04074909053742885, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:02, Epoch: 74, Batch: 750, Training Loss: 0.03872901052236557, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:02, Epoch: 74, Batch: 760, Training Loss: 0.04153149276971817, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:03, Epoch: 74, Batch: 770, Training Loss: 0.039016883820295334, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:04, Epoch: 74, Batch: 780, Training Loss: 0.03941237144172192, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:05, Epoch: 74, Batch: 790, Training Loss: 0.03943710774183273, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:05, Epoch: 74, Batch: 800, Training Loss: 0.03683928437530994, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:06, Epoch: 74, Batch: 810, Training Loss: 0.04589274227619171, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:07, Epoch: 74, Batch: 820, Training Loss: 0.04203561656177044, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:07, Epoch: 74, Batch: 830, Training Loss: 0.05687326565384865, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:08, Epoch: 74, Batch: 840, Training Loss: 0.050214937701821324, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:09, Epoch: 74, Batch: 850, Training Loss: 0.05411440953612327, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:10, Epoch: 74, Batch: 860, Training Loss: 0.033557460829615596, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:10, Epoch: 74, Batch: 870, Training Loss: 0.030627232789993287, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:11, Epoch: 74, Batch: 880, Training Loss: 0.050611629709601404, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:12, Epoch: 74, Batch: 890, Training Loss: 0.0430727694183588, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:12, Epoch: 74, Batch: 900, Training Loss: 0.05855246186256409, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:13, Epoch: 74, Batch: 910, Training Loss: 0.047502780333161354, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:14, Epoch: 74, Batch: 920, Training Loss: 0.04146653413772583, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:15, Epoch: 74, Batch: 930, Training Loss: 0.03873424157500267, LR: 0.00010000000000000003
Epoch: 74, Validation Top 1 acc: 98.84561920166016
Epoch: 74, Validation Top 5 acc: 99.99000549316406
Epoch: 74, Validation Set Loss: 0.04146286100149155
Start training epoch 75
Time, 2019-01-01T18:42:42, Epoch: 75, Batch: 10, Training Loss: 0.05356838330626488, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:43, Epoch: 75, Batch: 20, Training Loss: 0.04317360185086727, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:44, Epoch: 75, Batch: 30, Training Loss: 0.047291746735572814, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:45, Epoch: 75, Batch: 40, Training Loss: 0.05990169942378998, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:45, Epoch: 75, Batch: 50, Training Loss: 0.04317216016352177, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:46, Epoch: 75, Batch: 60, Training Loss: 0.036786897107958794, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:47, Epoch: 75, Batch: 70, Training Loss: 0.03951225206255913, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:48, Epoch: 75, Batch: 80, Training Loss: 0.04619642160832882, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:48, Epoch: 75, Batch: 90, Training Loss: 0.05515670143067837, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:49, Epoch: 75, Batch: 100, Training Loss: 0.03338629975914955, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:50, Epoch: 75, Batch: 110, Training Loss: 0.03915610052645206, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:50, Epoch: 75, Batch: 120, Training Loss: 0.024898037314414978, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:51, Epoch: 75, Batch: 130, Training Loss: 0.03621140345931053, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:52, Epoch: 75, Batch: 140, Training Loss: 0.04390471801161766, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:53, Epoch: 75, Batch: 150, Training Loss: 0.04881851188838482, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:53, Epoch: 75, Batch: 160, Training Loss: 0.044031678885221484, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:54, Epoch: 75, Batch: 170, Training Loss: 0.03952325731515884, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:55, Epoch: 75, Batch: 180, Training Loss: 0.039907587319612504, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:55, Epoch: 75, Batch: 190, Training Loss: 0.025490009039640427, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:56, Epoch: 75, Batch: 200, Training Loss: 0.05201951339840889, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:57, Epoch: 75, Batch: 210, Training Loss: 0.04284078516066074, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:58, Epoch: 75, Batch: 220, Training Loss: 0.043998194858431816, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:58, Epoch: 75, Batch: 230, Training Loss: 0.0423449769616127, LR: 0.00010000000000000003
Time, 2019-01-01T18:42:59, Epoch: 75, Batch: 240, Training Loss: 0.024476706981658936, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:00, Epoch: 75, Batch: 250, Training Loss: 0.04747525230050087, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:00, Epoch: 75, Batch: 260, Training Loss: 0.02838672660291195, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:01, Epoch: 75, Batch: 270, Training Loss: 0.0351354818791151, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:02, Epoch: 75, Batch: 280, Training Loss: 0.04479632303118706, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:03, Epoch: 75, Batch: 290, Training Loss: 0.031463929638266566, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:03, Epoch: 75, Batch: 300, Training Loss: 0.03931364007294178, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:04, Epoch: 75, Batch: 310, Training Loss: 0.04530211687088013, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:05, Epoch: 75, Batch: 320, Training Loss: 0.047699176147580145, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:05, Epoch: 75, Batch: 330, Training Loss: 0.039148684963583945, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:06, Epoch: 75, Batch: 340, Training Loss: 0.07346385791897773, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:07, Epoch: 75, Batch: 350, Training Loss: 0.030973609909415244, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:08, Epoch: 75, Batch: 360, Training Loss: 0.03850121423602104, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:08, Epoch: 75, Batch: 370, Training Loss: 0.036713390797376635, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:09, Epoch: 75, Batch: 380, Training Loss: 0.031638260930776596, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:10, Epoch: 75, Batch: 390, Training Loss: 0.03394070230424404, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:10, Epoch: 75, Batch: 400, Training Loss: 0.0350098192691803, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:11, Epoch: 75, Batch: 410, Training Loss: 0.03290055952966213, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:12, Epoch: 75, Batch: 420, Training Loss: 0.03813531436026096, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:13, Epoch: 75, Batch: 430, Training Loss: 0.0566774345934391, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:13, Epoch: 75, Batch: 440, Training Loss: 0.04460438899695873, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:14, Epoch: 75, Batch: 450, Training Loss: 0.05202995948493481, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:15, Epoch: 75, Batch: 460, Training Loss: 0.03286201059818268, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:16, Epoch: 75, Batch: 470, Training Loss: 0.034985891357064244, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:16, Epoch: 75, Batch: 480, Training Loss: 0.039668576419353486, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:17, Epoch: 75, Batch: 490, Training Loss: 0.03684328570961952, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:18, Epoch: 75, Batch: 500, Training Loss: 0.03981893733143806, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:18, Epoch: 75, Batch: 510, Training Loss: 0.0453280545771122, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:19, Epoch: 75, Batch: 520, Training Loss: 0.05056404881179333, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:20, Epoch: 75, Batch: 530, Training Loss: 0.032521427050232886, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:21, Epoch: 75, Batch: 540, Training Loss: 0.06159234121441841, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:21, Epoch: 75, Batch: 550, Training Loss: 0.029746996611356734, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:22, Epoch: 75, Batch: 560, Training Loss: 0.03438375145196915, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:23, Epoch: 75, Batch: 570, Training Loss: 0.03711651340126991, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:24, Epoch: 75, Batch: 580, Training Loss: 0.03389638625085354, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:24, Epoch: 75, Batch: 590, Training Loss: 0.05223589837551117, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:25, Epoch: 75, Batch: 600, Training Loss: 0.042084474489092825, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:26, Epoch: 75, Batch: 610, Training Loss: 0.05181019827723503, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:26, Epoch: 75, Batch: 620, Training Loss: 0.0396858636289835, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:27, Epoch: 75, Batch: 630, Training Loss: 0.06045916490256786, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:28, Epoch: 75, Batch: 640, Training Loss: 0.060195643454790115, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:29, Epoch: 75, Batch: 650, Training Loss: 0.028634949401021004, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:29, Epoch: 75, Batch: 660, Training Loss: 0.03764389380812645, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:30, Epoch: 75, Batch: 670, Training Loss: 0.05265493430197239, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:31, Epoch: 75, Batch: 680, Training Loss: 0.029508066177368165, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:31, Epoch: 75, Batch: 690, Training Loss: 0.04133071042597294, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:32, Epoch: 75, Batch: 700, Training Loss: 0.044120718538761136, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:33, Epoch: 75, Batch: 710, Training Loss: 0.034044599160552025, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:34, Epoch: 75, Batch: 720, Training Loss: 0.05770744793117046, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:34, Epoch: 75, Batch: 730, Training Loss: 0.051022940501570704, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:35, Epoch: 75, Batch: 740, Training Loss: 0.041855355724692345, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:36, Epoch: 75, Batch: 750, Training Loss: 0.042410014942288396, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:36, Epoch: 75, Batch: 760, Training Loss: 0.0343063659965992, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:37, Epoch: 75, Batch: 770, Training Loss: 0.02710953466594219, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:38, Epoch: 75, Batch: 780, Training Loss: 0.03370045870542526, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:39, Epoch: 75, Batch: 790, Training Loss: 0.05509889721870422, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:39, Epoch: 75, Batch: 800, Training Loss: 0.03239280357956886, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:40, Epoch: 75, Batch: 810, Training Loss: 0.051938601210713385, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:41, Epoch: 75, Batch: 820, Training Loss: 0.043113387748599054, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:41, Epoch: 75, Batch: 830, Training Loss: 0.040740296989679334, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:42, Epoch: 75, Batch: 840, Training Loss: 0.05433679521083832, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:43, Epoch: 75, Batch: 850, Training Loss: 0.029741399735212327, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:44, Epoch: 75, Batch: 860, Training Loss: 0.035499782487750056, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:44, Epoch: 75, Batch: 870, Training Loss: 0.04689554795622826, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:45, Epoch: 75, Batch: 880, Training Loss: 0.03307776525616646, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:46, Epoch: 75, Batch: 890, Training Loss: 0.0455179262906313, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:47, Epoch: 75, Batch: 900, Training Loss: 0.03904397934675217, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:47, Epoch: 75, Batch: 910, Training Loss: 0.04032892026007175, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:48, Epoch: 75, Batch: 920, Training Loss: 0.04357912950217724, LR: 0.00010000000000000003
Time, 2019-01-01T18:43:49, Epoch: 75, Batch: 930, Training Loss: 0.03740780204534531, LR: 0.00010000000000000003
Epoch: 75, Validation Top 1 acc: 98.84728240966797
Epoch: 75, Validation Top 5 acc: 99.99166870117188
Epoch: 75, Validation Set Loss: 0.041453536599874496
Start training epoch 76
Time, 2019-01-01T18:44:17, Epoch: 76, Batch: 10, Training Loss: 0.02675037793815136, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:17, Epoch: 76, Batch: 20, Training Loss: 0.043435484543442725, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:18, Epoch: 76, Batch: 30, Training Loss: 0.026630305498838425, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:19, Epoch: 76, Batch: 40, Training Loss: 0.047071028500795364, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:19, Epoch: 76, Batch: 50, Training Loss: 0.04942305907607079, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:20, Epoch: 76, Batch: 60, Training Loss: 0.0466873973608017, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:21, Epoch: 76, Batch: 70, Training Loss: 0.045303677767515184, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:22, Epoch: 76, Batch: 80, Training Loss: 0.031241913512349128, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:22, Epoch: 76, Batch: 90, Training Loss: 0.04124349355697632, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:23, Epoch: 76, Batch: 100, Training Loss: 0.03729266561567783, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:24, Epoch: 76, Batch: 110, Training Loss: 0.03944917023181915, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:25, Epoch: 76, Batch: 120, Training Loss: 0.025532301142811777, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:25, Epoch: 76, Batch: 130, Training Loss: 0.04135238341987133, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:26, Epoch: 76, Batch: 140, Training Loss: 0.037851229682564734, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:27, Epoch: 76, Batch: 150, Training Loss: 0.06766677759587765, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:27, Epoch: 76, Batch: 160, Training Loss: 0.029329372197389604, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:28, Epoch: 76, Batch: 170, Training Loss: 0.05650670006871224, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:29, Epoch: 76, Batch: 180, Training Loss: 0.04142247810959816, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:30, Epoch: 76, Batch: 190, Training Loss: 0.02772647514939308, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:30, Epoch: 76, Batch: 200, Training Loss: 0.035570936277508736, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:31, Epoch: 76, Batch: 210, Training Loss: 0.04516794793307781, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:32, Epoch: 76, Batch: 220, Training Loss: 0.054243383929133415, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:33, Epoch: 76, Batch: 230, Training Loss: 0.038387805223464966, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:33, Epoch: 76, Batch: 240, Training Loss: 0.028677626699209213, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:34, Epoch: 76, Batch: 250, Training Loss: 0.03133743144571781, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:35, Epoch: 76, Batch: 260, Training Loss: 0.03130372352898121, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:35, Epoch: 76, Batch: 270, Training Loss: 0.0599727176129818, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:36, Epoch: 76, Batch: 280, Training Loss: 0.039160659164190294, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:37, Epoch: 76, Batch: 290, Training Loss: 0.041658128052949904, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:38, Epoch: 76, Batch: 300, Training Loss: 0.04879192188382149, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:38, Epoch: 76, Batch: 310, Training Loss: 0.042475837469100955, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:39, Epoch: 76, Batch: 320, Training Loss: 0.035780007392168044, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:40, Epoch: 76, Batch: 330, Training Loss: 0.03500445038080215, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:41, Epoch: 76, Batch: 340, Training Loss: 0.04514478072524071, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:41, Epoch: 76, Batch: 350, Training Loss: 0.03897772617638111, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:42, Epoch: 76, Batch: 360, Training Loss: 0.037363918498158455, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:43, Epoch: 76, Batch: 370, Training Loss: 0.042843329906463626, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:44, Epoch: 76, Batch: 380, Training Loss: 0.03415163308382034, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:44, Epoch: 76, Batch: 390, Training Loss: 0.046491334587335585, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:45, Epoch: 76, Batch: 400, Training Loss: 0.0431195080280304, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:46, Epoch: 76, Batch: 410, Training Loss: 0.033911528810858724, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:46, Epoch: 76, Batch: 420, Training Loss: 0.03522152155637741, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:47, Epoch: 76, Batch: 430, Training Loss: 0.03587254546582699, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:48, Epoch: 76, Batch: 440, Training Loss: 0.046029360592365266, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:49, Epoch: 76, Batch: 450, Training Loss: 0.05050406605005264, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:49, Epoch: 76, Batch: 460, Training Loss: 0.05148872509598732, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:50, Epoch: 76, Batch: 470, Training Loss: 0.05475281439721584, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:51, Epoch: 76, Batch: 480, Training Loss: 0.03689250648021698, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:51, Epoch: 76, Batch: 490, Training Loss: 0.025161461532115938, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:52, Epoch: 76, Batch: 500, Training Loss: 0.06012625545263291, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:53, Epoch: 76, Batch: 510, Training Loss: 0.046814021468162534, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:54, Epoch: 76, Batch: 520, Training Loss: 0.04216428771615029, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:55, Epoch: 76, Batch: 530, Training Loss: 0.044987664371728894, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:56, Epoch: 76, Batch: 540, Training Loss: 0.046496983617544174, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:56, Epoch: 76, Batch: 550, Training Loss: 0.05886423662304878, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:57, Epoch: 76, Batch: 560, Training Loss: 0.04505706913769245, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:58, Epoch: 76, Batch: 570, Training Loss: 0.031891724467277525, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:59, Epoch: 76, Batch: 580, Training Loss: 0.04495810866355896, LR: 0.00010000000000000003
Time, 2019-01-01T18:44:59, Epoch: 76, Batch: 590, Training Loss: 0.04209242649376392, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:00, Epoch: 76, Batch: 600, Training Loss: 0.03677753694355488, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:01, Epoch: 76, Batch: 610, Training Loss: 0.028143931180238724, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:02, Epoch: 76, Batch: 620, Training Loss: 0.04315637834370136, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:02, Epoch: 76, Batch: 630, Training Loss: 0.03271579183638096, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:03, Epoch: 76, Batch: 640, Training Loss: 0.03209744393825531, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:04, Epoch: 76, Batch: 650, Training Loss: 0.05265812464058399, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:04, Epoch: 76, Batch: 660, Training Loss: 0.03384977467358112, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:05, Epoch: 76, Batch: 670, Training Loss: 0.043667832389473915, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:06, Epoch: 76, Batch: 680, Training Loss: 0.0519615177065134, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:07, Epoch: 76, Batch: 690, Training Loss: 0.04023677222430706, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:07, Epoch: 76, Batch: 700, Training Loss: 0.03929205983877182, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:08, Epoch: 76, Batch: 710, Training Loss: 0.03261899426579475, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:09, Epoch: 76, Batch: 720, Training Loss: 0.040397738292813304, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:10, Epoch: 76, Batch: 730, Training Loss: 0.028797836601734163, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:10, Epoch: 76, Batch: 740, Training Loss: 0.03464106470346451, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:11, Epoch: 76, Batch: 750, Training Loss: 0.038085267692804334, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:12, Epoch: 76, Batch: 760, Training Loss: 0.044807470962405206, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:13, Epoch: 76, Batch: 770, Training Loss: 0.047730501368641855, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:13, Epoch: 76, Batch: 780, Training Loss: 0.050944223254919055, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:14, Epoch: 76, Batch: 790, Training Loss: 0.03483852222561836, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:15, Epoch: 76, Batch: 800, Training Loss: 0.05867642648518086, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:16, Epoch: 76, Batch: 810, Training Loss: 0.03394059762358666, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:17, Epoch: 76, Batch: 820, Training Loss: 0.05762989297509193, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:17, Epoch: 76, Batch: 830, Training Loss: 0.04808840900659561, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:18, Epoch: 76, Batch: 840, Training Loss: 0.03561212792992592, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:19, Epoch: 76, Batch: 850, Training Loss: 0.042943736910820006, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:20, Epoch: 76, Batch: 860, Training Loss: 0.04551149345934391, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:21, Epoch: 76, Batch: 870, Training Loss: 0.03183507807552814, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:21, Epoch: 76, Batch: 880, Training Loss: 0.048527128621935846, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:22, Epoch: 76, Batch: 890, Training Loss: 0.03278357945382595, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:23, Epoch: 76, Batch: 900, Training Loss: 0.0562813151627779, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:23, Epoch: 76, Batch: 910, Training Loss: 0.044828998297452925, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:24, Epoch: 76, Batch: 920, Training Loss: 0.035993875935673714, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:25, Epoch: 76, Batch: 930, Training Loss: 0.036093243211507794, LR: 0.00010000000000000003
Epoch: 76, Validation Top 1 acc: 98.85394287109375
Epoch: 76, Validation Top 5 acc: 99.99166870117188
Epoch: 76, Validation Set Loss: 0.041388534009456635
Start training epoch 77
Time, 2019-01-01T18:45:53, Epoch: 77, Batch: 10, Training Loss: 0.06340535506606101, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:54, Epoch: 77, Batch: 20, Training Loss: 0.049227166920900345, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:55, Epoch: 77, Batch: 30, Training Loss: 0.05264788158237934, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:55, Epoch: 77, Batch: 40, Training Loss: 0.05094784237444401, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:56, Epoch: 77, Batch: 50, Training Loss: 0.03706178329885006, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:57, Epoch: 77, Batch: 60, Training Loss: 0.04160182885825634, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:58, Epoch: 77, Batch: 70, Training Loss: 0.06151557117700577, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:58, Epoch: 77, Batch: 80, Training Loss: 0.03155521750450134, LR: 0.00010000000000000003
Time, 2019-01-01T18:45:59, Epoch: 77, Batch: 90, Training Loss: 0.04233860895037651, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:00, Epoch: 77, Batch: 100, Training Loss: 0.03772740438580513, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:00, Epoch: 77, Batch: 110, Training Loss: 0.03055371791124344, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:01, Epoch: 77, Batch: 120, Training Loss: 0.04610614329576492, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:02, Epoch: 77, Batch: 130, Training Loss: 0.037227853760123254, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:03, Epoch: 77, Batch: 140, Training Loss: 0.031109368428587914, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:03, Epoch: 77, Batch: 150, Training Loss: 0.03503177538514137, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:04, Epoch: 77, Batch: 160, Training Loss: 0.04252995997667312, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:05, Epoch: 77, Batch: 170, Training Loss: 0.04135242849588394, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:05, Epoch: 77, Batch: 180, Training Loss: 0.03969411328434944, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:06, Epoch: 77, Batch: 190, Training Loss: 0.03196342401206494, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:07, Epoch: 77, Batch: 200, Training Loss: 0.03741342462599277, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:08, Epoch: 77, Batch: 210, Training Loss: 0.027793151885271074, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:08, Epoch: 77, Batch: 220, Training Loss: 0.03915867581963539, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:09, Epoch: 77, Batch: 230, Training Loss: 0.033732269704341886, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:10, Epoch: 77, Batch: 240, Training Loss: 0.06027549058198929, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:10, Epoch: 77, Batch: 250, Training Loss: 0.03623965829610824, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:11, Epoch: 77, Batch: 260, Training Loss: 0.03745359443128109, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:12, Epoch: 77, Batch: 270, Training Loss: 0.03717916347086429, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:13, Epoch: 77, Batch: 280, Training Loss: 0.03819731846451759, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:13, Epoch: 77, Batch: 290, Training Loss: 0.03850039876997471, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:14, Epoch: 77, Batch: 300, Training Loss: 0.04408159665763378, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:15, Epoch: 77, Batch: 310, Training Loss: 0.054953959211707114, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:15, Epoch: 77, Batch: 320, Training Loss: 0.04375041089951992, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:16, Epoch: 77, Batch: 330, Training Loss: 0.061004461348056795, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:17, Epoch: 77, Batch: 340, Training Loss: 0.03252956457436085, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:18, Epoch: 77, Batch: 350, Training Loss: 0.045608413964509965, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:18, Epoch: 77, Batch: 360, Training Loss: 0.048079436644911766, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:19, Epoch: 77, Batch: 370, Training Loss: 0.04258320294320583, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:20, Epoch: 77, Batch: 380, Training Loss: 0.05174432322382927, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:21, Epoch: 77, Batch: 390, Training Loss: 0.05252668261528015, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:21, Epoch: 77, Batch: 400, Training Loss: 0.041359687224030495, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:22, Epoch: 77, Batch: 410, Training Loss: 0.025977807864546777, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:23, Epoch: 77, Batch: 420, Training Loss: 0.043629137054085734, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:23, Epoch: 77, Batch: 430, Training Loss: 0.05400751382112503, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:24, Epoch: 77, Batch: 440, Training Loss: 0.045363498851656914, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:25, Epoch: 77, Batch: 450, Training Loss: 0.04544028416275978, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:26, Epoch: 77, Batch: 460, Training Loss: 0.04349483512341976, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:26, Epoch: 77, Batch: 470, Training Loss: 0.0438509039580822, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:27, Epoch: 77, Batch: 480, Training Loss: 0.02819601260125637, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:28, Epoch: 77, Batch: 490, Training Loss: 0.04524080865085125, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:28, Epoch: 77, Batch: 500, Training Loss: 0.027166446670889854, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:29, Epoch: 77, Batch: 510, Training Loss: 0.04272835068404675, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:30, Epoch: 77, Batch: 520, Training Loss: 0.05209662429988384, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:31, Epoch: 77, Batch: 530, Training Loss: 0.0356966819614172, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:31, Epoch: 77, Batch: 540, Training Loss: 0.04573003947734833, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:32, Epoch: 77, Batch: 550, Training Loss: 0.049601299315690996, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:33, Epoch: 77, Batch: 560, Training Loss: 0.04734474457800388, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:33, Epoch: 77, Batch: 570, Training Loss: 0.030011719092726707, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:34, Epoch: 77, Batch: 580, Training Loss: 0.047640809416770936, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:35, Epoch: 77, Batch: 590, Training Loss: 0.04042028486728668, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:36, Epoch: 77, Batch: 600, Training Loss: 0.03885713890194893, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:36, Epoch: 77, Batch: 610, Training Loss: 0.03390225172042847, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:37, Epoch: 77, Batch: 620, Training Loss: 0.03086574301123619, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:38, Epoch: 77, Batch: 630, Training Loss: 0.024375519901514053, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:38, Epoch: 77, Batch: 640, Training Loss: 0.04457584321498871, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:39, Epoch: 77, Batch: 650, Training Loss: 0.04087398760020733, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:40, Epoch: 77, Batch: 660, Training Loss: 0.0368084542453289, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:41, Epoch: 77, Batch: 670, Training Loss: 0.04328055754303932, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:41, Epoch: 77, Batch: 680, Training Loss: 0.02489922344684601, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:42, Epoch: 77, Batch: 690, Training Loss: 0.06514140740036964, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:43, Epoch: 77, Batch: 700, Training Loss: 0.053522444888949396, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:44, Epoch: 77, Batch: 710, Training Loss: 0.02120153419673443, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:44, Epoch: 77, Batch: 720, Training Loss: 0.049541813880205156, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:45, Epoch: 77, Batch: 730, Training Loss: 0.04137538932263851, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:46, Epoch: 77, Batch: 740, Training Loss: 0.050072720646858214, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:46, Epoch: 77, Batch: 750, Training Loss: 0.03865688368678093, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:47, Epoch: 77, Batch: 760, Training Loss: 0.0462293341755867, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:48, Epoch: 77, Batch: 770, Training Loss: 0.04564626961946487, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:49, Epoch: 77, Batch: 780, Training Loss: 0.04891906008124351, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:49, Epoch: 77, Batch: 790, Training Loss: 0.027527450025081633, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:50, Epoch: 77, Batch: 800, Training Loss: 0.03163385018706322, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:51, Epoch: 77, Batch: 810, Training Loss: 0.05544357635080814, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:51, Epoch: 77, Batch: 820, Training Loss: 0.03739336170256138, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:52, Epoch: 77, Batch: 830, Training Loss: 0.04699905291199684, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:53, Epoch: 77, Batch: 840, Training Loss: 0.04745093137025833, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:54, Epoch: 77, Batch: 850, Training Loss: 0.035494426265358925, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:54, Epoch: 77, Batch: 860, Training Loss: 0.027300604432821274, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:55, Epoch: 77, Batch: 870, Training Loss: 0.04404811635613441, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:56, Epoch: 77, Batch: 880, Training Loss: 0.026778633520007134, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:56, Epoch: 77, Batch: 890, Training Loss: 0.03737360015511513, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:57, Epoch: 77, Batch: 900, Training Loss: 0.06190182641148567, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:58, Epoch: 77, Batch: 910, Training Loss: 0.039217375591397284, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:59, Epoch: 77, Batch: 920, Training Loss: 0.042047394439578056, LR: 0.00010000000000000003
Time, 2019-01-01T18:46:59, Epoch: 77, Batch: 930, Training Loss: 0.03285182863473892, LR: 0.00010000000000000003
Epoch: 77, Validation Top 1 acc: 98.86061096191406
Epoch: 77, Validation Top 5 acc: 99.99166870117188
Epoch: 77, Validation Set Loss: 0.04138585552573204
Start training epoch 78
Time, 2019-01-01T18:47:27, Epoch: 78, Batch: 10, Training Loss: 0.035065339878201485, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:28, Epoch: 78, Batch: 20, Training Loss: 0.04411615431308746, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:28, Epoch: 78, Batch: 30, Training Loss: 0.043724522739648816, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:29, Epoch: 78, Batch: 40, Training Loss: 0.05308690592646599, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:30, Epoch: 78, Batch: 50, Training Loss: 0.043855854868888856, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:31, Epoch: 78, Batch: 60, Training Loss: 0.04297645129263401, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:32, Epoch: 78, Batch: 70, Training Loss: 0.05850749537348747, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:32, Epoch: 78, Batch: 80, Training Loss: 0.05237489528954029, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:33, Epoch: 78, Batch: 90, Training Loss: 0.030583006888628007, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:34, Epoch: 78, Batch: 100, Training Loss: 0.03129943162202835, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:35, Epoch: 78, Batch: 110, Training Loss: 0.04165619798004627, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:35, Epoch: 78, Batch: 120, Training Loss: 0.033829008042812345, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:36, Epoch: 78, Batch: 130, Training Loss: 0.055055049061775205, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:37, Epoch: 78, Batch: 140, Training Loss: 0.03897155039012432, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:38, Epoch: 78, Batch: 150, Training Loss: 0.05858737304806709, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:38, Epoch: 78, Batch: 160, Training Loss: 0.043004120141267775, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:39, Epoch: 78, Batch: 170, Training Loss: 0.02483951337635517, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:40, Epoch: 78, Batch: 180, Training Loss: 0.04078806415200233, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:41, Epoch: 78, Batch: 190, Training Loss: 0.026606756448745727, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:41, Epoch: 78, Batch: 200, Training Loss: 0.05082472786307335, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:42, Epoch: 78, Batch: 210, Training Loss: 0.04001951403915882, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:43, Epoch: 78, Batch: 220, Training Loss: 0.04097161293029785, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:44, Epoch: 78, Batch: 230, Training Loss: 0.0386914886534214, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:44, Epoch: 78, Batch: 240, Training Loss: 0.04183908589184284, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:45, Epoch: 78, Batch: 250, Training Loss: 0.03665803223848343, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:46, Epoch: 78, Batch: 260, Training Loss: 0.0531748291105032, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:47, Epoch: 78, Batch: 270, Training Loss: 0.03662794455885887, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:47, Epoch: 78, Batch: 280, Training Loss: 0.030642247945070266, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:48, Epoch: 78, Batch: 290, Training Loss: 0.03949522599577904, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:49, Epoch: 78, Batch: 300, Training Loss: 0.034398164600133896, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:49, Epoch: 78, Batch: 310, Training Loss: 0.05191880501806736, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:50, Epoch: 78, Batch: 320, Training Loss: 0.03344647139310837, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:51, Epoch: 78, Batch: 330, Training Loss: 0.05523090474307537, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:52, Epoch: 78, Batch: 340, Training Loss: 0.05507722161710262, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:52, Epoch: 78, Batch: 350, Training Loss: 0.05832053236663341, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:53, Epoch: 78, Batch: 360, Training Loss: 0.032144928351044655, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:54, Epoch: 78, Batch: 370, Training Loss: 0.025047482550144197, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:54, Epoch: 78, Batch: 380, Training Loss: 0.03617008253931999, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:55, Epoch: 78, Batch: 390, Training Loss: 0.026143377646803856, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:56, Epoch: 78, Batch: 400, Training Loss: 0.03342508971691131, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:57, Epoch: 78, Batch: 410, Training Loss: 0.04449728392064571, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:57, Epoch: 78, Batch: 420, Training Loss: 0.03287920653820038, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:58, Epoch: 78, Batch: 430, Training Loss: 0.03342837989330292, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:59, Epoch: 78, Batch: 440, Training Loss: 0.045031306147575376, LR: 0.00010000000000000003
Time, 2019-01-01T18:47:59, Epoch: 78, Batch: 450, Training Loss: 0.05815822742879391, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:00, Epoch: 78, Batch: 460, Training Loss: 0.045843901485204695, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:01, Epoch: 78, Batch: 470, Training Loss: 0.046315823122859, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:02, Epoch: 78, Batch: 480, Training Loss: 0.043385306000709535, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:02, Epoch: 78, Batch: 490, Training Loss: 0.037848057597875594, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:03, Epoch: 78, Batch: 500, Training Loss: 0.04833726510405541, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:04, Epoch: 78, Batch: 510, Training Loss: 0.027659563720226286, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:04, Epoch: 78, Batch: 520, Training Loss: 0.041846543177962305, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:05, Epoch: 78, Batch: 530, Training Loss: 0.04265514947474003, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:06, Epoch: 78, Batch: 540, Training Loss: 0.03598848171532154, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:07, Epoch: 78, Batch: 550, Training Loss: 0.07806478776037692, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:07, Epoch: 78, Batch: 560, Training Loss: 0.04138821698725224, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:08, Epoch: 78, Batch: 570, Training Loss: 0.046919552609324455, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:09, Epoch: 78, Batch: 580, Training Loss: 0.037564367800951, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:10, Epoch: 78, Batch: 590, Training Loss: 0.03246763274073601, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:10, Epoch: 78, Batch: 600, Training Loss: 0.055507612973451616, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:11, Epoch: 78, Batch: 610, Training Loss: 0.03777020536363125, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:12, Epoch: 78, Batch: 620, Training Loss: 0.02944239489734173, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:12, Epoch: 78, Batch: 630, Training Loss: 0.04640818387269974, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:13, Epoch: 78, Batch: 640, Training Loss: 0.04197332002222538, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:14, Epoch: 78, Batch: 650, Training Loss: 0.04005365073680878, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:15, Epoch: 78, Batch: 660, Training Loss: 0.031771072372794154, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:15, Epoch: 78, Batch: 670, Training Loss: 0.04620944857597351, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:16, Epoch: 78, Batch: 680, Training Loss: 0.05688414461910725, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:17, Epoch: 78, Batch: 690, Training Loss: 0.04526685178279877, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:17, Epoch: 78, Batch: 700, Training Loss: 0.042924250662326816, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:18, Epoch: 78, Batch: 710, Training Loss: 0.04808064512908459, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:19, Epoch: 78, Batch: 720, Training Loss: 0.037617054954171184, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:20, Epoch: 78, Batch: 730, Training Loss: 0.036883027851581575, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:20, Epoch: 78, Batch: 740, Training Loss: 0.05188005119562149, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:21, Epoch: 78, Batch: 750, Training Loss: 0.037377935275435446, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:22, Epoch: 78, Batch: 760, Training Loss: 0.04335070215165615, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:22, Epoch: 78, Batch: 770, Training Loss: 0.043046417832374576, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:23, Epoch: 78, Batch: 780, Training Loss: 0.045430293679237364, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:24, Epoch: 78, Batch: 790, Training Loss: 0.038025690242648125, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:25, Epoch: 78, Batch: 800, Training Loss: 0.041348497942090036, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:25, Epoch: 78, Batch: 810, Training Loss: 0.04254983142018318, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:26, Epoch: 78, Batch: 820, Training Loss: 0.05658521950244903, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:27, Epoch: 78, Batch: 830, Training Loss: 0.04620933048427105, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:27, Epoch: 78, Batch: 840, Training Loss: 0.03151233308017254, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:28, Epoch: 78, Batch: 850, Training Loss: 0.02715989835560322, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:29, Epoch: 78, Batch: 860, Training Loss: 0.04063923135399818, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:30, Epoch: 78, Batch: 870, Training Loss: 0.03278749324381351, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:30, Epoch: 78, Batch: 880, Training Loss: 0.030161142349243164, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:31, Epoch: 78, Batch: 890, Training Loss: 0.0441733367741108, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:32, Epoch: 78, Batch: 900, Training Loss: 0.0325168639421463, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:32, Epoch: 78, Batch: 910, Training Loss: 0.04803075529634952, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:33, Epoch: 78, Batch: 920, Training Loss: 0.02891007959842682, LR: 0.00010000000000000003
Time, 2019-01-01T18:48:34, Epoch: 78, Batch: 930, Training Loss: 0.029552313312888146, LR: 0.00010000000000000003
Epoch: 78, Validation Top 1 acc: 98.85061645507812
Epoch: 78, Validation Top 5 acc: 99.99000549316406
Epoch: 78, Validation Set Loss: 0.04134119674563408
Start training epoch 79
Time, 2019-01-01T18:49:02, Epoch: 79, Batch: 10, Training Loss: 0.0457308903336525, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:02, Epoch: 79, Batch: 20, Training Loss: 0.04633718505501747, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:03, Epoch: 79, Batch: 30, Training Loss: 0.03405558243393898, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:04, Epoch: 79, Batch: 40, Training Loss: 0.040816647559404375, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:05, Epoch: 79, Batch: 50, Training Loss: 0.040141372755169866, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:05, Epoch: 79, Batch: 60, Training Loss: 0.04700322151184082, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:06, Epoch: 79, Batch: 70, Training Loss: 0.027595588564872743, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:07, Epoch: 79, Batch: 80, Training Loss: 0.04139379076659679, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:07, Epoch: 79, Batch: 90, Training Loss: 0.06417794898152351, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:08, Epoch: 79, Batch: 100, Training Loss: 0.038988128677010535, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:09, Epoch: 79, Batch: 110, Training Loss: 0.04655447527766228, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:10, Epoch: 79, Batch: 120, Training Loss: 0.058827827125787734, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:10, Epoch: 79, Batch: 130, Training Loss: 0.04474678263068199, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:11, Epoch: 79, Batch: 140, Training Loss: 0.03225319236516953, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:12, Epoch: 79, Batch: 150, Training Loss: 0.04679032228887081, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:12, Epoch: 79, Batch: 160, Training Loss: 0.034918203949928284, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:13, Epoch: 79, Batch: 170, Training Loss: 0.030550263077020644, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:14, Epoch: 79, Batch: 180, Training Loss: 0.03227199763059616, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:15, Epoch: 79, Batch: 190, Training Loss: 0.037298211827874184, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:15, Epoch: 79, Batch: 200, Training Loss: 0.05925705209374428, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:16, Epoch: 79, Batch: 210, Training Loss: 0.04276319555938244, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:17, Epoch: 79, Batch: 220, Training Loss: 0.05152105800807476, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:18, Epoch: 79, Batch: 230, Training Loss: 0.02409488782286644, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:18, Epoch: 79, Batch: 240, Training Loss: 0.04299174025654793, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:19, Epoch: 79, Batch: 250, Training Loss: 0.059352767467498777, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:20, Epoch: 79, Batch: 260, Training Loss: 0.03927049823105335, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:20, Epoch: 79, Batch: 270, Training Loss: 0.03390936143696308, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:21, Epoch: 79, Batch: 280, Training Loss: 0.018598014116287233, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:22, Epoch: 79, Batch: 290, Training Loss: 0.03406814932823181, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:23, Epoch: 79, Batch: 300, Training Loss: 0.04180320017039776, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:23, Epoch: 79, Batch: 310, Training Loss: 0.023287778720259666, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:24, Epoch: 79, Batch: 320, Training Loss: 0.04763592779636383, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:25, Epoch: 79, Batch: 330, Training Loss: 0.03326299637556076, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:25, Epoch: 79, Batch: 340, Training Loss: 0.04323487728834152, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:26, Epoch: 79, Batch: 350, Training Loss: 0.03106712959706783, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:27, Epoch: 79, Batch: 360, Training Loss: 0.04020115099847317, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:28, Epoch: 79, Batch: 370, Training Loss: 0.03950345478951931, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:28, Epoch: 79, Batch: 380, Training Loss: 0.04481024444103241, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:29, Epoch: 79, Batch: 390, Training Loss: 0.03755649775266647, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:30, Epoch: 79, Batch: 400, Training Loss: 0.04568798765540123, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:31, Epoch: 79, Batch: 410, Training Loss: 0.04328352026641369, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:31, Epoch: 79, Batch: 420, Training Loss: 0.054975463822484014, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:32, Epoch: 79, Batch: 430, Training Loss: 0.04741935469210148, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:33, Epoch: 79, Batch: 440, Training Loss: 0.03743482381105423, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:33, Epoch: 79, Batch: 450, Training Loss: 0.02576874941587448, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:34, Epoch: 79, Batch: 460, Training Loss: 0.04372101053595543, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:35, Epoch: 79, Batch: 470, Training Loss: 0.039035641402006147, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:36, Epoch: 79, Batch: 480, Training Loss: 0.03459595702588558, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:36, Epoch: 79, Batch: 490, Training Loss: 0.04742278046905994, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:37, Epoch: 79, Batch: 500, Training Loss: 0.05914140120148659, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:38, Epoch: 79, Batch: 510, Training Loss: 0.03799097202718258, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:38, Epoch: 79, Batch: 520, Training Loss: 0.03749435544013977, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:39, Epoch: 79, Batch: 530, Training Loss: 0.03889577239751816, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:40, Epoch: 79, Batch: 540, Training Loss: 0.03969186022877693, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:41, Epoch: 79, Batch: 550, Training Loss: 0.030956193059682845, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:41, Epoch: 79, Batch: 560, Training Loss: 0.044857665151357654, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:42, Epoch: 79, Batch: 570, Training Loss: 0.04950187988579273, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:43, Epoch: 79, Batch: 580, Training Loss: 0.06128381155431271, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:44, Epoch: 79, Batch: 590, Training Loss: 0.0418057020753622, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:44, Epoch: 79, Batch: 600, Training Loss: 0.04068179540336132, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:45, Epoch: 79, Batch: 610, Training Loss: 0.03987281434237957, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:46, Epoch: 79, Batch: 620, Training Loss: 0.050053594261407854, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:46, Epoch: 79, Batch: 630, Training Loss: 0.056373465806245804, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:47, Epoch: 79, Batch: 640, Training Loss: 0.04714930541813374, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:48, Epoch: 79, Batch: 650, Training Loss: 0.04553915672004223, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:49, Epoch: 79, Batch: 660, Training Loss: 0.04992361329495907, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:49, Epoch: 79, Batch: 670, Training Loss: 0.03549923449754715, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:50, Epoch: 79, Batch: 680, Training Loss: 0.03916689082980156, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:51, Epoch: 79, Batch: 690, Training Loss: 0.03819823749363423, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:51, Epoch: 79, Batch: 700, Training Loss: 0.03886883147060871, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:52, Epoch: 79, Batch: 710, Training Loss: 0.06208515726029873, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:53, Epoch: 79, Batch: 720, Training Loss: 0.03483269363641739, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:54, Epoch: 79, Batch: 730, Training Loss: 0.024406559020280837, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:54, Epoch: 79, Batch: 740, Training Loss: 0.059122083336114885, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:55, Epoch: 79, Batch: 750, Training Loss: 0.04664416573941708, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:56, Epoch: 79, Batch: 760, Training Loss: 0.03832478187978268, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:56, Epoch: 79, Batch: 770, Training Loss: 0.03271220102906227, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:57, Epoch: 79, Batch: 780, Training Loss: 0.04694023691117764, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:58, Epoch: 79, Batch: 790, Training Loss: 0.0357303187251091, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:59, Epoch: 79, Batch: 800, Training Loss: 0.03661275468766689, LR: 0.00010000000000000003
Time, 2019-01-01T18:49:59, Epoch: 79, Batch: 810, Training Loss: 0.051015281677246095, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:00, Epoch: 79, Batch: 820, Training Loss: 0.032820770516991615, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:01, Epoch: 79, Batch: 830, Training Loss: 0.03452448919415474, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:01, Epoch: 79, Batch: 840, Training Loss: 0.03984976522624493, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:02, Epoch: 79, Batch: 850, Training Loss: 0.034045232087373735, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:03, Epoch: 79, Batch: 860, Training Loss: 0.038538287952542306, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:04, Epoch: 79, Batch: 870, Training Loss: 0.05199732482433319, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:04, Epoch: 79, Batch: 880, Training Loss: 0.055098379775881764, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:05, Epoch: 79, Batch: 890, Training Loss: 0.02643570527434349, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:06, Epoch: 79, Batch: 900, Training Loss: 0.028104639053344725, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:07, Epoch: 79, Batch: 910, Training Loss: 0.04115427397191525, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:07, Epoch: 79, Batch: 920, Training Loss: 0.030807244777679443, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:08, Epoch: 79, Batch: 930, Training Loss: 0.06341396160423755, LR: 0.00010000000000000003
Epoch: 79, Validation Top 1 acc: 98.87060546875
Epoch: 79, Validation Top 5 acc: 99.99166870117188
Epoch: 79, Validation Set Loss: 0.04135868698358536
Start training epoch 80
Time, 2019-01-01T18:50:36, Epoch: 80, Batch: 10, Training Loss: 0.04955966956913471, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:37, Epoch: 80, Batch: 20, Training Loss: 0.03988533839583397, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:38, Epoch: 80, Batch: 30, Training Loss: 0.03910537362098694, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:38, Epoch: 80, Batch: 40, Training Loss: 0.04500689655542374, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:39, Epoch: 80, Batch: 50, Training Loss: 0.04784014709293842, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:40, Epoch: 80, Batch: 60, Training Loss: 0.03426825292408466, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:40, Epoch: 80, Batch: 70, Training Loss: 0.04056900031864643, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:41, Epoch: 80, Batch: 80, Training Loss: 0.04084252193570137, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:42, Epoch: 80, Batch: 90, Training Loss: 0.043981530144810675, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:43, Epoch: 80, Batch: 100, Training Loss: 0.03892442248761654, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:43, Epoch: 80, Batch: 110, Training Loss: 0.04065568372607231, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:44, Epoch: 80, Batch: 120, Training Loss: 0.045762797072529796, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:45, Epoch: 80, Batch: 130, Training Loss: 0.06357190161943435, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:46, Epoch: 80, Batch: 140, Training Loss: 0.03228702321648598, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:46, Epoch: 80, Batch: 150, Training Loss: 0.03166128844022751, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:47, Epoch: 80, Batch: 160, Training Loss: 0.03934234343469143, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:48, Epoch: 80, Batch: 170, Training Loss: 0.05845508761703968, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:49, Epoch: 80, Batch: 180, Training Loss: 0.06135189272463322, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:49, Epoch: 80, Batch: 190, Training Loss: 0.03467499651014805, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:50, Epoch: 80, Batch: 200, Training Loss: 0.048503682017326355, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:51, Epoch: 80, Batch: 210, Training Loss: 0.028704895824193954, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:52, Epoch: 80, Batch: 220, Training Loss: 0.031372550129890445, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:52, Epoch: 80, Batch: 230, Training Loss: 0.0424794539809227, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:53, Epoch: 80, Batch: 240, Training Loss: 0.04689517319202423, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:54, Epoch: 80, Batch: 250, Training Loss: 0.03312471583485603, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:54, Epoch: 80, Batch: 260, Training Loss: 0.051779669150710105, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:55, Epoch: 80, Batch: 270, Training Loss: 0.03222092539072037, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:56, Epoch: 80, Batch: 280, Training Loss: 0.04371159821748734, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:57, Epoch: 80, Batch: 290, Training Loss: 0.05685518868267536, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:57, Epoch: 80, Batch: 300, Training Loss: 0.03311863951385021, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:58, Epoch: 80, Batch: 310, Training Loss: 0.05001010522246361, LR: 0.00010000000000000003
Time, 2019-01-01T18:50:59, Epoch: 80, Batch: 320, Training Loss: 0.04241039678454399, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:00, Epoch: 80, Batch: 330, Training Loss: 0.04374627880752087, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:00, Epoch: 80, Batch: 340, Training Loss: 0.030417471006512643, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:01, Epoch: 80, Batch: 350, Training Loss: 0.0377179741859436, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:02, Epoch: 80, Batch: 360, Training Loss: 0.050510569289326665, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:02, Epoch: 80, Batch: 370, Training Loss: 0.03512217067182064, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:03, Epoch: 80, Batch: 380, Training Loss: 0.04084557257592678, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:04, Epoch: 80, Batch: 390, Training Loss: 0.028422356024384497, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:05, Epoch: 80, Batch: 400, Training Loss: 0.030433812737464906, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:05, Epoch: 80, Batch: 410, Training Loss: 0.05623627118766308, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:06, Epoch: 80, Batch: 420, Training Loss: 0.038387080281972887, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:07, Epoch: 80, Batch: 430, Training Loss: 0.03361895829439163, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:07, Epoch: 80, Batch: 440, Training Loss: 0.05183174200356007, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:08, Epoch: 80, Batch: 450, Training Loss: 0.030997972190380096, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:09, Epoch: 80, Batch: 460, Training Loss: 0.03504679501056671, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:10, Epoch: 80, Batch: 470, Training Loss: 0.06540236733853817, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:10, Epoch: 80, Batch: 480, Training Loss: 0.04177493453025818, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:11, Epoch: 80, Batch: 490, Training Loss: 0.03261063508689403, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:12, Epoch: 80, Batch: 500, Training Loss: 0.04078143164515495, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:13, Epoch: 80, Batch: 510, Training Loss: 0.04731479249894619, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:13, Epoch: 80, Batch: 520, Training Loss: 0.03179541900753975, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:14, Epoch: 80, Batch: 530, Training Loss: 0.04956640303134918, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:15, Epoch: 80, Batch: 540, Training Loss: 0.021560458838939665, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:15, Epoch: 80, Batch: 550, Training Loss: 0.03971374705433846, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:16, Epoch: 80, Batch: 560, Training Loss: 0.03957168683409691, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:17, Epoch: 80, Batch: 570, Training Loss: 0.035924415662884714, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:18, Epoch: 80, Batch: 580, Training Loss: 0.046224376559257506, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:18, Epoch: 80, Batch: 590, Training Loss: 0.03382732383906841, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:19, Epoch: 80, Batch: 600, Training Loss: 0.0478132963180542, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:20, Epoch: 80, Batch: 610, Training Loss: 0.0705711580812931, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:20, Epoch: 80, Batch: 620, Training Loss: 0.03061050847172737, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:21, Epoch: 80, Batch: 630, Training Loss: 0.05043211355805397, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:22, Epoch: 80, Batch: 640, Training Loss: 0.0419360987842083, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:23, Epoch: 80, Batch: 650, Training Loss: 0.06038554050028324, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:24, Epoch: 80, Batch: 660, Training Loss: 0.0351948969066143, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:25, Epoch: 80, Batch: 670, Training Loss: 0.04173859655857086, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:25, Epoch: 80, Batch: 680, Training Loss: 0.0481794573366642, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:26, Epoch: 80, Batch: 690, Training Loss: 0.026795335114002228, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:27, Epoch: 80, Batch: 700, Training Loss: 0.04863852337002754, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:28, Epoch: 80, Batch: 710, Training Loss: 0.03661360628902912, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:28, Epoch: 80, Batch: 720, Training Loss: 0.03467379957437515, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:29, Epoch: 80, Batch: 730, Training Loss: 0.03221695646643639, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:30, Epoch: 80, Batch: 740, Training Loss: 0.04623455815017223, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:31, Epoch: 80, Batch: 750, Training Loss: 0.03238825649023056, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:31, Epoch: 80, Batch: 760, Training Loss: 0.030335532128810884, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:32, Epoch: 80, Batch: 770, Training Loss: 0.038608748465776443, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:33, Epoch: 80, Batch: 780, Training Loss: 0.0327177632600069, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:33, Epoch: 80, Batch: 790, Training Loss: 0.0626008678227663, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:34, Epoch: 80, Batch: 800, Training Loss: 0.040316008403897284, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:35, Epoch: 80, Batch: 810, Training Loss: 0.027817334234714507, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:36, Epoch: 80, Batch: 820, Training Loss: 0.054729709029197694, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:36, Epoch: 80, Batch: 830, Training Loss: 0.05211889892816544, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:37, Epoch: 80, Batch: 840, Training Loss: 0.028713596612215044, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:38, Epoch: 80, Batch: 850, Training Loss: 0.03588310740888119, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:39, Epoch: 80, Batch: 860, Training Loss: 0.04261164516210556, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:39, Epoch: 80, Batch: 870, Training Loss: 0.04263053461909294, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:40, Epoch: 80, Batch: 880, Training Loss: 0.03981791436672211, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:41, Epoch: 80, Batch: 890, Training Loss: 0.03060252293944359, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:41, Epoch: 80, Batch: 900, Training Loss: 0.04945041611790657, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:42, Epoch: 80, Batch: 910, Training Loss: 0.0386339046061039, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:43, Epoch: 80, Batch: 920, Training Loss: 0.04912355057895183, LR: 0.00010000000000000003
Time, 2019-01-01T18:51:44, Epoch: 80, Batch: 930, Training Loss: 0.04669888727366924, LR: 0.00010000000000000003
Epoch: 80, Validation Top 1 acc: 98.87226867675781
Epoch: 80, Validation Top 5 acc: 99.99166870117188
Epoch: 80, Validation Set Loss: 0.04139310494065285
Start training epoch 81
Time, 2019-01-01T18:52:11, Epoch: 81, Batch: 10, Training Loss: 0.0487191092222929, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:12, Epoch: 81, Batch: 20, Training Loss: 0.04105338156223297, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:13, Epoch: 81, Batch: 30, Training Loss: 0.035329056158661845, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:13, Epoch: 81, Batch: 40, Training Loss: 0.025885892659425737, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:14, Epoch: 81, Batch: 50, Training Loss: 0.038411252573132514, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:15, Epoch: 81, Batch: 60, Training Loss: 0.04103809632360935, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:16, Epoch: 81, Batch: 70, Training Loss: 0.0326244942843914, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:16, Epoch: 81, Batch: 80, Training Loss: 0.03713785745203495, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:17, Epoch: 81, Batch: 90, Training Loss: 0.06011925339698791, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:18, Epoch: 81, Batch: 100, Training Loss: 0.038828660175204276, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:19, Epoch: 81, Batch: 110, Training Loss: 0.04824037179350853, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:19, Epoch: 81, Batch: 120, Training Loss: 0.04051961936056614, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:20, Epoch: 81, Batch: 130, Training Loss: 0.04468972235918045, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:21, Epoch: 81, Batch: 140, Training Loss: 0.035629420354962346, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:21, Epoch: 81, Batch: 150, Training Loss: 0.035026013106107715, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:22, Epoch: 81, Batch: 160, Training Loss: 0.050421278551220895, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:23, Epoch: 81, Batch: 170, Training Loss: 0.04245645590126514, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:24, Epoch: 81, Batch: 180, Training Loss: 0.06179511249065399, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:24, Epoch: 81, Batch: 190, Training Loss: 0.03298076875507831, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:25, Epoch: 81, Batch: 200, Training Loss: 0.04008045196533203, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:26, Epoch: 81, Batch: 210, Training Loss: 0.0350998766720295, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:26, Epoch: 81, Batch: 220, Training Loss: 0.043153053522109984, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:27, Epoch: 81, Batch: 230, Training Loss: 0.06127675510942936, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:28, Epoch: 81, Batch: 240, Training Loss: 0.03975828662514687, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:29, Epoch: 81, Batch: 250, Training Loss: 0.0461974885314703, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:29, Epoch: 81, Batch: 260, Training Loss: 0.022312643006443977, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:30, Epoch: 81, Batch: 270, Training Loss: 0.034856153279542924, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:31, Epoch: 81, Batch: 280, Training Loss: 0.05786576643586159, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:31, Epoch: 81, Batch: 290, Training Loss: 0.017504147440195083, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:32, Epoch: 81, Batch: 300, Training Loss: 0.03555555418133736, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:33, Epoch: 81, Batch: 310, Training Loss: 0.044249352440238, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:34, Epoch: 81, Batch: 320, Training Loss: 0.029251930862665178, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:34, Epoch: 81, Batch: 330, Training Loss: 0.042950625717639926, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:35, Epoch: 81, Batch: 340, Training Loss: 0.041970884799957274, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:36, Epoch: 81, Batch: 350, Training Loss: 0.03993309289216995, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:36, Epoch: 81, Batch: 360, Training Loss: 0.04229716546833515, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:37, Epoch: 81, Batch: 370, Training Loss: 0.04545290768146515, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:38, Epoch: 81, Batch: 380, Training Loss: 0.04807341173291206, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:39, Epoch: 81, Batch: 390, Training Loss: 0.04031323492527008, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:39, Epoch: 81, Batch: 400, Training Loss: 0.029495686665177344, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:40, Epoch: 81, Batch: 410, Training Loss: 0.043315823376178744, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:41, Epoch: 81, Batch: 420, Training Loss: 0.044562391191720965, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:42, Epoch: 81, Batch: 430, Training Loss: 0.060334577783942224, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:42, Epoch: 81, Batch: 440, Training Loss: 0.040023474395275115, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:43, Epoch: 81, Batch: 450, Training Loss: 0.044623814150691034, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:44, Epoch: 81, Batch: 460, Training Loss: 0.04766591340303421, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:44, Epoch: 81, Batch: 470, Training Loss: 0.03650915026664734, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:45, Epoch: 81, Batch: 480, Training Loss: 0.035703206062316896, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:46, Epoch: 81, Batch: 490, Training Loss: 0.030961259827017786, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:47, Epoch: 81, Batch: 500, Training Loss: 0.02605154290795326, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:47, Epoch: 81, Batch: 510, Training Loss: 0.04505511112511158, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:48, Epoch: 81, Batch: 520, Training Loss: 0.03276591971516609, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:49, Epoch: 81, Batch: 530, Training Loss: 0.031955789774656296, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:49, Epoch: 81, Batch: 540, Training Loss: 0.03741910792887211, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:50, Epoch: 81, Batch: 550, Training Loss: 0.03719778284430504, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:51, Epoch: 81, Batch: 560, Training Loss: 0.04147575497627258, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:52, Epoch: 81, Batch: 570, Training Loss: 0.040553174540400506, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:52, Epoch: 81, Batch: 580, Training Loss: 0.046897686272859576, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:53, Epoch: 81, Batch: 590, Training Loss: 0.045211580023169516, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:54, Epoch: 81, Batch: 600, Training Loss: 0.03993939980864525, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:54, Epoch: 81, Batch: 610, Training Loss: 0.04163190387189388, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:55, Epoch: 81, Batch: 620, Training Loss: 0.03815179243683815, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:56, Epoch: 81, Batch: 630, Training Loss: 0.035237687081098555, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:57, Epoch: 81, Batch: 640, Training Loss: 0.04982892759144306, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:57, Epoch: 81, Batch: 650, Training Loss: 0.031096887588500977, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:58, Epoch: 81, Batch: 660, Training Loss: 0.03955204710364342, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:59, Epoch: 81, Batch: 670, Training Loss: 0.05544345900416374, LR: 0.00010000000000000003
Time, 2019-01-01T18:52:59, Epoch: 81, Batch: 680, Training Loss: 0.03451165072619915, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:00, Epoch: 81, Batch: 690, Training Loss: 0.045410298928618434, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:01, Epoch: 81, Batch: 700, Training Loss: 0.04048959836363793, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:02, Epoch: 81, Batch: 710, Training Loss: 0.03376760557293892, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:02, Epoch: 81, Batch: 720, Training Loss: 0.030557937547564508, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:03, Epoch: 81, Batch: 730, Training Loss: 0.04984987713396549, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:04, Epoch: 81, Batch: 740, Training Loss: 0.04251728802919388, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:04, Epoch: 81, Batch: 750, Training Loss: 0.0382258303463459, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:05, Epoch: 81, Batch: 760, Training Loss: 0.03144360780715942, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:06, Epoch: 81, Batch: 770, Training Loss: 0.046382748335599897, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:07, Epoch: 81, Batch: 780, Training Loss: 0.02795216143131256, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:07, Epoch: 81, Batch: 790, Training Loss: 0.059657220542430875, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:08, Epoch: 81, Batch: 800, Training Loss: 0.06280210539698601, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:09, Epoch: 81, Batch: 810, Training Loss: 0.038756522536277774, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:09, Epoch: 81, Batch: 820, Training Loss: 0.04063237532973289, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:10, Epoch: 81, Batch: 830, Training Loss: 0.04259241446852684, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:11, Epoch: 81, Batch: 840, Training Loss: 0.05580938011407852, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:12, Epoch: 81, Batch: 850, Training Loss: 0.05725760757923126, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:12, Epoch: 81, Batch: 860, Training Loss: 0.04687731936573982, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:13, Epoch: 81, Batch: 870, Training Loss: 0.05139530524611473, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:14, Epoch: 81, Batch: 880, Training Loss: 0.04240750931203365, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:14, Epoch: 81, Batch: 890, Training Loss: 0.044476979225873944, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:15, Epoch: 81, Batch: 900, Training Loss: 0.04104132354259491, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:16, Epoch: 81, Batch: 910, Training Loss: 0.05055280812084675, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:17, Epoch: 81, Batch: 920, Training Loss: 0.044991873949766156, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:17, Epoch: 81, Batch: 930, Training Loss: 0.03563107326626778, LR: 0.00010000000000000003
Epoch: 81, Validation Top 1 acc: 98.86393737792969
Epoch: 81, Validation Top 5 acc: 99.99000549316406
Epoch: 81, Validation Set Loss: 0.04130711033940315
Start training epoch 82
Time, 2019-01-01T18:53:45, Epoch: 82, Batch: 10, Training Loss: 0.047868264466524126, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:46, Epoch: 82, Batch: 20, Training Loss: 0.02811955362558365, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:47, Epoch: 82, Batch: 30, Training Loss: 0.054209979623556136, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:47, Epoch: 82, Batch: 40, Training Loss: 0.047263885661959645, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:48, Epoch: 82, Batch: 50, Training Loss: 0.03907034918665886, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:49, Epoch: 82, Batch: 60, Training Loss: 0.03629589602351189, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:49, Epoch: 82, Batch: 70, Training Loss: 0.04209751524031162, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:50, Epoch: 82, Batch: 80, Training Loss: 0.03204139024019241, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:51, Epoch: 82, Batch: 90, Training Loss: 0.029619715735316278, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:52, Epoch: 82, Batch: 100, Training Loss: 0.03344865776598453, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:52, Epoch: 82, Batch: 110, Training Loss: 0.03466375358402729, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:53, Epoch: 82, Batch: 120, Training Loss: 0.04217638187110424, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:54, Epoch: 82, Batch: 130, Training Loss: 0.03960160501301289, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:55, Epoch: 82, Batch: 140, Training Loss: 0.04326233379542828, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:55, Epoch: 82, Batch: 150, Training Loss: 0.04552544094622135, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:56, Epoch: 82, Batch: 160, Training Loss: 0.039095989614725116, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:57, Epoch: 82, Batch: 170, Training Loss: 0.035848488286137584, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:57, Epoch: 82, Batch: 180, Training Loss: 0.04259681962430477, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:58, Epoch: 82, Batch: 190, Training Loss: 0.03411708325147629, LR: 0.00010000000000000003
Time, 2019-01-01T18:53:59, Epoch: 82, Batch: 200, Training Loss: 0.041773320734500886, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:00, Epoch: 82, Batch: 210, Training Loss: 0.02824891433119774, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:00, Epoch: 82, Batch: 220, Training Loss: 0.024562250077724456, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:01, Epoch: 82, Batch: 230, Training Loss: 0.02978723794221878, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:02, Epoch: 82, Batch: 240, Training Loss: 0.04278437346220017, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:02, Epoch: 82, Batch: 250, Training Loss: 0.04927611462771893, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:03, Epoch: 82, Batch: 260, Training Loss: 0.06289115883409976, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:04, Epoch: 82, Batch: 270, Training Loss: 0.035807186365127565, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:05, Epoch: 82, Batch: 280, Training Loss: 0.03779837116599083, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:05, Epoch: 82, Batch: 290, Training Loss: 0.022957442700862883, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:06, Epoch: 82, Batch: 300, Training Loss: 0.033440927788615224, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:07, Epoch: 82, Batch: 310, Training Loss: 0.04621279239654541, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:07, Epoch: 82, Batch: 320, Training Loss: 0.05220609605312347, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:08, Epoch: 82, Batch: 330, Training Loss: 0.03275950700044632, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:09, Epoch: 82, Batch: 340, Training Loss: 0.04055567495524883, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:10, Epoch: 82, Batch: 350, Training Loss: 0.031041743606328963, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:10, Epoch: 82, Batch: 360, Training Loss: 0.0312599178403616, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:11, Epoch: 82, Batch: 370, Training Loss: 0.04538062289357185, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:12, Epoch: 82, Batch: 380, Training Loss: 0.040356279164552686, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:12, Epoch: 82, Batch: 390, Training Loss: 0.04897163212299347, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:13, Epoch: 82, Batch: 400, Training Loss: 0.03866969794034958, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:14, Epoch: 82, Batch: 410, Training Loss: 0.029637736082077027, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:15, Epoch: 82, Batch: 420, Training Loss: 0.03832961246371269, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:15, Epoch: 82, Batch: 430, Training Loss: 0.04872954785823822, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:16, Epoch: 82, Batch: 440, Training Loss: 0.0292069248855114, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:17, Epoch: 82, Batch: 450, Training Loss: 0.04676632955670357, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:17, Epoch: 82, Batch: 460, Training Loss: 0.027206336706876756, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:18, Epoch: 82, Batch: 470, Training Loss: 0.040225870162248614, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:19, Epoch: 82, Batch: 480, Training Loss: 0.04684006683528423, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:20, Epoch: 82, Batch: 490, Training Loss: 0.03346812501549721, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:20, Epoch: 82, Batch: 500, Training Loss: 0.0461306344717741, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:21, Epoch: 82, Batch: 510, Training Loss: 0.06902523674070835, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:22, Epoch: 82, Batch: 520, Training Loss: 0.03983030691742897, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:23, Epoch: 82, Batch: 530, Training Loss: 0.058273830264806745, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:23, Epoch: 82, Batch: 540, Training Loss: 0.06772674545645714, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:24, Epoch: 82, Batch: 550, Training Loss: 0.048090696707367896, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:25, Epoch: 82, Batch: 560, Training Loss: 0.02741159461438656, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:25, Epoch: 82, Batch: 570, Training Loss: 0.03281558230519295, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:26, Epoch: 82, Batch: 580, Training Loss: 0.058450684696435926, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:27, Epoch: 82, Batch: 590, Training Loss: 0.055388647690415384, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:28, Epoch: 82, Batch: 600, Training Loss: 0.06058738306164742, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:28, Epoch: 82, Batch: 610, Training Loss: 0.034649293124675754, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:29, Epoch: 82, Batch: 620, Training Loss: 0.05452897287905216, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:30, Epoch: 82, Batch: 630, Training Loss: 0.0631848569959402, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:30, Epoch: 82, Batch: 640, Training Loss: 0.06612936928868293, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:31, Epoch: 82, Batch: 650, Training Loss: 0.04612707048654556, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:32, Epoch: 82, Batch: 660, Training Loss: 0.04233192950487137, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:33, Epoch: 82, Batch: 670, Training Loss: 0.04119964763522148, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:33, Epoch: 82, Batch: 680, Training Loss: 0.04215481206774711, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:34, Epoch: 82, Batch: 690, Training Loss: 0.04045062735676765, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:35, Epoch: 82, Batch: 700, Training Loss: 0.03864617869257927, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:35, Epoch: 82, Batch: 710, Training Loss: 0.02942046970129013, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:36, Epoch: 82, Batch: 720, Training Loss: 0.04323486909270287, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:37, Epoch: 82, Batch: 730, Training Loss: 0.058830279484391214, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:38, Epoch: 82, Batch: 740, Training Loss: 0.050543397665023804, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:38, Epoch: 82, Batch: 750, Training Loss: 0.04971078634262085, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:39, Epoch: 82, Batch: 760, Training Loss: 0.033163589611649516, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:40, Epoch: 82, Batch: 770, Training Loss: 0.028411030769348145, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:41, Epoch: 82, Batch: 780, Training Loss: 0.03412916138768196, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:41, Epoch: 82, Batch: 790, Training Loss: 0.039796946570277214, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:42, Epoch: 82, Batch: 800, Training Loss: 0.030114684253931046, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:43, Epoch: 82, Batch: 810, Training Loss: 0.03829093873500824, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:43, Epoch: 82, Batch: 820, Training Loss: 0.04509805217385292, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:44, Epoch: 82, Batch: 830, Training Loss: 0.019017232954502104, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:45, Epoch: 82, Batch: 840, Training Loss: 0.04008170440793037, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:46, Epoch: 82, Batch: 850, Training Loss: 0.051315855234861374, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:46, Epoch: 82, Batch: 860, Training Loss: 0.02340255379676819, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:47, Epoch: 82, Batch: 870, Training Loss: 0.042667874693870546, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:48, Epoch: 82, Batch: 880, Training Loss: 0.041040175780653956, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:48, Epoch: 82, Batch: 890, Training Loss: 0.0403546329587698, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:49, Epoch: 82, Batch: 900, Training Loss: 0.04159032739698887, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:50, Epoch: 82, Batch: 910, Training Loss: 0.047914425283670424, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:51, Epoch: 82, Batch: 920, Training Loss: 0.03144740760326385, LR: 0.00010000000000000003
Time, 2019-01-01T18:54:51, Epoch: 82, Batch: 930, Training Loss: 0.07119907960295677, LR: 0.00010000000000000003
Epoch: 82, Validation Top 1 acc: 98.86893463134766
Epoch: 82, Validation Top 5 acc: 99.99000549316406
Epoch: 82, Validation Set Loss: 0.041284795850515366
Start training epoch 83
Time, 2019-01-01T18:55:19, Epoch: 83, Batch: 10, Training Loss: 0.043570806831121446, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:20, Epoch: 83, Batch: 20, Training Loss: 0.05240506902337074, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:20, Epoch: 83, Batch: 30, Training Loss: 0.06481757760047913, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:21, Epoch: 83, Batch: 40, Training Loss: 0.026231265068054198, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:22, Epoch: 83, Batch: 50, Training Loss: 0.041377334669232366, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:23, Epoch: 83, Batch: 60, Training Loss: 0.03225460462272167, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:23, Epoch: 83, Batch: 70, Training Loss: 0.039447064697742465, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:24, Epoch: 83, Batch: 80, Training Loss: 0.0345297884196043, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:25, Epoch: 83, Batch: 90, Training Loss: 0.038172387331724164, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:26, Epoch: 83, Batch: 100, Training Loss: 0.05082329362630844, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:26, Epoch: 83, Batch: 110, Training Loss: 0.047526280581951144, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:27, Epoch: 83, Batch: 120, Training Loss: 0.05061931163072586, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:28, Epoch: 83, Batch: 130, Training Loss: 0.04876836128532887, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:28, Epoch: 83, Batch: 140, Training Loss: 0.024211395531892776, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:29, Epoch: 83, Batch: 150, Training Loss: 0.04370543844997883, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:30, Epoch: 83, Batch: 160, Training Loss: 0.044843877851963046, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:31, Epoch: 83, Batch: 170, Training Loss: 0.02052692323923111, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:31, Epoch: 83, Batch: 180, Training Loss: 0.0333803690969944, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:32, Epoch: 83, Batch: 190, Training Loss: 0.02590223290026188, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:33, Epoch: 83, Batch: 200, Training Loss: 0.0339606624096632, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:33, Epoch: 83, Batch: 210, Training Loss: 0.05650702193379402, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:34, Epoch: 83, Batch: 220, Training Loss: 0.03059863708913326, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:35, Epoch: 83, Batch: 230, Training Loss: 0.03135784417390823, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:36, Epoch: 83, Batch: 240, Training Loss: 0.05266879796981812, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:36, Epoch: 83, Batch: 250, Training Loss: 0.04475790336728096, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:37, Epoch: 83, Batch: 260, Training Loss: 0.037261124700307846, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:38, Epoch: 83, Batch: 270, Training Loss: 0.037278526648879054, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:38, Epoch: 83, Batch: 280, Training Loss: 0.0418534629046917, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:39, Epoch: 83, Batch: 290, Training Loss: 0.037845037132501605, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:40, Epoch: 83, Batch: 300, Training Loss: 0.030498876050114632, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:41, Epoch: 83, Batch: 310, Training Loss: 0.0679420806467533, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:41, Epoch: 83, Batch: 320, Training Loss: 0.046730120107531546, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:42, Epoch: 83, Batch: 330, Training Loss: 0.04108260869979859, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:43, Epoch: 83, Batch: 340, Training Loss: 0.055181640386581424, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:44, Epoch: 83, Batch: 350, Training Loss: 0.035821913927793506, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:44, Epoch: 83, Batch: 360, Training Loss: 0.04535769075155258, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:45, Epoch: 83, Batch: 370, Training Loss: 0.04078882150352001, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:46, Epoch: 83, Batch: 380, Training Loss: 0.03760361596941948, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:46, Epoch: 83, Batch: 390, Training Loss: 0.047481196373701094, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:47, Epoch: 83, Batch: 400, Training Loss: 0.06536981053650379, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:48, Epoch: 83, Batch: 410, Training Loss: 0.04885350167751312, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:49, Epoch: 83, Batch: 420, Training Loss: 0.05688210278749466, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:49, Epoch: 83, Batch: 430, Training Loss: 0.04999551773071289, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:50, Epoch: 83, Batch: 440, Training Loss: 0.040247971564531325, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:51, Epoch: 83, Batch: 450, Training Loss: 0.03977054581046104, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:51, Epoch: 83, Batch: 460, Training Loss: 0.021569134294986726, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:52, Epoch: 83, Batch: 470, Training Loss: 0.052351648733019826, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:53, Epoch: 83, Batch: 480, Training Loss: 0.05478002019226551, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:54, Epoch: 83, Batch: 490, Training Loss: 0.03497146144509315, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:54, Epoch: 83, Batch: 500, Training Loss: 0.028006812930107115, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:55, Epoch: 83, Batch: 510, Training Loss: 0.03373049199581146, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:56, Epoch: 83, Batch: 520, Training Loss: 0.05857863873243332, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:57, Epoch: 83, Batch: 530, Training Loss: 0.053183683380484584, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:57, Epoch: 83, Batch: 540, Training Loss: 0.046551716327667234, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:58, Epoch: 83, Batch: 550, Training Loss: 0.03221924379467964, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:59, Epoch: 83, Batch: 560, Training Loss: 0.028376369923353194, LR: 0.00010000000000000003
Time, 2019-01-01T18:55:59, Epoch: 83, Batch: 570, Training Loss: 0.029735147207975387, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:00, Epoch: 83, Batch: 580, Training Loss: 0.04044533409178257, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:01, Epoch: 83, Batch: 590, Training Loss: 0.045282450318336484, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:02, Epoch: 83, Batch: 600, Training Loss: 0.042654109001159665, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:02, Epoch: 83, Batch: 610, Training Loss: 0.05512661449611187, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:03, Epoch: 83, Batch: 620, Training Loss: 0.028436927869915963, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:04, Epoch: 83, Batch: 630, Training Loss: 0.06918383799493313, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:04, Epoch: 83, Batch: 640, Training Loss: 0.04136971831321716, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:05, Epoch: 83, Batch: 650, Training Loss: 0.036522785946726796, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:06, Epoch: 83, Batch: 660, Training Loss: 0.04349132105708122, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:07, Epoch: 83, Batch: 670, Training Loss: 0.04541622698307037, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:07, Epoch: 83, Batch: 680, Training Loss: 0.04005135521292687, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:08, Epoch: 83, Batch: 690, Training Loss: 0.03755229488015175, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:09, Epoch: 83, Batch: 700, Training Loss: 0.04845940247178078, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:09, Epoch: 83, Batch: 710, Training Loss: 0.04486734122037887, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:10, Epoch: 83, Batch: 720, Training Loss: 0.023318138718605042, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:11, Epoch: 83, Batch: 730, Training Loss: 0.03607101812958717, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:12, Epoch: 83, Batch: 740, Training Loss: 0.04382501505315304, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:12, Epoch: 83, Batch: 750, Training Loss: 0.045367206633090976, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:13, Epoch: 83, Batch: 760, Training Loss: 0.0440755020827055, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:14, Epoch: 83, Batch: 770, Training Loss: 0.03395535908639431, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:14, Epoch: 83, Batch: 780, Training Loss: 0.04648850038647652, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:15, Epoch: 83, Batch: 790, Training Loss: 0.03615889847278595, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:16, Epoch: 83, Batch: 800, Training Loss: 0.028988365828990937, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:17, Epoch: 83, Batch: 810, Training Loss: 0.04108567461371422, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:17, Epoch: 83, Batch: 820, Training Loss: 0.027644886076450347, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:18, Epoch: 83, Batch: 830, Training Loss: 0.026530428603291512, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:19, Epoch: 83, Batch: 840, Training Loss: 0.06610974222421646, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:20, Epoch: 83, Batch: 850, Training Loss: 0.030165406689047814, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:20, Epoch: 83, Batch: 860, Training Loss: 0.04694739431142807, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:21, Epoch: 83, Batch: 870, Training Loss: 0.048571468144655225, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:22, Epoch: 83, Batch: 880, Training Loss: 0.04362992979586124, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:22, Epoch: 83, Batch: 890, Training Loss: 0.02827550135552883, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:23, Epoch: 83, Batch: 900, Training Loss: 0.03745147362351418, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:24, Epoch: 83, Batch: 910, Training Loss: 0.030688007175922394, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:25, Epoch: 83, Batch: 920, Training Loss: 0.03549013808369637, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:25, Epoch: 83, Batch: 930, Training Loss: 0.03894844762980938, LR: 0.00010000000000000003
Epoch: 83, Validation Top 1 acc: 98.87726593017578
Epoch: 83, Validation Top 5 acc: 99.99166870117188
Epoch: 83, Validation Set Loss: 0.04124727100133896
Start training epoch 84
Time, 2019-01-01T18:56:55, Epoch: 84, Batch: 10, Training Loss: 0.034397856891155244, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:56, Epoch: 84, Batch: 20, Training Loss: 0.036948304250836374, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:56, Epoch: 84, Batch: 30, Training Loss: 0.038906443119049075, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:57, Epoch: 84, Batch: 40, Training Loss: 0.04473530799150467, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:58, Epoch: 84, Batch: 50, Training Loss: 0.02628719210624695, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:58, Epoch: 84, Batch: 60, Training Loss: 0.031112832576036455, LR: 0.00010000000000000003
Time, 2019-01-01T18:56:59, Epoch: 84, Batch: 70, Training Loss: 0.05389583483338356, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:00, Epoch: 84, Batch: 80, Training Loss: 0.025150906294584274, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:01, Epoch: 84, Batch: 90, Training Loss: 0.04103127531707287, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:01, Epoch: 84, Batch: 100, Training Loss: 0.05354805551469326, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:02, Epoch: 84, Batch: 110, Training Loss: 0.06494341976940632, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:03, Epoch: 84, Batch: 120, Training Loss: 0.046563025936484335, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:03, Epoch: 84, Batch: 130, Training Loss: 0.045118728280067445, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:04, Epoch: 84, Batch: 140, Training Loss: 0.040489387139678, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:05, Epoch: 84, Batch: 150, Training Loss: 0.040969622880220415, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:06, Epoch: 84, Batch: 160, Training Loss: 0.027462006732821464, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:06, Epoch: 84, Batch: 170, Training Loss: 0.052033097669482234, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:07, Epoch: 84, Batch: 180, Training Loss: 0.04330202601850033, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:08, Epoch: 84, Batch: 190, Training Loss: 0.04442049339413643, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:08, Epoch: 84, Batch: 200, Training Loss: 0.04719534069299698, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:09, Epoch: 84, Batch: 210, Training Loss: 0.04929364621639252, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:10, Epoch: 84, Batch: 220, Training Loss: 0.03549705110490322, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:11, Epoch: 84, Batch: 230, Training Loss: 0.035840126872062686, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:11, Epoch: 84, Batch: 240, Training Loss: 0.03499026782810688, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:12, Epoch: 84, Batch: 250, Training Loss: 0.03649631068110466, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:13, Epoch: 84, Batch: 260, Training Loss: 0.043354911357164384, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:13, Epoch: 84, Batch: 270, Training Loss: 0.04560446739196777, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:14, Epoch: 84, Batch: 280, Training Loss: 0.03367511332035065, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:15, Epoch: 84, Batch: 290, Training Loss: 0.038149061426520345, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:16, Epoch: 84, Batch: 300, Training Loss: 0.05779193006455898, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:16, Epoch: 84, Batch: 310, Training Loss: 0.059524345025420186, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:17, Epoch: 84, Batch: 320, Training Loss: 0.019903183728456498, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:18, Epoch: 84, Batch: 330, Training Loss: 0.045546936616301534, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:18, Epoch: 84, Batch: 340, Training Loss: 0.043046265840530396, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:19, Epoch: 84, Batch: 350, Training Loss: 0.06881809383630752, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:20, Epoch: 84, Batch: 360, Training Loss: 0.03779851533472538, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:21, Epoch: 84, Batch: 370, Training Loss: 0.037099336832761766, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:21, Epoch: 84, Batch: 380, Training Loss: 0.044017934054136273, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:22, Epoch: 84, Batch: 390, Training Loss: 0.051809778064489366, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:23, Epoch: 84, Batch: 400, Training Loss: 0.05540599264204502, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:24, Epoch: 84, Batch: 410, Training Loss: 0.03566676378250122, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:24, Epoch: 84, Batch: 420, Training Loss: 0.04517646618187428, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:25, Epoch: 84, Batch: 430, Training Loss: 0.023970917239785193, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:26, Epoch: 84, Batch: 440, Training Loss: 0.04863131046295166, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:26, Epoch: 84, Batch: 450, Training Loss: 0.03797863647341728, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:27, Epoch: 84, Batch: 460, Training Loss: 0.05020135752856732, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:28, Epoch: 84, Batch: 470, Training Loss: 0.04139962382614613, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:29, Epoch: 84, Batch: 480, Training Loss: 0.040839610248804094, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:29, Epoch: 84, Batch: 490, Training Loss: 0.059603326767683026, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:30, Epoch: 84, Batch: 500, Training Loss: 0.03473102077841759, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:31, Epoch: 84, Batch: 510, Training Loss: 0.04635061621665955, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:31, Epoch: 84, Batch: 520, Training Loss: 0.03423212803900242, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:32, Epoch: 84, Batch: 530, Training Loss: 0.040417705476284024, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:33, Epoch: 84, Batch: 540, Training Loss: 0.02522728592157364, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:34, Epoch: 84, Batch: 550, Training Loss: 0.02768438421189785, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:34, Epoch: 84, Batch: 560, Training Loss: 0.037388209626078604, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:35, Epoch: 84, Batch: 570, Training Loss: 0.05521412752568722, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:36, Epoch: 84, Batch: 580, Training Loss: 0.04016475789248943, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:36, Epoch: 84, Batch: 590, Training Loss: 0.03837785869836807, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:37, Epoch: 84, Batch: 600, Training Loss: 0.042076514661312105, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:38, Epoch: 84, Batch: 610, Training Loss: 0.03697977066040039, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:39, Epoch: 84, Batch: 620, Training Loss: 0.05046316906809807, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:39, Epoch: 84, Batch: 630, Training Loss: 0.036332723870873454, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:40, Epoch: 84, Batch: 640, Training Loss: 0.03714971169829369, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:41, Epoch: 84, Batch: 650, Training Loss: 0.028523129224777222, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:41, Epoch: 84, Batch: 660, Training Loss: 0.039812272787094115, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:42, Epoch: 84, Batch: 670, Training Loss: 0.03268240839242935, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:43, Epoch: 84, Batch: 680, Training Loss: 0.0678051881492138, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:44, Epoch: 84, Batch: 690, Training Loss: 0.03607477992773056, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:44, Epoch: 84, Batch: 700, Training Loss: 0.031434650719165805, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:45, Epoch: 84, Batch: 710, Training Loss: 0.038307157903909685, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:46, Epoch: 84, Batch: 720, Training Loss: 0.05631577745079994, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:47, Epoch: 84, Batch: 730, Training Loss: 0.037785978987812996, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:47, Epoch: 84, Batch: 740, Training Loss: 0.04490390531718731, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:48, Epoch: 84, Batch: 750, Training Loss: 0.0519653856754303, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:49, Epoch: 84, Batch: 760, Training Loss: 0.0440315380692482, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:49, Epoch: 84, Batch: 770, Training Loss: 0.033447616174817084, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:50, Epoch: 84, Batch: 780, Training Loss: 0.04154555015265941, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:51, Epoch: 84, Batch: 790, Training Loss: 0.042852514609694484, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:52, Epoch: 84, Batch: 800, Training Loss: 0.04696576073765755, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:52, Epoch: 84, Batch: 810, Training Loss: 0.039517777785658834, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:53, Epoch: 84, Batch: 820, Training Loss: 0.028946327790617944, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:54, Epoch: 84, Batch: 830, Training Loss: 0.049189281463623044, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:54, Epoch: 84, Batch: 840, Training Loss: 0.06209922544658184, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:55, Epoch: 84, Batch: 850, Training Loss: 0.03020901419222355, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:56, Epoch: 84, Batch: 860, Training Loss: 0.03166065402328968, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:57, Epoch: 84, Batch: 870, Training Loss: 0.05053641125559807, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:57, Epoch: 84, Batch: 880, Training Loss: 0.028705836087465287, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:58, Epoch: 84, Batch: 890, Training Loss: 0.0528906624764204, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:59, Epoch: 84, Batch: 900, Training Loss: 0.022373948618769644, LR: 0.00010000000000000003
Time, 2019-01-01T18:57:59, Epoch: 84, Batch: 910, Training Loss: 0.02447161078453064, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:00, Epoch: 84, Batch: 920, Training Loss: 0.04010867699980736, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:01, Epoch: 84, Batch: 930, Training Loss: 0.03533269762992859, LR: 0.00010000000000000003
Epoch: 84, Validation Top 1 acc: 98.86227416992188
Epoch: 84, Validation Top 5 acc: 99.99000549316406
Epoch: 84, Validation Set Loss: 0.04123605042695999
Start training epoch 85
Time, 2019-01-01T18:58:29, Epoch: 85, Batch: 10, Training Loss: 0.03542838431894779, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:30, Epoch: 85, Batch: 20, Training Loss: 0.03609649240970612, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:31, Epoch: 85, Batch: 30, Training Loss: 0.035607264190912244, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:31, Epoch: 85, Batch: 40, Training Loss: 0.05340414606034756, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:32, Epoch: 85, Batch: 50, Training Loss: 0.0315419290214777, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:33, Epoch: 85, Batch: 60, Training Loss: 0.034314585849642754, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:33, Epoch: 85, Batch: 70, Training Loss: 0.022966205328702926, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:34, Epoch: 85, Batch: 80, Training Loss: 0.06967502012848854, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:35, Epoch: 85, Batch: 90, Training Loss: 0.0564622450619936, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:36, Epoch: 85, Batch: 100, Training Loss: 0.034811148047447206, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:36, Epoch: 85, Batch: 110, Training Loss: 0.044354519620537755, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:37, Epoch: 85, Batch: 120, Training Loss: 0.04768563099205494, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:38, Epoch: 85, Batch: 130, Training Loss: 0.04750235490500927, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:39, Epoch: 85, Batch: 140, Training Loss: 0.052386044338345525, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:39, Epoch: 85, Batch: 150, Training Loss: 0.05463027693331242, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:40, Epoch: 85, Batch: 160, Training Loss: 0.03003026694059372, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:41, Epoch: 85, Batch: 170, Training Loss: 0.05607962645590305, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:41, Epoch: 85, Batch: 180, Training Loss: 0.045808783173561095, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:42, Epoch: 85, Batch: 190, Training Loss: 0.04844425581395626, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:43, Epoch: 85, Batch: 200, Training Loss: 0.04394877403974533, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:44, Epoch: 85, Batch: 210, Training Loss: 0.041651802882552144, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:44, Epoch: 85, Batch: 220, Training Loss: 0.0380920086055994, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:45, Epoch: 85, Batch: 230, Training Loss: 0.057602322474122045, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:46, Epoch: 85, Batch: 240, Training Loss: 0.035261955857276914, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:46, Epoch: 85, Batch: 250, Training Loss: 0.029996974021196367, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:47, Epoch: 85, Batch: 260, Training Loss: 0.05665148012340069, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:48, Epoch: 85, Batch: 270, Training Loss: 0.03415754586458206, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:49, Epoch: 85, Batch: 280, Training Loss: 0.04470805861055851, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:49, Epoch: 85, Batch: 290, Training Loss: 0.04257902279496193, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:50, Epoch: 85, Batch: 300, Training Loss: 0.05773799791932106, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:51, Epoch: 85, Batch: 310, Training Loss: 0.03382184095680714, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:52, Epoch: 85, Batch: 320, Training Loss: 0.04099433161318302, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:52, Epoch: 85, Batch: 330, Training Loss: 0.04201146773993969, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:53, Epoch: 85, Batch: 340, Training Loss: 0.04410935752093792, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:54, Epoch: 85, Batch: 350, Training Loss: 0.020569730550050735, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:54, Epoch: 85, Batch: 360, Training Loss: 0.026999836415052415, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:55, Epoch: 85, Batch: 370, Training Loss: 0.04114801995456219, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:56, Epoch: 85, Batch: 380, Training Loss: 0.05172558277845383, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:57, Epoch: 85, Batch: 390, Training Loss: 0.04510861039161682, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:57, Epoch: 85, Batch: 400, Training Loss: 0.05856198370456696, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:58, Epoch: 85, Batch: 410, Training Loss: 0.030350027605891228, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:59, Epoch: 85, Batch: 420, Training Loss: 0.0397891853004694, LR: 0.00010000000000000003
Time, 2019-01-01T18:58:59, Epoch: 85, Batch: 430, Training Loss: 0.03009672984480858, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:00, Epoch: 85, Batch: 440, Training Loss: 0.04363812170922756, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:01, Epoch: 85, Batch: 450, Training Loss: 0.056181172281503676, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:02, Epoch: 85, Batch: 460, Training Loss: 0.02714075744152069, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:02, Epoch: 85, Batch: 470, Training Loss: 0.06545518413186073, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:03, Epoch: 85, Batch: 480, Training Loss: 0.028827663511037827, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:04, Epoch: 85, Batch: 490, Training Loss: 0.0249916885048151, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:04, Epoch: 85, Batch: 500, Training Loss: 0.04976041540503502, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:05, Epoch: 85, Batch: 510, Training Loss: 0.04118983000516892, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:06, Epoch: 85, Batch: 520, Training Loss: 0.03390204757452011, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:07, Epoch: 85, Batch: 530, Training Loss: 0.03954241499304771, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:07, Epoch: 85, Batch: 540, Training Loss: 0.03343912288546562, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:08, Epoch: 85, Batch: 550, Training Loss: 0.028113360702991485, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:09, Epoch: 85, Batch: 560, Training Loss: 0.05767308622598648, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:09, Epoch: 85, Batch: 570, Training Loss: 0.04013854749500752, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:10, Epoch: 85, Batch: 580, Training Loss: 0.042913009971380235, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:11, Epoch: 85, Batch: 590, Training Loss: 0.04005522578954697, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:12, Epoch: 85, Batch: 600, Training Loss: 0.04534556344151497, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:12, Epoch: 85, Batch: 610, Training Loss: 0.038508358597755435, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:13, Epoch: 85, Batch: 620, Training Loss: 0.039388281479477885, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:14, Epoch: 85, Batch: 630, Training Loss: 0.0462813425809145, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:15, Epoch: 85, Batch: 640, Training Loss: 0.02854880318045616, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:15, Epoch: 85, Batch: 650, Training Loss: 0.04236210212111473, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:16, Epoch: 85, Batch: 660, Training Loss: 0.03106345236301422, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:17, Epoch: 85, Batch: 670, Training Loss: 0.04127566479146481, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:17, Epoch: 85, Batch: 680, Training Loss: 0.03809053152799606, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:18, Epoch: 85, Batch: 690, Training Loss: 0.03816851526498795, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:19, Epoch: 85, Batch: 700, Training Loss: 0.04538104794919491, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:20, Epoch: 85, Batch: 710, Training Loss: 0.06696886941790581, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:20, Epoch: 85, Batch: 720, Training Loss: 0.02190440148115158, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:21, Epoch: 85, Batch: 730, Training Loss: 0.044395289942622186, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:22, Epoch: 85, Batch: 740, Training Loss: 0.050245441123843194, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:22, Epoch: 85, Batch: 750, Training Loss: 0.04670254662632942, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:23, Epoch: 85, Batch: 760, Training Loss: 0.033638247102499005, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:24, Epoch: 85, Batch: 770, Training Loss: 0.04282908588647842, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:25, Epoch: 85, Batch: 780, Training Loss: 0.03494415953755379, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:25, Epoch: 85, Batch: 790, Training Loss: 0.050894496217370036, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:26, Epoch: 85, Batch: 800, Training Loss: 0.04902077466249466, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:27, Epoch: 85, Batch: 810, Training Loss: 0.03392160758376121, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:27, Epoch: 85, Batch: 820, Training Loss: 0.032554876431822775, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:28, Epoch: 85, Batch: 830, Training Loss: 0.040198391675949095, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:29, Epoch: 85, Batch: 840, Training Loss: 0.029640864580869675, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:30, Epoch: 85, Batch: 850, Training Loss: 0.04199387356638908, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:30, Epoch: 85, Batch: 860, Training Loss: 0.05527720116078853, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:31, Epoch: 85, Batch: 870, Training Loss: 0.04426452293992043, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:32, Epoch: 85, Batch: 880, Training Loss: 0.041158358380198476, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:32, Epoch: 85, Batch: 890, Training Loss: 0.03831970728933811, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:33, Epoch: 85, Batch: 900, Training Loss: 0.041139909997582436, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:34, Epoch: 85, Batch: 910, Training Loss: 0.04537266343832016, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:35, Epoch: 85, Batch: 920, Training Loss: 0.02742670737206936, LR: 0.00010000000000000003
Time, 2019-01-01T18:59:35, Epoch: 85, Batch: 930, Training Loss: 0.02672399431467056, LR: 0.00010000000000000003
Epoch: 85, Validation Top 1 acc: 98.86893463134766
Epoch: 85, Validation Top 5 acc: 99.99166870117188
Epoch: 85, Validation Set Loss: 0.04117253050208092
Start training epoch 86
Time, 2019-01-01T19:00:03, Epoch: 86, Batch: 10, Training Loss: 0.03408215530216694, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:04, Epoch: 86, Batch: 20, Training Loss: 0.04550698548555374, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:05, Epoch: 86, Batch: 30, Training Loss: 0.04404896721243858, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:05, Epoch: 86, Batch: 40, Training Loss: 0.03922726400196552, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:06, Epoch: 86, Batch: 50, Training Loss: 0.03597630076110363, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:07, Epoch: 86, Batch: 60, Training Loss: 0.051953985914587976, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:07, Epoch: 86, Batch: 70, Training Loss: 0.03793762400746346, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:08, Epoch: 86, Batch: 80, Training Loss: 0.04271295443177223, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:09, Epoch: 86, Batch: 90, Training Loss: 0.05383168011903763, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:10, Epoch: 86, Batch: 100, Training Loss: 0.024640391767024993, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:10, Epoch: 86, Batch: 110, Training Loss: 0.05268407389521599, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:11, Epoch: 86, Batch: 120, Training Loss: 0.04152471870183945, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:12, Epoch: 86, Batch: 130, Training Loss: 0.04385419562458992, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:13, Epoch: 86, Batch: 140, Training Loss: 0.04530286267399788, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:13, Epoch: 86, Batch: 150, Training Loss: 0.05185943022370339, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:14, Epoch: 86, Batch: 160, Training Loss: 0.03609725721180439, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:15, Epoch: 86, Batch: 170, Training Loss: 0.05860501304268837, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:15, Epoch: 86, Batch: 180, Training Loss: 0.03301753103733063, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:16, Epoch: 86, Batch: 190, Training Loss: 0.04064012058079243, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:17, Epoch: 86, Batch: 200, Training Loss: 0.054232373088598254, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:18, Epoch: 86, Batch: 210, Training Loss: 0.03277011439204216, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:18, Epoch: 86, Batch: 220, Training Loss: 0.04214052259922028, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:19, Epoch: 86, Batch: 230, Training Loss: 0.0447459913790226, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:20, Epoch: 86, Batch: 240, Training Loss: 0.03291945159435272, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:20, Epoch: 86, Batch: 250, Training Loss: 0.028274742141366004, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:21, Epoch: 86, Batch: 260, Training Loss: 0.0419399444013834, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:22, Epoch: 86, Batch: 270, Training Loss: 0.031148416548967363, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:23, Epoch: 86, Batch: 280, Training Loss: 0.037088101357221605, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:23, Epoch: 86, Batch: 290, Training Loss: 0.05513067990541458, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:24, Epoch: 86, Batch: 300, Training Loss: 0.03140046074986458, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:25, Epoch: 86, Batch: 310, Training Loss: 0.03616020530462265, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:25, Epoch: 86, Batch: 320, Training Loss: 0.06113583594560623, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:26, Epoch: 86, Batch: 330, Training Loss: 0.039105416461825374, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:27, Epoch: 86, Batch: 340, Training Loss: 0.02948533818125725, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:28, Epoch: 86, Batch: 350, Training Loss: 0.02791851684451103, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:28, Epoch: 86, Batch: 360, Training Loss: 0.035251638293266295, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:29, Epoch: 86, Batch: 370, Training Loss: 0.03481813706457615, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:30, Epoch: 86, Batch: 380, Training Loss: 0.06329878568649291, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:30, Epoch: 86, Batch: 390, Training Loss: 0.032301215827465056, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:31, Epoch: 86, Batch: 400, Training Loss: 0.04453582391142845, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:32, Epoch: 86, Batch: 410, Training Loss: 0.02838165536522865, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:33, Epoch: 86, Batch: 420, Training Loss: 0.04619947597384453, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:33, Epoch: 86, Batch: 430, Training Loss: 0.05575178600847721, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:34, Epoch: 86, Batch: 440, Training Loss: 0.031022698432207108, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:35, Epoch: 86, Batch: 450, Training Loss: 0.05149188376963139, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:36, Epoch: 86, Batch: 460, Training Loss: 0.042754759266972545, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:36, Epoch: 86, Batch: 470, Training Loss: 0.03716154545545578, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:37, Epoch: 86, Batch: 480, Training Loss: 0.04061531648039818, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:38, Epoch: 86, Batch: 490, Training Loss: 0.05198943428695202, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:38, Epoch: 86, Batch: 500, Training Loss: 0.034847821295261386, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:39, Epoch: 86, Batch: 510, Training Loss: 0.04231880754232407, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:40, Epoch: 86, Batch: 520, Training Loss: 0.044394709169864655, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:41, Epoch: 86, Batch: 530, Training Loss: 0.053583193197846414, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:41, Epoch: 86, Batch: 540, Training Loss: 0.03847095929086208, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:42, Epoch: 86, Batch: 550, Training Loss: 0.046801316738128665, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:43, Epoch: 86, Batch: 560, Training Loss: 0.038759177550673485, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:43, Epoch: 86, Batch: 570, Training Loss: 0.02770870365202427, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:44, Epoch: 86, Batch: 580, Training Loss: 0.04153778553009033, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:45, Epoch: 86, Batch: 590, Training Loss: 0.05174788422882557, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:46, Epoch: 86, Batch: 600, Training Loss: 0.03872792720794678, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:46, Epoch: 86, Batch: 610, Training Loss: 0.041964541748166086, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:47, Epoch: 86, Batch: 620, Training Loss: 0.05379363931715488, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:48, Epoch: 86, Batch: 630, Training Loss: 0.03548879250884056, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:49, Epoch: 86, Batch: 640, Training Loss: 0.034698599576950075, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:49, Epoch: 86, Batch: 650, Training Loss: 0.02588057704269886, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:50, Epoch: 86, Batch: 660, Training Loss: 0.03942587450146675, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:51, Epoch: 86, Batch: 670, Training Loss: 0.04371130727231502, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:51, Epoch: 86, Batch: 680, Training Loss: 0.060740864276885985, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:52, Epoch: 86, Batch: 690, Training Loss: 0.02434520870447159, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:53, Epoch: 86, Batch: 700, Training Loss: 0.04222202748060226, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:54, Epoch: 86, Batch: 710, Training Loss: 0.035692301020026206, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:54, Epoch: 86, Batch: 720, Training Loss: 0.045405831187963486, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:55, Epoch: 86, Batch: 730, Training Loss: 0.04682813920080662, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:56, Epoch: 86, Batch: 740, Training Loss: 0.054929924011230466, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:56, Epoch: 86, Batch: 750, Training Loss: 0.03321104645729065, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:57, Epoch: 86, Batch: 760, Training Loss: 0.038509152829647064, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:58, Epoch: 86, Batch: 770, Training Loss: 0.04445683509111405, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:59, Epoch: 86, Batch: 780, Training Loss: 0.03106747940182686, LR: 0.00010000000000000003
Time, 2019-01-01T19:00:59, Epoch: 86, Batch: 790, Training Loss: 0.04336467757821083, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:00, Epoch: 86, Batch: 800, Training Loss: 0.04463456943631172, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:01, Epoch: 86, Batch: 810, Training Loss: 0.04107673391699791, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:01, Epoch: 86, Batch: 820, Training Loss: 0.048205454275012015, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:02, Epoch: 86, Batch: 830, Training Loss: 0.05784648098051548, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:03, Epoch: 86, Batch: 840, Training Loss: 0.03481258787214756, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:04, Epoch: 86, Batch: 850, Training Loss: 0.038838278874754904, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:04, Epoch: 86, Batch: 860, Training Loss: 0.029197868704795838, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:05, Epoch: 86, Batch: 870, Training Loss: 0.04424335733056069, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:06, Epoch: 86, Batch: 880, Training Loss: 0.02548972927033901, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:06, Epoch: 86, Batch: 890, Training Loss: 0.051184839010238646, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:07, Epoch: 86, Batch: 900, Training Loss: 0.03852264955639839, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:08, Epoch: 86, Batch: 910, Training Loss: 0.044960541650652885, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:09, Epoch: 86, Batch: 920, Training Loss: 0.050307849794626235, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:09, Epoch: 86, Batch: 930, Training Loss: 0.03945975601673126, LR: 0.00010000000000000003
Epoch: 86, Validation Top 1 acc: 98.86393737792969
Epoch: 86, Validation Top 5 acc: 99.99166870117188
Epoch: 86, Validation Set Loss: 0.04122677445411682
Start training epoch 87
Time, 2019-01-01T19:01:37, Epoch: 87, Batch: 10, Training Loss: 0.03381770178675651, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:38, Epoch: 87, Batch: 20, Training Loss: 0.03815710097551346, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:39, Epoch: 87, Batch: 30, Training Loss: 0.046581140160560607, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:40, Epoch: 87, Batch: 40, Training Loss: 0.035384805500507356, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:40, Epoch: 87, Batch: 50, Training Loss: 0.048544400930404664, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:41, Epoch: 87, Batch: 60, Training Loss: 0.055679652467370035, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:42, Epoch: 87, Batch: 70, Training Loss: 0.03480275236070156, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:43, Epoch: 87, Batch: 80, Training Loss: 0.04893391206860542, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:43, Epoch: 87, Batch: 90, Training Loss: 0.05135371759533882, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:44, Epoch: 87, Batch: 100, Training Loss: 0.029475482925772668, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:45, Epoch: 87, Batch: 110, Training Loss: 0.04330208748579025, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:45, Epoch: 87, Batch: 120, Training Loss: 0.04115237519145012, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:46, Epoch: 87, Batch: 130, Training Loss: 0.07030250057578087, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:47, Epoch: 87, Batch: 140, Training Loss: 0.040930455550551414, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:48, Epoch: 87, Batch: 150, Training Loss: 0.03183983713388443, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:48, Epoch: 87, Batch: 160, Training Loss: 0.032751040905714034, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:49, Epoch: 87, Batch: 170, Training Loss: 0.04380344673991203, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:50, Epoch: 87, Batch: 180, Training Loss: 0.03798711895942688, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:50, Epoch: 87, Batch: 190, Training Loss: 0.053910822793841365, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:51, Epoch: 87, Batch: 200, Training Loss: 0.042934907227754594, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:52, Epoch: 87, Batch: 210, Training Loss: 0.04365604259073734, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:53, Epoch: 87, Batch: 220, Training Loss: 0.06555828340351581, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:53, Epoch: 87, Batch: 230, Training Loss: 0.022801417112350463, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:54, Epoch: 87, Batch: 240, Training Loss: 0.038659676909446716, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:55, Epoch: 87, Batch: 250, Training Loss: 0.034924878552556035, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:55, Epoch: 87, Batch: 260, Training Loss: 0.027340012043714522, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:56, Epoch: 87, Batch: 270, Training Loss: 0.04031185582280159, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:57, Epoch: 87, Batch: 280, Training Loss: 0.03400575555860996, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:58, Epoch: 87, Batch: 290, Training Loss: 0.030701034516096116, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:58, Epoch: 87, Batch: 300, Training Loss: 0.044024025276303294, LR: 0.00010000000000000003
Time, 2019-01-01T19:01:59, Epoch: 87, Batch: 310, Training Loss: 0.0250699520111084, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:00, Epoch: 87, Batch: 320, Training Loss: 0.04054371230304241, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:01, Epoch: 87, Batch: 330, Training Loss: 0.03596872389316559, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:01, Epoch: 87, Batch: 340, Training Loss: 0.05215611271560192, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:02, Epoch: 87, Batch: 350, Training Loss: 0.0317565742880106, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:03, Epoch: 87, Batch: 360, Training Loss: 0.04333324395120144, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:04, Epoch: 87, Batch: 370, Training Loss: 0.028700393810868263, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:04, Epoch: 87, Batch: 380, Training Loss: 0.04391515292227268, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:05, Epoch: 87, Batch: 390, Training Loss: 0.04689091667532921, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:06, Epoch: 87, Batch: 400, Training Loss: 0.05013773925602436, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:06, Epoch: 87, Batch: 410, Training Loss: 0.0634075053036213, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:07, Epoch: 87, Batch: 420, Training Loss: 0.043894415721297264, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:08, Epoch: 87, Batch: 430, Training Loss: 0.027160578221082688, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:09, Epoch: 87, Batch: 440, Training Loss: 0.04745953008532524, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:09, Epoch: 87, Batch: 450, Training Loss: 0.05248284563422203, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:10, Epoch: 87, Batch: 460, Training Loss: 0.040974270179867746, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:11, Epoch: 87, Batch: 470, Training Loss: 0.026891440153121948, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:11, Epoch: 87, Batch: 480, Training Loss: 0.0492182083427906, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:12, Epoch: 87, Batch: 490, Training Loss: 0.02887023240327835, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:13, Epoch: 87, Batch: 500, Training Loss: 0.03658664859831333, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:14, Epoch: 87, Batch: 510, Training Loss: 0.059515885263681415, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:14, Epoch: 87, Batch: 520, Training Loss: 0.04367323964834213, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:15, Epoch: 87, Batch: 530, Training Loss: 0.04765515103936195, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:16, Epoch: 87, Batch: 540, Training Loss: 0.035934647917747496, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:17, Epoch: 87, Batch: 550, Training Loss: 0.03156617395579815, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:17, Epoch: 87, Batch: 560, Training Loss: 0.05204455181956291, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:18, Epoch: 87, Batch: 570, Training Loss: 0.03138975761830807, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:19, Epoch: 87, Batch: 580, Training Loss: 0.029592129215598107, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:19, Epoch: 87, Batch: 590, Training Loss: 0.04162728637456894, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:20, Epoch: 87, Batch: 600, Training Loss: 0.037510374933481215, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:21, Epoch: 87, Batch: 610, Training Loss: 0.05669616013765335, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:22, Epoch: 87, Batch: 620, Training Loss: 0.03939828351140022, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:22, Epoch: 87, Batch: 630, Training Loss: 0.036092912405729295, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:23, Epoch: 87, Batch: 640, Training Loss: 0.036830770596861837, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:24, Epoch: 87, Batch: 650, Training Loss: 0.04144135415554047, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:24, Epoch: 87, Batch: 660, Training Loss: 0.03833577074110508, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:25, Epoch: 87, Batch: 670, Training Loss: 0.04605536125600338, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:26, Epoch: 87, Batch: 680, Training Loss: 0.03588383123278618, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:27, Epoch: 87, Batch: 690, Training Loss: 0.04262551069259644, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:27, Epoch: 87, Batch: 700, Training Loss: 0.048905112966895106, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:28, Epoch: 87, Batch: 710, Training Loss: 0.040741190686821936, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:29, Epoch: 87, Batch: 720, Training Loss: 0.04870343767106533, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:29, Epoch: 87, Batch: 730, Training Loss: 0.05294807590544224, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:30, Epoch: 87, Batch: 740, Training Loss: 0.044265822321176526, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:31, Epoch: 87, Batch: 750, Training Loss: 0.025838715210556983, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:32, Epoch: 87, Batch: 760, Training Loss: 0.06233828440308571, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:32, Epoch: 87, Batch: 770, Training Loss: 0.023038864135742188, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:33, Epoch: 87, Batch: 780, Training Loss: 0.03814980685710907, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:34, Epoch: 87, Batch: 790, Training Loss: 0.027599358186125755, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:34, Epoch: 87, Batch: 800, Training Loss: 0.03650286272168159, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:35, Epoch: 87, Batch: 810, Training Loss: 0.03502713553607464, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:36, Epoch: 87, Batch: 820, Training Loss: 0.032765720039606094, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:37, Epoch: 87, Batch: 830, Training Loss: 0.06218659281730652, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:37, Epoch: 87, Batch: 840, Training Loss: 0.0500396691262722, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:38, Epoch: 87, Batch: 850, Training Loss: 0.04292215071618557, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:39, Epoch: 87, Batch: 860, Training Loss: 0.04479247629642487, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:40, Epoch: 87, Batch: 870, Training Loss: 0.039148901030421256, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:40, Epoch: 87, Batch: 880, Training Loss: 0.031277085468173024, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:41, Epoch: 87, Batch: 890, Training Loss: 0.038764703646302225, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:42, Epoch: 87, Batch: 900, Training Loss: 0.04298081398010254, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:42, Epoch: 87, Batch: 910, Training Loss: 0.03240382149815559, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:43, Epoch: 87, Batch: 920, Training Loss: 0.05223949514329433, LR: 0.00010000000000000003
Time, 2019-01-01T19:02:44, Epoch: 87, Batch: 930, Training Loss: 0.043594100326299665, LR: 0.00010000000000000003
Epoch: 87, Validation Top 1 acc: 98.87060546875
Epoch: 87, Validation Top 5 acc: 99.99166870117188
Epoch: 87, Validation Set Loss: 0.04113701730966568
Start training epoch 88
Time, 2019-01-01T19:03:12, Epoch: 88, Batch: 10, Training Loss: 0.0327240951359272, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:12, Epoch: 88, Batch: 20, Training Loss: 0.04249881505966187, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:13, Epoch: 88, Batch: 30, Training Loss: 0.030235259979963302, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:14, Epoch: 88, Batch: 40, Training Loss: 0.03002472370862961, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:15, Epoch: 88, Batch: 50, Training Loss: 0.04911630861461162, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:15, Epoch: 88, Batch: 60, Training Loss: 0.04040299020707607, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:16, Epoch: 88, Batch: 70, Training Loss: 0.035306168347597124, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:17, Epoch: 88, Batch: 80, Training Loss: 0.047761493176221845, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:17, Epoch: 88, Batch: 90, Training Loss: 0.06580051444470883, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:18, Epoch: 88, Batch: 100, Training Loss: 0.04889335855841637, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:19, Epoch: 88, Batch: 110, Training Loss: 0.04842702262103558, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:20, Epoch: 88, Batch: 120, Training Loss: 0.0369040884077549, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:20, Epoch: 88, Batch: 130, Training Loss: 0.061389054358005526, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:21, Epoch: 88, Batch: 140, Training Loss: 0.04226929619908333, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:22, Epoch: 88, Batch: 150, Training Loss: 0.04962350577116013, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:22, Epoch: 88, Batch: 160, Training Loss: 0.034367921948432925, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:23, Epoch: 88, Batch: 170, Training Loss: 0.03517933934926987, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:24, Epoch: 88, Batch: 180, Training Loss: 0.03548825904726982, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:25, Epoch: 88, Batch: 190, Training Loss: 0.03172246627509594, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:25, Epoch: 88, Batch: 200, Training Loss: 0.04470738358795643, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:26, Epoch: 88, Batch: 210, Training Loss: 0.04233826547861099, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:27, Epoch: 88, Batch: 220, Training Loss: 0.043287241458892824, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:27, Epoch: 88, Batch: 230, Training Loss: 0.033388666808605194, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:28, Epoch: 88, Batch: 240, Training Loss: 0.03245591968297958, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:29, Epoch: 88, Batch: 250, Training Loss: 0.04500585235655308, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:30, Epoch: 88, Batch: 260, Training Loss: 0.03477784171700478, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:30, Epoch: 88, Batch: 270, Training Loss: 0.04080223888158798, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:31, Epoch: 88, Batch: 280, Training Loss: 0.03757064826786518, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:32, Epoch: 88, Batch: 290, Training Loss: 0.02965044416487217, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:33, Epoch: 88, Batch: 300, Training Loss: 0.05251721888780594, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:33, Epoch: 88, Batch: 310, Training Loss: 0.036359281092882154, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:34, Epoch: 88, Batch: 320, Training Loss: 0.040305967628955844, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:35, Epoch: 88, Batch: 330, Training Loss: 0.04088466092944145, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:35, Epoch: 88, Batch: 340, Training Loss: 0.050569505989551546, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:36, Epoch: 88, Batch: 350, Training Loss: 0.028717804700136185, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:37, Epoch: 88, Batch: 360, Training Loss: 0.0575863566249609, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:38, Epoch: 88, Batch: 370, Training Loss: 0.04042062386870384, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:38, Epoch: 88, Batch: 380, Training Loss: 0.06327154822647571, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:39, Epoch: 88, Batch: 390, Training Loss: 0.04480927810072899, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:40, Epoch: 88, Batch: 400, Training Loss: 0.025116158276796342, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:41, Epoch: 88, Batch: 410, Training Loss: 0.027414652705192565, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:41, Epoch: 88, Batch: 420, Training Loss: 0.023221465945243835, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:42, Epoch: 88, Batch: 430, Training Loss: 0.03401156924664974, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:43, Epoch: 88, Batch: 440, Training Loss: 0.02896985411643982, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:43, Epoch: 88, Batch: 450, Training Loss: 0.03781699277460575, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:44, Epoch: 88, Batch: 460, Training Loss: 0.046587858349084854, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:45, Epoch: 88, Batch: 470, Training Loss: 0.08972200639545917, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:46, Epoch: 88, Batch: 480, Training Loss: 0.04614013805985451, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:46, Epoch: 88, Batch: 490, Training Loss: 0.04152956865727901, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:47, Epoch: 88, Batch: 500, Training Loss: 0.037895045801997186, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:48, Epoch: 88, Batch: 510, Training Loss: 0.03864546231925488, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:48, Epoch: 88, Batch: 520, Training Loss: 0.042578593641519544, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:49, Epoch: 88, Batch: 530, Training Loss: 0.03567367754876614, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:50, Epoch: 88, Batch: 540, Training Loss: 0.03944474384188652, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:51, Epoch: 88, Batch: 550, Training Loss: 0.04739643707871437, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:51, Epoch: 88, Batch: 560, Training Loss: 0.038973034173250196, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:52, Epoch: 88, Batch: 570, Training Loss: 0.03898345082998276, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:53, Epoch: 88, Batch: 580, Training Loss: 0.03729049637913704, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:53, Epoch: 88, Batch: 590, Training Loss: 0.03742008879780769, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:54, Epoch: 88, Batch: 600, Training Loss: 0.04084434509277344, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:55, Epoch: 88, Batch: 610, Training Loss: 0.033994553983211516, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:56, Epoch: 88, Batch: 620, Training Loss: 0.03366926908493042, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:56, Epoch: 88, Batch: 630, Training Loss: 0.03406127877533436, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:57, Epoch: 88, Batch: 640, Training Loss: 0.034806220605969426, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:58, Epoch: 88, Batch: 650, Training Loss: 0.03549985289573669, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:59, Epoch: 88, Batch: 660, Training Loss: 0.05516867376863956, LR: 0.00010000000000000003
Time, 2019-01-01T19:03:59, Epoch: 88, Batch: 670, Training Loss: 0.05861031860113144, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:00, Epoch: 88, Batch: 680, Training Loss: 0.04628608152270317, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:01, Epoch: 88, Batch: 690, Training Loss: 0.043921053782105444, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:01, Epoch: 88, Batch: 700, Training Loss: 0.040534405037760735, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:02, Epoch: 88, Batch: 710, Training Loss: 0.03682830855250359, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:03, Epoch: 88, Batch: 720, Training Loss: 0.04653327465057373, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:04, Epoch: 88, Batch: 730, Training Loss: 0.04728804081678391, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:04, Epoch: 88, Batch: 740, Training Loss: 0.035794259235262874, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:05, Epoch: 88, Batch: 750, Training Loss: 0.03217573091387749, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:06, Epoch: 88, Batch: 760, Training Loss: 0.04018233269453049, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:06, Epoch: 88, Batch: 770, Training Loss: 0.0474741343408823, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:07, Epoch: 88, Batch: 780, Training Loss: 0.038200030103325844, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:08, Epoch: 88, Batch: 790, Training Loss: 0.029645077511668207, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:09, Epoch: 88, Batch: 800, Training Loss: 0.04704280570149422, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:09, Epoch: 88, Batch: 810, Training Loss: 0.04394703693687916, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:10, Epoch: 88, Batch: 820, Training Loss: 0.02491854317486286, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:11, Epoch: 88, Batch: 830, Training Loss: 0.056030132621526715, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:12, Epoch: 88, Batch: 840, Training Loss: 0.0501464881002903, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:12, Epoch: 88, Batch: 850, Training Loss: 0.054472552984952925, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:13, Epoch: 88, Batch: 860, Training Loss: 0.06250007972121238, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:14, Epoch: 88, Batch: 870, Training Loss: 0.04479679316282272, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:14, Epoch: 88, Batch: 880, Training Loss: 0.036297696828842166, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:15, Epoch: 88, Batch: 890, Training Loss: 0.022383243963122367, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:16, Epoch: 88, Batch: 900, Training Loss: 0.02819196619093418, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:17, Epoch: 88, Batch: 910, Training Loss: 0.0364396158605814, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:17, Epoch: 88, Batch: 920, Training Loss: 0.03633069284260273, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:18, Epoch: 88, Batch: 930, Training Loss: 0.06015518382191658, LR: 0.00010000000000000003
Epoch: 88, Validation Top 1 acc: 98.87393188476562
Epoch: 88, Validation Top 5 acc: 99.99166870117188
Epoch: 88, Validation Set Loss: 0.04119610786437988
Start training epoch 89
Time, 2019-01-01T19:04:46, Epoch: 89, Batch: 10, Training Loss: 0.045980751141905786, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:46, Epoch: 89, Batch: 20, Training Loss: 0.0437178909778595, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:47, Epoch: 89, Batch: 30, Training Loss: 0.03325918912887573, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:48, Epoch: 89, Batch: 40, Training Loss: 0.02804197557270527, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:49, Epoch: 89, Batch: 50, Training Loss: 0.026403696089982987, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:49, Epoch: 89, Batch: 60, Training Loss: 0.05085399486124516, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:50, Epoch: 89, Batch: 70, Training Loss: 0.0359940517693758, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:51, Epoch: 89, Batch: 80, Training Loss: 0.04976182579994202, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:51, Epoch: 89, Batch: 90, Training Loss: 0.04330109171569348, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:52, Epoch: 89, Batch: 100, Training Loss: 0.035718636214733125, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:53, Epoch: 89, Batch: 110, Training Loss: 0.03489632532000542, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:54, Epoch: 89, Batch: 120, Training Loss: 0.04838112853467465, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:54, Epoch: 89, Batch: 130, Training Loss: 0.03430144675076008, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:55, Epoch: 89, Batch: 140, Training Loss: 0.04089989960193634, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:56, Epoch: 89, Batch: 150, Training Loss: 0.03432414121925831, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:57, Epoch: 89, Batch: 160, Training Loss: 0.06680596470832825, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:57, Epoch: 89, Batch: 170, Training Loss: 0.03786033652722835, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:58, Epoch: 89, Batch: 180, Training Loss: 0.04505218490958214, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:59, Epoch: 89, Batch: 190, Training Loss: 0.05927886702120304, LR: 0.00010000000000000003
Time, 2019-01-01T19:04:59, Epoch: 89, Batch: 200, Training Loss: 0.04886094443500042, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:00, Epoch: 89, Batch: 210, Training Loss: 0.052507975697517396, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:01, Epoch: 89, Batch: 220, Training Loss: 0.04865811802446842, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:02, Epoch: 89, Batch: 230, Training Loss: 0.050932098925113675, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:02, Epoch: 89, Batch: 240, Training Loss: 0.03304264694452286, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:03, Epoch: 89, Batch: 250, Training Loss: 0.03702159896492958, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:04, Epoch: 89, Batch: 260, Training Loss: 0.02714051716029644, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:04, Epoch: 89, Batch: 270, Training Loss: 0.04574713483452797, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:05, Epoch: 89, Batch: 280, Training Loss: 0.044981949776411054, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:06, Epoch: 89, Batch: 290, Training Loss: 0.044606827571988104, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:07, Epoch: 89, Batch: 300, Training Loss: 0.05520222559571266, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:07, Epoch: 89, Batch: 310, Training Loss: 0.04643437303602695, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:08, Epoch: 89, Batch: 320, Training Loss: 0.05216232575476169, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:09, Epoch: 89, Batch: 330, Training Loss: 0.04038462862372398, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:10, Epoch: 89, Batch: 340, Training Loss: 0.06264994107186794, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:10, Epoch: 89, Batch: 350, Training Loss: 0.05696664154529572, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:11, Epoch: 89, Batch: 360, Training Loss: 0.02715498358011246, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:12, Epoch: 89, Batch: 370, Training Loss: 0.024904174357652666, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:12, Epoch: 89, Batch: 380, Training Loss: 0.03854396939277649, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:13, Epoch: 89, Batch: 390, Training Loss: 0.04076794423162937, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:14, Epoch: 89, Batch: 400, Training Loss: 0.04022886157035828, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:15, Epoch: 89, Batch: 410, Training Loss: 0.05777539908885956, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:15, Epoch: 89, Batch: 420, Training Loss: 0.02511310614645481, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:16, Epoch: 89, Batch: 430, Training Loss: 0.025393251329660416, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:17, Epoch: 89, Batch: 440, Training Loss: 0.06634868532419205, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:17, Epoch: 89, Batch: 450, Training Loss: 0.04320097416639328, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:18, Epoch: 89, Batch: 460, Training Loss: 0.035016676783561705, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:19, Epoch: 89, Batch: 470, Training Loss: 0.04986503124237061, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:20, Epoch: 89, Batch: 480, Training Loss: 0.04378668293356895, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:20, Epoch: 89, Batch: 490, Training Loss: 0.036143505573272706, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:21, Epoch: 89, Batch: 500, Training Loss: 0.04582545682787895, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:22, Epoch: 89, Batch: 510, Training Loss: 0.04623765908181667, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:22, Epoch: 89, Batch: 520, Training Loss: 0.035370805859565736, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:23, Epoch: 89, Batch: 530, Training Loss: 0.039124347269535065, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:24, Epoch: 89, Batch: 540, Training Loss: 0.03850894905626774, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:25, Epoch: 89, Batch: 550, Training Loss: 0.020991836860775947, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:25, Epoch: 89, Batch: 560, Training Loss: 0.059650999307632444, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:26, Epoch: 89, Batch: 570, Training Loss: 0.0353982113301754, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:27, Epoch: 89, Batch: 580, Training Loss: 0.04671062603592872, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:27, Epoch: 89, Batch: 590, Training Loss: 0.04817691221833229, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:28, Epoch: 89, Batch: 600, Training Loss: 0.031039258092641832, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:29, Epoch: 89, Batch: 610, Training Loss: 0.03734258785843849, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:30, Epoch: 89, Batch: 620, Training Loss: 0.04157880991697312, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:30, Epoch: 89, Batch: 630, Training Loss: 0.03707731664180756, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:31, Epoch: 89, Batch: 640, Training Loss: 0.038403820246458054, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:32, Epoch: 89, Batch: 650, Training Loss: 0.03416054546833038, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:32, Epoch: 89, Batch: 660, Training Loss: 0.03162391819059849, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:33, Epoch: 89, Batch: 670, Training Loss: 0.03959171213209629, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:34, Epoch: 89, Batch: 680, Training Loss: 0.04838659055531025, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:35, Epoch: 89, Batch: 690, Training Loss: 0.03420677036046982, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:35, Epoch: 89, Batch: 700, Training Loss: 0.02946929708123207, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:36, Epoch: 89, Batch: 710, Training Loss: 0.030737397074699403, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:37, Epoch: 89, Batch: 720, Training Loss: 0.03677362650632858, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:38, Epoch: 89, Batch: 730, Training Loss: 0.031111040338873863, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:38, Epoch: 89, Batch: 740, Training Loss: 0.0406213253736496, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:39, Epoch: 89, Batch: 750, Training Loss: 0.035937707871198654, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:40, Epoch: 89, Batch: 760, Training Loss: 0.038591858744621274, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:40, Epoch: 89, Batch: 770, Training Loss: 0.0366375632584095, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:41, Epoch: 89, Batch: 780, Training Loss: 0.03655933067202568, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:42, Epoch: 89, Batch: 790, Training Loss: 0.05636227764189243, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:43, Epoch: 89, Batch: 800, Training Loss: 0.028469985350966454, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:43, Epoch: 89, Batch: 810, Training Loss: 0.03589553683996201, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:44, Epoch: 89, Batch: 820, Training Loss: 0.026710107550024988, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:45, Epoch: 89, Batch: 830, Training Loss: 0.047120902314782144, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:45, Epoch: 89, Batch: 840, Training Loss: 0.033331313729286195, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:46, Epoch: 89, Batch: 850, Training Loss: 0.04890849404036999, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:47, Epoch: 89, Batch: 860, Training Loss: 0.051646371558308604, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:48, Epoch: 89, Batch: 870, Training Loss: 0.028162434697151184, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:48, Epoch: 89, Batch: 880, Training Loss: 0.0320544209331274, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:49, Epoch: 89, Batch: 890, Training Loss: 0.05189397633075714, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:50, Epoch: 89, Batch: 900, Training Loss: 0.05601239427924156, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:50, Epoch: 89, Batch: 910, Training Loss: 0.05118732526898384, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:51, Epoch: 89, Batch: 920, Training Loss: 0.04712584801018238, LR: 0.00010000000000000003
Time, 2019-01-01T19:05:52, Epoch: 89, Batch: 930, Training Loss: 0.0459485299885273, LR: 0.00010000000000000003
Epoch: 89, Validation Top 1 acc: 98.86893463134766
Epoch: 89, Validation Top 5 acc: 99.99166870117188
Epoch: 89, Validation Set Loss: 0.04117089882493019
Start training epoch 90
Time, 2019-01-01T19:06:20, Epoch: 90, Batch: 10, Training Loss: 0.06383609473705291, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:20, Epoch: 90, Batch: 20, Training Loss: 0.04339909180998802, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:21, Epoch: 90, Batch: 30, Training Loss: 0.058081111311912535, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:22, Epoch: 90, Batch: 40, Training Loss: 0.04405579045414924, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:23, Epoch: 90, Batch: 50, Training Loss: 0.026714340224862097, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:23, Epoch: 90, Batch: 60, Training Loss: 0.02548677586019039, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:24, Epoch: 90, Batch: 70, Training Loss: 0.038520161807537076, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:25, Epoch: 90, Batch: 80, Training Loss: 0.03489706292748451, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:26, Epoch: 90, Batch: 90, Training Loss: 0.03366459384560585, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:26, Epoch: 90, Batch: 100, Training Loss: 0.03350292555987835, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:27, Epoch: 90, Batch: 110, Training Loss: 0.055622877180576326, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:28, Epoch: 90, Batch: 120, Training Loss: 0.03597437664866447, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:28, Epoch: 90, Batch: 130, Training Loss: 0.053975703194737434, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:29, Epoch: 90, Batch: 140, Training Loss: 0.04083450362086296, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:30, Epoch: 90, Batch: 150, Training Loss: 0.05459925904870033, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:31, Epoch: 90, Batch: 160, Training Loss: 0.03130964934825897, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:31, Epoch: 90, Batch: 170, Training Loss: 0.045182302594184875, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:32, Epoch: 90, Batch: 180, Training Loss: 0.04842638149857521, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:33, Epoch: 90, Batch: 190, Training Loss: 0.03733845129609108, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:33, Epoch: 90, Batch: 200, Training Loss: 0.055117645859718324, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:34, Epoch: 90, Batch: 210, Training Loss: 0.0459738738834858, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:35, Epoch: 90, Batch: 220, Training Loss: 0.055350524932146074, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:36, Epoch: 90, Batch: 230, Training Loss: 0.02769316956400871, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:36, Epoch: 90, Batch: 240, Training Loss: 0.036774882674217226, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:37, Epoch: 90, Batch: 250, Training Loss: 0.039444050192832945, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:38, Epoch: 90, Batch: 260, Training Loss: 0.023745892196893693, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:38, Epoch: 90, Batch: 270, Training Loss: 0.02944611981511116, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:39, Epoch: 90, Batch: 280, Training Loss: 0.06293569877743721, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:40, Epoch: 90, Batch: 290, Training Loss: 0.02443101666867733, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:41, Epoch: 90, Batch: 300, Training Loss: 0.03736408911645413, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:41, Epoch: 90, Batch: 310, Training Loss: 0.04084584824740887, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:42, Epoch: 90, Batch: 320, Training Loss: 0.06132727712392807, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:43, Epoch: 90, Batch: 330, Training Loss: 0.04680683873593807, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:43, Epoch: 90, Batch: 340, Training Loss: 0.05596954897046089, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:44, Epoch: 90, Batch: 350, Training Loss: 0.03084065318107605, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:45, Epoch: 90, Batch: 360, Training Loss: 0.03323336578905582, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:46, Epoch: 90, Batch: 370, Training Loss: 0.031812213361263275, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:46, Epoch: 90, Batch: 380, Training Loss: 0.04086751788854599, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:47, Epoch: 90, Batch: 390, Training Loss: 0.03606163635849953, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:48, Epoch: 90, Batch: 400, Training Loss: 0.06336477547883987, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:48, Epoch: 90, Batch: 410, Training Loss: 0.03897010460495949, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:49, Epoch: 90, Batch: 420, Training Loss: 0.05018723346292973, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:50, Epoch: 90, Batch: 430, Training Loss: 0.03716657906770706, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:51, Epoch: 90, Batch: 440, Training Loss: 0.046794593334198, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:51, Epoch: 90, Batch: 450, Training Loss: 0.031999605149030684, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:52, Epoch: 90, Batch: 460, Training Loss: 0.04032617136836052, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:53, Epoch: 90, Batch: 470, Training Loss: 0.037006822600960734, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:54, Epoch: 90, Batch: 480, Training Loss: 0.03985603637993336, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:54, Epoch: 90, Batch: 490, Training Loss: 0.03906841389834881, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:55, Epoch: 90, Batch: 500, Training Loss: 0.030029439181089402, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:56, Epoch: 90, Batch: 510, Training Loss: 0.03595305234193802, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:56, Epoch: 90, Batch: 520, Training Loss: 0.03873328939080238, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:57, Epoch: 90, Batch: 530, Training Loss: 0.04107613228261471, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:58, Epoch: 90, Batch: 540, Training Loss: 0.047873805090785024, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:59, Epoch: 90, Batch: 550, Training Loss: 0.029367828369140626, LR: 0.00010000000000000003
Time, 2019-01-01T19:06:59, Epoch: 90, Batch: 560, Training Loss: 0.050090429931879045, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:00, Epoch: 90, Batch: 570, Training Loss: 0.04074075892567634, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:01, Epoch: 90, Batch: 580, Training Loss: 0.03306833878159523, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:02, Epoch: 90, Batch: 590, Training Loss: 0.035123807936906816, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:02, Epoch: 90, Batch: 600, Training Loss: 0.039911114424467084, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:03, Epoch: 90, Batch: 610, Training Loss: 0.027850471436977386, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:04, Epoch: 90, Batch: 620, Training Loss: 0.05000408925116062, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:04, Epoch: 90, Batch: 630, Training Loss: 0.03705444596707821, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:05, Epoch: 90, Batch: 640, Training Loss: 0.048550485819578174, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:06, Epoch: 90, Batch: 650, Training Loss: 0.039473893493413924, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:07, Epoch: 90, Batch: 660, Training Loss: 0.042561358958482745, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:07, Epoch: 90, Batch: 670, Training Loss: 0.03406999297440052, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:08, Epoch: 90, Batch: 680, Training Loss: 0.03199029676616192, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:09, Epoch: 90, Batch: 690, Training Loss: 0.04816492609679699, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:09, Epoch: 90, Batch: 700, Training Loss: 0.026863736659288408, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:10, Epoch: 90, Batch: 710, Training Loss: 0.038739731162786485, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:11, Epoch: 90, Batch: 720, Training Loss: 0.04436536282300949, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:12, Epoch: 90, Batch: 730, Training Loss: 0.03013934791088104, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:12, Epoch: 90, Batch: 740, Training Loss: 0.03928348906338215, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:13, Epoch: 90, Batch: 750, Training Loss: 0.04974318742752075, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:14, Epoch: 90, Batch: 760, Training Loss: 0.03964637741446495, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:14, Epoch: 90, Batch: 770, Training Loss: 0.04009978324174881, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:15, Epoch: 90, Batch: 780, Training Loss: 0.041241138428449634, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:16, Epoch: 90, Batch: 790, Training Loss: 0.046832680329680444, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:17, Epoch: 90, Batch: 800, Training Loss: 0.05076103396713734, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:17, Epoch: 90, Batch: 810, Training Loss: 0.03695913888514042, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:18, Epoch: 90, Batch: 820, Training Loss: 0.03702927753329277, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:19, Epoch: 90, Batch: 830, Training Loss: 0.05490297675132751, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:19, Epoch: 90, Batch: 840, Training Loss: 0.04025506414473057, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:20, Epoch: 90, Batch: 850, Training Loss: 0.0450481116771698, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:21, Epoch: 90, Batch: 860, Training Loss: 0.05065875574946403, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:22, Epoch: 90, Batch: 870, Training Loss: 0.034923581033945085, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:22, Epoch: 90, Batch: 880, Training Loss: 0.0329466849565506, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:23, Epoch: 90, Batch: 890, Training Loss: 0.04322001039981842, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:24, Epoch: 90, Batch: 900, Training Loss: 0.03919990882277489, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:25, Epoch: 90, Batch: 910, Training Loss: 0.039817388728260995, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:25, Epoch: 90, Batch: 920, Training Loss: 0.05820127949118614, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:26, Epoch: 90, Batch: 930, Training Loss: 0.04354699775576591, LR: 0.00010000000000000003
Epoch: 90, Validation Top 1 acc: 98.86393737792969
Epoch: 90, Validation Top 5 acc: 99.99000549316406
Epoch: 90, Validation Set Loss: 0.04115711525082588
Start training epoch 91
Time, 2019-01-01T19:07:54, Epoch: 91, Batch: 10, Training Loss: 0.044310306757688524, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:54, Epoch: 91, Batch: 20, Training Loss: 0.034269051253795625, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:55, Epoch: 91, Batch: 30, Training Loss: 0.030159108340740204, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:56, Epoch: 91, Batch: 40, Training Loss: 0.04624466374516487, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:57, Epoch: 91, Batch: 50, Training Loss: 0.037361356616020205, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:57, Epoch: 91, Batch: 60, Training Loss: 0.03622791059315204, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:58, Epoch: 91, Batch: 70, Training Loss: 0.0692296601831913, LR: 0.00010000000000000003
Time, 2019-01-01T19:07:59, Epoch: 91, Batch: 80, Training Loss: 0.03516026586294174, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:00, Epoch: 91, Batch: 90, Training Loss: 0.03512696698307991, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:00, Epoch: 91, Batch: 100, Training Loss: 0.04769046120345592, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:01, Epoch: 91, Batch: 110, Training Loss: 0.034809484332799914, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:02, Epoch: 91, Batch: 120, Training Loss: 0.0326375737786293, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:02, Epoch: 91, Batch: 130, Training Loss: 0.0450836069881916, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:03, Epoch: 91, Batch: 140, Training Loss: 0.031940095126628876, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:04, Epoch: 91, Batch: 150, Training Loss: 0.05194952934980392, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:05, Epoch: 91, Batch: 160, Training Loss: 0.03782450743019581, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:05, Epoch: 91, Batch: 170, Training Loss: 0.07584480755031109, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:06, Epoch: 91, Batch: 180, Training Loss: 0.030295151099562646, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:07, Epoch: 91, Batch: 190, Training Loss: 0.032813649624586105, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:07, Epoch: 91, Batch: 200, Training Loss: 0.03349645547568798, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:08, Epoch: 91, Batch: 210, Training Loss: 0.02905941791832447, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:09, Epoch: 91, Batch: 220, Training Loss: 0.041073834896087645, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:10, Epoch: 91, Batch: 230, Training Loss: 0.05414639487862587, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:10, Epoch: 91, Batch: 240, Training Loss: 0.043548818305134775, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:11, Epoch: 91, Batch: 250, Training Loss: 0.043425187841057776, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:12, Epoch: 91, Batch: 260, Training Loss: 0.04716629683971405, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:13, Epoch: 91, Batch: 270, Training Loss: 0.04518329985439777, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:13, Epoch: 91, Batch: 280, Training Loss: 0.04647787362337112, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:14, Epoch: 91, Batch: 290, Training Loss: 0.03551097437739372, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:15, Epoch: 91, Batch: 300, Training Loss: 0.05113142356276512, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:15, Epoch: 91, Batch: 310, Training Loss: 0.03994233943521976, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:16, Epoch: 91, Batch: 320, Training Loss: 0.031077926605939867, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:17, Epoch: 91, Batch: 330, Training Loss: 0.026980334892868996, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:18, Epoch: 91, Batch: 340, Training Loss: 0.038534026220440866, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:18, Epoch: 91, Batch: 350, Training Loss: 0.03278001919388771, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:19, Epoch: 91, Batch: 360, Training Loss: 0.04202512577176094, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:20, Epoch: 91, Batch: 370, Training Loss: 0.030545199289917946, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:20, Epoch: 91, Batch: 380, Training Loss: 0.038118918612599376, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:21, Epoch: 91, Batch: 390, Training Loss: 0.042456676810979845, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:22, Epoch: 91, Batch: 400, Training Loss: 0.04339070841670036, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:23, Epoch: 91, Batch: 410, Training Loss: 0.041967084258794786, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:23, Epoch: 91, Batch: 420, Training Loss: 0.041022207215428354, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:24, Epoch: 91, Batch: 430, Training Loss: 0.036646437272429465, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:25, Epoch: 91, Batch: 440, Training Loss: 0.020447346568107604, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:25, Epoch: 91, Batch: 450, Training Loss: 0.02602311931550503, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:26, Epoch: 91, Batch: 460, Training Loss: 0.048809859156608584, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:27, Epoch: 91, Batch: 470, Training Loss: 0.03500602096319198, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:28, Epoch: 91, Batch: 480, Training Loss: 0.05344058088958263, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:28, Epoch: 91, Batch: 490, Training Loss: 0.03850136622786522, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:29, Epoch: 91, Batch: 500, Training Loss: 0.03512183763086796, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:30, Epoch: 91, Batch: 510, Training Loss: 0.08274277001619339, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:30, Epoch: 91, Batch: 520, Training Loss: 0.035685057193040846, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:31, Epoch: 91, Batch: 530, Training Loss: 0.03846849799156189, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:32, Epoch: 91, Batch: 540, Training Loss: 0.0454075887799263, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:33, Epoch: 91, Batch: 550, Training Loss: 0.04868269972503185, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:33, Epoch: 91, Batch: 560, Training Loss: 0.04065147340297699, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:34, Epoch: 91, Batch: 570, Training Loss: 0.031150345504283906, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:35, Epoch: 91, Batch: 580, Training Loss: 0.028118008747696877, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:35, Epoch: 91, Batch: 590, Training Loss: 0.04879031032323837, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:36, Epoch: 91, Batch: 600, Training Loss: 0.0414414431899786, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:37, Epoch: 91, Batch: 610, Training Loss: 0.058733397349715236, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:38, Epoch: 91, Batch: 620, Training Loss: 0.030450641736388208, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:38, Epoch: 91, Batch: 630, Training Loss: 0.04504591077566147, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:39, Epoch: 91, Batch: 640, Training Loss: 0.041352713108062746, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:40, Epoch: 91, Batch: 650, Training Loss: 0.05450172945857048, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:40, Epoch: 91, Batch: 660, Training Loss: 0.04315255992114544, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:41, Epoch: 91, Batch: 670, Training Loss: 0.0726479485630989, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:42, Epoch: 91, Batch: 680, Training Loss: 0.0311124250292778, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:43, Epoch: 91, Batch: 690, Training Loss: 0.026330748945474623, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:43, Epoch: 91, Batch: 700, Training Loss: 0.04207078590989113, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:44, Epoch: 91, Batch: 710, Training Loss: 0.059092236682772636, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:45, Epoch: 91, Batch: 720, Training Loss: 0.04301752485334873, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:45, Epoch: 91, Batch: 730, Training Loss: 0.034974949061870576, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:46, Epoch: 91, Batch: 740, Training Loss: 0.04117889478802681, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:47, Epoch: 91, Batch: 750, Training Loss: 0.04800520911812782, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:48, Epoch: 91, Batch: 760, Training Loss: 0.053222567588090894, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:48, Epoch: 91, Batch: 770, Training Loss: 0.036174492165446284, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:49, Epoch: 91, Batch: 780, Training Loss: 0.03883873000741005, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:50, Epoch: 91, Batch: 790, Training Loss: 0.043520157039165494, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:51, Epoch: 91, Batch: 800, Training Loss: 0.03927774131298065, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:51, Epoch: 91, Batch: 810, Training Loss: 0.04153841435909271, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:52, Epoch: 91, Batch: 820, Training Loss: 0.042528093233704566, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:53, Epoch: 91, Batch: 830, Training Loss: 0.039986920356750486, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:53, Epoch: 91, Batch: 840, Training Loss: 0.03968676514923573, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:54, Epoch: 91, Batch: 850, Training Loss: 0.04127015396952629, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:55, Epoch: 91, Batch: 860, Training Loss: 0.043434707820415495, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:56, Epoch: 91, Batch: 870, Training Loss: 0.05072314329445362, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:56, Epoch: 91, Batch: 880, Training Loss: 0.019419387727975846, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:57, Epoch: 91, Batch: 890, Training Loss: 0.01985713019967079, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:58, Epoch: 91, Batch: 900, Training Loss: 0.031495945528149605, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:58, Epoch: 91, Batch: 910, Training Loss: 0.046636255085468294, LR: 0.00010000000000000003
Time, 2019-01-01T19:08:59, Epoch: 91, Batch: 920, Training Loss: 0.04238832257688045, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:00, Epoch: 91, Batch: 930, Training Loss: 0.05030379444360733, LR: 0.00010000000000000003
Epoch: 91, Validation Top 1 acc: 98.87560272216797
Epoch: 91, Validation Top 5 acc: 99.99000549316406
Epoch: 91, Validation Set Loss: 0.04109591990709305
Start training epoch 92
Time, 2019-01-01T19:09:28, Epoch: 92, Batch: 10, Training Loss: 0.0610308401286602, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:29, Epoch: 92, Batch: 20, Training Loss: 0.03964236117899418, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:29, Epoch: 92, Batch: 30, Training Loss: 0.04304616972804069, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:30, Epoch: 92, Batch: 40, Training Loss: 0.045094762742519376, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:31, Epoch: 92, Batch: 50, Training Loss: 0.05341322273015976, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:31, Epoch: 92, Batch: 60, Training Loss: 0.03279534429311752, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:32, Epoch: 92, Batch: 70, Training Loss: 0.039715978875756265, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:33, Epoch: 92, Batch: 80, Training Loss: 0.030392083898186685, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:34, Epoch: 92, Batch: 90, Training Loss: 0.03882185816764831, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:34, Epoch: 92, Batch: 100, Training Loss: 0.0332243163138628, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:35, Epoch: 92, Batch: 110, Training Loss: 0.04456762224435806, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:36, Epoch: 92, Batch: 120, Training Loss: 0.03869287297129631, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:36, Epoch: 92, Batch: 130, Training Loss: 0.04694742187857628, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:37, Epoch: 92, Batch: 140, Training Loss: 0.027708230167627336, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:38, Epoch: 92, Batch: 150, Training Loss: 0.038643128052353856, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:39, Epoch: 92, Batch: 160, Training Loss: 0.03648630790412426, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:39, Epoch: 92, Batch: 170, Training Loss: 0.031189139932394028, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:40, Epoch: 92, Batch: 180, Training Loss: 0.04249558784067631, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:41, Epoch: 92, Batch: 190, Training Loss: 0.04347732290625572, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:41, Epoch: 92, Batch: 200, Training Loss: 0.05937814973294735, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:42, Epoch: 92, Batch: 210, Training Loss: 0.04762645401060581, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:43, Epoch: 92, Batch: 220, Training Loss: 0.020475979894399643, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:44, Epoch: 92, Batch: 230, Training Loss: 0.04314594008028507, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:44, Epoch: 92, Batch: 240, Training Loss: 0.028118662908673285, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:45, Epoch: 92, Batch: 250, Training Loss: 0.040295911207795146, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:46, Epoch: 92, Batch: 260, Training Loss: 0.04741123989224434, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:46, Epoch: 92, Batch: 270, Training Loss: 0.03264487348496914, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:47, Epoch: 92, Batch: 280, Training Loss: 0.03913877978920936, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:48, Epoch: 92, Batch: 290, Training Loss: 0.03456542231142521, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:49, Epoch: 92, Batch: 300, Training Loss: 0.04673155508935452, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:49, Epoch: 92, Batch: 310, Training Loss: 0.03323765322566032, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:50, Epoch: 92, Batch: 320, Training Loss: 0.03818340934813023, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:51, Epoch: 92, Batch: 330, Training Loss: 0.03768527545034885, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:51, Epoch: 92, Batch: 340, Training Loss: 0.042842359840869905, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:52, Epoch: 92, Batch: 350, Training Loss: 0.021360398456454276, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:53, Epoch: 92, Batch: 360, Training Loss: 0.05303192026913166, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:54, Epoch: 92, Batch: 370, Training Loss: 0.03299726322293282, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:54, Epoch: 92, Batch: 380, Training Loss: 0.04029500037431717, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:55, Epoch: 92, Batch: 390, Training Loss: 0.05010425932705402, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:56, Epoch: 92, Batch: 400, Training Loss: 0.04361442103981972, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:57, Epoch: 92, Batch: 410, Training Loss: 0.04596262201666832, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:57, Epoch: 92, Batch: 420, Training Loss: 0.03834005631506443, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:58, Epoch: 92, Batch: 430, Training Loss: 0.03511860482394695, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:59, Epoch: 92, Batch: 440, Training Loss: 0.025829566270112993, LR: 0.00010000000000000003
Time, 2019-01-01T19:09:59, Epoch: 92, Batch: 450, Training Loss: 0.045699624717235564, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:00, Epoch: 92, Batch: 460, Training Loss: 0.0460717935115099, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:01, Epoch: 92, Batch: 470, Training Loss: 0.030820641666650772, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:02, Epoch: 92, Batch: 480, Training Loss: 0.040886237472295764, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:02, Epoch: 92, Batch: 490, Training Loss: 0.05089871659874916, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:03, Epoch: 92, Batch: 500, Training Loss: 0.04526637233793736, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:04, Epoch: 92, Batch: 510, Training Loss: 0.0348435815423727, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:04, Epoch: 92, Batch: 520, Training Loss: 0.0448152095079422, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:05, Epoch: 92, Batch: 530, Training Loss: 0.059845267236232756, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:06, Epoch: 92, Batch: 540, Training Loss: 0.03147007003426552, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:07, Epoch: 92, Batch: 550, Training Loss: 0.05580002889037132, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:07, Epoch: 92, Batch: 560, Training Loss: 0.04028533101081848, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:08, Epoch: 92, Batch: 570, Training Loss: 0.041011089086532594, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:09, Epoch: 92, Batch: 580, Training Loss: 0.04207206107676029, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:10, Epoch: 92, Batch: 590, Training Loss: 0.0461936105042696, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:10, Epoch: 92, Batch: 600, Training Loss: 0.04596988447010517, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:11, Epoch: 92, Batch: 610, Training Loss: 0.05614197105169296, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:12, Epoch: 92, Batch: 620, Training Loss: 0.023367646709084512, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:12, Epoch: 92, Batch: 630, Training Loss: 0.05453905984759331, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:13, Epoch: 92, Batch: 640, Training Loss: 0.03339083455502987, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:14, Epoch: 92, Batch: 650, Training Loss: 0.03712239935994148, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:15, Epoch: 92, Batch: 660, Training Loss: 0.05160786509513855, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:15, Epoch: 92, Batch: 670, Training Loss: 0.039882635697722435, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:16, Epoch: 92, Batch: 680, Training Loss: 0.038212196156382564, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:17, Epoch: 92, Batch: 690, Training Loss: 0.037381839379668234, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:17, Epoch: 92, Batch: 700, Training Loss: 0.033191030472517015, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:18, Epoch: 92, Batch: 710, Training Loss: 0.04798957705497742, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:19, Epoch: 92, Batch: 720, Training Loss: 0.030734948441386224, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:20, Epoch: 92, Batch: 730, Training Loss: 0.028485699743032455, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:20, Epoch: 92, Batch: 740, Training Loss: 0.0386460542678833, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:21, Epoch: 92, Batch: 750, Training Loss: 0.04939153082668781, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:22, Epoch: 92, Batch: 760, Training Loss: 0.03007533252239227, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:22, Epoch: 92, Batch: 770, Training Loss: 0.04187941960990429, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:23, Epoch: 92, Batch: 780, Training Loss: 0.05531301498413086, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:24, Epoch: 92, Batch: 790, Training Loss: 0.04199206717312336, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:25, Epoch: 92, Batch: 800, Training Loss: 0.05946265161037445, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:25, Epoch: 92, Batch: 810, Training Loss: 0.0432788297533989, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:26, Epoch: 92, Batch: 820, Training Loss: 0.03304334282875061, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:27, Epoch: 92, Batch: 830, Training Loss: 0.044611118361353876, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:27, Epoch: 92, Batch: 840, Training Loss: 0.05145544074475765, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:28, Epoch: 92, Batch: 850, Training Loss: 0.052151535823941234, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:29, Epoch: 92, Batch: 860, Training Loss: 0.034958697855472565, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:30, Epoch: 92, Batch: 870, Training Loss: 0.05519365556538105, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:30, Epoch: 92, Batch: 880, Training Loss: 0.04062209092080593, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:31, Epoch: 92, Batch: 890, Training Loss: 0.0412639569491148, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:32, Epoch: 92, Batch: 900, Training Loss: 0.03491640388965607, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:33, Epoch: 92, Batch: 910, Training Loss: 0.06379422880709171, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:33, Epoch: 92, Batch: 920, Training Loss: 0.042000887170434, LR: 0.00010000000000000003
Time, 2019-01-01T19:10:34, Epoch: 92, Batch: 930, Training Loss: 0.024885143712162973, LR: 0.00010000000000000003
Epoch: 92, Validation Top 1 acc: 98.87560272216797
Epoch: 92, Validation Top 5 acc: 99.99000549316406
Epoch: 92, Validation Set Loss: 0.04101857170462608
Start training epoch 93
Time, 2019-01-01T19:11:02, Epoch: 93, Batch: 10, Training Loss: 0.029148009419441224, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:02, Epoch: 93, Batch: 20, Training Loss: 0.046766317635774615, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:03, Epoch: 93, Batch: 30, Training Loss: 0.028982036188244818, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:04, Epoch: 93, Batch: 40, Training Loss: 0.03445186167955398, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:05, Epoch: 93, Batch: 50, Training Loss: 0.03778090626001358, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:05, Epoch: 93, Batch: 60, Training Loss: 0.023368612676858903, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:06, Epoch: 93, Batch: 70, Training Loss: 0.0462486632168293, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:07, Epoch: 93, Batch: 80, Training Loss: 0.04564220793545246, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:07, Epoch: 93, Batch: 90, Training Loss: 0.0354572020471096, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:08, Epoch: 93, Batch: 100, Training Loss: 0.0360691238194704, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:09, Epoch: 93, Batch: 110, Training Loss: 0.05930902399122715, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:10, Epoch: 93, Batch: 120, Training Loss: 0.03915261924266815, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:10, Epoch: 93, Batch: 130, Training Loss: 0.050217506289482114, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:11, Epoch: 93, Batch: 140, Training Loss: 0.029153340309858323, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:12, Epoch: 93, Batch: 150, Training Loss: 0.05142948552966118, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:13, Epoch: 93, Batch: 160, Training Loss: 0.04228772930800915, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:13, Epoch: 93, Batch: 170, Training Loss: 0.03413765206933021, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:14, Epoch: 93, Batch: 180, Training Loss: 0.024962390214204787, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:15, Epoch: 93, Batch: 190, Training Loss: 0.03152348510921001, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:15, Epoch: 93, Batch: 200, Training Loss: 0.050325723364949226, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:16, Epoch: 93, Batch: 210, Training Loss: 0.039942674711346625, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:17, Epoch: 93, Batch: 220, Training Loss: 0.04748839549720287, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:18, Epoch: 93, Batch: 230, Training Loss: 0.05392515249550343, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:18, Epoch: 93, Batch: 240, Training Loss: 0.05045920386910439, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:19, Epoch: 93, Batch: 250, Training Loss: 0.032765185460448265, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:20, Epoch: 93, Batch: 260, Training Loss: 0.03630293644964695, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:20, Epoch: 93, Batch: 270, Training Loss: 0.03958090916275978, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:21, Epoch: 93, Batch: 280, Training Loss: 0.048249881342053415, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:22, Epoch: 93, Batch: 290, Training Loss: 0.03821780122816563, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:23, Epoch: 93, Batch: 300, Training Loss: 0.041298863291740415, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:23, Epoch: 93, Batch: 310, Training Loss: 0.040242656320333484, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:24, Epoch: 93, Batch: 320, Training Loss: 0.037068134918808934, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:25, Epoch: 93, Batch: 330, Training Loss: 0.033256135135889056, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:25, Epoch: 93, Batch: 340, Training Loss: 0.041680720075964926, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:26, Epoch: 93, Batch: 350, Training Loss: 0.03661726787686348, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:27, Epoch: 93, Batch: 360, Training Loss: 0.04091892503201962, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:28, Epoch: 93, Batch: 370, Training Loss: 0.037849842384457585, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:28, Epoch: 93, Batch: 380, Training Loss: 0.071726144105196, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:29, Epoch: 93, Batch: 390, Training Loss: 0.056810345500707626, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:30, Epoch: 93, Batch: 400, Training Loss: 0.05136648342013359, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:31, Epoch: 93, Batch: 410, Training Loss: 0.03431771844625473, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:31, Epoch: 93, Batch: 420, Training Loss: 0.042919541522860524, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:32, Epoch: 93, Batch: 430, Training Loss: 0.036984455585479734, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:33, Epoch: 93, Batch: 440, Training Loss: 0.04685242138803005, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:33, Epoch: 93, Batch: 450, Training Loss: 0.0276151392608881, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:34, Epoch: 93, Batch: 460, Training Loss: 0.04572066590189934, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:35, Epoch: 93, Batch: 470, Training Loss: 0.03013930022716522, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:36, Epoch: 93, Batch: 480, Training Loss: 0.04801813662052155, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:36, Epoch: 93, Batch: 490, Training Loss: 0.035005932301282884, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:37, Epoch: 93, Batch: 500, Training Loss: 0.03519412390887737, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:38, Epoch: 93, Batch: 510, Training Loss: 0.04040047824382782, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:38, Epoch: 93, Batch: 520, Training Loss: 0.057217025011777875, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:39, Epoch: 93, Batch: 530, Training Loss: 0.03281334787607193, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:40, Epoch: 93, Batch: 540, Training Loss: 0.040566857904195786, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:41, Epoch: 93, Batch: 550, Training Loss: 0.03946691192686558, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:41, Epoch: 93, Batch: 560, Training Loss: 0.04484494403004646, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:42, Epoch: 93, Batch: 570, Training Loss: 0.05343053340911865, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:43, Epoch: 93, Batch: 580, Training Loss: 0.04642956145107746, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:43, Epoch: 93, Batch: 590, Training Loss: 0.06044398620724678, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:44, Epoch: 93, Batch: 600, Training Loss: 0.03796960301697254, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:45, Epoch: 93, Batch: 610, Training Loss: 0.038950759917497635, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:46, Epoch: 93, Batch: 620, Training Loss: 0.040950026363134384, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:46, Epoch: 93, Batch: 630, Training Loss: 0.05238659083843231, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:47, Epoch: 93, Batch: 640, Training Loss: 0.05378360822796822, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:48, Epoch: 93, Batch: 650, Training Loss: 0.04797717109322548, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:48, Epoch: 93, Batch: 660, Training Loss: 0.04175980314612389, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:49, Epoch: 93, Batch: 670, Training Loss: 0.032343045249581336, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:50, Epoch: 93, Batch: 680, Training Loss: 0.04586907364428043, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:51, Epoch: 93, Batch: 690, Training Loss: 0.04325704872608185, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:51, Epoch: 93, Batch: 700, Training Loss: 0.04479687064886093, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:52, Epoch: 93, Batch: 710, Training Loss: 0.026847198233008386, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:53, Epoch: 93, Batch: 720, Training Loss: 0.03462300524115562, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:53, Epoch: 93, Batch: 730, Training Loss: 0.047846078872680664, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:54, Epoch: 93, Batch: 740, Training Loss: 0.03532636761665344, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:55, Epoch: 93, Batch: 750, Training Loss: 0.04669544734060764, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:56, Epoch: 93, Batch: 760, Training Loss: 0.035691185668110845, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:56, Epoch: 93, Batch: 770, Training Loss: 0.039680981263518336, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:57, Epoch: 93, Batch: 780, Training Loss: 0.0415139053016901, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:58, Epoch: 93, Batch: 790, Training Loss: 0.045632192865014076, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:59, Epoch: 93, Batch: 800, Training Loss: 0.05244589075446129, LR: 0.00010000000000000003
Time, 2019-01-01T19:11:59, Epoch: 93, Batch: 810, Training Loss: 0.03464934974908829, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:00, Epoch: 93, Batch: 820, Training Loss: 0.045682710409164426, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:01, Epoch: 93, Batch: 830, Training Loss: 0.04299558401107788, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:01, Epoch: 93, Batch: 840, Training Loss: 0.04483657330274582, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:02, Epoch: 93, Batch: 850, Training Loss: 0.0283773273229599, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:03, Epoch: 93, Batch: 860, Training Loss: 0.0380280252546072, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:04, Epoch: 93, Batch: 870, Training Loss: 0.038614651188254356, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:04, Epoch: 93, Batch: 880, Training Loss: 0.042663493752479555, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:05, Epoch: 93, Batch: 890, Training Loss: 0.03401236645877361, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:06, Epoch: 93, Batch: 900, Training Loss: 0.028472049906849862, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:06, Epoch: 93, Batch: 910, Training Loss: 0.04001306779682636, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:07, Epoch: 93, Batch: 920, Training Loss: 0.03911256492137909, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:08, Epoch: 93, Batch: 930, Training Loss: 0.037908874824643134, LR: 0.00010000000000000003
Epoch: 93, Validation Top 1 acc: 98.87393188476562
Epoch: 93, Validation Top 5 acc: 99.99166870117188
Epoch: 93, Validation Set Loss: 0.040999095886945724
Start training epoch 94
Time, 2019-01-01T19:12:36, Epoch: 94, Batch: 10, Training Loss: 0.052085283771157266, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:36, Epoch: 94, Batch: 20, Training Loss: 0.02761341482400894, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:37, Epoch: 94, Batch: 30, Training Loss: 0.04696105383336544, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:38, Epoch: 94, Batch: 40, Training Loss: 0.04504563361406326, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:39, Epoch: 94, Batch: 50, Training Loss: 0.04166368432343006, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:39, Epoch: 94, Batch: 60, Training Loss: 0.05979671403765678, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:40, Epoch: 94, Batch: 70, Training Loss: 0.04144720770418644, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:41, Epoch: 94, Batch: 80, Training Loss: 0.046059136465191844, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:41, Epoch: 94, Batch: 90, Training Loss: 0.04143601879477501, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:42, Epoch: 94, Batch: 100, Training Loss: 0.0513780452311039, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:43, Epoch: 94, Batch: 110, Training Loss: 0.04260508492588997, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:44, Epoch: 94, Batch: 120, Training Loss: 0.04046850204467774, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:44, Epoch: 94, Batch: 130, Training Loss: 0.0428085446357727, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:45, Epoch: 94, Batch: 140, Training Loss: 0.04375236183404922, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:46, Epoch: 94, Batch: 150, Training Loss: 0.04090092182159424, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:46, Epoch: 94, Batch: 160, Training Loss: 0.029312990605831146, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:47, Epoch: 94, Batch: 170, Training Loss: 0.05099311619997025, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:48, Epoch: 94, Batch: 180, Training Loss: 0.048307427763938905, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:49, Epoch: 94, Batch: 190, Training Loss: 0.03197674192488194, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:49, Epoch: 94, Batch: 200, Training Loss: 0.03880369737744331, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:50, Epoch: 94, Batch: 210, Training Loss: 0.03677043281495571, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:51, Epoch: 94, Batch: 220, Training Loss: 0.03788719438016415, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:52, Epoch: 94, Batch: 230, Training Loss: 0.027182988822460175, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:52, Epoch: 94, Batch: 240, Training Loss: 0.054422702267766, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:53, Epoch: 94, Batch: 250, Training Loss: 0.02592899091541767, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:54, Epoch: 94, Batch: 260, Training Loss: 0.047345562279224394, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:54, Epoch: 94, Batch: 270, Training Loss: 0.03330384269356727, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:55, Epoch: 94, Batch: 280, Training Loss: 0.036809330061078074, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:56, Epoch: 94, Batch: 290, Training Loss: 0.03186262920498848, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:57, Epoch: 94, Batch: 300, Training Loss: 0.04080240651965141, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:57, Epoch: 94, Batch: 310, Training Loss: 0.046564546227455136, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:58, Epoch: 94, Batch: 320, Training Loss: 0.02739480324089527, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:59, Epoch: 94, Batch: 330, Training Loss: 0.031015631556510926, LR: 0.00010000000000000003
Time, 2019-01-01T19:12:59, Epoch: 94, Batch: 340, Training Loss: 0.04841183610260487, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:00, Epoch: 94, Batch: 350, Training Loss: 0.03902727887034416, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:01, Epoch: 94, Batch: 360, Training Loss: 0.05724484995007515, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:02, Epoch: 94, Batch: 370, Training Loss: 0.04607399180531502, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:02, Epoch: 94, Batch: 380, Training Loss: 0.05647091940045357, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:03, Epoch: 94, Batch: 390, Training Loss: 0.031074048578739168, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:04, Epoch: 94, Batch: 400, Training Loss: 0.04881744161248207, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:04, Epoch: 94, Batch: 410, Training Loss: 0.040132146701216696, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:05, Epoch: 94, Batch: 420, Training Loss: 0.04554748572409153, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:06, Epoch: 94, Batch: 430, Training Loss: 0.040492397546768186, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:07, Epoch: 94, Batch: 440, Training Loss: 0.048475495725870135, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:07, Epoch: 94, Batch: 450, Training Loss: 0.03589804470539093, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:08, Epoch: 94, Batch: 460, Training Loss: 0.03338121846318245, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:09, Epoch: 94, Batch: 470, Training Loss: 0.05207090452313423, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:09, Epoch: 94, Batch: 480, Training Loss: 0.0470901507884264, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:10, Epoch: 94, Batch: 490, Training Loss: 0.03431911319494248, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:11, Epoch: 94, Batch: 500, Training Loss: 0.030335866659879685, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:12, Epoch: 94, Batch: 510, Training Loss: 0.03453927077353001, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:12, Epoch: 94, Batch: 520, Training Loss: 0.04907224662601948, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:13, Epoch: 94, Batch: 530, Training Loss: 0.041446325927972795, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:14, Epoch: 94, Batch: 540, Training Loss: 0.043330127373337746, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:15, Epoch: 94, Batch: 550, Training Loss: 0.0450533926486969, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:16, Epoch: 94, Batch: 560, Training Loss: 0.061081298440694806, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:16, Epoch: 94, Batch: 570, Training Loss: 0.027279913425445557, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:17, Epoch: 94, Batch: 580, Training Loss: 0.020905765891075134, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:18, Epoch: 94, Batch: 590, Training Loss: 0.05501836985349655, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:19, Epoch: 94, Batch: 600, Training Loss: 0.043317248672246934, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:19, Epoch: 94, Batch: 610, Training Loss: 0.03892437294125557, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:20, Epoch: 94, Batch: 620, Training Loss: 0.04734518527984619, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:21, Epoch: 94, Batch: 630, Training Loss: 0.04373657740652561, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:22, Epoch: 94, Batch: 640, Training Loss: 0.05114372000098229, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:23, Epoch: 94, Batch: 650, Training Loss: 0.026701633259654044, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:24, Epoch: 94, Batch: 660, Training Loss: 0.03108319342136383, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:24, Epoch: 94, Batch: 670, Training Loss: 0.03411448672413826, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:25, Epoch: 94, Batch: 680, Training Loss: 0.04653159044682979, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:26, Epoch: 94, Batch: 690, Training Loss: 0.05043533258140087, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:27, Epoch: 94, Batch: 700, Training Loss: 0.03526070863008499, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:27, Epoch: 94, Batch: 710, Training Loss: 0.035822001099586484, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:28, Epoch: 94, Batch: 720, Training Loss: 0.04808545000851154, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:29, Epoch: 94, Batch: 730, Training Loss: 0.04838391467928886, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:30, Epoch: 94, Batch: 740, Training Loss: 0.04510194174945355, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:30, Epoch: 94, Batch: 750, Training Loss: 0.03431662544608116, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:31, Epoch: 94, Batch: 760, Training Loss: 0.025534985959529875, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:32, Epoch: 94, Batch: 770, Training Loss: 0.04213174507021904, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:33, Epoch: 94, Batch: 780, Training Loss: 0.03910319320857525, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:33, Epoch: 94, Batch: 790, Training Loss: 0.04068246856331825, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:34, Epoch: 94, Batch: 800, Training Loss: 0.04424266442656517, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:35, Epoch: 94, Batch: 810, Training Loss: 0.04493118524551391, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:35, Epoch: 94, Batch: 820, Training Loss: 0.04602289497852326, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:36, Epoch: 94, Batch: 830, Training Loss: 0.038079289346933366, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:37, Epoch: 94, Batch: 840, Training Loss: 0.05194833278656006, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:38, Epoch: 94, Batch: 850, Training Loss: 0.03680523298680782, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:38, Epoch: 94, Batch: 860, Training Loss: 0.054194411262869835, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:39, Epoch: 94, Batch: 870, Training Loss: 0.03188092075288296, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:40, Epoch: 94, Batch: 880, Training Loss: 0.05672143548727036, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:40, Epoch: 94, Batch: 890, Training Loss: 0.03039429858326912, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:41, Epoch: 94, Batch: 900, Training Loss: 0.031176504120230673, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:42, Epoch: 94, Batch: 910, Training Loss: 0.03145243637263775, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:43, Epoch: 94, Batch: 920, Training Loss: 0.037779008597135545, LR: 0.00010000000000000003
Time, 2019-01-01T19:13:43, Epoch: 94, Batch: 930, Training Loss: 0.03783483505249023, LR: 0.00010000000000000003
Epoch: 94, Validation Top 1 acc: 98.86727142333984
Epoch: 94, Validation Top 5 acc: 99.99166870117188
Epoch: 94, Validation Set Loss: 0.04102978855371475
Start training epoch 95
Time, 2019-01-01T19:14:12, Epoch: 95, Batch: 10, Training Loss: 0.03497376330196857, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:12, Epoch: 95, Batch: 20, Training Loss: 0.04163213931024075, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:13, Epoch: 95, Batch: 30, Training Loss: 0.043215297162532806, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:14, Epoch: 95, Batch: 40, Training Loss: 0.043613943830132486, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:14, Epoch: 95, Batch: 50, Training Loss: 0.0477644469588995, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:15, Epoch: 95, Batch: 60, Training Loss: 0.07405115440487861, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:16, Epoch: 95, Batch: 70, Training Loss: 0.023322221264243125, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:17, Epoch: 95, Batch: 80, Training Loss: 0.03888697624206543, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:17, Epoch: 95, Batch: 90, Training Loss: 0.0416934423148632, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:18, Epoch: 95, Batch: 100, Training Loss: 0.04733263924717903, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:19, Epoch: 95, Batch: 110, Training Loss: 0.031469447538256645, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:19, Epoch: 95, Batch: 120, Training Loss: 0.03542069867253304, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:20, Epoch: 95, Batch: 130, Training Loss: 0.03945024684071541, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:21, Epoch: 95, Batch: 140, Training Loss: 0.03567656688392162, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:22, Epoch: 95, Batch: 150, Training Loss: 0.05509716235101223, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:22, Epoch: 95, Batch: 160, Training Loss: 0.04155083075165748, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:23, Epoch: 95, Batch: 170, Training Loss: 0.04466895163059235, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:24, Epoch: 95, Batch: 180, Training Loss: 0.04259376600384712, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:25, Epoch: 95, Batch: 190, Training Loss: 0.044220343232154846, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:25, Epoch: 95, Batch: 200, Training Loss: 0.061943274736404416, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:26, Epoch: 95, Batch: 210, Training Loss: 0.04041637443006039, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:27, Epoch: 95, Batch: 220, Training Loss: 0.031685144454240796, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:27, Epoch: 95, Batch: 230, Training Loss: 0.056153503432869914, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:28, Epoch: 95, Batch: 240, Training Loss: 0.04106403701007366, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:29, Epoch: 95, Batch: 250, Training Loss: 0.057523682713508606, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:30, Epoch: 95, Batch: 260, Training Loss: 0.04601593501865864, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:30, Epoch: 95, Batch: 270, Training Loss: 0.030735373497009277, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:31, Epoch: 95, Batch: 280, Training Loss: 0.04737724661827088, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:32, Epoch: 95, Batch: 290, Training Loss: 0.05839803218841553, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:32, Epoch: 95, Batch: 300, Training Loss: 0.03705189824104309, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:33, Epoch: 95, Batch: 310, Training Loss: 0.044507354497909546, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:34, Epoch: 95, Batch: 320, Training Loss: 0.03919249773025513, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:35, Epoch: 95, Batch: 330, Training Loss: 0.042383383587002756, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:35, Epoch: 95, Batch: 340, Training Loss: 0.037729066982865336, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:36, Epoch: 95, Batch: 350, Training Loss: 0.03688166439533234, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:37, Epoch: 95, Batch: 360, Training Loss: 0.036169301718473434, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:37, Epoch: 95, Batch: 370, Training Loss: 0.0337177325040102, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:38, Epoch: 95, Batch: 380, Training Loss: 0.04574911184608936, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:39, Epoch: 95, Batch: 390, Training Loss: 0.02132207974791527, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:40, Epoch: 95, Batch: 400, Training Loss: 0.047678669542074205, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:40, Epoch: 95, Batch: 410, Training Loss: 0.050251588225364685, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:41, Epoch: 95, Batch: 420, Training Loss: 0.03472472950816154, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:42, Epoch: 95, Batch: 430, Training Loss: 0.04666858986020088, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:43, Epoch: 95, Batch: 440, Training Loss: 0.04132014811038971, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:43, Epoch: 95, Batch: 450, Training Loss: 0.04775704145431518, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:44, Epoch: 95, Batch: 460, Training Loss: 0.054203950613737104, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:45, Epoch: 95, Batch: 470, Training Loss: 0.027528870105743408, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:45, Epoch: 95, Batch: 480, Training Loss: 0.0347761794924736, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:46, Epoch: 95, Batch: 490, Training Loss: 0.026977140456438065, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:47, Epoch: 95, Batch: 500, Training Loss: 0.04017378389835358, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:48, Epoch: 95, Batch: 510, Training Loss: 0.047298190742731096, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:48, Epoch: 95, Batch: 520, Training Loss: 0.048051918670535085, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:49, Epoch: 95, Batch: 530, Training Loss: 0.03367680534720421, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:50, Epoch: 95, Batch: 540, Training Loss: 0.03569067269563675, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:50, Epoch: 95, Batch: 550, Training Loss: 0.03536590412259102, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:51, Epoch: 95, Batch: 560, Training Loss: 0.028642559051513673, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:52, Epoch: 95, Batch: 570, Training Loss: 0.02373039536178112, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:53, Epoch: 95, Batch: 580, Training Loss: 0.04016549028456211, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:53, Epoch: 95, Batch: 590, Training Loss: 0.051733477786183354, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:54, Epoch: 95, Batch: 600, Training Loss: 0.043848438560962676, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:55, Epoch: 95, Batch: 610, Training Loss: 0.04249071814119816, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:55, Epoch: 95, Batch: 620, Training Loss: 0.04470171332359314, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:56, Epoch: 95, Batch: 630, Training Loss: 0.042629709094762804, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:57, Epoch: 95, Batch: 640, Training Loss: 0.04157214388251305, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:58, Epoch: 95, Batch: 650, Training Loss: 0.05545538775622845, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:58, Epoch: 95, Batch: 660, Training Loss: 0.03949514776468277, LR: 0.00010000000000000003
Time, 2019-01-01T19:14:59, Epoch: 95, Batch: 670, Training Loss: 0.04819734469056129, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:00, Epoch: 95, Batch: 680, Training Loss: 0.027702025696635248, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:01, Epoch: 95, Batch: 690, Training Loss: 0.04805658683180809, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:01, Epoch: 95, Batch: 700, Training Loss: 0.03436349369585514, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:02, Epoch: 95, Batch: 710, Training Loss: 0.05415353514254093, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:03, Epoch: 95, Batch: 720, Training Loss: 0.0431359987705946, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:03, Epoch: 95, Batch: 730, Training Loss: 0.042255451902747154, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:04, Epoch: 95, Batch: 740, Training Loss: 0.033051136136054995, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:05, Epoch: 95, Batch: 750, Training Loss: 0.039490063488483426, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:06, Epoch: 95, Batch: 760, Training Loss: 0.03840818777680397, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:06, Epoch: 95, Batch: 770, Training Loss: 0.031309259682893754, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:07, Epoch: 95, Batch: 780, Training Loss: 0.040800494328141215, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:08, Epoch: 95, Batch: 790, Training Loss: 0.030607638508081438, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:08, Epoch: 95, Batch: 800, Training Loss: 0.04304149523377419, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:09, Epoch: 95, Batch: 810, Training Loss: 0.028773585706949233, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:10, Epoch: 95, Batch: 820, Training Loss: 0.04893412031233311, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:11, Epoch: 95, Batch: 830, Training Loss: 0.06294755972921848, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:11, Epoch: 95, Batch: 840, Training Loss: 0.03784608170390129, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:12, Epoch: 95, Batch: 850, Training Loss: 0.03723292760550976, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:13, Epoch: 95, Batch: 860, Training Loss: 0.03699366934597492, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:13, Epoch: 95, Batch: 870, Training Loss: 0.040234130993485454, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:14, Epoch: 95, Batch: 880, Training Loss: 0.026383868604898452, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:15, Epoch: 95, Batch: 890, Training Loss: 0.048735373094677924, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:16, Epoch: 95, Batch: 900, Training Loss: 0.036278028041124344, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:16, Epoch: 95, Batch: 910, Training Loss: 0.04506695792078972, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:17, Epoch: 95, Batch: 920, Training Loss: 0.028913643956184388, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:18, Epoch: 95, Batch: 930, Training Loss: 0.03835624866187572, LR: 0.00010000000000000003
Epoch: 95, Validation Top 1 acc: 98.87560272216797
Epoch: 95, Validation Top 5 acc: 99.99000549316406
Epoch: 95, Validation Set Loss: 0.04098359867930412
Start training epoch 96
Time, 2019-01-01T19:15:46, Epoch: 96, Batch: 10, Training Loss: 0.04020573832094669, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:47, Epoch: 96, Batch: 20, Training Loss: 0.023244940489530564, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:48, Epoch: 96, Batch: 30, Training Loss: 0.04872854202985764, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:48, Epoch: 96, Batch: 40, Training Loss: 0.037177682295441626, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:49, Epoch: 96, Batch: 50, Training Loss: 0.04288365244865418, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:50, Epoch: 96, Batch: 60, Training Loss: 0.046852133423089984, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:51, Epoch: 96, Batch: 70, Training Loss: 0.0404535785317421, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:51, Epoch: 96, Batch: 80, Training Loss: 0.055238164216279986, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:52, Epoch: 96, Batch: 90, Training Loss: 0.042180029302835466, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:53, Epoch: 96, Batch: 100, Training Loss: 0.03595166504383087, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:53, Epoch: 96, Batch: 110, Training Loss: 0.028071116656064987, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:54, Epoch: 96, Batch: 120, Training Loss: 0.04959417134523392, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:55, Epoch: 96, Batch: 130, Training Loss: 0.050210758671164514, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:56, Epoch: 96, Batch: 140, Training Loss: 0.04885188266634941, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:56, Epoch: 96, Batch: 150, Training Loss: 0.041238414496183394, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:57, Epoch: 96, Batch: 160, Training Loss: 0.03314555287361145, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:58, Epoch: 96, Batch: 170, Training Loss: 0.03902857527136803, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:59, Epoch: 96, Batch: 180, Training Loss: 0.041961969435215, LR: 0.00010000000000000003
Time, 2019-01-01T19:15:59, Epoch: 96, Batch: 190, Training Loss: 0.0382616575807333, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:00, Epoch: 96, Batch: 200, Training Loss: 0.061166852712631226, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:01, Epoch: 96, Batch: 210, Training Loss: 0.06539696231484413, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:01, Epoch: 96, Batch: 220, Training Loss: 0.04676241129636764, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:02, Epoch: 96, Batch: 230, Training Loss: 0.050890175998210906, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:03, Epoch: 96, Batch: 240, Training Loss: 0.03516407236456871, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:04, Epoch: 96, Batch: 250, Training Loss: 0.035253243520855904, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:04, Epoch: 96, Batch: 260, Training Loss: 0.03412804938852787, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:05, Epoch: 96, Batch: 270, Training Loss: 0.047105104103684424, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:06, Epoch: 96, Batch: 280, Training Loss: 0.03455934002995491, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:07, Epoch: 96, Batch: 290, Training Loss: 0.04003972113132477, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:07, Epoch: 96, Batch: 300, Training Loss: 0.057897938415408134, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:08, Epoch: 96, Batch: 310, Training Loss: 0.04004162847995758, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:09, Epoch: 96, Batch: 320, Training Loss: 0.02960277795791626, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:10, Epoch: 96, Batch: 330, Training Loss: 0.06899000033736229, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:10, Epoch: 96, Batch: 340, Training Loss: 0.030676355957984923, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:11, Epoch: 96, Batch: 350, Training Loss: 0.0350084874778986, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:12, Epoch: 96, Batch: 360, Training Loss: 0.05274372324347496, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:12, Epoch: 96, Batch: 370, Training Loss: 0.05134875327348709, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:13, Epoch: 96, Batch: 380, Training Loss: 0.049768735840916636, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:14, Epoch: 96, Batch: 390, Training Loss: 0.050394130498170854, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:15, Epoch: 96, Batch: 400, Training Loss: 0.025557146221399308, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:15, Epoch: 96, Batch: 410, Training Loss: 0.041768334805965424, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:16, Epoch: 96, Batch: 420, Training Loss: 0.020982158184051514, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:17, Epoch: 96, Batch: 430, Training Loss: 0.049581137299537656, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:18, Epoch: 96, Batch: 440, Training Loss: 0.028995008766651155, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:18, Epoch: 96, Batch: 450, Training Loss: 0.04446934424340725, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:19, Epoch: 96, Batch: 460, Training Loss: 0.06206395141780376, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:20, Epoch: 96, Batch: 470, Training Loss: 0.062008672207593915, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:21, Epoch: 96, Batch: 480, Training Loss: 0.035598385334014895, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:21, Epoch: 96, Batch: 490, Training Loss: 0.03242216967046261, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:22, Epoch: 96, Batch: 500, Training Loss: 0.035449568182229996, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:23, Epoch: 96, Batch: 510, Training Loss: 0.038757036253809926, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:24, Epoch: 96, Batch: 520, Training Loss: 0.03289875537157059, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:24, Epoch: 96, Batch: 530, Training Loss: 0.027689889073371887, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:25, Epoch: 96, Batch: 540, Training Loss: 0.04964864030480385, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:26, Epoch: 96, Batch: 550, Training Loss: 0.04508582018315792, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:26, Epoch: 96, Batch: 560, Training Loss: 0.049791131541132926, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:27, Epoch: 96, Batch: 570, Training Loss: 0.029264388605952263, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:28, Epoch: 96, Batch: 580, Training Loss: 0.049504698067903516, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:29, Epoch: 96, Batch: 590, Training Loss: 0.044587784260511396, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:29, Epoch: 96, Batch: 600, Training Loss: 0.02692611739039421, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:30, Epoch: 96, Batch: 610, Training Loss: 0.05807423517107964, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:31, Epoch: 96, Batch: 620, Training Loss: 0.041999314725399015, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:32, Epoch: 96, Batch: 630, Training Loss: 0.044226211681962016, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:32, Epoch: 96, Batch: 640, Training Loss: 0.03560321554541588, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:33, Epoch: 96, Batch: 650, Training Loss: 0.038503497838974, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:34, Epoch: 96, Batch: 660, Training Loss: 0.02095527797937393, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:34, Epoch: 96, Batch: 670, Training Loss: 0.03440089374780655, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:35, Epoch: 96, Batch: 680, Training Loss: 0.03386729098856449, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:36, Epoch: 96, Batch: 690, Training Loss: 0.028689178079366683, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:37, Epoch: 96, Batch: 700, Training Loss: 0.029456481337547302, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:37, Epoch: 96, Batch: 710, Training Loss: 0.0343584381043911, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:38, Epoch: 96, Batch: 720, Training Loss: 0.04622551277279854, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:39, Epoch: 96, Batch: 730, Training Loss: 0.04197474867105484, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:39, Epoch: 96, Batch: 740, Training Loss: 0.033412526547908786, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:40, Epoch: 96, Batch: 750, Training Loss: 0.04294458590447903, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:41, Epoch: 96, Batch: 760, Training Loss: 0.048083188757300374, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:42, Epoch: 96, Batch: 770, Training Loss: 0.02820228524506092, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:42, Epoch: 96, Batch: 780, Training Loss: 0.034894279390573504, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:43, Epoch: 96, Batch: 790, Training Loss: 0.041308663040399554, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:44, Epoch: 96, Batch: 800, Training Loss: 0.060229108482599256, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:44, Epoch: 96, Batch: 810, Training Loss: 0.06720021367073059, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:45, Epoch: 96, Batch: 820, Training Loss: 0.04859618544578552, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:46, Epoch: 96, Batch: 830, Training Loss: 0.04332787990570068, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:47, Epoch: 96, Batch: 840, Training Loss: 0.024247496202588082, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:47, Epoch: 96, Batch: 850, Training Loss: 0.030823149532079697, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:48, Epoch: 96, Batch: 860, Training Loss: 0.04815044961869717, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:49, Epoch: 96, Batch: 870, Training Loss: 0.029654010385274886, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:50, Epoch: 96, Batch: 880, Training Loss: 0.033482329174876216, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:50, Epoch: 96, Batch: 890, Training Loss: 0.040128623321652415, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:51, Epoch: 96, Batch: 900, Training Loss: 0.029999705404043196, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:52, Epoch: 96, Batch: 910, Training Loss: 0.055151449888944624, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:52, Epoch: 96, Batch: 920, Training Loss: 0.035181169584393504, LR: 0.00010000000000000003
Time, 2019-01-01T19:16:53, Epoch: 96, Batch: 930, Training Loss: 0.04147113151848316, LR: 0.00010000000000000003
Epoch: 96, Validation Top 1 acc: 98.88392639160156
Epoch: 96, Validation Top 5 acc: 99.99000549316406
Epoch: 96, Validation Set Loss: 0.04100725054740906
Start training epoch 97
Time, 2019-01-01T19:17:21, Epoch: 97, Batch: 10, Training Loss: 0.04968385696411133, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:22, Epoch: 97, Batch: 20, Training Loss: 0.05135656259953976, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:22, Epoch: 97, Batch: 30, Training Loss: 0.03749663420021534, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:23, Epoch: 97, Batch: 40, Training Loss: 0.04421912617981434, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:24, Epoch: 97, Batch: 50, Training Loss: 0.04727853238582611, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:24, Epoch: 97, Batch: 60, Training Loss: 0.03439826741814613, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:25, Epoch: 97, Batch: 70, Training Loss: 0.0253979429602623, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:26, Epoch: 97, Batch: 80, Training Loss: 0.04321167804300785, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:27, Epoch: 97, Batch: 90, Training Loss: 0.03983938060700894, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:27, Epoch: 97, Batch: 100, Training Loss: 0.03810176514089107, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:28, Epoch: 97, Batch: 110, Training Loss: 0.03463918454945088, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:29, Epoch: 97, Batch: 120, Training Loss: 0.027062224224209785, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:30, Epoch: 97, Batch: 130, Training Loss: 0.050032833218574525, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:30, Epoch: 97, Batch: 140, Training Loss: 0.06258839294314385, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:31, Epoch: 97, Batch: 150, Training Loss: 0.038050421699881556, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:32, Epoch: 97, Batch: 160, Training Loss: 0.04195166528224945, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:32, Epoch: 97, Batch: 170, Training Loss: 0.03893852084875107, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:33, Epoch: 97, Batch: 180, Training Loss: 0.04149192348122597, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:34, Epoch: 97, Batch: 190, Training Loss: 0.04755722619593143, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:35, Epoch: 97, Batch: 200, Training Loss: 0.040589718893170354, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:35, Epoch: 97, Batch: 210, Training Loss: 0.03834219090640545, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:36, Epoch: 97, Batch: 220, Training Loss: 0.05073601864278317, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:37, Epoch: 97, Batch: 230, Training Loss: 0.04258909113705158, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:37, Epoch: 97, Batch: 240, Training Loss: 0.04205332100391388, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:38, Epoch: 97, Batch: 250, Training Loss: 0.03623659908771515, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:39, Epoch: 97, Batch: 260, Training Loss: 0.047673863172531125, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:40, Epoch: 97, Batch: 270, Training Loss: 0.06457798518240451, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:40, Epoch: 97, Batch: 280, Training Loss: 0.04638309702277184, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:41, Epoch: 97, Batch: 290, Training Loss: 0.053865018486976626, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:42, Epoch: 97, Batch: 300, Training Loss: 0.04580242075026035, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:42, Epoch: 97, Batch: 310, Training Loss: 0.05075019672513008, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:43, Epoch: 97, Batch: 320, Training Loss: 0.03309257216751575, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:44, Epoch: 97, Batch: 330, Training Loss: 0.02734244614839554, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:45, Epoch: 97, Batch: 340, Training Loss: 0.04965820424258709, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:45, Epoch: 97, Batch: 350, Training Loss: 0.027716827020049094, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:46, Epoch: 97, Batch: 360, Training Loss: 0.04582338184118271, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:47, Epoch: 97, Batch: 370, Training Loss: 0.0413587037473917, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:48, Epoch: 97, Batch: 380, Training Loss: 0.03841629512608051, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:48, Epoch: 97, Batch: 390, Training Loss: 0.039590995758771896, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:49, Epoch: 97, Batch: 400, Training Loss: 0.03255759067833423, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:50, Epoch: 97, Batch: 410, Training Loss: 0.03651519566774368, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:50, Epoch: 97, Batch: 420, Training Loss: 0.027777903899550437, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:51, Epoch: 97, Batch: 430, Training Loss: 0.035491416603326796, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:52, Epoch: 97, Batch: 440, Training Loss: 0.05375950764864683, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:53, Epoch: 97, Batch: 450, Training Loss: 0.03448728322982788, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:53, Epoch: 97, Batch: 460, Training Loss: 0.0517708383500576, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:54, Epoch: 97, Batch: 470, Training Loss: 0.04182194247841835, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:55, Epoch: 97, Batch: 480, Training Loss: 0.0375470407307148, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:55, Epoch: 97, Batch: 490, Training Loss: 0.03914409726858139, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:56, Epoch: 97, Batch: 500, Training Loss: 0.03180871345102787, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:57, Epoch: 97, Batch: 510, Training Loss: 0.042260411009192464, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:58, Epoch: 97, Batch: 520, Training Loss: 0.04692261517047882, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:58, Epoch: 97, Batch: 530, Training Loss: 0.04390102364122868, LR: 0.00010000000000000003
Time, 2019-01-01T19:17:59, Epoch: 97, Batch: 540, Training Loss: 0.02989954724907875, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:00, Epoch: 97, Batch: 550, Training Loss: 0.029398084431886674, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:00, Epoch: 97, Batch: 560, Training Loss: 0.03096976950764656, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:01, Epoch: 97, Batch: 570, Training Loss: 0.04186019115149975, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:02, Epoch: 97, Batch: 580, Training Loss: 0.03917085751891136, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:03, Epoch: 97, Batch: 590, Training Loss: 0.059863504394888876, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:03, Epoch: 97, Batch: 600, Training Loss: 0.04281497038900852, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:04, Epoch: 97, Batch: 610, Training Loss: 0.03827830143272877, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:05, Epoch: 97, Batch: 620, Training Loss: 0.03696198984980583, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:05, Epoch: 97, Batch: 630, Training Loss: 0.04363467805087566, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:06, Epoch: 97, Batch: 640, Training Loss: 0.02912813127040863, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:07, Epoch: 97, Batch: 650, Training Loss: 0.0389082245528698, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:08, Epoch: 97, Batch: 660, Training Loss: 0.04673349857330322, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:08, Epoch: 97, Batch: 670, Training Loss: 0.040659571066498756, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:09, Epoch: 97, Batch: 680, Training Loss: 0.03844967558979988, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:10, Epoch: 97, Batch: 690, Training Loss: 0.04607986211776734, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:11, Epoch: 97, Batch: 700, Training Loss: 0.03834809586405754, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:11, Epoch: 97, Batch: 710, Training Loss: 0.0516658928245306, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:12, Epoch: 97, Batch: 720, Training Loss: 0.03550885617733002, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:13, Epoch: 97, Batch: 730, Training Loss: 0.0407173965126276, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:13, Epoch: 97, Batch: 740, Training Loss: 0.04398612156510353, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:14, Epoch: 97, Batch: 750, Training Loss: 0.04540620036423206, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:15, Epoch: 97, Batch: 760, Training Loss: 0.06236649714410305, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:16, Epoch: 97, Batch: 770, Training Loss: 0.04327099472284317, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:16, Epoch: 97, Batch: 780, Training Loss: 0.02835414595901966, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:17, Epoch: 97, Batch: 790, Training Loss: 0.04508181586861611, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:18, Epoch: 97, Batch: 800, Training Loss: 0.041076673194766045, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:18, Epoch: 97, Batch: 810, Training Loss: 0.06989194899797439, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:19, Epoch: 97, Batch: 820, Training Loss: 0.04229367263615132, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:20, Epoch: 97, Batch: 830, Training Loss: 0.04334266409277916, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:21, Epoch: 97, Batch: 840, Training Loss: 0.03452891334891319, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:21, Epoch: 97, Batch: 850, Training Loss: 0.046636210009455684, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:22, Epoch: 97, Batch: 860, Training Loss: 0.026341186463832857, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:23, Epoch: 97, Batch: 870, Training Loss: 0.02954532876610756, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:23, Epoch: 97, Batch: 880, Training Loss: 0.028523357585072517, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:24, Epoch: 97, Batch: 890, Training Loss: 0.031652648001909256, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:25, Epoch: 97, Batch: 900, Training Loss: 0.0451690249145031, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:26, Epoch: 97, Batch: 910, Training Loss: 0.0371667642146349, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:26, Epoch: 97, Batch: 920, Training Loss: 0.04139094464480877, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:27, Epoch: 97, Batch: 930, Training Loss: 0.03721040263772011, LR: 0.00010000000000000003
Epoch: 97, Validation Top 1 acc: 98.87726593017578
Epoch: 97, Validation Top 5 acc: 99.99000549316406
Epoch: 97, Validation Set Loss: 0.04098090901970863
Start training epoch 98
Time, 2019-01-01T19:18:55, Epoch: 98, Batch: 10, Training Loss: 0.03286651484668255, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:56, Epoch: 98, Batch: 20, Training Loss: 0.030602947995066642, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:57, Epoch: 98, Batch: 30, Training Loss: 0.03558136150240898, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:57, Epoch: 98, Batch: 40, Training Loss: 0.03528523184359074, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:58, Epoch: 98, Batch: 50, Training Loss: 0.05002623349428177, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:59, Epoch: 98, Batch: 60, Training Loss: 0.04837570264935494, LR: 0.00010000000000000003
Time, 2019-01-01T19:18:59, Epoch: 98, Batch: 70, Training Loss: 0.041036350280046464, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:00, Epoch: 98, Batch: 80, Training Loss: 0.04670102670788765, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:01, Epoch: 98, Batch: 90, Training Loss: 0.028671233728528023, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:02, Epoch: 98, Batch: 100, Training Loss: 0.04700952582061291, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:02, Epoch: 98, Batch: 110, Training Loss: 0.040959416329860686, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:03, Epoch: 98, Batch: 120, Training Loss: 0.028922094404697417, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:04, Epoch: 98, Batch: 130, Training Loss: 0.049103991314768794, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:04, Epoch: 98, Batch: 140, Training Loss: 0.04731597416102886, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:05, Epoch: 98, Batch: 150, Training Loss: 0.03413510769605636, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:06, Epoch: 98, Batch: 160, Training Loss: 0.029669109731912613, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:07, Epoch: 98, Batch: 170, Training Loss: 0.04309373646974564, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:07, Epoch: 98, Batch: 180, Training Loss: 0.035851388424634936, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:08, Epoch: 98, Batch: 190, Training Loss: 0.045508773252367976, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:09, Epoch: 98, Batch: 200, Training Loss: 0.03630202524363994, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:09, Epoch: 98, Batch: 210, Training Loss: 0.03960265070199966, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:10, Epoch: 98, Batch: 220, Training Loss: 0.03222553133964538, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:11, Epoch: 98, Batch: 230, Training Loss: 0.0527077928185463, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:12, Epoch: 98, Batch: 240, Training Loss: 0.03692252822220325, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:12, Epoch: 98, Batch: 250, Training Loss: 0.02524753287434578, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:13, Epoch: 98, Batch: 260, Training Loss: 0.05180442780256271, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:14, Epoch: 98, Batch: 270, Training Loss: 0.0244595929980278, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:14, Epoch: 98, Batch: 280, Training Loss: 0.046929705142974856, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:15, Epoch: 98, Batch: 290, Training Loss: 0.029804636165499686, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:16, Epoch: 98, Batch: 300, Training Loss: 0.038996922224760054, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:17, Epoch: 98, Batch: 310, Training Loss: 0.025427481159567833, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:17, Epoch: 98, Batch: 320, Training Loss: 0.028677331656217574, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:18, Epoch: 98, Batch: 330, Training Loss: 0.05588329657912254, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:19, Epoch: 98, Batch: 340, Training Loss: 0.03852242156863213, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:20, Epoch: 98, Batch: 350, Training Loss: 0.0530837707221508, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:20, Epoch: 98, Batch: 360, Training Loss: 0.05835535153746605, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:21, Epoch: 98, Batch: 370, Training Loss: 0.0440638467669487, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:22, Epoch: 98, Batch: 380, Training Loss: 0.032224439457058905, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:22, Epoch: 98, Batch: 390, Training Loss: 0.04890157245099545, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:23, Epoch: 98, Batch: 400, Training Loss: 0.03665782622992993, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:24, Epoch: 98, Batch: 410, Training Loss: 0.03701736591756344, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:25, Epoch: 98, Batch: 420, Training Loss: 0.028431636467576026, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:25, Epoch: 98, Batch: 430, Training Loss: 0.05101578235626221, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:26, Epoch: 98, Batch: 440, Training Loss: 0.04774562790989876, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:27, Epoch: 98, Batch: 450, Training Loss: 0.04859746694564819, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:27, Epoch: 98, Batch: 460, Training Loss: 0.04753984548151493, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:28, Epoch: 98, Batch: 470, Training Loss: 0.043248103186488154, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:29, Epoch: 98, Batch: 480, Training Loss: 0.023104190453886987, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:30, Epoch: 98, Batch: 490, Training Loss: 0.05750583559274673, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:30, Epoch: 98, Batch: 500, Training Loss: 0.04608679972589016, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:31, Epoch: 98, Batch: 510, Training Loss: 0.03203299045562744, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:32, Epoch: 98, Batch: 520, Training Loss: 0.05641792267560959, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:33, Epoch: 98, Batch: 530, Training Loss: 0.031370875984430314, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:33, Epoch: 98, Batch: 540, Training Loss: 0.044683804363012315, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:34, Epoch: 98, Batch: 550, Training Loss: 0.050265001505613326, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:35, Epoch: 98, Batch: 560, Training Loss: 0.03998389393091202, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:35, Epoch: 98, Batch: 570, Training Loss: 0.05329490639269352, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:36, Epoch: 98, Batch: 580, Training Loss: 0.04234001599252224, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:37, Epoch: 98, Batch: 590, Training Loss: 0.03441908061504364, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:38, Epoch: 98, Batch: 600, Training Loss: 0.041492222994565967, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:38, Epoch: 98, Batch: 610, Training Loss: 0.05102103129029274, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:39, Epoch: 98, Batch: 620, Training Loss: 0.038962263613939285, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:40, Epoch: 98, Batch: 630, Training Loss: 0.039797975495457646, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:41, Epoch: 98, Batch: 640, Training Loss: 0.037353530526161194, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:41, Epoch: 98, Batch: 650, Training Loss: 0.03417793922126293, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:42, Epoch: 98, Batch: 660, Training Loss: 0.04286932274699211, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:43, Epoch: 98, Batch: 670, Training Loss: 0.0337899774312973, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:43, Epoch: 98, Batch: 680, Training Loss: 0.041031225398182866, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:44, Epoch: 98, Batch: 690, Training Loss: 0.03596109375357628, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:45, Epoch: 98, Batch: 700, Training Loss: 0.05469570383429527, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:46, Epoch: 98, Batch: 710, Training Loss: 0.06367004737257957, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:46, Epoch: 98, Batch: 720, Training Loss: 0.0406696829944849, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:47, Epoch: 98, Batch: 730, Training Loss: 0.0521659217774868, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:48, Epoch: 98, Batch: 740, Training Loss: 0.028430153056979178, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:48, Epoch: 98, Batch: 750, Training Loss: 0.043199000880122185, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:49, Epoch: 98, Batch: 760, Training Loss: 0.060431667417287824, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:50, Epoch: 98, Batch: 770, Training Loss: 0.03285490050911903, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:51, Epoch: 98, Batch: 780, Training Loss: 0.051098034158349036, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:51, Epoch: 98, Batch: 790, Training Loss: 0.0320461917668581, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:52, Epoch: 98, Batch: 800, Training Loss: 0.044809342548251155, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:53, Epoch: 98, Batch: 810, Training Loss: 0.040396498888731, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:53, Epoch: 98, Batch: 820, Training Loss: 0.06866482570767403, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:54, Epoch: 98, Batch: 830, Training Loss: 0.053182366862893106, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:55, Epoch: 98, Batch: 840, Training Loss: 0.03338935188949108, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:56, Epoch: 98, Batch: 850, Training Loss: 0.048860258981585504, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:56, Epoch: 98, Batch: 860, Training Loss: 0.032158590853214264, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:57, Epoch: 98, Batch: 870, Training Loss: 0.02631015405058861, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:58, Epoch: 98, Batch: 880, Training Loss: 0.0578142024576664, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:58, Epoch: 98, Batch: 890, Training Loss: 0.031939854472875596, LR: 0.00010000000000000003
Time, 2019-01-01T19:19:59, Epoch: 98, Batch: 900, Training Loss: 0.019427621364593507, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:00, Epoch: 98, Batch: 910, Training Loss: 0.03741295412182808, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:01, Epoch: 98, Batch: 920, Training Loss: 0.03196587227284908, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:01, Epoch: 98, Batch: 930, Training Loss: 0.036148430034518245, LR: 0.00010000000000000003
Epoch: 98, Validation Top 1 acc: 98.8789291381836
Epoch: 98, Validation Top 5 acc: 99.99000549316406
Epoch: 98, Validation Set Loss: 0.04095226898789406
Start training epoch 99
Time, 2019-01-01T19:20:29, Epoch: 99, Batch: 10, Training Loss: 0.04337602704763412, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:30, Epoch: 99, Batch: 20, Training Loss: 0.03469328135251999, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:31, Epoch: 99, Batch: 30, Training Loss: 0.03676958568394184, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:32, Epoch: 99, Batch: 40, Training Loss: 0.04219743423163891, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:32, Epoch: 99, Batch: 50, Training Loss: 0.021823053434491157, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:33, Epoch: 99, Batch: 60, Training Loss: 0.03576430045068264, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:34, Epoch: 99, Batch: 70, Training Loss: 0.04912812188267708, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:34, Epoch: 99, Batch: 80, Training Loss: 0.04595692157745361, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:35, Epoch: 99, Batch: 90, Training Loss: 0.03534097671508789, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:36, Epoch: 99, Batch: 100, Training Loss: 0.039151716977357864, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:37, Epoch: 99, Batch: 110, Training Loss: 0.04521111063659191, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:37, Epoch: 99, Batch: 120, Training Loss: 0.03065302185714245, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:38, Epoch: 99, Batch: 130, Training Loss: 0.039101407304406165, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:39, Epoch: 99, Batch: 140, Training Loss: 0.05540550649166107, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:39, Epoch: 99, Batch: 150, Training Loss: 0.03364725299179554, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:40, Epoch: 99, Batch: 160, Training Loss: 0.050441556796431544, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:41, Epoch: 99, Batch: 170, Training Loss: 0.030307164043188096, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:42, Epoch: 99, Batch: 180, Training Loss: 0.038630923628807066, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:42, Epoch: 99, Batch: 190, Training Loss: 0.03162873238325119, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:43, Epoch: 99, Batch: 200, Training Loss: 0.04052932932972908, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:44, Epoch: 99, Batch: 210, Training Loss: 0.04534540362656116, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:44, Epoch: 99, Batch: 220, Training Loss: 0.043799753114581105, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:45, Epoch: 99, Batch: 230, Training Loss: 0.04270606748759746, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:46, Epoch: 99, Batch: 240, Training Loss: 0.039178040251135825, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:47, Epoch: 99, Batch: 250, Training Loss: 0.03774164505302906, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:47, Epoch: 99, Batch: 260, Training Loss: 0.047952033951878546, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:48, Epoch: 99, Batch: 270, Training Loss: 0.04708705395460129, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:49, Epoch: 99, Batch: 280, Training Loss: 0.040130230411887166, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:50, Epoch: 99, Batch: 290, Training Loss: 0.04936199747025967, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:50, Epoch: 99, Batch: 300, Training Loss: 0.04199696183204651, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:51, Epoch: 99, Batch: 310, Training Loss: 0.03413071259856224, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:52, Epoch: 99, Batch: 320, Training Loss: 0.05260810032486916, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:52, Epoch: 99, Batch: 330, Training Loss: 0.04769769310951233, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:53, Epoch: 99, Batch: 340, Training Loss: 0.056308959424495694, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:54, Epoch: 99, Batch: 350, Training Loss: 0.03294587507843971, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:55, Epoch: 99, Batch: 360, Training Loss: 0.031209857016801835, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:55, Epoch: 99, Batch: 370, Training Loss: 0.03984764963388443, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:56, Epoch: 99, Batch: 380, Training Loss: 0.02546043209731579, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:57, Epoch: 99, Batch: 390, Training Loss: 0.02631691247224808, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:57, Epoch: 99, Batch: 400, Training Loss: 0.04421636275947094, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:58, Epoch: 99, Batch: 410, Training Loss: 0.027045655995607376, LR: 0.00010000000000000003
Time, 2019-01-01T19:20:59, Epoch: 99, Batch: 420, Training Loss: 0.05378307476639747, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:00, Epoch: 99, Batch: 430, Training Loss: 0.06020934879779816, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:00, Epoch: 99, Batch: 440, Training Loss: 0.03727343045175076, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:01, Epoch: 99, Batch: 450, Training Loss: 0.040854868665337565, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:02, Epoch: 99, Batch: 460, Training Loss: 0.03103974424302578, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:02, Epoch: 99, Batch: 470, Training Loss: 0.04075066782534122, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:03, Epoch: 99, Batch: 480, Training Loss: 0.04145846664905548, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:04, Epoch: 99, Batch: 490, Training Loss: 0.02207299917936325, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:05, Epoch: 99, Batch: 500, Training Loss: 0.04534982442855835, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:05, Epoch: 99, Batch: 510, Training Loss: 0.03885259069502354, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:06, Epoch: 99, Batch: 520, Training Loss: 0.046667379513382914, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:07, Epoch: 99, Batch: 530, Training Loss: 0.04749397486448288, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:07, Epoch: 99, Batch: 540, Training Loss: 0.053233249485492705, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:08, Epoch: 99, Batch: 550, Training Loss: 0.04265731126070023, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:09, Epoch: 99, Batch: 560, Training Loss: 0.030297376587986945, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:10, Epoch: 99, Batch: 570, Training Loss: 0.035953367874026296, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:10, Epoch: 99, Batch: 580, Training Loss: 0.04525975808501244, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:11, Epoch: 99, Batch: 590, Training Loss: 0.04037435427308082, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:12, Epoch: 99, Batch: 600, Training Loss: 0.04997370839118957, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:13, Epoch: 99, Batch: 610, Training Loss: 0.046329493448138234, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:13, Epoch: 99, Batch: 620, Training Loss: 0.03145438879728317, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:14, Epoch: 99, Batch: 630, Training Loss: 0.03865238204598427, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:15, Epoch: 99, Batch: 640, Training Loss: 0.05336909592151642, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:15, Epoch: 99, Batch: 650, Training Loss: 0.04434230588376522, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:16, Epoch: 99, Batch: 660, Training Loss: 0.03950086943805218, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:17, Epoch: 99, Batch: 670, Training Loss: 0.032874253764748576, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:18, Epoch: 99, Batch: 680, Training Loss: 0.03203511014580727, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:18, Epoch: 99, Batch: 690, Training Loss: 0.04372565820813179, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:19, Epoch: 99, Batch: 700, Training Loss: 0.0361389271914959, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:20, Epoch: 99, Batch: 710, Training Loss: 0.04121722877025604, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:20, Epoch: 99, Batch: 720, Training Loss: 0.03357670567929745, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:21, Epoch: 99, Batch: 730, Training Loss: 0.05294826477766037, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:22, Epoch: 99, Batch: 740, Training Loss: 0.022292817011475563, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:23, Epoch: 99, Batch: 750, Training Loss: 0.03899743780493736, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:23, Epoch: 99, Batch: 760, Training Loss: 0.047347329556941986, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:24, Epoch: 99, Batch: 770, Training Loss: 0.02301902659237385, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:25, Epoch: 99, Batch: 780, Training Loss: 0.048989488184452055, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:25, Epoch: 99, Batch: 790, Training Loss: 0.045310941711068156, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:26, Epoch: 99, Batch: 800, Training Loss: 0.060170800983905794, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:27, Epoch: 99, Batch: 810, Training Loss: 0.039168549701571465, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:28, Epoch: 99, Batch: 820, Training Loss: 0.034731216728687286, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:28, Epoch: 99, Batch: 830, Training Loss: 0.05335945971310139, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:29, Epoch: 99, Batch: 840, Training Loss: 0.04419821575284004, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:30, Epoch: 99, Batch: 850, Training Loss: 0.0314235508441925, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:30, Epoch: 99, Batch: 860, Training Loss: 0.04352797418832779, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:31, Epoch: 99, Batch: 870, Training Loss: 0.051326562836766246, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:32, Epoch: 99, Batch: 880, Training Loss: 0.04652757942676544, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:33, Epoch: 99, Batch: 890, Training Loss: 0.03800518475472927, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:33, Epoch: 99, Batch: 900, Training Loss: 0.04421551041305065, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:34, Epoch: 99, Batch: 910, Training Loss: 0.04783236011862755, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:35, Epoch: 99, Batch: 920, Training Loss: 0.03314107768237591, LR: 0.00010000000000000003
Time, 2019-01-01T19:21:35, Epoch: 99, Batch: 930, Training Loss: 0.05724332630634308, LR: 0.00010000000000000003
Epoch: 99, Validation Top 1 acc: 98.8789291381836
Epoch: 99, Validation Top 5 acc: 99.99000549316406
Epoch: 99, Validation Set Loss: 0.041013557463884354
Start training epoch 100
Time, 2019-01-01T19:22:03, Epoch: 100, Batch: 10, Training Loss: 0.03546038046479225, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:04, Epoch: 100, Batch: 20, Training Loss: 0.02702750749886036, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:05, Epoch: 100, Batch: 30, Training Loss: 0.029071833938360214, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:05, Epoch: 100, Batch: 40, Training Loss: 0.032961097359657285, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:06, Epoch: 100, Batch: 50, Training Loss: 0.04744840487837791, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:07, Epoch: 100, Batch: 60, Training Loss: 0.03688819482922554, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:08, Epoch: 100, Batch: 70, Training Loss: 0.045258273184299466, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:08, Epoch: 100, Batch: 80, Training Loss: 0.027822837978601456, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:09, Epoch: 100, Batch: 90, Training Loss: 0.03981155753135681, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:10, Epoch: 100, Batch: 100, Training Loss: 0.04727906510233879, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:10, Epoch: 100, Batch: 110, Training Loss: 0.04838964082300663, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:11, Epoch: 100, Batch: 120, Training Loss: 0.045929769054055214, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:12, Epoch: 100, Batch: 130, Training Loss: 0.05366462543606758, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:13, Epoch: 100, Batch: 140, Training Loss: 0.03326890617609024, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:13, Epoch: 100, Batch: 150, Training Loss: 0.051072650402784345, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:14, Epoch: 100, Batch: 160, Training Loss: 0.04290277436375618, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:15, Epoch: 100, Batch: 170, Training Loss: 0.025919569283723833, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:16, Epoch: 100, Batch: 180, Training Loss: 0.04572941511869431, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:16, Epoch: 100, Batch: 190, Training Loss: 0.039256719127297404, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:17, Epoch: 100, Batch: 200, Training Loss: 0.0531923770904541, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:18, Epoch: 100, Batch: 210, Training Loss: 0.026812975108623505, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:18, Epoch: 100, Batch: 220, Training Loss: 0.0522538922727108, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:19, Epoch: 100, Batch: 230, Training Loss: 0.0449072003364563, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:20, Epoch: 100, Batch: 240, Training Loss: 0.043279720842838286, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:21, Epoch: 100, Batch: 250, Training Loss: 0.047345945984125136, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:21, Epoch: 100, Batch: 260, Training Loss: 0.050171701982617375, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:22, Epoch: 100, Batch: 270, Training Loss: 0.027477701008319855, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:23, Epoch: 100, Batch: 280, Training Loss: 0.034062277525663376, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:23, Epoch: 100, Batch: 290, Training Loss: 0.05344686694443226, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:24, Epoch: 100, Batch: 300, Training Loss: 0.042073092237114904, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:25, Epoch: 100, Batch: 310, Training Loss: 0.037712917849421504, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:26, Epoch: 100, Batch: 320, Training Loss: 0.03644064292311668, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:26, Epoch: 100, Batch: 330, Training Loss: 0.05435447692871094, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:27, Epoch: 100, Batch: 340, Training Loss: 0.03433417454361916, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:28, Epoch: 100, Batch: 350, Training Loss: 0.03583595454692841, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:28, Epoch: 100, Batch: 360, Training Loss: 0.0321378581225872, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:29, Epoch: 100, Batch: 370, Training Loss: 0.03490614220499992, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:30, Epoch: 100, Batch: 380, Training Loss: 0.032945653423666954, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:31, Epoch: 100, Batch: 390, Training Loss: 0.052482080459594724, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:31, Epoch: 100, Batch: 400, Training Loss: 0.03736275061964989, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:32, Epoch: 100, Batch: 410, Training Loss: 0.0537695586681366, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:33, Epoch: 100, Batch: 420, Training Loss: 0.04479750096797943, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:34, Epoch: 100, Batch: 430, Training Loss: 0.03168957345187664, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:34, Epoch: 100, Batch: 440, Training Loss: 0.05028584599494934, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:35, Epoch: 100, Batch: 450, Training Loss: 0.0459549468010664, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:36, Epoch: 100, Batch: 460, Training Loss: 0.058669842407107355, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:36, Epoch: 100, Batch: 470, Training Loss: 0.04458165280520916, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:37, Epoch: 100, Batch: 480, Training Loss: 0.05077551975846291, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:38, Epoch: 100, Batch: 490, Training Loss: 0.041993673890829086, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:39, Epoch: 100, Batch: 500, Training Loss: 0.03495688214898109, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:39, Epoch: 100, Batch: 510, Training Loss: 0.031559991836547854, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:40, Epoch: 100, Batch: 520, Training Loss: 0.03297824785113335, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:41, Epoch: 100, Batch: 530, Training Loss: 0.03994280621409416, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:41, Epoch: 100, Batch: 540, Training Loss: 0.06188973113894462, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:42, Epoch: 100, Batch: 550, Training Loss: 0.04095874987542629, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:43, Epoch: 100, Batch: 560, Training Loss: 0.06730535998940468, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:44, Epoch: 100, Batch: 570, Training Loss: 0.048657133057713506, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:44, Epoch: 100, Batch: 580, Training Loss: 0.034778973832726476, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:45, Epoch: 100, Batch: 590, Training Loss: 0.05149298757314682, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:46, Epoch: 100, Batch: 600, Training Loss: 0.0322800301015377, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:47, Epoch: 100, Batch: 610, Training Loss: 0.06408419013023377, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:47, Epoch: 100, Batch: 620, Training Loss: 0.05096641406416893, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:48, Epoch: 100, Batch: 630, Training Loss: 0.03705041892826557, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:49, Epoch: 100, Batch: 640, Training Loss: 0.041623755171895024, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:49, Epoch: 100, Batch: 650, Training Loss: 0.053504638373851776, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:50, Epoch: 100, Batch: 660, Training Loss: 0.03300192430615425, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:51, Epoch: 100, Batch: 670, Training Loss: 0.033745432272553444, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:52, Epoch: 100, Batch: 680, Training Loss: 0.03158475197851658, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:52, Epoch: 100, Batch: 690, Training Loss: 0.039075064659118655, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:53, Epoch: 100, Batch: 700, Training Loss: 0.03200877495110035, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:54, Epoch: 100, Batch: 710, Training Loss: 0.02654997892677784, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:54, Epoch: 100, Batch: 720, Training Loss: 0.04176084473729134, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:55, Epoch: 100, Batch: 730, Training Loss: 0.050024450197815894, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:56, Epoch: 100, Batch: 740, Training Loss: 0.029663747176527977, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:57, Epoch: 100, Batch: 750, Training Loss: 0.04165053479373455, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:57, Epoch: 100, Batch: 760, Training Loss: 0.04322931058704853, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:58, Epoch: 100, Batch: 770, Training Loss: 0.044326229766011235, LR: 0.00010000000000000003
Time, 2019-01-01T19:22:59, Epoch: 100, Batch: 780, Training Loss: 0.03374650403857231, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:00, Epoch: 100, Batch: 790, Training Loss: 0.023346227407455445, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:00, Epoch: 100, Batch: 800, Training Loss: 0.04645023792982102, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:01, Epoch: 100, Batch: 810, Training Loss: 0.03687259703874588, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:02, Epoch: 100, Batch: 820, Training Loss: 0.04176875911653042, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:02, Epoch: 100, Batch: 830, Training Loss: 0.05604202039539814, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:03, Epoch: 100, Batch: 840, Training Loss: 0.03350004889070988, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:04, Epoch: 100, Batch: 850, Training Loss: 0.05320263877511024, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:05, Epoch: 100, Batch: 860, Training Loss: 0.05268019661307335, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:05, Epoch: 100, Batch: 870, Training Loss: 0.03171398118138313, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:06, Epoch: 100, Batch: 880, Training Loss: 0.0300806675106287, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:07, Epoch: 100, Batch: 890, Training Loss: 0.04178715236485005, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:07, Epoch: 100, Batch: 900, Training Loss: 0.04465416595339775, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:08, Epoch: 100, Batch: 910, Training Loss: 0.02123137041926384, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:09, Epoch: 100, Batch: 920, Training Loss: 0.04333062320947647, LR: 0.00010000000000000003
Time, 2019-01-01T19:23:10, Epoch: 100, Batch: 930, Training Loss: 0.03762252852320671, LR: 0.00010000000000000003
Epoch: 100, Validation Top 1 acc: 98.88226318359375
Epoch: 100, Validation Top 5 acc: 99.99000549316406
Epoch: 100, Validation Set Loss: 0.04099052771925926
Start training epoch 101
Time, 2019-01-01T19:48:36, Epoch: 101, Batch: 10, Training Loss: 0.023742782697081567, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:37, Epoch: 101, Batch: 20, Training Loss: 0.043402180820703504, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:37, Epoch: 101, Batch: 30, Training Loss: 0.04664214551448822, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:38, Epoch: 101, Batch: 40, Training Loss: 0.036055267602205274, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:39, Epoch: 101, Batch: 50, Training Loss: 0.058742758631706235, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:40, Epoch: 101, Batch: 60, Training Loss: 0.03993899635970592, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:40, Epoch: 101, Batch: 70, Training Loss: 0.0317904956638813, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:41, Epoch: 101, Batch: 80, Training Loss: 0.04035585261881351, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:42, Epoch: 101, Batch: 90, Training Loss: 0.04320930503308773, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:42, Epoch: 101, Batch: 100, Training Loss: 0.05162506885826588, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:43, Epoch: 101, Batch: 110, Training Loss: 0.04138206243515015, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:44, Epoch: 101, Batch: 120, Training Loss: 0.054447613656520844, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:45, Epoch: 101, Batch: 130, Training Loss: 0.039218387380242345, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:45, Epoch: 101, Batch: 140, Training Loss: 0.07100254446268081, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:46, Epoch: 101, Batch: 150, Training Loss: 0.04125197157263756, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:47, Epoch: 101, Batch: 160, Training Loss: 0.04724207259714604, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:48, Epoch: 101, Batch: 170, Training Loss: 0.025291458517313004, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:48, Epoch: 101, Batch: 180, Training Loss: 0.03771073594689369, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:49, Epoch: 101, Batch: 190, Training Loss: 0.03131569102406502, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:50, Epoch: 101, Batch: 200, Training Loss: 0.034714553877711295, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:51, Epoch: 101, Batch: 210, Training Loss: 0.03482120297849178, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:51, Epoch: 101, Batch: 220, Training Loss: 0.034193172678351404, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:52, Epoch: 101, Batch: 230, Training Loss: 0.05084705427289009, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:53, Epoch: 101, Batch: 240, Training Loss: 0.04312545470893383, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:54, Epoch: 101, Batch: 250, Training Loss: 0.03854773938655853, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:54, Epoch: 101, Batch: 260, Training Loss: 0.051668021082878116, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:55, Epoch: 101, Batch: 270, Training Loss: 0.041635560989379886, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:56, Epoch: 101, Batch: 280, Training Loss: 0.037068961560726164, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:57, Epoch: 101, Batch: 290, Training Loss: 0.04250093847513199, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:57, Epoch: 101, Batch: 300, Training Loss: 0.034412865340709684, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:58, Epoch: 101, Batch: 310, Training Loss: 0.026319383457303048, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:59, Epoch: 101, Batch: 320, Training Loss: 0.03138615936040878, LR: 0.00010000000000000003
Time, 2019-01-01T19:48:59, Epoch: 101, Batch: 330, Training Loss: 0.039097030833363536, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:00, Epoch: 101, Batch: 340, Training Loss: 0.04088085703551769, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:01, Epoch: 101, Batch: 350, Training Loss: 0.022819175198674203, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:02, Epoch: 101, Batch: 360, Training Loss: 0.05443450696766376, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:03, Epoch: 101, Batch: 370, Training Loss: 0.038055671751499175, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:03, Epoch: 101, Batch: 380, Training Loss: 0.03996545672416687, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:04, Epoch: 101, Batch: 390, Training Loss: 0.04254600591957569, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:05, Epoch: 101, Batch: 400, Training Loss: 0.0453035831451416, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:05, Epoch: 101, Batch: 410, Training Loss: 0.03229320198297501, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:06, Epoch: 101, Batch: 420, Training Loss: 0.03914132229983806, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:07, Epoch: 101, Batch: 430, Training Loss: 0.036374807357788086, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:08, Epoch: 101, Batch: 440, Training Loss: 0.030266471207141876, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:09, Epoch: 101, Batch: 450, Training Loss: 0.03085060976445675, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:09, Epoch: 101, Batch: 460, Training Loss: 0.03331274166703224, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:10, Epoch: 101, Batch: 470, Training Loss: 0.0490561455488205, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:11, Epoch: 101, Batch: 480, Training Loss: 0.045209828019142154, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:11, Epoch: 101, Batch: 490, Training Loss: 0.04198708981275558, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:12, Epoch: 101, Batch: 500, Training Loss: 0.03303452394902706, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:13, Epoch: 101, Batch: 510, Training Loss: 0.047554518282413485, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:14, Epoch: 101, Batch: 520, Training Loss: 0.0359190683811903, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:14, Epoch: 101, Batch: 530, Training Loss: 0.03937127701938152, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:15, Epoch: 101, Batch: 540, Training Loss: 0.022137392684817313, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:16, Epoch: 101, Batch: 550, Training Loss: 0.061842259019613266, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:17, Epoch: 101, Batch: 560, Training Loss: 0.02549019455909729, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:17, Epoch: 101, Batch: 570, Training Loss: 0.0582524161785841, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:18, Epoch: 101, Batch: 580, Training Loss: 0.03970032297074795, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:19, Epoch: 101, Batch: 590, Training Loss: 0.051650528237223624, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:20, Epoch: 101, Batch: 600, Training Loss: 0.03985106535255909, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:21, Epoch: 101, Batch: 610, Training Loss: 0.032423398643732074, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:22, Epoch: 101, Batch: 620, Training Loss: 0.049541836604475975, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:23, Epoch: 101, Batch: 630, Training Loss: 0.041133633628487584, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:24, Epoch: 101, Batch: 640, Training Loss: 0.07973639108240604, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:25, Epoch: 101, Batch: 650, Training Loss: 0.04425374865531921, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:25, Epoch: 101, Batch: 660, Training Loss: 0.044171012938022614, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:26, Epoch: 101, Batch: 670, Training Loss: 0.03374320380389691, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:27, Epoch: 101, Batch: 680, Training Loss: 0.04223686642944813, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:28, Epoch: 101, Batch: 690, Training Loss: 0.05997116416692734, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:30, Epoch: 101, Batch: 700, Training Loss: 0.02706793397665024, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:30, Epoch: 101, Batch: 710, Training Loss: 0.02950763553380966, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:31, Epoch: 101, Batch: 720, Training Loss: 0.031782492995262146, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:32, Epoch: 101, Batch: 730, Training Loss: 0.05037876330316067, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:33, Epoch: 101, Batch: 740, Training Loss: 0.057340686395764354, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:34, Epoch: 101, Batch: 750, Training Loss: 0.04974299110472202, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:34, Epoch: 101, Batch: 760, Training Loss: 0.03730942755937576, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:35, Epoch: 101, Batch: 770, Training Loss: 0.04924016147851944, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:36, Epoch: 101, Batch: 780, Training Loss: 0.04421909227967262, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:37, Epoch: 101, Batch: 790, Training Loss: 0.03346489518880844, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:37, Epoch: 101, Batch: 800, Training Loss: 0.043649404495954516, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:38, Epoch: 101, Batch: 810, Training Loss: 0.05101534649729729, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:39, Epoch: 101, Batch: 820, Training Loss: 0.03519183062016964, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:40, Epoch: 101, Batch: 830, Training Loss: 0.04126977548003197, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:40, Epoch: 101, Batch: 840, Training Loss: 0.029740283638238905, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:41, Epoch: 101, Batch: 850, Training Loss: 0.036961707100272176, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:42, Epoch: 101, Batch: 860, Training Loss: 0.03574302010238171, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:43, Epoch: 101, Batch: 870, Training Loss: 0.03263838067650795, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:43, Epoch: 101, Batch: 880, Training Loss: 0.050564758479595184, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:44, Epoch: 101, Batch: 890, Training Loss: 0.035104737430810926, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:45, Epoch: 101, Batch: 900, Training Loss: 0.04146266393363476, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:46, Epoch: 101, Batch: 910, Training Loss: 0.045346430689096454, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:47, Epoch: 101, Batch: 920, Training Loss: 0.04148345328867435, LR: 0.00010000000000000003
Time, 2019-01-01T19:49:47, Epoch: 101, Batch: 930, Training Loss: 0.04871345907449722, LR: 0.00010000000000000003
Epoch: 101, Validation Top 1 acc: 98.8939208984375
Epoch: 101, Validation Top 5 acc: 99.99166870117188
Epoch: 101, Validation Set Loss: 0.04094843938946724
Start training epoch 102
Time, 2019-01-01T19:50:16, Epoch: 102, Batch: 10, Training Loss: 0.032293112203478816, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:17, Epoch: 102, Batch: 20, Training Loss: 0.041170758008956906, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:18, Epoch: 102, Batch: 30, Training Loss: 0.03290879763662815, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:19, Epoch: 102, Batch: 40, Training Loss: 0.04391719959676266, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:19, Epoch: 102, Batch: 50, Training Loss: 0.027815845608711243, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:20, Epoch: 102, Batch: 60, Training Loss: 0.03735367171466351, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:21, Epoch: 102, Batch: 70, Training Loss: 0.034081284701824185, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:22, Epoch: 102, Batch: 80, Training Loss: 0.06827775686979294, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:23, Epoch: 102, Batch: 90, Training Loss: 0.02531767152249813, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:23, Epoch: 102, Batch: 100, Training Loss: 0.06708652712404728, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:24, Epoch: 102, Batch: 110, Training Loss: 0.045967945083975795, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:25, Epoch: 102, Batch: 120, Training Loss: 0.036653246358036996, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:26, Epoch: 102, Batch: 130, Training Loss: 0.0550297848880291, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:26, Epoch: 102, Batch: 140, Training Loss: 0.03425665274262428, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:27, Epoch: 102, Batch: 150, Training Loss: 0.04805022105574608, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:28, Epoch: 102, Batch: 160, Training Loss: 0.04351052641868591, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:28, Epoch: 102, Batch: 170, Training Loss: 0.03745548725128174, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:29, Epoch: 102, Batch: 180, Training Loss: 0.0401463009417057, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:30, Epoch: 102, Batch: 190, Training Loss: 0.040000219270586966, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:31, Epoch: 102, Batch: 200, Training Loss: 0.04201128594577312, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:31, Epoch: 102, Batch: 210, Training Loss: 0.05314731746912003, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:32, Epoch: 102, Batch: 220, Training Loss: 0.028005889803171157, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:33, Epoch: 102, Batch: 230, Training Loss: 0.04817801415920257, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:33, Epoch: 102, Batch: 240, Training Loss: 0.03996000997722149, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:34, Epoch: 102, Batch: 250, Training Loss: 0.05807738341391087, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:35, Epoch: 102, Batch: 260, Training Loss: 0.0559196587651968, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:36, Epoch: 102, Batch: 270, Training Loss: 0.02910953015089035, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:36, Epoch: 102, Batch: 280, Training Loss: 0.0311222106218338, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:37, Epoch: 102, Batch: 290, Training Loss: 0.03609815239906311, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:38, Epoch: 102, Batch: 300, Training Loss: 0.04162878319621086, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:39, Epoch: 102, Batch: 310, Training Loss: 0.04975968450307846, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:39, Epoch: 102, Batch: 320, Training Loss: 0.054006625339388846, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:40, Epoch: 102, Batch: 330, Training Loss: 0.049707578122615816, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:41, Epoch: 102, Batch: 340, Training Loss: 0.05362420864403248, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:41, Epoch: 102, Batch: 350, Training Loss: 0.028794924542307854, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:42, Epoch: 102, Batch: 360, Training Loss: 0.06363267563283444, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:43, Epoch: 102, Batch: 370, Training Loss: 0.04545657224953174, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:44, Epoch: 102, Batch: 380, Training Loss: 0.04114540815353394, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:44, Epoch: 102, Batch: 390, Training Loss: 0.032601813599467275, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:45, Epoch: 102, Batch: 400, Training Loss: 0.05416426137089729, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:46, Epoch: 102, Batch: 410, Training Loss: 0.060263313353061676, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:47, Epoch: 102, Batch: 420, Training Loss: 0.045414718240499495, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:47, Epoch: 102, Batch: 430, Training Loss: 0.049774742871522906, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:48, Epoch: 102, Batch: 440, Training Loss: 0.04283719435334206, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:49, Epoch: 102, Batch: 450, Training Loss: 0.05090323425829411, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:49, Epoch: 102, Batch: 460, Training Loss: 0.04156751371920109, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:50, Epoch: 102, Batch: 470, Training Loss: 0.03234620913863182, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:51, Epoch: 102, Batch: 480, Training Loss: 0.03573058247566223, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:52, Epoch: 102, Batch: 490, Training Loss: 0.03965237438678741, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:52, Epoch: 102, Batch: 500, Training Loss: 0.031743576377630235, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:53, Epoch: 102, Batch: 510, Training Loss: 0.037889867648482324, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:54, Epoch: 102, Batch: 520, Training Loss: 0.03712676651775837, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:54, Epoch: 102, Batch: 530, Training Loss: 0.044151785969734195, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:55, Epoch: 102, Batch: 540, Training Loss: 0.04111328125, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:56, Epoch: 102, Batch: 550, Training Loss: 0.030566348135471343, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:57, Epoch: 102, Batch: 560, Training Loss: 0.03251400403678417, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:57, Epoch: 102, Batch: 570, Training Loss: 0.03947274088859558, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:58, Epoch: 102, Batch: 580, Training Loss: 0.0337179247289896, LR: 0.00010000000000000003
Time, 2019-01-01T19:50:59, Epoch: 102, Batch: 590, Training Loss: 0.03916601836681366, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:00, Epoch: 102, Batch: 600, Training Loss: 0.0406821247190237, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:00, Epoch: 102, Batch: 610, Training Loss: 0.02303725332021713, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:01, Epoch: 102, Batch: 620, Training Loss: 0.05264784656465053, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:02, Epoch: 102, Batch: 630, Training Loss: 0.04526043236255646, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:02, Epoch: 102, Batch: 640, Training Loss: 0.035474810376763345, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:03, Epoch: 102, Batch: 650, Training Loss: 0.018334170430898668, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:04, Epoch: 102, Batch: 660, Training Loss: 0.024757836759090424, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:05, Epoch: 102, Batch: 670, Training Loss: 0.022964862734079362, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:05, Epoch: 102, Batch: 680, Training Loss: 0.03734494484961033, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:06, Epoch: 102, Batch: 690, Training Loss: 0.03793270960450172, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:07, Epoch: 102, Batch: 700, Training Loss: 0.041431257128715517, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:08, Epoch: 102, Batch: 710, Training Loss: 0.044024805352091786, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:08, Epoch: 102, Batch: 720, Training Loss: 0.025279606506228448, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:09, Epoch: 102, Batch: 730, Training Loss: 0.0460112027823925, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:10, Epoch: 102, Batch: 740, Training Loss: 0.04272669479250908, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:11, Epoch: 102, Batch: 750, Training Loss: 0.043140888959169385, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:11, Epoch: 102, Batch: 760, Training Loss: 0.04266703799366951, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:12, Epoch: 102, Batch: 770, Training Loss: 0.051167917996644975, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:13, Epoch: 102, Batch: 780, Training Loss: 0.0343627829104662, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:13, Epoch: 102, Batch: 790, Training Loss: 0.05219069570302963, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:14, Epoch: 102, Batch: 800, Training Loss: 0.03166402690112591, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:15, Epoch: 102, Batch: 810, Training Loss: 0.03224675431847572, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:16, Epoch: 102, Batch: 820, Training Loss: 0.0484319843351841, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:16, Epoch: 102, Batch: 830, Training Loss: 0.03495400100946426, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:17, Epoch: 102, Batch: 840, Training Loss: 0.022486815601587294, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:18, Epoch: 102, Batch: 850, Training Loss: 0.05006976872682571, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:18, Epoch: 102, Batch: 860, Training Loss: 0.039562496915459634, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:19, Epoch: 102, Batch: 870, Training Loss: 0.029572136327624322, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:20, Epoch: 102, Batch: 880, Training Loss: 0.0411975059658289, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:21, Epoch: 102, Batch: 890, Training Loss: 0.04277340546250343, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:22, Epoch: 102, Batch: 900, Training Loss: 0.05615394562482834, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:22, Epoch: 102, Batch: 910, Training Loss: 0.04602500982582569, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:23, Epoch: 102, Batch: 920, Training Loss: 0.06020587719976902, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:24, Epoch: 102, Batch: 930, Training Loss: 0.03836205899715424, LR: 0.00010000000000000003
Epoch: 102, Validation Top 1 acc: 98.87726593017578
Epoch: 102, Validation Top 5 acc: 99.99000549316406
Epoch: 102, Validation Set Loss: 0.0409807413816452
Start training epoch 103
Time, 2019-01-01T19:51:53, Epoch: 103, Batch: 10, Training Loss: 0.05525592863559723, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:54, Epoch: 103, Batch: 20, Training Loss: 0.04433397985994816, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:54, Epoch: 103, Batch: 30, Training Loss: 0.040760792791843414, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:55, Epoch: 103, Batch: 40, Training Loss: 0.023344896361231805, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:56, Epoch: 103, Batch: 50, Training Loss: 0.03384391665458679, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:57, Epoch: 103, Batch: 60, Training Loss: 0.04290651455521584, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:57, Epoch: 103, Batch: 70, Training Loss: 0.044444179907441136, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:58, Epoch: 103, Batch: 80, Training Loss: 0.052072925865650176, LR: 0.00010000000000000003
Time, 2019-01-01T19:51:59, Epoch: 103, Batch: 90, Training Loss: 0.04321285411715507, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:00, Epoch: 103, Batch: 100, Training Loss: 0.02319685071706772, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:00, Epoch: 103, Batch: 110, Training Loss: 0.033825723081827165, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:01, Epoch: 103, Batch: 120, Training Loss: 0.03816131167113781, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:02, Epoch: 103, Batch: 130, Training Loss: 0.04597205221652985, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:03, Epoch: 103, Batch: 140, Training Loss: 0.042100349068641664, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:03, Epoch: 103, Batch: 150, Training Loss: 0.05525136962532997, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:04, Epoch: 103, Batch: 160, Training Loss: 0.03849704600870609, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:05, Epoch: 103, Batch: 170, Training Loss: 0.030425600707530975, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:05, Epoch: 103, Batch: 180, Training Loss: 0.05018053948879242, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:06, Epoch: 103, Batch: 190, Training Loss: 0.06036454066634178, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:07, Epoch: 103, Batch: 200, Training Loss: 0.04734936617314815, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:08, Epoch: 103, Batch: 210, Training Loss: 0.03986933790147305, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:08, Epoch: 103, Batch: 220, Training Loss: 0.042877963930368426, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:09, Epoch: 103, Batch: 230, Training Loss: 0.04810865931212902, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:10, Epoch: 103, Batch: 240, Training Loss: 0.03133973255753517, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:11, Epoch: 103, Batch: 250, Training Loss: 0.029609492421150206, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:11, Epoch: 103, Batch: 260, Training Loss: 0.04533431008458137, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:12, Epoch: 103, Batch: 270, Training Loss: 0.05078873112797737, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:13, Epoch: 103, Batch: 280, Training Loss: 0.040487752109766004, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:14, Epoch: 103, Batch: 290, Training Loss: 0.0325478620827198, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:14, Epoch: 103, Batch: 300, Training Loss: 0.044823525846004485, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:15, Epoch: 103, Batch: 310, Training Loss: 0.03962463662028313, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:16, Epoch: 103, Batch: 320, Training Loss: 0.04414106830954552, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:16, Epoch: 103, Batch: 330, Training Loss: 0.048294855281710625, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:17, Epoch: 103, Batch: 340, Training Loss: 0.041802532970905304, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:18, Epoch: 103, Batch: 350, Training Loss: 0.030694029107689858, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:19, Epoch: 103, Batch: 360, Training Loss: 0.04952046647667885, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:19, Epoch: 103, Batch: 370, Training Loss: 0.03187335506081581, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:20, Epoch: 103, Batch: 380, Training Loss: 0.04370700195431709, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:21, Epoch: 103, Batch: 390, Training Loss: 0.034894564747810365, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:22, Epoch: 103, Batch: 400, Training Loss: 0.03067108877003193, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:22, Epoch: 103, Batch: 410, Training Loss: 0.03486394137144089, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:23, Epoch: 103, Batch: 420, Training Loss: 0.035367999225854874, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:24, Epoch: 103, Batch: 430, Training Loss: 0.04545337781310081, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:24, Epoch: 103, Batch: 440, Training Loss: 0.0594038650393486, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:25, Epoch: 103, Batch: 450, Training Loss: 0.048933638632297514, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:26, Epoch: 103, Batch: 460, Training Loss: 0.028498012572526932, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:27, Epoch: 103, Batch: 470, Training Loss: 0.04139437191188335, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:27, Epoch: 103, Batch: 480, Training Loss: 0.05522572323679924, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:28, Epoch: 103, Batch: 490, Training Loss: 0.03731528222560883, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:29, Epoch: 103, Batch: 500, Training Loss: 0.0297512236982584, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:30, Epoch: 103, Batch: 510, Training Loss: 0.03827434703707695, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:30, Epoch: 103, Batch: 520, Training Loss: 0.022913455218076705, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:31, Epoch: 103, Batch: 530, Training Loss: 0.0440248616039753, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:32, Epoch: 103, Batch: 540, Training Loss: 0.03715486079454422, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:33, Epoch: 103, Batch: 550, Training Loss: 0.04253482967615128, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:33, Epoch: 103, Batch: 560, Training Loss: 0.031878304481506345, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:34, Epoch: 103, Batch: 570, Training Loss: 0.029037166759371756, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:35, Epoch: 103, Batch: 580, Training Loss: 0.03866283297538757, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:36, Epoch: 103, Batch: 590, Training Loss: 0.031991332024335864, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:36, Epoch: 103, Batch: 600, Training Loss: 0.027776696905493736, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:37, Epoch: 103, Batch: 610, Training Loss: 0.06861677318811417, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:38, Epoch: 103, Batch: 620, Training Loss: 0.02816873788833618, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:38, Epoch: 103, Batch: 630, Training Loss: 0.04305689334869385, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:39, Epoch: 103, Batch: 640, Training Loss: 0.044894351810216906, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:40, Epoch: 103, Batch: 650, Training Loss: 0.052801124006509784, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:41, Epoch: 103, Batch: 660, Training Loss: 0.03023630902171135, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:41, Epoch: 103, Batch: 670, Training Loss: 0.04525626972317696, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:42, Epoch: 103, Batch: 680, Training Loss: 0.044909586757421495, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:43, Epoch: 103, Batch: 690, Training Loss: 0.058112065121531484, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:44, Epoch: 103, Batch: 700, Training Loss: 0.028381406515836715, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:44, Epoch: 103, Batch: 710, Training Loss: 0.034483646228909495, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:45, Epoch: 103, Batch: 720, Training Loss: 0.02903531678020954, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:46, Epoch: 103, Batch: 730, Training Loss: 0.03676796220242977, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:46, Epoch: 103, Batch: 740, Training Loss: 0.048913581669330596, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:47, Epoch: 103, Batch: 750, Training Loss: 0.03696683794260025, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:48, Epoch: 103, Batch: 760, Training Loss: 0.04208065792918205, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:49, Epoch: 103, Batch: 770, Training Loss: 0.034285316988825795, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:50, Epoch: 103, Batch: 780, Training Loss: 0.03799743950366974, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:50, Epoch: 103, Batch: 790, Training Loss: 0.05171673521399498, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:51, Epoch: 103, Batch: 800, Training Loss: 0.03196409344673157, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:52, Epoch: 103, Batch: 810, Training Loss: 0.04024139679968357, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:53, Epoch: 103, Batch: 820, Training Loss: 0.03504604734480381, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:53, Epoch: 103, Batch: 830, Training Loss: 0.04645738266408443, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:54, Epoch: 103, Batch: 840, Training Loss: 0.043229322880506516, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:55, Epoch: 103, Batch: 850, Training Loss: 0.03705205544829369, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:55, Epoch: 103, Batch: 860, Training Loss: 0.06226966828107834, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:56, Epoch: 103, Batch: 870, Training Loss: 0.04450986124575138, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:57, Epoch: 103, Batch: 880, Training Loss: 0.05550471544265747, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:58, Epoch: 103, Batch: 890, Training Loss: 0.03731741607189178, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:58, Epoch: 103, Batch: 900, Training Loss: 0.05143573433160782, LR: 0.00010000000000000003
Time, 2019-01-01T19:52:59, Epoch: 103, Batch: 910, Training Loss: 0.04190410636365414, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:00, Epoch: 103, Batch: 920, Training Loss: 0.044048289954662326, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:01, Epoch: 103, Batch: 930, Training Loss: 0.02976815402507782, LR: 0.00010000000000000003
Epoch: 103, Validation Top 1 acc: 98.8939208984375
Epoch: 103, Validation Top 5 acc: 99.99166870117188
Epoch: 103, Validation Set Loss: 0.04090780019760132
Start training epoch 104
Time, 2019-01-01T19:53:30, Epoch: 104, Batch: 10, Training Loss: 0.07606326676905155, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:31, Epoch: 104, Batch: 20, Training Loss: 0.05502689406275749, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:31, Epoch: 104, Batch: 30, Training Loss: 0.03053676076233387, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:32, Epoch: 104, Batch: 40, Training Loss: 0.036706993356347084, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:33, Epoch: 104, Batch: 50, Training Loss: 0.044246864318847653, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:34, Epoch: 104, Batch: 60, Training Loss: 0.06746884956955909, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:34, Epoch: 104, Batch: 70, Training Loss: 0.056244618445634845, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:35, Epoch: 104, Batch: 80, Training Loss: 0.05298622436821461, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:36, Epoch: 104, Batch: 90, Training Loss: 0.03916046284139156, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:36, Epoch: 104, Batch: 100, Training Loss: 0.05364826656877995, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:37, Epoch: 104, Batch: 110, Training Loss: 0.04857998229563236, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:38, Epoch: 104, Batch: 120, Training Loss: 0.05976918749511242, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:39, Epoch: 104, Batch: 130, Training Loss: 0.04168900437653065, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:39, Epoch: 104, Batch: 140, Training Loss: 0.06483082883059979, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:40, Epoch: 104, Batch: 150, Training Loss: 0.03388201706111431, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:41, Epoch: 104, Batch: 160, Training Loss: 0.06430846676230431, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:42, Epoch: 104, Batch: 170, Training Loss: 0.02239253930747509, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:42, Epoch: 104, Batch: 180, Training Loss: 0.030978137999773024, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:43, Epoch: 104, Batch: 190, Training Loss: 0.04315461479127407, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:44, Epoch: 104, Batch: 200, Training Loss: 0.04498583897948265, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:44, Epoch: 104, Batch: 210, Training Loss: 0.02416992709040642, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:45, Epoch: 104, Batch: 220, Training Loss: 0.04001695774495602, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:46, Epoch: 104, Batch: 230, Training Loss: 0.06560968235135078, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:47, Epoch: 104, Batch: 240, Training Loss: 0.026514671742916107, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:47, Epoch: 104, Batch: 250, Training Loss: 0.036723009496927264, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:48, Epoch: 104, Batch: 260, Training Loss: 0.042388033121824265, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:49, Epoch: 104, Batch: 270, Training Loss: 0.0393154326826334, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:49, Epoch: 104, Batch: 280, Training Loss: 0.04507375881075859, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:50, Epoch: 104, Batch: 290, Training Loss: 0.027664394676685335, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:51, Epoch: 104, Batch: 300, Training Loss: 0.03481663577258587, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:52, Epoch: 104, Batch: 310, Training Loss: 0.03637556880712509, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:52, Epoch: 104, Batch: 320, Training Loss: 0.05148929581046104, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:53, Epoch: 104, Batch: 330, Training Loss: 0.036598659679293634, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:54, Epoch: 104, Batch: 340, Training Loss: 0.040147972479462626, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:55, Epoch: 104, Batch: 350, Training Loss: 0.04004971012473106, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:55, Epoch: 104, Batch: 360, Training Loss: 0.04486654102802277, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:56, Epoch: 104, Batch: 370, Training Loss: 0.030137930065393448, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:57, Epoch: 104, Batch: 380, Training Loss: 0.028298500180244445, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:58, Epoch: 104, Batch: 390, Training Loss: 0.04117393456399441, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:58, Epoch: 104, Batch: 400, Training Loss: 0.030950605869293213, LR: 0.00010000000000000003
Time, 2019-01-01T19:53:59, Epoch: 104, Batch: 410, Training Loss: 0.05141826532781124, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:00, Epoch: 104, Batch: 420, Training Loss: 0.032309386506676675, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:01, Epoch: 104, Batch: 430, Training Loss: 0.03060152381658554, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:01, Epoch: 104, Batch: 440, Training Loss: 0.03457423597574234, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:02, Epoch: 104, Batch: 450, Training Loss: 0.025680768489837646, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:03, Epoch: 104, Batch: 460, Training Loss: 0.0465236708521843, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:04, Epoch: 104, Batch: 470, Training Loss: 0.019754862785339354, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:04, Epoch: 104, Batch: 480, Training Loss: 0.04469287060201168, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:05, Epoch: 104, Batch: 490, Training Loss: 0.034728503599762914, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:06, Epoch: 104, Batch: 500, Training Loss: 0.043003744632005694, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:06, Epoch: 104, Batch: 510, Training Loss: 0.04161787033081055, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:07, Epoch: 104, Batch: 520, Training Loss: 0.05248784944415093, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:08, Epoch: 104, Batch: 530, Training Loss: 0.05344134718179703, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:09, Epoch: 104, Batch: 540, Training Loss: 0.037293515354394915, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:09, Epoch: 104, Batch: 550, Training Loss: 0.0308120422065258, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:10, Epoch: 104, Batch: 560, Training Loss: 0.04817303493618965, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:11, Epoch: 104, Batch: 570, Training Loss: 0.04153171107172966, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:12, Epoch: 104, Batch: 580, Training Loss: 0.039419325068593025, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:12, Epoch: 104, Batch: 590, Training Loss: 0.028085484355688094, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:13, Epoch: 104, Batch: 600, Training Loss: 0.049982161819934846, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:14, Epoch: 104, Batch: 610, Training Loss: 0.04207870177924633, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:14, Epoch: 104, Batch: 620, Training Loss: 0.02554294317960739, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:15, Epoch: 104, Batch: 630, Training Loss: 0.04668959937989712, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:16, Epoch: 104, Batch: 640, Training Loss: 0.04276311621069908, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:17, Epoch: 104, Batch: 650, Training Loss: 0.044832592085003856, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:17, Epoch: 104, Batch: 660, Training Loss: 0.04793084040284157, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:18, Epoch: 104, Batch: 670, Training Loss: 0.03862991258502006, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:19, Epoch: 104, Batch: 680, Training Loss: 0.06081293039023876, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:20, Epoch: 104, Batch: 690, Training Loss: 0.031057444214820863, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:20, Epoch: 104, Batch: 700, Training Loss: 0.05150782912969589, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:21, Epoch: 104, Batch: 710, Training Loss: 0.03444497510790825, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:22, Epoch: 104, Batch: 720, Training Loss: 0.02558874525129795, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:23, Epoch: 104, Batch: 730, Training Loss: 0.02981688119471073, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:23, Epoch: 104, Batch: 740, Training Loss: 0.03006204590201378, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:24, Epoch: 104, Batch: 750, Training Loss: 0.04042475409805775, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:25, Epoch: 104, Batch: 760, Training Loss: 0.03675782158970833, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:25, Epoch: 104, Batch: 770, Training Loss: 0.027796488255262375, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:26, Epoch: 104, Batch: 780, Training Loss: 0.0429193489253521, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:27, Epoch: 104, Batch: 790, Training Loss: 0.056157037243247034, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:28, Epoch: 104, Batch: 800, Training Loss: 0.036370063200592995, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:28, Epoch: 104, Batch: 810, Training Loss: 0.052257508412003516, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:29, Epoch: 104, Batch: 820, Training Loss: 0.02850443236529827, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:30, Epoch: 104, Batch: 830, Training Loss: 0.04764567241072655, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:31, Epoch: 104, Batch: 840, Training Loss: 0.03466496802866459, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:32, Epoch: 104, Batch: 850, Training Loss: 0.04496444314718247, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:32, Epoch: 104, Batch: 860, Training Loss: 0.03339893147349358, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:33, Epoch: 104, Batch: 870, Training Loss: 0.0316572479903698, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:34, Epoch: 104, Batch: 880, Training Loss: 0.034155746549367906, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:35, Epoch: 104, Batch: 890, Training Loss: 0.044314037263393405, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:35, Epoch: 104, Batch: 900, Training Loss: 0.035869797319173814, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:36, Epoch: 104, Batch: 910, Training Loss: 0.04368961416184902, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:37, Epoch: 104, Batch: 920, Training Loss: 0.050769511982798575, LR: 0.00010000000000000003
Time, 2019-01-01T19:54:38, Epoch: 104, Batch: 930, Training Loss: 0.02997853197157383, LR: 0.00010000000000000003
Epoch: 104, Validation Top 1 acc: 98.8789291381836
Epoch: 104, Validation Top 5 acc: 99.99000549316406
Epoch: 104, Validation Set Loss: 0.04091469570994377
Start training epoch 105
Time, 2019-01-01T19:55:07, Epoch: 105, Batch: 10, Training Loss: 0.036660774052143096, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:08, Epoch: 105, Batch: 20, Training Loss: 0.03949660584330559, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:09, Epoch: 105, Batch: 30, Training Loss: 0.03607143871486187, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:09, Epoch: 105, Batch: 40, Training Loss: 0.04938565045595169, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:10, Epoch: 105, Batch: 50, Training Loss: 0.04069560952484608, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:11, Epoch: 105, Batch: 60, Training Loss: 0.04393186792731285, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:11, Epoch: 105, Batch: 70, Training Loss: 0.04018353745341301, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:12, Epoch: 105, Batch: 80, Training Loss: 0.046649398282170296, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:13, Epoch: 105, Batch: 90, Training Loss: 0.042269694805145266, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:14, Epoch: 105, Batch: 100, Training Loss: 0.04978955313563347, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:14, Epoch: 105, Batch: 110, Training Loss: 0.03027016222476959, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:15, Epoch: 105, Batch: 120, Training Loss: 0.05171686187386513, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:16, Epoch: 105, Batch: 130, Training Loss: 0.023982197418808936, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:17, Epoch: 105, Batch: 140, Training Loss: 0.027786481752991678, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:17, Epoch: 105, Batch: 150, Training Loss: 0.028751184791326524, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:18, Epoch: 105, Batch: 160, Training Loss: 0.04167811907827854, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:19, Epoch: 105, Batch: 170, Training Loss: 0.03206481486558914, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:20, Epoch: 105, Batch: 180, Training Loss: 0.03877515122294426, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:21, Epoch: 105, Batch: 190, Training Loss: 0.046446231007575986, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:21, Epoch: 105, Batch: 200, Training Loss: 0.038704138994216916, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:22, Epoch: 105, Batch: 210, Training Loss: 0.0343551080673933, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:23, Epoch: 105, Batch: 220, Training Loss: 0.026550418138504027, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:23, Epoch: 105, Batch: 230, Training Loss: 0.037566138058900835, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:24, Epoch: 105, Batch: 240, Training Loss: 0.03532906584441662, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:25, Epoch: 105, Batch: 250, Training Loss: 0.05262642614543438, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:26, Epoch: 105, Batch: 260, Training Loss: 0.06745842769742012, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:26, Epoch: 105, Batch: 270, Training Loss: 0.04820101894438267, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:27, Epoch: 105, Batch: 280, Training Loss: 0.030681345611810684, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:28, Epoch: 105, Batch: 290, Training Loss: 0.049175909534096715, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:29, Epoch: 105, Batch: 300, Training Loss: 0.05275297239422798, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:29, Epoch: 105, Batch: 310, Training Loss: 0.04408824779093266, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:30, Epoch: 105, Batch: 320, Training Loss: 0.05824732929468155, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:31, Epoch: 105, Batch: 330, Training Loss: 0.037909602746367455, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:31, Epoch: 105, Batch: 340, Training Loss: 0.030628561228513717, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:32, Epoch: 105, Batch: 350, Training Loss: 0.05691033750772476, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:33, Epoch: 105, Batch: 360, Training Loss: 0.04207832142710686, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:34, Epoch: 105, Batch: 370, Training Loss: 0.048703161627054216, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:34, Epoch: 105, Batch: 380, Training Loss: 0.042168738692998885, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:35, Epoch: 105, Batch: 390, Training Loss: 0.043045001849532125, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:36, Epoch: 105, Batch: 400, Training Loss: 0.0370702039450407, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:37, Epoch: 105, Batch: 410, Training Loss: 0.04334619492292404, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:37, Epoch: 105, Batch: 420, Training Loss: 0.049924449622631074, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:38, Epoch: 105, Batch: 430, Training Loss: 0.0394671555608511, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:39, Epoch: 105, Batch: 440, Training Loss: 0.023838845640420915, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:39, Epoch: 105, Batch: 450, Training Loss: 0.038743631914258, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:40, Epoch: 105, Batch: 460, Training Loss: 0.04562614001333713, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:41, Epoch: 105, Batch: 470, Training Loss: 0.04251359403133392, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:42, Epoch: 105, Batch: 480, Training Loss: 0.03479026407003403, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:42, Epoch: 105, Batch: 490, Training Loss: 0.02975563295185566, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:43, Epoch: 105, Batch: 500, Training Loss: 0.032948487997055055, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:44, Epoch: 105, Batch: 510, Training Loss: 0.04838268905878067, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:45, Epoch: 105, Batch: 520, Training Loss: 0.03752608522772789, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:45, Epoch: 105, Batch: 530, Training Loss: 0.03575987182557583, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:46, Epoch: 105, Batch: 540, Training Loss: 0.04071196280419827, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:47, Epoch: 105, Batch: 550, Training Loss: 0.041372529417276385, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:47, Epoch: 105, Batch: 560, Training Loss: 0.037783296406269075, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:48, Epoch: 105, Batch: 570, Training Loss: 0.025872331112623215, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:49, Epoch: 105, Batch: 580, Training Loss: 0.026348563283681868, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:50, Epoch: 105, Batch: 590, Training Loss: 0.04521964006125927, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:50, Epoch: 105, Batch: 600, Training Loss: 0.0424322847276926, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:51, Epoch: 105, Batch: 610, Training Loss: 0.028477505221962927, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:52, Epoch: 105, Batch: 620, Training Loss: 0.04573170095682144, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:53, Epoch: 105, Batch: 630, Training Loss: 0.04233382754027844, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:53, Epoch: 105, Batch: 640, Training Loss: 0.05634442418813705, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:54, Epoch: 105, Batch: 650, Training Loss: 0.050337452441453934, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:55, Epoch: 105, Batch: 660, Training Loss: 0.04466301836073398, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:55, Epoch: 105, Batch: 670, Training Loss: 0.05522095263004303, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:56, Epoch: 105, Batch: 680, Training Loss: 0.06035493239760399, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:57, Epoch: 105, Batch: 690, Training Loss: 0.034090204536914824, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:58, Epoch: 105, Batch: 700, Training Loss: 0.032393141835927966, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:58, Epoch: 105, Batch: 710, Training Loss: 0.029954466968774796, LR: 0.00010000000000000003
Time, 2019-01-01T19:55:59, Epoch: 105, Batch: 720, Training Loss: 0.047305451706051826, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:00, Epoch: 105, Batch: 730, Training Loss: 0.05205204635858536, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:01, Epoch: 105, Batch: 740, Training Loss: 0.05105222389101982, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:01, Epoch: 105, Batch: 750, Training Loss: 0.04006192609667778, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:02, Epoch: 105, Batch: 760, Training Loss: 0.046705467626452446, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:03, Epoch: 105, Batch: 770, Training Loss: 0.04228562265634537, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:03, Epoch: 105, Batch: 780, Training Loss: 0.04207133948802948, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:04, Epoch: 105, Batch: 790, Training Loss: 0.030992211028933524, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:05, Epoch: 105, Batch: 800, Training Loss: 0.0448412086814642, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:06, Epoch: 105, Batch: 810, Training Loss: 0.0492240559309721, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:06, Epoch: 105, Batch: 820, Training Loss: 0.04503764808177948, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:07, Epoch: 105, Batch: 830, Training Loss: 0.047868703678250316, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:08, Epoch: 105, Batch: 840, Training Loss: 0.03678876906633377, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:09, Epoch: 105, Batch: 850, Training Loss: 0.03802936151623726, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:09, Epoch: 105, Batch: 860, Training Loss: 0.0372643381357193, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:10, Epoch: 105, Batch: 870, Training Loss: 0.026416511833667757, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:11, Epoch: 105, Batch: 880, Training Loss: 0.0365446113049984, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:12, Epoch: 105, Batch: 890, Training Loss: 0.0478326179087162, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:13, Epoch: 105, Batch: 900, Training Loss: 0.03301621973514557, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:13, Epoch: 105, Batch: 910, Training Loss: 0.04602156244218349, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:14, Epoch: 105, Batch: 920, Training Loss: 0.042739661782979964, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:15, Epoch: 105, Batch: 930, Training Loss: 0.042664700746536256, LR: 0.00010000000000000003
Epoch: 105, Validation Top 1 acc: 98.8855972290039
Epoch: 105, Validation Top 5 acc: 99.99000549316406
Epoch: 105, Validation Set Loss: 0.04089407995343208
Start training epoch 106
Time, 2019-01-01T19:56:45, Epoch: 106, Batch: 10, Training Loss: 0.06462255492806435, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:45, Epoch: 106, Batch: 20, Training Loss: 0.05640502534806728, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:46, Epoch: 106, Batch: 30, Training Loss: 0.04483229443430901, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:47, Epoch: 106, Batch: 40, Training Loss: 0.04132094569504261, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:48, Epoch: 106, Batch: 50, Training Loss: 0.05257765129208565, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:48, Epoch: 106, Batch: 60, Training Loss: 0.03363186344504356, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:49, Epoch: 106, Batch: 70, Training Loss: 0.04531207457184792, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:50, Epoch: 106, Batch: 80, Training Loss: 0.06971358507871628, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:51, Epoch: 106, Batch: 90, Training Loss: 0.023078983277082445, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:51, Epoch: 106, Batch: 100, Training Loss: 0.044298002123832704, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:52, Epoch: 106, Batch: 110, Training Loss: 0.02703205682337284, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:53, Epoch: 106, Batch: 120, Training Loss: 0.051011976599693296, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:54, Epoch: 106, Batch: 130, Training Loss: 0.031816371530294416, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:55, Epoch: 106, Batch: 140, Training Loss: 0.04720702692866326, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:55, Epoch: 106, Batch: 150, Training Loss: 0.04354618974030018, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:56, Epoch: 106, Batch: 160, Training Loss: 0.04409787282347679, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:57, Epoch: 106, Batch: 170, Training Loss: 0.025902021676301956, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:57, Epoch: 106, Batch: 180, Training Loss: 0.041424728184938434, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:58, Epoch: 106, Batch: 190, Training Loss: 0.0403156504034996, LR: 0.00010000000000000003
Time, 2019-01-01T19:56:59, Epoch: 106, Batch: 200, Training Loss: 0.04307032227516174, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:00, Epoch: 106, Batch: 210, Training Loss: 0.040422141924500464, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:00, Epoch: 106, Batch: 220, Training Loss: 0.03966551832854748, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:01, Epoch: 106, Batch: 230, Training Loss: 0.04230956211686134, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:02, Epoch: 106, Batch: 240, Training Loss: 0.03306005224585533, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:03, Epoch: 106, Batch: 250, Training Loss: 0.050533054023981096, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:03, Epoch: 106, Batch: 260, Training Loss: 0.04630146324634552, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:04, Epoch: 106, Batch: 270, Training Loss: 0.04003493003547191, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:05, Epoch: 106, Batch: 280, Training Loss: 0.04256086423993111, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:06, Epoch: 106, Batch: 290, Training Loss: 0.03645554855465889, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:07, Epoch: 106, Batch: 300, Training Loss: 0.028425084426999092, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:07, Epoch: 106, Batch: 310, Training Loss: 0.0449204720556736, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:08, Epoch: 106, Batch: 320, Training Loss: 0.039075514674186705, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:09, Epoch: 106, Batch: 330, Training Loss: 0.042170295119285585, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:10, Epoch: 106, Batch: 340, Training Loss: 0.031997811049222946, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:11, Epoch: 106, Batch: 350, Training Loss: 0.05017221607267856, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:11, Epoch: 106, Batch: 360, Training Loss: 0.030550632253289224, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:12, Epoch: 106, Batch: 370, Training Loss: 0.05508778616786003, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:13, Epoch: 106, Batch: 380, Training Loss: 0.032336863502860066, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:14, Epoch: 106, Batch: 390, Training Loss: 0.04652852714061737, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:14, Epoch: 106, Batch: 400, Training Loss: 0.022080754116177558, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:15, Epoch: 106, Batch: 410, Training Loss: 0.025819844007492064, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:16, Epoch: 106, Batch: 420, Training Loss: 0.043675290793180464, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:17, Epoch: 106, Batch: 430, Training Loss: 0.04994483739137649, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:17, Epoch: 106, Batch: 440, Training Loss: 0.04860810749232769, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:18, Epoch: 106, Batch: 450, Training Loss: 0.03494612127542496, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:19, Epoch: 106, Batch: 460, Training Loss: 0.03225783631205559, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:20, Epoch: 106, Batch: 470, Training Loss: 0.02879854179918766, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:20, Epoch: 106, Batch: 480, Training Loss: 0.05446368157863617, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:21, Epoch: 106, Batch: 490, Training Loss: 0.0334891777485609, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:22, Epoch: 106, Batch: 500, Training Loss: 0.03232613727450371, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:23, Epoch: 106, Batch: 510, Training Loss: 0.04661100544035435, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:23, Epoch: 106, Batch: 520, Training Loss: 0.026884448155760764, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:24, Epoch: 106, Batch: 530, Training Loss: 0.04414097219705582, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:25, Epoch: 106, Batch: 540, Training Loss: 0.04506270177662373, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:25, Epoch: 106, Batch: 550, Training Loss: 0.04634869620203972, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:26, Epoch: 106, Batch: 560, Training Loss: 0.04122581854462624, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:27, Epoch: 106, Batch: 570, Training Loss: 0.026665200293064118, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:28, Epoch: 106, Batch: 580, Training Loss: 0.07093343213200569, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:28, Epoch: 106, Batch: 590, Training Loss: 0.03869779743254185, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:29, Epoch: 106, Batch: 600, Training Loss: 0.0453514888882637, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:30, Epoch: 106, Batch: 610, Training Loss: 0.030702556297183037, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:31, Epoch: 106, Batch: 620, Training Loss: 0.042960512638092044, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:32, Epoch: 106, Batch: 630, Training Loss: 0.04770180806517601, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:32, Epoch: 106, Batch: 640, Training Loss: 0.03347063474357128, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:33, Epoch: 106, Batch: 650, Training Loss: 0.0473062951117754, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:34, Epoch: 106, Batch: 660, Training Loss: 0.043376707658171657, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:35, Epoch: 106, Batch: 670, Training Loss: 0.052638623490929605, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:35, Epoch: 106, Batch: 680, Training Loss: 0.0312685526907444, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:36, Epoch: 106, Batch: 690, Training Loss: 0.04954482614994049, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:37, Epoch: 106, Batch: 700, Training Loss: 0.053553443774580954, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:38, Epoch: 106, Batch: 710, Training Loss: 0.036386404559016226, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:38, Epoch: 106, Batch: 720, Training Loss: 0.03248678520321846, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:39, Epoch: 106, Batch: 730, Training Loss: 0.03948744684457779, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:40, Epoch: 106, Batch: 740, Training Loss: 0.03356458134949207, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:41, Epoch: 106, Batch: 750, Training Loss: 0.03859383687376976, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:41, Epoch: 106, Batch: 760, Training Loss: 0.0395071979612112, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:42, Epoch: 106, Batch: 770, Training Loss: 0.04352381378412247, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:43, Epoch: 106, Batch: 780, Training Loss: 0.0422274999320507, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:44, Epoch: 106, Batch: 790, Training Loss: 0.04552714675664902, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:44, Epoch: 106, Batch: 800, Training Loss: 0.05165606252849102, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:45, Epoch: 106, Batch: 810, Training Loss: 0.055740084126591684, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:46, Epoch: 106, Batch: 820, Training Loss: 0.03759790137410164, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:47, Epoch: 106, Batch: 830, Training Loss: 0.022594381868839265, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:47, Epoch: 106, Batch: 840, Training Loss: 0.030136185139417647, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:48, Epoch: 106, Batch: 850, Training Loss: 0.04950811006128788, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:49, Epoch: 106, Batch: 860, Training Loss: 0.04604614116251469, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:50, Epoch: 106, Batch: 870, Training Loss: 0.03319813124835491, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:50, Epoch: 106, Batch: 880, Training Loss: 0.05130992271006107, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:51, Epoch: 106, Batch: 890, Training Loss: 0.04066283106803894, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:52, Epoch: 106, Batch: 900, Training Loss: 0.01806265115737915, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:53, Epoch: 106, Batch: 910, Training Loss: 0.028601540625095366, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:53, Epoch: 106, Batch: 920, Training Loss: 0.04886140078306198, LR: 0.00010000000000000003
Time, 2019-01-01T19:57:54, Epoch: 106, Batch: 930, Training Loss: 0.0408817395567894, LR: 0.00010000000000000003
Epoch: 106, Validation Top 1 acc: 98.8939208984375
Epoch: 106, Validation Top 5 acc: 99.99000549316406
Epoch: 106, Validation Set Loss: 0.04087487608194351
Start training epoch 107
Time, 2019-01-01T19:58:24, Epoch: 107, Batch: 10, Training Loss: 0.04106573276221752, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:24, Epoch: 107, Batch: 20, Training Loss: 0.04162856712937355, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:25, Epoch: 107, Batch: 30, Training Loss: 0.026099241524934768, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:26, Epoch: 107, Batch: 40, Training Loss: 0.036710790172219274, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:27, Epoch: 107, Batch: 50, Training Loss: 0.0482848696410656, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:27, Epoch: 107, Batch: 60, Training Loss: 0.042582695558667186, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:28, Epoch: 107, Batch: 70, Training Loss: 0.03200483918190002, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:29, Epoch: 107, Batch: 80, Training Loss: 0.042444226890802385, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:30, Epoch: 107, Batch: 90, Training Loss: 0.032062904164195064, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:30, Epoch: 107, Batch: 100, Training Loss: 0.02918771542608738, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:31, Epoch: 107, Batch: 110, Training Loss: 0.039358111843466756, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:32, Epoch: 107, Batch: 120, Training Loss: 0.04388379231095314, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:33, Epoch: 107, Batch: 130, Training Loss: 0.057395318150520326, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:33, Epoch: 107, Batch: 140, Training Loss: 0.044222913682460785, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:34, Epoch: 107, Batch: 150, Training Loss: 0.03557811975479126, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:35, Epoch: 107, Batch: 160, Training Loss: 0.04298194870352745, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:36, Epoch: 107, Batch: 170, Training Loss: 0.038627342134714124, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:37, Epoch: 107, Batch: 180, Training Loss: 0.03842046447098255, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:37, Epoch: 107, Batch: 190, Training Loss: 0.02116982564330101, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:38, Epoch: 107, Batch: 200, Training Loss: 0.04198322780430317, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:39, Epoch: 107, Batch: 210, Training Loss: 0.04142039343714714, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:40, Epoch: 107, Batch: 220, Training Loss: 0.03994460254907608, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:40, Epoch: 107, Batch: 230, Training Loss: 0.03813942298293114, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:41, Epoch: 107, Batch: 240, Training Loss: 0.03213939070701599, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:42, Epoch: 107, Batch: 250, Training Loss: 0.030911508202552795, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:43, Epoch: 107, Batch: 260, Training Loss: 0.026160918548703192, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:43, Epoch: 107, Batch: 270, Training Loss: 0.04361999705433846, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:44, Epoch: 107, Batch: 280, Training Loss: 0.038878947496414185, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:45, Epoch: 107, Batch: 290, Training Loss: 0.04082901179790497, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:45, Epoch: 107, Batch: 300, Training Loss: 0.037881539389491084, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:46, Epoch: 107, Batch: 310, Training Loss: 0.0424329899251461, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:47, Epoch: 107, Batch: 320, Training Loss: 0.03458028808236122, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:48, Epoch: 107, Batch: 330, Training Loss: 0.0511099960654974, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:48, Epoch: 107, Batch: 340, Training Loss: 0.05700569078326225, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:49, Epoch: 107, Batch: 350, Training Loss: 0.05172780118882656, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:50, Epoch: 107, Batch: 360, Training Loss: 0.03154773339629173, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:51, Epoch: 107, Batch: 370, Training Loss: 0.027595628052949905, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:51, Epoch: 107, Batch: 380, Training Loss: 0.029557560011744498, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:52, Epoch: 107, Batch: 390, Training Loss: 0.0315703272819519, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:53, Epoch: 107, Batch: 400, Training Loss: 0.037743497639894485, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:54, Epoch: 107, Batch: 410, Training Loss: 0.05422946251928806, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:54, Epoch: 107, Batch: 420, Training Loss: 0.05606259815394878, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:55, Epoch: 107, Batch: 430, Training Loss: 0.04138401448726654, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:56, Epoch: 107, Batch: 440, Training Loss: 0.05152531750500202, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:57, Epoch: 107, Batch: 450, Training Loss: 0.059467734023928645, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:57, Epoch: 107, Batch: 460, Training Loss: 0.03607977740466595, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:58, Epoch: 107, Batch: 470, Training Loss: 0.03234788589179516, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:59, Epoch: 107, Batch: 480, Training Loss: 0.03706938847899437, LR: 0.00010000000000000003
Time, 2019-01-01T19:58:59, Epoch: 107, Batch: 490, Training Loss: 0.044906963035464285, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:00, Epoch: 107, Batch: 500, Training Loss: 0.042590481042861936, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:01, Epoch: 107, Batch: 510, Training Loss: 0.033955253288149835, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:02, Epoch: 107, Batch: 520, Training Loss: 0.03568602688610554, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:02, Epoch: 107, Batch: 530, Training Loss: 0.03707187175750733, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:03, Epoch: 107, Batch: 540, Training Loss: 0.05045765712857246, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:04, Epoch: 107, Batch: 550, Training Loss: 0.04420581385493279, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:05, Epoch: 107, Batch: 560, Training Loss: 0.048892583698034286, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:05, Epoch: 107, Batch: 570, Training Loss: 0.0366477906703949, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:06, Epoch: 107, Batch: 580, Training Loss: 0.04802025705575943, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:07, Epoch: 107, Batch: 590, Training Loss: 0.06790319755673409, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:08, Epoch: 107, Batch: 600, Training Loss: 0.03739965446293354, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:08, Epoch: 107, Batch: 610, Training Loss: 0.032250428944826125, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:09, Epoch: 107, Batch: 620, Training Loss: 0.04023093730211258, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:10, Epoch: 107, Batch: 630, Training Loss: 0.07234477140009403, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:11, Epoch: 107, Batch: 640, Training Loss: 0.03487279303371906, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:11, Epoch: 107, Batch: 650, Training Loss: 0.040833863988518716, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:12, Epoch: 107, Batch: 660, Training Loss: 0.05172433331608772, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:13, Epoch: 107, Batch: 670, Training Loss: 0.034356129541993144, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:13, Epoch: 107, Batch: 680, Training Loss: 0.039875658228993416, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:14, Epoch: 107, Batch: 690, Training Loss: 0.0582315418869257, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:15, Epoch: 107, Batch: 700, Training Loss: 0.03217475265264511, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:16, Epoch: 107, Batch: 710, Training Loss: 0.028148093447089195, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:17, Epoch: 107, Batch: 720, Training Loss: 0.023226935416460037, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:18, Epoch: 107, Batch: 730, Training Loss: 0.04253992885351181, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:18, Epoch: 107, Batch: 740, Training Loss: 0.047741687297821044, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:19, Epoch: 107, Batch: 750, Training Loss: 0.04454096965491772, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:20, Epoch: 107, Batch: 760, Training Loss: 0.03031744733452797, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:21, Epoch: 107, Batch: 770, Training Loss: 0.04058142341673374, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:21, Epoch: 107, Batch: 780, Training Loss: 0.047634460777044293, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:22, Epoch: 107, Batch: 790, Training Loss: 0.043931536749005316, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:23, Epoch: 107, Batch: 800, Training Loss: 0.051355069130659105, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:24, Epoch: 107, Batch: 810, Training Loss: 0.027660636603832243, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:24, Epoch: 107, Batch: 820, Training Loss: 0.03616022765636444, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:25, Epoch: 107, Batch: 830, Training Loss: 0.046248506754636765, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:26, Epoch: 107, Batch: 840, Training Loss: 0.03711377196013928, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:27, Epoch: 107, Batch: 850, Training Loss: 0.0480251207947731, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:27, Epoch: 107, Batch: 860, Training Loss: 0.027459412813186646, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:28, Epoch: 107, Batch: 870, Training Loss: 0.03363679051399231, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:29, Epoch: 107, Batch: 880, Training Loss: 0.05888034962117672, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:29, Epoch: 107, Batch: 890, Training Loss: 0.044524159654974936, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:30, Epoch: 107, Batch: 900, Training Loss: 0.04363453835248947, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:31, Epoch: 107, Batch: 910, Training Loss: 0.045421027764678004, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:32, Epoch: 107, Batch: 920, Training Loss: 0.05822530463337898, LR: 0.00010000000000000003
Time, 2019-01-01T19:59:32, Epoch: 107, Batch: 930, Training Loss: 0.04875176176428795, LR: 0.00010000000000000003
Epoch: 107, Validation Top 1 acc: 98.88726043701172
Epoch: 107, Validation Top 5 acc: 99.99000549316406
Epoch: 107, Validation Set Loss: 0.040878232568502426
Start training epoch 108
Time, 2019-01-01T20:00:02, Epoch: 108, Batch: 10, Training Loss: 0.029836513102054596, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:03, Epoch: 108, Batch: 20, Training Loss: 0.041823525354266165, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:03, Epoch: 108, Batch: 30, Training Loss: 0.060978198796510695, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:04, Epoch: 108, Batch: 40, Training Loss: 0.04504361301660538, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:05, Epoch: 108, Batch: 50, Training Loss: 0.03164492435753345, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:05, Epoch: 108, Batch: 60, Training Loss: 0.05056587979197502, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:06, Epoch: 108, Batch: 70, Training Loss: 0.0258936308324337, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:07, Epoch: 108, Batch: 80, Training Loss: 0.038875000551342964, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:08, Epoch: 108, Batch: 90, Training Loss: 0.04414079189300537, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:08, Epoch: 108, Batch: 100, Training Loss: 0.04072617888450623, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:09, Epoch: 108, Batch: 110, Training Loss: 0.03966868296265602, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:10, Epoch: 108, Batch: 120, Training Loss: 0.047407133132219316, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:11, Epoch: 108, Batch: 130, Training Loss: 0.03025161996483803, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:11, Epoch: 108, Batch: 140, Training Loss: 0.04122985266149044, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:12, Epoch: 108, Batch: 150, Training Loss: 0.0342269841581583, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:13, Epoch: 108, Batch: 160, Training Loss: 0.02097122184932232, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:14, Epoch: 108, Batch: 170, Training Loss: 0.054211368411779405, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:14, Epoch: 108, Batch: 180, Training Loss: 0.0309746690094471, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:15, Epoch: 108, Batch: 190, Training Loss: 0.049657297879457475, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:16, Epoch: 108, Batch: 200, Training Loss: 0.03463768921792507, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:17, Epoch: 108, Batch: 210, Training Loss: 0.03922911584377289, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:17, Epoch: 108, Batch: 220, Training Loss: 0.05221558287739754, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:18, Epoch: 108, Batch: 230, Training Loss: 0.042498698085546495, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:19, Epoch: 108, Batch: 240, Training Loss: 0.048792269080877304, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:20, Epoch: 108, Batch: 250, Training Loss: 0.03780754953622818, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:21, Epoch: 108, Batch: 260, Training Loss: 0.044472620636224744, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:22, Epoch: 108, Batch: 270, Training Loss: 0.041578149795532225, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:22, Epoch: 108, Batch: 280, Training Loss: 0.03940548300743103, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:23, Epoch: 108, Batch: 290, Training Loss: 0.05091252736747265, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:24, Epoch: 108, Batch: 300, Training Loss: 0.04024766236543655, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:25, Epoch: 108, Batch: 310, Training Loss: 0.07148099467158317, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:25, Epoch: 108, Batch: 320, Training Loss: 0.04641047865152359, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:26, Epoch: 108, Batch: 330, Training Loss: 0.03707226812839508, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:27, Epoch: 108, Batch: 340, Training Loss: 0.04402726255357266, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:28, Epoch: 108, Batch: 350, Training Loss: 0.04072810113430023, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:29, Epoch: 108, Batch: 360, Training Loss: 0.06829576157033443, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:29, Epoch: 108, Batch: 370, Training Loss: 0.033461625874042514, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:30, Epoch: 108, Batch: 380, Training Loss: 0.04420899823307991, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:31, Epoch: 108, Batch: 390, Training Loss: 0.04818716906011104, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:32, Epoch: 108, Batch: 400, Training Loss: 0.03403147719800472, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:33, Epoch: 108, Batch: 410, Training Loss: 0.042862849310040474, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:33, Epoch: 108, Batch: 420, Training Loss: 0.043215740099549295, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:34, Epoch: 108, Batch: 430, Training Loss: 0.03561719991266728, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:35, Epoch: 108, Batch: 440, Training Loss: 0.03924574218690395, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:36, Epoch: 108, Batch: 450, Training Loss: 0.03765305951237678, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:36, Epoch: 108, Batch: 460, Training Loss: 0.03448857516050339, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:37, Epoch: 108, Batch: 470, Training Loss: 0.04133437648415565, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:38, Epoch: 108, Batch: 480, Training Loss: 0.04117130227386952, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:39, Epoch: 108, Batch: 490, Training Loss: 0.038597482815384866, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:39, Epoch: 108, Batch: 500, Training Loss: 0.042015109583735465, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:40, Epoch: 108, Batch: 510, Training Loss: 0.05245918184518814, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:41, Epoch: 108, Batch: 520, Training Loss: 0.05752442814409733, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:41, Epoch: 108, Batch: 530, Training Loss: 0.03951160162687302, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:42, Epoch: 108, Batch: 540, Training Loss: 0.020927394181489943, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:43, Epoch: 108, Batch: 550, Training Loss: 0.03796609789133072, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:44, Epoch: 108, Batch: 560, Training Loss: 0.03531679622828961, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:45, Epoch: 108, Batch: 570, Training Loss: 0.03213651850819588, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:45, Epoch: 108, Batch: 580, Training Loss: 0.04190112091600895, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:46, Epoch: 108, Batch: 590, Training Loss: 0.05142421722412109, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:47, Epoch: 108, Batch: 600, Training Loss: 0.04575214236974716, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:47, Epoch: 108, Batch: 610, Training Loss: 0.04624791666865349, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:48, Epoch: 108, Batch: 620, Training Loss: 0.028535472601652144, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:49, Epoch: 108, Batch: 630, Training Loss: 0.04920893386006355, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:50, Epoch: 108, Batch: 640, Training Loss: 0.05064702369272709, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:50, Epoch: 108, Batch: 650, Training Loss: 0.03193838819861412, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:51, Epoch: 108, Batch: 660, Training Loss: 0.045361734554171564, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:52, Epoch: 108, Batch: 670, Training Loss: 0.025863216444849967, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:53, Epoch: 108, Batch: 680, Training Loss: 0.051458432152867314, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:53, Epoch: 108, Batch: 690, Training Loss: 0.03341448716819286, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:54, Epoch: 108, Batch: 700, Training Loss: 0.039916303008794785, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:55, Epoch: 108, Batch: 710, Training Loss: 0.05066691339015961, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:56, Epoch: 108, Batch: 720, Training Loss: 0.048260205611586574, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:57, Epoch: 108, Batch: 730, Training Loss: 0.03655471913516521, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:57, Epoch: 108, Batch: 740, Training Loss: 0.02886548787355423, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:58, Epoch: 108, Batch: 750, Training Loss: 0.02504684366285801, LR: 0.00010000000000000003
Time, 2019-01-01T20:00:59, Epoch: 108, Batch: 760, Training Loss: 0.0362282358109951, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:00, Epoch: 108, Batch: 770, Training Loss: 0.0517652589827776, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:00, Epoch: 108, Batch: 780, Training Loss: 0.0414202805608511, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:01, Epoch: 108, Batch: 790, Training Loss: 0.056810808926820756, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:02, Epoch: 108, Batch: 800, Training Loss: 0.036771193891763684, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:03, Epoch: 108, Batch: 810, Training Loss: 0.0389648899435997, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:03, Epoch: 108, Batch: 820, Training Loss: 0.03361265584826469, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:04, Epoch: 108, Batch: 830, Training Loss: 0.0345251377671957, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:05, Epoch: 108, Batch: 840, Training Loss: 0.04938356056809425, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:06, Epoch: 108, Batch: 850, Training Loss: 0.029452205076813696, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:08, Epoch: 108, Batch: 860, Training Loss: 0.03322891406714916, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:09, Epoch: 108, Batch: 870, Training Loss: 0.02639893740415573, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:10, Epoch: 108, Batch: 880, Training Loss: 0.03609838597476482, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:12, Epoch: 108, Batch: 890, Training Loss: 0.04779911302030086, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:13, Epoch: 108, Batch: 900, Training Loss: 0.04221633672714233, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:14, Epoch: 108, Batch: 910, Training Loss: 0.030604013055562974, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:15, Epoch: 108, Batch: 920, Training Loss: 0.04761761091649532, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:16, Epoch: 108, Batch: 930, Training Loss: 0.03534423336386681, LR: 0.00010000000000000003
Epoch: 108, Validation Top 1 acc: 98.88726043701172
Epoch: 108, Validation Top 5 acc: 99.99166870117188
Epoch: 108, Validation Set Loss: 0.04086010158061981
Start training epoch 109
Time, 2019-01-01T20:01:44, Epoch: 109, Batch: 10, Training Loss: 0.04275526702404022, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:45, Epoch: 109, Batch: 20, Training Loss: 0.03221069723367691, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:46, Epoch: 109, Batch: 30, Training Loss: 0.03758867084980011, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:46, Epoch: 109, Batch: 40, Training Loss: 0.0387303251773119, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:47, Epoch: 109, Batch: 50, Training Loss: 0.04665292762219906, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:48, Epoch: 109, Batch: 60, Training Loss: 0.03415931910276413, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:49, Epoch: 109, Batch: 70, Training Loss: 0.028291025757789613, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:49, Epoch: 109, Batch: 80, Training Loss: 0.039583969861269, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:50, Epoch: 109, Batch: 90, Training Loss: 0.03281327374279499, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:51, Epoch: 109, Batch: 100, Training Loss: 0.05394026227295399, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:51, Epoch: 109, Batch: 110, Training Loss: 0.03035036288201809, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:52, Epoch: 109, Batch: 120, Training Loss: 0.033569822832942006, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:53, Epoch: 109, Batch: 130, Training Loss: 0.03859807029366493, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:54, Epoch: 109, Batch: 140, Training Loss: 0.053695745393633844, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:54, Epoch: 109, Batch: 150, Training Loss: 0.041268514841794966, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:55, Epoch: 109, Batch: 160, Training Loss: 0.029488415643572807, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:56, Epoch: 109, Batch: 170, Training Loss: 0.05329064801335335, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:57, Epoch: 109, Batch: 180, Training Loss: 0.03598318882286548, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:57, Epoch: 109, Batch: 190, Training Loss: 0.03384532928466797, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:58, Epoch: 109, Batch: 200, Training Loss: 0.030647101625800133, LR: 0.00010000000000000003
Time, 2019-01-01T20:01:59, Epoch: 109, Batch: 210, Training Loss: 0.03634181618690491, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:00, Epoch: 109, Batch: 220, Training Loss: 0.03160669468343258, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:00, Epoch: 109, Batch: 230, Training Loss: 0.0627843052148819, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:01, Epoch: 109, Batch: 240, Training Loss: 0.04870329536497593, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:02, Epoch: 109, Batch: 250, Training Loss: 0.050837316736578944, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:02, Epoch: 109, Batch: 260, Training Loss: 0.03996065482497215, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:03, Epoch: 109, Batch: 270, Training Loss: 0.04034389965236187, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:04, Epoch: 109, Batch: 280, Training Loss: 0.047258084267377855, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:05, Epoch: 109, Batch: 290, Training Loss: 0.043782299384474754, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:05, Epoch: 109, Batch: 300, Training Loss: 0.045174887776374816, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:06, Epoch: 109, Batch: 310, Training Loss: 0.03922837488353252, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:07, Epoch: 109, Batch: 320, Training Loss: 0.02554555833339691, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:08, Epoch: 109, Batch: 330, Training Loss: 0.026147303730249406, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:09, Epoch: 109, Batch: 340, Training Loss: 0.02634883001446724, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:09, Epoch: 109, Batch: 350, Training Loss: 0.04736756049096584, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:10, Epoch: 109, Batch: 360, Training Loss: 0.04266980029642582, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:11, Epoch: 109, Batch: 370, Training Loss: 0.04271064065396786, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:12, Epoch: 109, Batch: 380, Training Loss: 0.03619775474071503, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:13, Epoch: 109, Batch: 390, Training Loss: 0.03758286163210869, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:13, Epoch: 109, Batch: 400, Training Loss: 0.03516077250242233, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:14, Epoch: 109, Batch: 410, Training Loss: 0.03491333350539207, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:15, Epoch: 109, Batch: 420, Training Loss: 0.040535660833120345, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:16, Epoch: 109, Batch: 430, Training Loss: 0.031206424906849863, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:17, Epoch: 109, Batch: 440, Training Loss: 0.03856795951724053, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:17, Epoch: 109, Batch: 450, Training Loss: 0.04300948306918144, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:18, Epoch: 109, Batch: 460, Training Loss: 0.03602149337530136, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:19, Epoch: 109, Batch: 470, Training Loss: 0.03839159868657589, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:20, Epoch: 109, Batch: 480, Training Loss: 0.055562379956245425, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:21, Epoch: 109, Batch: 490, Training Loss: 0.03460806272923946, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:22, Epoch: 109, Batch: 500, Training Loss: 0.05407119654119015, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:23, Epoch: 109, Batch: 510, Training Loss: 0.05941073559224606, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:24, Epoch: 109, Batch: 520, Training Loss: 0.05672326609492302, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:25, Epoch: 109, Batch: 530, Training Loss: 0.04896088279783726, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:25, Epoch: 109, Batch: 540, Training Loss: 0.04445827268064022, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:26, Epoch: 109, Batch: 550, Training Loss: 0.03588434308767319, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:27, Epoch: 109, Batch: 560, Training Loss: 0.0425223208963871, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:28, Epoch: 109, Batch: 570, Training Loss: 0.03291758522391319, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:29, Epoch: 109, Batch: 580, Training Loss: 0.03345344290137291, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:29, Epoch: 109, Batch: 590, Training Loss: 0.035155969113111495, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:30, Epoch: 109, Batch: 600, Training Loss: 0.03404567018151283, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:31, Epoch: 109, Batch: 610, Training Loss: 0.05116605050861835, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:32, Epoch: 109, Batch: 620, Training Loss: 0.022146338224411012, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:33, Epoch: 109, Batch: 630, Training Loss: 0.04397772364318371, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:33, Epoch: 109, Batch: 640, Training Loss: 0.04363694787025452, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:34, Epoch: 109, Batch: 650, Training Loss: 0.04635342061519623, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:35, Epoch: 109, Batch: 660, Training Loss: 0.040562234818935394, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:36, Epoch: 109, Batch: 670, Training Loss: 0.03839734047651291, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:37, Epoch: 109, Batch: 680, Training Loss: 0.04542414024472237, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:37, Epoch: 109, Batch: 690, Training Loss: 0.0331126157194376, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:38, Epoch: 109, Batch: 700, Training Loss: 0.044120625406503675, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:39, Epoch: 109, Batch: 710, Training Loss: 0.04270257875323295, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:40, Epoch: 109, Batch: 720, Training Loss: 0.054994800686836244, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:40, Epoch: 109, Batch: 730, Training Loss: 0.0427792377769947, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:41, Epoch: 109, Batch: 740, Training Loss: 0.05088711380958557, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:42, Epoch: 109, Batch: 750, Training Loss: 0.03676225394010544, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:43, Epoch: 109, Batch: 760, Training Loss: 0.05272403433918953, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:43, Epoch: 109, Batch: 770, Training Loss: 0.06742698475718498, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:44, Epoch: 109, Batch: 780, Training Loss: 0.044091008603572845, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:45, Epoch: 109, Batch: 790, Training Loss: 0.06289114505052566, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:46, Epoch: 109, Batch: 800, Training Loss: 0.0333119235932827, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:46, Epoch: 109, Batch: 810, Training Loss: 0.046337515115737915, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:47, Epoch: 109, Batch: 820, Training Loss: 0.03653333447873593, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:48, Epoch: 109, Batch: 830, Training Loss: 0.027494264394044877, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:49, Epoch: 109, Batch: 840, Training Loss: 0.02968851923942566, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:50, Epoch: 109, Batch: 850, Training Loss: 0.06643791683018208, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:50, Epoch: 109, Batch: 860, Training Loss: 0.040032795444130895, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:51, Epoch: 109, Batch: 870, Training Loss: 0.034633044898509976, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:52, Epoch: 109, Batch: 880, Training Loss: 0.035724830627441403, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:52, Epoch: 109, Batch: 890, Training Loss: 0.04459171444177627, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:53, Epoch: 109, Batch: 900, Training Loss: 0.04794449619948864, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:54, Epoch: 109, Batch: 910, Training Loss: 0.03561687506735325, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:55, Epoch: 109, Batch: 920, Training Loss: 0.034042326360940935, LR: 0.00010000000000000003
Time, 2019-01-01T20:02:55, Epoch: 109, Batch: 930, Training Loss: 0.04306001327931881, LR: 0.00010000000000000003
Epoch: 109, Validation Top 1 acc: 98.88892364501953
Epoch: 109, Validation Top 5 acc: 99.99166870117188
Epoch: 109, Validation Set Loss: 0.040929850190877914
Start training epoch 110
Time, 2019-01-01T20:03:29, Epoch: 110, Batch: 10, Training Loss: 0.038963374122977255, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:30, Epoch: 110, Batch: 20, Training Loss: 0.038206937909126285, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:31, Epoch: 110, Batch: 30, Training Loss: 0.05127234682440758, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:32, Epoch: 110, Batch: 40, Training Loss: 0.03409341350197792, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:32, Epoch: 110, Batch: 50, Training Loss: 0.03538868092000484, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:33, Epoch: 110, Batch: 60, Training Loss: 0.04792409241199493, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:34, Epoch: 110, Batch: 70, Training Loss: 0.0536201823502779, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:35, Epoch: 110, Batch: 80, Training Loss: 0.034313442558050154, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:35, Epoch: 110, Batch: 90, Training Loss: 0.03940693959593773, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:36, Epoch: 110, Batch: 100, Training Loss: 0.04614710919559002, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:37, Epoch: 110, Batch: 110, Training Loss: 0.059131060540676114, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:38, Epoch: 110, Batch: 120, Training Loss: 0.050820858031511304, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:38, Epoch: 110, Batch: 130, Training Loss: 0.028429048880934716, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:39, Epoch: 110, Batch: 140, Training Loss: 0.037355192750692365, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:40, Epoch: 110, Batch: 150, Training Loss: 0.029101625829935075, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:41, Epoch: 110, Batch: 160, Training Loss: 0.03726712986826897, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:41, Epoch: 110, Batch: 170, Training Loss: 0.04130521863698959, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:42, Epoch: 110, Batch: 180, Training Loss: 0.03410945236682892, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:43, Epoch: 110, Batch: 190, Training Loss: 0.04908559769392014, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:44, Epoch: 110, Batch: 200, Training Loss: 0.03970644623041153, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:44, Epoch: 110, Batch: 210, Training Loss: 0.06035987436771393, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:45, Epoch: 110, Batch: 220, Training Loss: 0.03810702860355377, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:46, Epoch: 110, Batch: 230, Training Loss: 0.024682770669460296, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:47, Epoch: 110, Batch: 240, Training Loss: 0.0392131295055151, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:47, Epoch: 110, Batch: 250, Training Loss: 0.0395085409283638, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:48, Epoch: 110, Batch: 260, Training Loss: 0.02741583026945591, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:49, Epoch: 110, Batch: 270, Training Loss: 0.030606245249509813, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:50, Epoch: 110, Batch: 280, Training Loss: 0.0402508832514286, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:50, Epoch: 110, Batch: 290, Training Loss: 0.033825232461094853, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:51, Epoch: 110, Batch: 300, Training Loss: 0.0452548049390316, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:52, Epoch: 110, Batch: 310, Training Loss: 0.03964461535215378, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:53, Epoch: 110, Batch: 320, Training Loss: 0.04192086346447468, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:53, Epoch: 110, Batch: 330, Training Loss: 0.031038227304816247, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:54, Epoch: 110, Batch: 340, Training Loss: 0.04919134899973869, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:55, Epoch: 110, Batch: 350, Training Loss: 0.024415167793631554, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:56, Epoch: 110, Batch: 360, Training Loss: 0.04440319165587425, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:57, Epoch: 110, Batch: 370, Training Loss: 0.027245526015758515, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:58, Epoch: 110, Batch: 380, Training Loss: 0.030306008458137513, LR: 0.00010000000000000003
Time, 2019-01-01T20:03:59, Epoch: 110, Batch: 390, Training Loss: 0.020480196923017502, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:00, Epoch: 110, Batch: 400, Training Loss: 0.02995051145553589, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:00, Epoch: 110, Batch: 410, Training Loss: 0.0421346802264452, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:01, Epoch: 110, Batch: 420, Training Loss: 0.025379948318004608, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:02, Epoch: 110, Batch: 430, Training Loss: 0.04205912426114082, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:03, Epoch: 110, Batch: 440, Training Loss: 0.05011372342705726, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:03, Epoch: 110, Batch: 450, Training Loss: 0.039229176193475726, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:04, Epoch: 110, Batch: 460, Training Loss: 0.04006411656737328, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:05, Epoch: 110, Batch: 470, Training Loss: 0.045450055971741676, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:06, Epoch: 110, Batch: 480, Training Loss: 0.043387963250279424, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:07, Epoch: 110, Batch: 490, Training Loss: 0.035450476408004764, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:08, Epoch: 110, Batch: 500, Training Loss: 0.023605768382549287, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:09, Epoch: 110, Batch: 510, Training Loss: 0.03338077962398529, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:10, Epoch: 110, Batch: 520, Training Loss: 0.03420886099338531, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:11, Epoch: 110, Batch: 530, Training Loss: 0.036821230128407476, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:11, Epoch: 110, Batch: 540, Training Loss: 0.035801198333501816, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:12, Epoch: 110, Batch: 550, Training Loss: 0.03951954767107964, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:13, Epoch: 110, Batch: 560, Training Loss: 0.038870345801115036, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:14, Epoch: 110, Batch: 570, Training Loss: 0.042939324676990506, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:14, Epoch: 110, Batch: 580, Training Loss: 0.06131166666746139, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:15, Epoch: 110, Batch: 590, Training Loss: 0.03366893045604229, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:16, Epoch: 110, Batch: 600, Training Loss: 0.03668041825294495, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:17, Epoch: 110, Batch: 610, Training Loss: 0.04689354486763477, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:17, Epoch: 110, Batch: 620, Training Loss: 0.05276417694985867, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:18, Epoch: 110, Batch: 630, Training Loss: 0.03811267949640751, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:19, Epoch: 110, Batch: 640, Training Loss: 0.052696701139211655, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:20, Epoch: 110, Batch: 650, Training Loss: 0.05032614767551422, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:21, Epoch: 110, Batch: 660, Training Loss: 0.05931190215051174, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:22, Epoch: 110, Batch: 670, Training Loss: 0.04831970930099487, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:22, Epoch: 110, Batch: 680, Training Loss: 0.034703629463911055, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:23, Epoch: 110, Batch: 690, Training Loss: 0.039672767370939256, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:24, Epoch: 110, Batch: 700, Training Loss: 0.04910764992237091, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:25, Epoch: 110, Batch: 710, Training Loss: 0.03941460363566875, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:26, Epoch: 110, Batch: 720, Training Loss: 0.06940684691071511, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:27, Epoch: 110, Batch: 730, Training Loss: 0.039526082947850226, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:27, Epoch: 110, Batch: 740, Training Loss: 0.052939267829060555, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:28, Epoch: 110, Batch: 750, Training Loss: 0.03702596426010132, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:29, Epoch: 110, Batch: 760, Training Loss: 0.04586007036268711, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:30, Epoch: 110, Batch: 770, Training Loss: 0.049772490933537486, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:31, Epoch: 110, Batch: 780, Training Loss: 0.020926183462142943, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:31, Epoch: 110, Batch: 790, Training Loss: 0.05437212102115154, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:32, Epoch: 110, Batch: 800, Training Loss: 0.04529266096651554, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:33, Epoch: 110, Batch: 810, Training Loss: 0.04247466400265694, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:34, Epoch: 110, Batch: 820, Training Loss: 0.036679094284772874, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:34, Epoch: 110, Batch: 830, Training Loss: 0.05427944287657738, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:35, Epoch: 110, Batch: 840, Training Loss: 0.05935155898332596, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:36, Epoch: 110, Batch: 850, Training Loss: 0.029307833313941954, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:37, Epoch: 110, Batch: 860, Training Loss: 0.03212173953652382, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:38, Epoch: 110, Batch: 870, Training Loss: 0.03621695339679718, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:38, Epoch: 110, Batch: 880, Training Loss: 0.06481638923287392, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:39, Epoch: 110, Batch: 890, Training Loss: 0.0565428763628006, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:40, Epoch: 110, Batch: 900, Training Loss: 0.04450145810842514, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:41, Epoch: 110, Batch: 910, Training Loss: 0.04045746624469757, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:42, Epoch: 110, Batch: 920, Training Loss: 0.0351887185126543, LR: 0.00010000000000000003
Time, 2019-01-01T20:04:42, Epoch: 110, Batch: 930, Training Loss: 0.03540111556649208, LR: 0.00010000000000000003
Epoch: 110, Validation Top 1 acc: 98.88726043701172
Epoch: 110, Validation Top 5 acc: 99.99000549316406
Epoch: 110, Validation Set Loss: 0.04083970561623573
Start training epoch 111
Time, 2019-01-01T20:05:12, Epoch: 111, Batch: 10, Training Loss: 0.045495215058326724, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:13, Epoch: 111, Batch: 20, Training Loss: 0.03611324578523636, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:13, Epoch: 111, Batch: 30, Training Loss: 0.03514282964169979, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:14, Epoch: 111, Batch: 40, Training Loss: 0.04439650177955627, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:15, Epoch: 111, Batch: 50, Training Loss: 0.05286276750266552, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:16, Epoch: 111, Batch: 60, Training Loss: 0.04380456209182739, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:17, Epoch: 111, Batch: 70, Training Loss: 0.045601533353328706, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:17, Epoch: 111, Batch: 80, Training Loss: 0.03552264049649238, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:18, Epoch: 111, Batch: 90, Training Loss: 0.03279602043330669, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:19, Epoch: 111, Batch: 100, Training Loss: 0.03911158032715321, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:20, Epoch: 111, Batch: 110, Training Loss: 0.032653491199016574, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:20, Epoch: 111, Batch: 120, Training Loss: 0.03896948881447315, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:21, Epoch: 111, Batch: 130, Training Loss: 0.02828289270401001, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:22, Epoch: 111, Batch: 140, Training Loss: 0.03825469426810742, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:23, Epoch: 111, Batch: 150, Training Loss: 0.0341315072029829, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:23, Epoch: 111, Batch: 160, Training Loss: 0.038977858424186704, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:24, Epoch: 111, Batch: 170, Training Loss: 0.04004997834563255, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:25, Epoch: 111, Batch: 180, Training Loss: 0.048292724415659904, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:26, Epoch: 111, Batch: 190, Training Loss: 0.04704643711447716, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:26, Epoch: 111, Batch: 200, Training Loss: 0.032171135395765306, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:27, Epoch: 111, Batch: 210, Training Loss: 0.03293350115418434, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:28, Epoch: 111, Batch: 220, Training Loss: 0.028420043736696245, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:29, Epoch: 111, Batch: 230, Training Loss: 0.034698976948857305, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:30, Epoch: 111, Batch: 240, Training Loss: 0.047795823961496356, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:30, Epoch: 111, Batch: 250, Training Loss: 0.030946512892842292, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:31, Epoch: 111, Batch: 260, Training Loss: 0.051355140283703804, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:32, Epoch: 111, Batch: 270, Training Loss: 0.04764124602079391, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:33, Epoch: 111, Batch: 280, Training Loss: 0.03463437296450138, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:33, Epoch: 111, Batch: 290, Training Loss: 0.035480933263897896, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:34, Epoch: 111, Batch: 300, Training Loss: 0.0587157167494297, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:35, Epoch: 111, Batch: 310, Training Loss: 0.03834005668759346, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:36, Epoch: 111, Batch: 320, Training Loss: 0.019580917060375215, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:36, Epoch: 111, Batch: 330, Training Loss: 0.032587510719895366, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:37, Epoch: 111, Batch: 340, Training Loss: 0.034045171737670896, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:38, Epoch: 111, Batch: 350, Training Loss: 0.05417763292789459, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:39, Epoch: 111, Batch: 360, Training Loss: 0.035609982535243036, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:39, Epoch: 111, Batch: 370, Training Loss: 0.036645176261663436, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:40, Epoch: 111, Batch: 380, Training Loss: 0.03179437071084976, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:41, Epoch: 111, Batch: 390, Training Loss: 0.03353096581995487, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:42, Epoch: 111, Batch: 400, Training Loss: 0.03465547412633896, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:42, Epoch: 111, Batch: 410, Training Loss: 0.02832893505692482, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:43, Epoch: 111, Batch: 420, Training Loss: 0.047194761037826535, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:44, Epoch: 111, Batch: 430, Training Loss: 0.03949447646737099, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:45, Epoch: 111, Batch: 440, Training Loss: 0.02452799342572689, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:45, Epoch: 111, Batch: 450, Training Loss: 0.03501860909163952, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:46, Epoch: 111, Batch: 460, Training Loss: 0.04883928447961807, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:47, Epoch: 111, Batch: 470, Training Loss: 0.054790865257382394, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:48, Epoch: 111, Batch: 480, Training Loss: 0.04910395890474319, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:48, Epoch: 111, Batch: 490, Training Loss: 0.04424928799271584, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:49, Epoch: 111, Batch: 500, Training Loss: 0.046485630050301555, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:50, Epoch: 111, Batch: 510, Training Loss: 0.03845415897667408, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:51, Epoch: 111, Batch: 520, Training Loss: 0.04478795900940895, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:51, Epoch: 111, Batch: 530, Training Loss: 0.049844378232955934, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:52, Epoch: 111, Batch: 540, Training Loss: 0.043934274837374686, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:53, Epoch: 111, Batch: 550, Training Loss: 0.03183183334767818, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:54, Epoch: 111, Batch: 560, Training Loss: 0.03663212209939957, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:54, Epoch: 111, Batch: 570, Training Loss: 0.030012764036655426, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:55, Epoch: 111, Batch: 580, Training Loss: 0.03801574520766735, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:56, Epoch: 111, Batch: 590, Training Loss: 0.03184780515730381, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:57, Epoch: 111, Batch: 600, Training Loss: 0.047861356288194656, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:57, Epoch: 111, Batch: 610, Training Loss: 0.04675772786140442, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:58, Epoch: 111, Batch: 620, Training Loss: 0.04434517659246921, LR: 0.00010000000000000003
Time, 2019-01-01T20:05:59, Epoch: 111, Batch: 630, Training Loss: 0.06760264337062835, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:00, Epoch: 111, Batch: 640, Training Loss: 0.05380906090140343, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:00, Epoch: 111, Batch: 650, Training Loss: 0.05027608275413513, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:01, Epoch: 111, Batch: 660, Training Loss: 0.04319789074361324, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:02, Epoch: 111, Batch: 670, Training Loss: 0.04253617152571678, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:03, Epoch: 111, Batch: 680, Training Loss: 0.04409807100892067, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:03, Epoch: 111, Batch: 690, Training Loss: 0.04599145203828812, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:04, Epoch: 111, Batch: 700, Training Loss: 0.04132064692676067, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:05, Epoch: 111, Batch: 710, Training Loss: 0.04634906612336635, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:06, Epoch: 111, Batch: 720, Training Loss: 0.03398403339087963, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:06, Epoch: 111, Batch: 730, Training Loss: 0.04115985929965973, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:07, Epoch: 111, Batch: 740, Training Loss: 0.03748398013412953, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:08, Epoch: 111, Batch: 750, Training Loss: 0.02550138235092163, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:09, Epoch: 111, Batch: 760, Training Loss: 0.05905720964074135, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:10, Epoch: 111, Batch: 770, Training Loss: 0.029402612149715422, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:10, Epoch: 111, Batch: 780, Training Loss: 0.040822362154722215, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:11, Epoch: 111, Batch: 790, Training Loss: 0.0367327019572258, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:12, Epoch: 111, Batch: 800, Training Loss: 0.03329220451414585, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:13, Epoch: 111, Batch: 810, Training Loss: 0.040934186428785324, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:13, Epoch: 111, Batch: 820, Training Loss: 0.04382653534412384, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:14, Epoch: 111, Batch: 830, Training Loss: 0.05332593806087971, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:15, Epoch: 111, Batch: 840, Training Loss: 0.04668392986059189, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:16, Epoch: 111, Batch: 850, Training Loss: 0.046003245562314984, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:16, Epoch: 111, Batch: 860, Training Loss: 0.05914944894611836, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:17, Epoch: 111, Batch: 870, Training Loss: 0.03222903721034527, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:18, Epoch: 111, Batch: 880, Training Loss: 0.03922606259584427, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:19, Epoch: 111, Batch: 890, Training Loss: 0.07250095941126347, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:19, Epoch: 111, Batch: 900, Training Loss: 0.039306513220071795, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:20, Epoch: 111, Batch: 910, Training Loss: 0.05006223320960999, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:21, Epoch: 111, Batch: 920, Training Loss: 0.053435396403074265, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:22, Epoch: 111, Batch: 930, Training Loss: 0.030858549848198892, LR: 0.00010000000000000003
Epoch: 111, Validation Top 1 acc: 98.88226318359375
Epoch: 111, Validation Top 5 acc: 99.99166870117188
Epoch: 111, Validation Set Loss: 0.04085354134440422
Start training epoch 112
Time, 2019-01-01T20:06:49, Epoch: 112, Batch: 10, Training Loss: 0.038485223427414894, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:50, Epoch: 112, Batch: 20, Training Loss: 0.05195109471678734, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:51, Epoch: 112, Batch: 30, Training Loss: 0.042819733172655104, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:51, Epoch: 112, Batch: 40, Training Loss: 0.029689495265483857, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:52, Epoch: 112, Batch: 50, Training Loss: 0.03174534700810909, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:53, Epoch: 112, Batch: 60, Training Loss: 0.03107698857784271, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:54, Epoch: 112, Batch: 70, Training Loss: 0.04220221787691116, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:54, Epoch: 112, Batch: 80, Training Loss: 0.04186603538691998, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:55, Epoch: 112, Batch: 90, Training Loss: 0.041094958782196045, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:56, Epoch: 112, Batch: 100, Training Loss: 0.03977471925318241, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:56, Epoch: 112, Batch: 110, Training Loss: 0.03939305283129215, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:57, Epoch: 112, Batch: 120, Training Loss: 0.026715822145342828, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:58, Epoch: 112, Batch: 130, Training Loss: 0.041958785802125934, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:59, Epoch: 112, Batch: 140, Training Loss: 0.048935729265213015, LR: 0.00010000000000000003
Time, 2019-01-01T20:06:59, Epoch: 112, Batch: 150, Training Loss: 0.03295078277587891, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:00, Epoch: 112, Batch: 160, Training Loss: 0.024524538218975066, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:01, Epoch: 112, Batch: 170, Training Loss: 0.03227778561413288, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:02, Epoch: 112, Batch: 180, Training Loss: 0.03627660162746906, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:03, Epoch: 112, Batch: 190, Training Loss: 0.04194525256752968, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:03, Epoch: 112, Batch: 200, Training Loss: 0.030448731407523154, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:04, Epoch: 112, Batch: 210, Training Loss: 0.04229230210185051, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:05, Epoch: 112, Batch: 220, Training Loss: 0.03899146430194378, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:06, Epoch: 112, Batch: 230, Training Loss: 0.03719408884644508, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:06, Epoch: 112, Batch: 240, Training Loss: 0.03501729741692543, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:07, Epoch: 112, Batch: 250, Training Loss: 0.042030935361981395, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:08, Epoch: 112, Batch: 260, Training Loss: 0.05186557024717331, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:09, Epoch: 112, Batch: 270, Training Loss: 0.04592027068138123, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:09, Epoch: 112, Batch: 280, Training Loss: 0.03045484833419323, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:10, Epoch: 112, Batch: 290, Training Loss: 0.04433907978236675, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:11, Epoch: 112, Batch: 300, Training Loss: 0.05537583865225315, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:12, Epoch: 112, Batch: 310, Training Loss: 0.05012259967625141, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:12, Epoch: 112, Batch: 320, Training Loss: 0.03508967012166977, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:13, Epoch: 112, Batch: 330, Training Loss: 0.043099869042634964, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:14, Epoch: 112, Batch: 340, Training Loss: 0.02937578931450844, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:15, Epoch: 112, Batch: 350, Training Loss: 0.042397104948759076, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:15, Epoch: 112, Batch: 360, Training Loss: 0.025093139708042146, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:16, Epoch: 112, Batch: 370, Training Loss: 0.03871450498700142, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:17, Epoch: 112, Batch: 380, Training Loss: 0.05552142448723316, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:18, Epoch: 112, Batch: 390, Training Loss: 0.02969779446721077, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:18, Epoch: 112, Batch: 400, Training Loss: 0.03636056333780289, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:19, Epoch: 112, Batch: 410, Training Loss: 0.06847699955105782, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:20, Epoch: 112, Batch: 420, Training Loss: 0.04391094297170639, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:21, Epoch: 112, Batch: 430, Training Loss: 0.06992565095424652, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:21, Epoch: 112, Batch: 440, Training Loss: 0.030887024477124214, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:22, Epoch: 112, Batch: 450, Training Loss: 0.04993534348905086, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:23, Epoch: 112, Batch: 460, Training Loss: 0.034968866035342216, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:23, Epoch: 112, Batch: 470, Training Loss: 0.03430228717625141, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:24, Epoch: 112, Batch: 480, Training Loss: 0.0345084797590971, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:25, Epoch: 112, Batch: 490, Training Loss: 0.06147197559475899, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:26, Epoch: 112, Batch: 500, Training Loss: 0.05165414661169052, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:26, Epoch: 112, Batch: 510, Training Loss: 0.04566022902727127, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:27, Epoch: 112, Batch: 520, Training Loss: 0.03786539100110531, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:28, Epoch: 112, Batch: 530, Training Loss: 0.042154548317193986, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:29, Epoch: 112, Batch: 540, Training Loss: 0.048014918342232704, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:29, Epoch: 112, Batch: 550, Training Loss: 0.03655265048146248, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:30, Epoch: 112, Batch: 560, Training Loss: 0.033025088161230086, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:31, Epoch: 112, Batch: 570, Training Loss: 0.043245188519358636, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:32, Epoch: 112, Batch: 580, Training Loss: 0.030090643838047982, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:32, Epoch: 112, Batch: 590, Training Loss: 0.031807553768157956, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:33, Epoch: 112, Batch: 600, Training Loss: 0.029428140819072725, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:34, Epoch: 112, Batch: 610, Training Loss: 0.03473386578261852, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:35, Epoch: 112, Batch: 620, Training Loss: 0.04286498539149761, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:35, Epoch: 112, Batch: 630, Training Loss: 0.03716627657413483, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:36, Epoch: 112, Batch: 640, Training Loss: 0.05110891908407211, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:37, Epoch: 112, Batch: 650, Training Loss: 0.04736301004886627, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:37, Epoch: 112, Batch: 660, Training Loss: 0.041933977231383326, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:38, Epoch: 112, Batch: 670, Training Loss: 0.043314408510923386, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:39, Epoch: 112, Batch: 680, Training Loss: 0.051474202424287796, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:40, Epoch: 112, Batch: 690, Training Loss: 0.04515614472329617, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:41, Epoch: 112, Batch: 700, Training Loss: 0.04510423727333546, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:42, Epoch: 112, Batch: 710, Training Loss: 0.029346973448991776, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:42, Epoch: 112, Batch: 720, Training Loss: 0.05011444352567196, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:43, Epoch: 112, Batch: 730, Training Loss: 0.03734327591955662, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:44, Epoch: 112, Batch: 740, Training Loss: 0.06852524653077126, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:45, Epoch: 112, Batch: 750, Training Loss: 0.03578447923064232, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:46, Epoch: 112, Batch: 760, Training Loss: 0.032092849537730214, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:46, Epoch: 112, Batch: 770, Training Loss: 0.041359171643853186, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:47, Epoch: 112, Batch: 780, Training Loss: 0.05203282721340656, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:48, Epoch: 112, Batch: 790, Training Loss: 0.060099485144019124, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:49, Epoch: 112, Batch: 800, Training Loss: 0.02907244935631752, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:50, Epoch: 112, Batch: 810, Training Loss: 0.036033845692873004, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:50, Epoch: 112, Batch: 820, Training Loss: 0.03193908110260964, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:51, Epoch: 112, Batch: 830, Training Loss: 0.03398007750511169, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:52, Epoch: 112, Batch: 840, Training Loss: 0.04908495023846626, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:53, Epoch: 112, Batch: 850, Training Loss: 0.04020625576376915, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:54, Epoch: 112, Batch: 860, Training Loss: 0.02324955314397812, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:55, Epoch: 112, Batch: 870, Training Loss: 0.04434933140873909, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:56, Epoch: 112, Batch: 880, Training Loss: 0.053681302815675735, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:58, Epoch: 112, Batch: 890, Training Loss: 0.05462659262120724, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:58, Epoch: 112, Batch: 900, Training Loss: 0.03394319973886013, LR: 0.00010000000000000003
Time, 2019-01-01T20:07:59, Epoch: 112, Batch: 910, Training Loss: 0.04238445684313774, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:00, Epoch: 112, Batch: 920, Training Loss: 0.026123158633708954, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:01, Epoch: 112, Batch: 930, Training Loss: 0.060942419618368146, LR: 0.00010000000000000003
Epoch: 112, Validation Top 1 acc: 98.8789291381836
Epoch: 112, Validation Top 5 acc: 99.99000549316406
Epoch: 112, Validation Set Loss: 0.04080550745129585
Start training epoch 113
Time, 2019-01-01T20:08:28, Epoch: 113, Batch: 10, Training Loss: 0.04440634474158287, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:29, Epoch: 113, Batch: 20, Training Loss: 0.03604726977646351, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:29, Epoch: 113, Batch: 30, Training Loss: 0.04647784270346165, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:30, Epoch: 113, Batch: 40, Training Loss: 0.03012574277818203, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:31, Epoch: 113, Batch: 50, Training Loss: 0.03618563152849674, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:32, Epoch: 113, Batch: 60, Training Loss: 0.06684184856712819, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:32, Epoch: 113, Batch: 70, Training Loss: 0.04113708473742008, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:33, Epoch: 113, Batch: 80, Training Loss: 0.02929146997630596, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:34, Epoch: 113, Batch: 90, Training Loss: 0.04531749337911606, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:35, Epoch: 113, Batch: 100, Training Loss: 0.03210227675735951, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:35, Epoch: 113, Batch: 110, Training Loss: 0.0538222499191761, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:36, Epoch: 113, Batch: 120, Training Loss: 0.04489037543535233, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:37, Epoch: 113, Batch: 130, Training Loss: 0.030889403820037842, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:37, Epoch: 113, Batch: 140, Training Loss: 0.039073404669761655, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:38, Epoch: 113, Batch: 150, Training Loss: 0.039194241911172864, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:39, Epoch: 113, Batch: 160, Training Loss: 0.04201299175620079, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:40, Epoch: 113, Batch: 170, Training Loss: 0.06552674658596516, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:40, Epoch: 113, Batch: 180, Training Loss: 0.028469619527459144, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:41, Epoch: 113, Batch: 190, Training Loss: 0.04528203047811985, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:42, Epoch: 113, Batch: 200, Training Loss: 0.03262810930609703, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:43, Epoch: 113, Batch: 210, Training Loss: 0.032106099650263786, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:43, Epoch: 113, Batch: 220, Training Loss: 0.03738352432847023, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:44, Epoch: 113, Batch: 230, Training Loss: 0.030483685806393622, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:45, Epoch: 113, Batch: 240, Training Loss: 0.046448884159326555, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:46, Epoch: 113, Batch: 250, Training Loss: 0.04295717664062977, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:47, Epoch: 113, Batch: 260, Training Loss: 0.03800392150878906, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:47, Epoch: 113, Batch: 270, Training Loss: 0.03388950526714325, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:48, Epoch: 113, Batch: 280, Training Loss: 0.043479832261800765, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:49, Epoch: 113, Batch: 290, Training Loss: 0.04894585534930229, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:49, Epoch: 113, Batch: 300, Training Loss: 0.04437601901590824, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:50, Epoch: 113, Batch: 310, Training Loss: 0.04889653995633125, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:51, Epoch: 113, Batch: 320, Training Loss: 0.05097882077097893, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:52, Epoch: 113, Batch: 330, Training Loss: 0.04352402165532112, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:52, Epoch: 113, Batch: 340, Training Loss: 0.055233307927846906, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:53, Epoch: 113, Batch: 350, Training Loss: 0.04651217684149742, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:54, Epoch: 113, Batch: 360, Training Loss: 0.042824335396289825, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:55, Epoch: 113, Batch: 370, Training Loss: 0.040822529792785646, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:55, Epoch: 113, Batch: 380, Training Loss: 0.034787265956401824, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:56, Epoch: 113, Batch: 390, Training Loss: 0.0361157275736332, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:57, Epoch: 113, Batch: 400, Training Loss: 0.035359949618577954, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:58, Epoch: 113, Batch: 410, Training Loss: 0.042930950224399564, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:58, Epoch: 113, Batch: 420, Training Loss: 0.04477934651076794, LR: 0.00010000000000000003
Time, 2019-01-01T20:08:59, Epoch: 113, Batch: 430, Training Loss: 0.047703005373477936, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:00, Epoch: 113, Batch: 440, Training Loss: 0.04188735261559486, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:00, Epoch: 113, Batch: 450, Training Loss: 0.03659795969724655, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:01, Epoch: 113, Batch: 460, Training Loss: 0.03318275213241577, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:02, Epoch: 113, Batch: 470, Training Loss: 0.027568334713578224, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:03, Epoch: 113, Batch: 480, Training Loss: 0.044383341819047926, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:03, Epoch: 113, Batch: 490, Training Loss: 0.042839810624718665, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:04, Epoch: 113, Batch: 500, Training Loss: 0.04550280570983887, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:05, Epoch: 113, Batch: 510, Training Loss: 0.05155001506209374, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:06, Epoch: 113, Batch: 520, Training Loss: 0.05227023437619209, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:06, Epoch: 113, Batch: 530, Training Loss: 0.043197373300790785, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:07, Epoch: 113, Batch: 540, Training Loss: 0.051404181867837906, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:08, Epoch: 113, Batch: 550, Training Loss: 0.035855451971292494, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:08, Epoch: 113, Batch: 560, Training Loss: 0.039387752488255504, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:09, Epoch: 113, Batch: 570, Training Loss: 0.027318177744746208, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:10, Epoch: 113, Batch: 580, Training Loss: 0.03957888446748257, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:11, Epoch: 113, Batch: 590, Training Loss: 0.03944976627826691, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:11, Epoch: 113, Batch: 600, Training Loss: 0.03419376686215401, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:12, Epoch: 113, Batch: 610, Training Loss: 0.030699485912919044, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:13, Epoch: 113, Batch: 620, Training Loss: 0.031038358807563782, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:14, Epoch: 113, Batch: 630, Training Loss: 0.048774178326129916, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:14, Epoch: 113, Batch: 640, Training Loss: 0.037274906784296034, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:15, Epoch: 113, Batch: 650, Training Loss: 0.041098573803901674, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:16, Epoch: 113, Batch: 660, Training Loss: 0.045613115280866624, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:16, Epoch: 113, Batch: 670, Training Loss: 0.033971100300550464, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:17, Epoch: 113, Batch: 680, Training Loss: 0.036877914890646936, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:18, Epoch: 113, Batch: 690, Training Loss: 0.02979125827550888, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:19, Epoch: 113, Batch: 700, Training Loss: 0.0402114886790514, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:19, Epoch: 113, Batch: 710, Training Loss: 0.04264197796583176, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:20, Epoch: 113, Batch: 720, Training Loss: 0.04968390949070454, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:21, Epoch: 113, Batch: 730, Training Loss: 0.04264233335852623, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:22, Epoch: 113, Batch: 740, Training Loss: 0.03154894672334194, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:22, Epoch: 113, Batch: 750, Training Loss: 0.03736623264849186, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:23, Epoch: 113, Batch: 760, Training Loss: 0.05195842161774635, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:24, Epoch: 113, Batch: 770, Training Loss: 0.046262817829847334, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:25, Epoch: 113, Batch: 780, Training Loss: 0.02642389349639416, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:25, Epoch: 113, Batch: 790, Training Loss: 0.0372158020734787, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:26, Epoch: 113, Batch: 800, Training Loss: 0.044859679043293, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:27, Epoch: 113, Batch: 810, Training Loss: 0.03235886320471763, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:28, Epoch: 113, Batch: 820, Training Loss: 0.06100633665919304, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:28, Epoch: 113, Batch: 830, Training Loss: 0.04017549082636833, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:29, Epoch: 113, Batch: 840, Training Loss: 0.0425383560359478, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:30, Epoch: 113, Batch: 850, Training Loss: 0.038543276861310004, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:31, Epoch: 113, Batch: 860, Training Loss: 0.03470251522958279, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:31, Epoch: 113, Batch: 870, Training Loss: 0.04794108457863331, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:32, Epoch: 113, Batch: 880, Training Loss: 0.05605540350079537, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:33, Epoch: 113, Batch: 890, Training Loss: 0.02824830785393715, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:34, Epoch: 113, Batch: 900, Training Loss: 0.05172251872718334, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:34, Epoch: 113, Batch: 910, Training Loss: 0.04309862330555916, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:35, Epoch: 113, Batch: 920, Training Loss: 0.04568170495331288, LR: 0.00010000000000000003
Time, 2019-01-01T20:09:36, Epoch: 113, Batch: 930, Training Loss: 0.026584815979003907, LR: 0.00010000000000000003
Epoch: 113, Validation Top 1 acc: 98.88892364501953
Epoch: 113, Validation Top 5 acc: 99.99166870117188
Epoch: 113, Validation Set Loss: 0.040843091905117035
Start training epoch 114
Time, 2019-01-01T20:10:03, Epoch: 114, Batch: 10, Training Loss: 0.04188661649823189, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:04, Epoch: 114, Batch: 20, Training Loss: 0.04062330909073353, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:05, Epoch: 114, Batch: 30, Training Loss: 0.027092406526207924, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:05, Epoch: 114, Batch: 40, Training Loss: 0.05331713519990444, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:06, Epoch: 114, Batch: 50, Training Loss: 0.0426527313888073, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:07, Epoch: 114, Batch: 60, Training Loss: 0.052088203653693196, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:08, Epoch: 114, Batch: 70, Training Loss: 0.025977661088109016, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:08, Epoch: 114, Batch: 80, Training Loss: 0.04203876182436943, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:09, Epoch: 114, Batch: 90, Training Loss: 0.06457745730876922, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:10, Epoch: 114, Batch: 100, Training Loss: 0.024722900241613388, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:11, Epoch: 114, Batch: 110, Training Loss: 0.040049588307738304, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:11, Epoch: 114, Batch: 120, Training Loss: 0.05746694952249527, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:12, Epoch: 114, Batch: 130, Training Loss: 0.04118513204157352, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:13, Epoch: 114, Batch: 140, Training Loss: 0.04750397428870201, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:14, Epoch: 114, Batch: 150, Training Loss: 0.04515860751271248, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:14, Epoch: 114, Batch: 160, Training Loss: 0.04654942527413368, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:15, Epoch: 114, Batch: 170, Training Loss: 0.04295606277883053, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:16, Epoch: 114, Batch: 180, Training Loss: 0.05478348806500435, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:16, Epoch: 114, Batch: 190, Training Loss: 0.05825973115861416, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:17, Epoch: 114, Batch: 200, Training Loss: 0.03644130863249302, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:18, Epoch: 114, Batch: 210, Training Loss: 0.06850483864545823, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:19, Epoch: 114, Batch: 220, Training Loss: 0.042215832695364955, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:19, Epoch: 114, Batch: 230, Training Loss: 0.04008582085371017, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:20, Epoch: 114, Batch: 240, Training Loss: 0.025752684473991393, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:21, Epoch: 114, Batch: 250, Training Loss: 0.02972576916217804, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:22, Epoch: 114, Batch: 260, Training Loss: 0.03182678148150444, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:22, Epoch: 114, Batch: 270, Training Loss: 0.06711484640836715, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:23, Epoch: 114, Batch: 280, Training Loss: 0.03657832406461239, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:24, Epoch: 114, Batch: 290, Training Loss: 0.026851512491703033, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:24, Epoch: 114, Batch: 300, Training Loss: 0.03701450973749161, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:25, Epoch: 114, Batch: 310, Training Loss: 0.03192055635154247, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:26, Epoch: 114, Batch: 320, Training Loss: 0.03959289900958538, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:27, Epoch: 114, Batch: 330, Training Loss: 0.030976847931742667, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:27, Epoch: 114, Batch: 340, Training Loss: 0.04673190712928772, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:28, Epoch: 114, Batch: 350, Training Loss: 0.04001263231039047, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:29, Epoch: 114, Batch: 360, Training Loss: 0.05311244800686836, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:30, Epoch: 114, Batch: 370, Training Loss: 0.04070807099342346, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:30, Epoch: 114, Batch: 380, Training Loss: 0.0480596587061882, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:31, Epoch: 114, Batch: 390, Training Loss: 0.04866107031702995, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:32, Epoch: 114, Batch: 400, Training Loss: 0.06425403356552124, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:33, Epoch: 114, Batch: 410, Training Loss: 0.03574243038892746, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:33, Epoch: 114, Batch: 420, Training Loss: 0.03415527045726776, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:34, Epoch: 114, Batch: 430, Training Loss: 0.04914778396487236, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:35, Epoch: 114, Batch: 440, Training Loss: 0.038183679804205894, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:35, Epoch: 114, Batch: 450, Training Loss: 0.0298928152769804, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:36, Epoch: 114, Batch: 460, Training Loss: 0.0506625235080719, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:37, Epoch: 114, Batch: 470, Training Loss: 0.03138712123036384, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:38, Epoch: 114, Batch: 480, Training Loss: 0.05797617956995964, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:38, Epoch: 114, Batch: 490, Training Loss: 0.029994606971740723, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:39, Epoch: 114, Batch: 500, Training Loss: 0.040752853453159335, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:40, Epoch: 114, Batch: 510, Training Loss: 0.045060301572084425, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:40, Epoch: 114, Batch: 520, Training Loss: 0.05233343839645386, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:41, Epoch: 114, Batch: 530, Training Loss: 0.026199566200375557, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:42, Epoch: 114, Batch: 540, Training Loss: 0.04127944186329842, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:43, Epoch: 114, Batch: 550, Training Loss: 0.04107425808906555, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:43, Epoch: 114, Batch: 560, Training Loss: 0.03287753388285637, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:44, Epoch: 114, Batch: 570, Training Loss: 0.057069691643118856, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:45, Epoch: 114, Batch: 580, Training Loss: 0.03358039483428001, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:46, Epoch: 114, Batch: 590, Training Loss: 0.03110722340643406, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:46, Epoch: 114, Batch: 600, Training Loss: 0.054552240669727324, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:47, Epoch: 114, Batch: 610, Training Loss: 0.03037944994866848, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:48, Epoch: 114, Batch: 620, Training Loss: 0.03076837919652462, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:48, Epoch: 114, Batch: 630, Training Loss: 0.03855628445744515, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:50, Epoch: 114, Batch: 640, Training Loss: 0.05035595968365669, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:50, Epoch: 114, Batch: 650, Training Loss: 0.06321938931941987, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:51, Epoch: 114, Batch: 660, Training Loss: 0.04496640786528587, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:52, Epoch: 114, Batch: 670, Training Loss: 0.029740504175424575, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:53, Epoch: 114, Batch: 680, Training Loss: 0.0395045205950737, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:54, Epoch: 114, Batch: 690, Training Loss: 0.02403464838862419, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:55, Epoch: 114, Batch: 700, Training Loss: 0.04150477387011051, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:56, Epoch: 114, Batch: 710, Training Loss: 0.034554319083690645, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:57, Epoch: 114, Batch: 720, Training Loss: 0.02649020552635193, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:57, Epoch: 114, Batch: 730, Training Loss: 0.05350251868367195, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:58, Epoch: 114, Batch: 740, Training Loss: 0.05224559605121613, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:59, Epoch: 114, Batch: 750, Training Loss: 0.03174478262662887, LR: 0.00010000000000000003
Time, 2019-01-01T20:10:59, Epoch: 114, Batch: 760, Training Loss: 0.03461698405444622, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:00, Epoch: 114, Batch: 770, Training Loss: 0.0420887291431427, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:01, Epoch: 114, Batch: 780, Training Loss: 0.02918160371482372, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:02, Epoch: 114, Batch: 790, Training Loss: 0.031411242112517355, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:02, Epoch: 114, Batch: 800, Training Loss: 0.03973252847790718, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:03, Epoch: 114, Batch: 810, Training Loss: 0.03202860094606876, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:04, Epoch: 114, Batch: 820, Training Loss: 0.044089020043611524, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:05, Epoch: 114, Batch: 830, Training Loss: 0.03047531396150589, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:05, Epoch: 114, Batch: 840, Training Loss: 0.03685704246163368, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:06, Epoch: 114, Batch: 850, Training Loss: 0.05817989856004715, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:07, Epoch: 114, Batch: 860, Training Loss: 0.0301309309899807, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:08, Epoch: 114, Batch: 870, Training Loss: 0.03979567363858223, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:08, Epoch: 114, Batch: 880, Training Loss: 0.032700097933411595, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:09, Epoch: 114, Batch: 890, Training Loss: 0.0304966576397419, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:10, Epoch: 114, Batch: 900, Training Loss: 0.03533368073403835, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:10, Epoch: 114, Batch: 910, Training Loss: 0.046054303646087646, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:11, Epoch: 114, Batch: 920, Training Loss: 0.04907965064048767, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:12, Epoch: 114, Batch: 930, Training Loss: 0.04219974987208843, LR: 0.00010000000000000003
Epoch: 114, Validation Top 1 acc: 98.89059448242188
Epoch: 114, Validation Top 5 acc: 99.99000549316406
Epoch: 114, Validation Set Loss: 0.04083343222737312
Start training epoch 115
Time, 2019-01-01T20:11:41, Epoch: 115, Batch: 10, Training Loss: 0.03728383779525757, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:42, Epoch: 115, Batch: 20, Training Loss: 0.030964555591344832, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:43, Epoch: 115, Batch: 30, Training Loss: 0.05428815074265003, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:43, Epoch: 115, Batch: 40, Training Loss: 0.05590163692831993, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:44, Epoch: 115, Batch: 50, Training Loss: 0.052414070814847946, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:45, Epoch: 115, Batch: 60, Training Loss: 0.03453893661499023, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:46, Epoch: 115, Batch: 70, Training Loss: 0.033095283806324004, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:46, Epoch: 115, Batch: 80, Training Loss: 0.04565674178302288, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:47, Epoch: 115, Batch: 90, Training Loss: 0.04158020317554474, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:48, Epoch: 115, Batch: 100, Training Loss: 0.04408059902489185, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:49, Epoch: 115, Batch: 110, Training Loss: 0.031951117888092996, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:49, Epoch: 115, Batch: 120, Training Loss: 0.038408273831009865, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:50, Epoch: 115, Batch: 130, Training Loss: 0.04973302930593491, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:51, Epoch: 115, Batch: 140, Training Loss: 0.05411304906010628, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:51, Epoch: 115, Batch: 150, Training Loss: 0.04300518818199635, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:52, Epoch: 115, Batch: 160, Training Loss: 0.045014326274394986, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:53, Epoch: 115, Batch: 170, Training Loss: 0.04597744531929493, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:54, Epoch: 115, Batch: 180, Training Loss: 0.03259482607245445, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:54, Epoch: 115, Batch: 190, Training Loss: 0.03865495808422566, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:55, Epoch: 115, Batch: 200, Training Loss: 0.03768507838249206, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:56, Epoch: 115, Batch: 210, Training Loss: 0.03870537467300892, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:57, Epoch: 115, Batch: 220, Training Loss: 0.05256334282457829, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:57, Epoch: 115, Batch: 230, Training Loss: 0.03466640524566174, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:58, Epoch: 115, Batch: 240, Training Loss: 0.05190396830439568, LR: 0.00010000000000000003
Time, 2019-01-01T20:11:59, Epoch: 115, Batch: 250, Training Loss: 0.029365450143814087, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:00, Epoch: 115, Batch: 260, Training Loss: 0.05036850310862064, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:00, Epoch: 115, Batch: 270, Training Loss: 0.038566801324486734, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:01, Epoch: 115, Batch: 280, Training Loss: 0.03908407352864742, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:02, Epoch: 115, Batch: 290, Training Loss: 0.04856856167316437, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:03, Epoch: 115, Batch: 300, Training Loss: 0.03491649329662323, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:03, Epoch: 115, Batch: 310, Training Loss: 0.04300958663225174, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:04, Epoch: 115, Batch: 320, Training Loss: 0.0549075610935688, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:05, Epoch: 115, Batch: 330, Training Loss: 0.05239363312721253, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:06, Epoch: 115, Batch: 340, Training Loss: 0.01957486495375633, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:06, Epoch: 115, Batch: 350, Training Loss: 0.027351244539022445, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:07, Epoch: 115, Batch: 360, Training Loss: 0.040900233760476114, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:08, Epoch: 115, Batch: 370, Training Loss: 0.04298308864235878, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:09, Epoch: 115, Batch: 380, Training Loss: 0.0343201145529747, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:09, Epoch: 115, Batch: 390, Training Loss: 0.04265753962099552, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:10, Epoch: 115, Batch: 400, Training Loss: 0.04848138391971588, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:11, Epoch: 115, Batch: 410, Training Loss: 0.03787841536104679, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:11, Epoch: 115, Batch: 420, Training Loss: 0.03508489578962326, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:12, Epoch: 115, Batch: 430, Training Loss: 0.030353401973843574, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:13, Epoch: 115, Batch: 440, Training Loss: 0.03521291762590408, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:14, Epoch: 115, Batch: 450, Training Loss: 0.054034240543842316, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:14, Epoch: 115, Batch: 460, Training Loss: 0.03222681991755962, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:15, Epoch: 115, Batch: 470, Training Loss: 0.0319775752723217, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:16, Epoch: 115, Batch: 480, Training Loss: 0.026376862078905106, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:17, Epoch: 115, Batch: 490, Training Loss: 0.03338452130556106, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:17, Epoch: 115, Batch: 500, Training Loss: 0.042266291379928586, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:18, Epoch: 115, Batch: 510, Training Loss: 0.047430495545268056, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:19, Epoch: 115, Batch: 520, Training Loss: 0.042874546349048616, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:19, Epoch: 115, Batch: 530, Training Loss: 0.022388234734535217, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:20, Epoch: 115, Batch: 540, Training Loss: 0.03627987690269947, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:21, Epoch: 115, Batch: 550, Training Loss: 0.04066482707858086, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:22, Epoch: 115, Batch: 560, Training Loss: 0.03905446603894234, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:22, Epoch: 115, Batch: 570, Training Loss: 0.03490671440958977, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:23, Epoch: 115, Batch: 580, Training Loss: 0.05163161307573318, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:24, Epoch: 115, Batch: 590, Training Loss: 0.047661164030432704, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:24, Epoch: 115, Batch: 600, Training Loss: 0.04137636087834835, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:25, Epoch: 115, Batch: 610, Training Loss: 0.037192390859127046, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:26, Epoch: 115, Batch: 620, Training Loss: 0.04618685804307461, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:27, Epoch: 115, Batch: 630, Training Loss: 0.02949138581752777, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:27, Epoch: 115, Batch: 640, Training Loss: 0.029918378219008446, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:28, Epoch: 115, Batch: 650, Training Loss: 0.029915253445506097, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:29, Epoch: 115, Batch: 660, Training Loss: 0.029156428948044776, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:29, Epoch: 115, Batch: 670, Training Loss: 0.04732804521918297, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:30, Epoch: 115, Batch: 680, Training Loss: 0.04962953329086304, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:31, Epoch: 115, Batch: 690, Training Loss: 0.05030921995639801, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:32, Epoch: 115, Batch: 700, Training Loss: 0.04021233022212982, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:32, Epoch: 115, Batch: 710, Training Loss: 0.028032781928777693, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:33, Epoch: 115, Batch: 720, Training Loss: 0.06773914285004139, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:34, Epoch: 115, Batch: 730, Training Loss: 0.031550553441047666, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:34, Epoch: 115, Batch: 740, Training Loss: 0.033869563788175586, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:35, Epoch: 115, Batch: 750, Training Loss: 0.05693090334534645, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:36, Epoch: 115, Batch: 760, Training Loss: 0.04543393552303314, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:37, Epoch: 115, Batch: 770, Training Loss: 0.03211187459528446, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:37, Epoch: 115, Batch: 780, Training Loss: 0.039040620997548105, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:38, Epoch: 115, Batch: 790, Training Loss: 0.04303320795297623, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:39, Epoch: 115, Batch: 800, Training Loss: 0.04588567987084389, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:39, Epoch: 115, Batch: 810, Training Loss: 0.06756649501621723, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:40, Epoch: 115, Batch: 820, Training Loss: 0.03973467536270618, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:41, Epoch: 115, Batch: 830, Training Loss: 0.05510335937142372, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:42, Epoch: 115, Batch: 840, Training Loss: 0.037258928269147874, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:42, Epoch: 115, Batch: 850, Training Loss: 0.02179744727909565, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:43, Epoch: 115, Batch: 860, Training Loss: 0.04351713061332703, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:44, Epoch: 115, Batch: 870, Training Loss: 0.04848124757409096, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:44, Epoch: 115, Batch: 880, Training Loss: 0.03844913579523564, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:45, Epoch: 115, Batch: 890, Training Loss: 0.05898051708936691, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:46, Epoch: 115, Batch: 900, Training Loss: 0.03644147180020809, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:47, Epoch: 115, Batch: 910, Training Loss: 0.05153440572321415, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:47, Epoch: 115, Batch: 920, Training Loss: 0.03268243968486786, LR: 0.00010000000000000003
Time, 2019-01-01T20:12:48, Epoch: 115, Batch: 930, Training Loss: 0.03125704526901245, LR: 0.00010000000000000003
Epoch: 115, Validation Top 1 acc: 98.88892364501953
Epoch: 115, Validation Top 5 acc: 99.99000549316406
Epoch: 115, Validation Set Loss: 0.04076431691646576
Start training epoch 116
Time, 2019-01-01T20:13:15, Epoch: 116, Batch: 10, Training Loss: 0.04609627053141594, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:16, Epoch: 116, Batch: 20, Training Loss: 0.028184984996914863, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:16, Epoch: 116, Batch: 30, Training Loss: 0.04708594381809235, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:17, Epoch: 116, Batch: 40, Training Loss: 0.07216915525496007, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:18, Epoch: 116, Batch: 50, Training Loss: 0.040998471155762675, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:19, Epoch: 116, Batch: 60, Training Loss: 0.04470580331981182, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:19, Epoch: 116, Batch: 70, Training Loss: 0.031080079078674317, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:20, Epoch: 116, Batch: 80, Training Loss: 0.034683768451213834, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:21, Epoch: 116, Batch: 90, Training Loss: 0.05663424432277679, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:21, Epoch: 116, Batch: 100, Training Loss: 0.05508831404149532, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:22, Epoch: 116, Batch: 110, Training Loss: 0.04113572649657726, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:23, Epoch: 116, Batch: 120, Training Loss: 0.0524876669049263, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:24, Epoch: 116, Batch: 130, Training Loss: 0.0644296396523714, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:24, Epoch: 116, Batch: 140, Training Loss: 0.03324803039431572, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:25, Epoch: 116, Batch: 150, Training Loss: 0.04123116619884968, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:26, Epoch: 116, Batch: 160, Training Loss: 0.035972027108073235, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:26, Epoch: 116, Batch: 170, Training Loss: 0.03330766744911671, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:27, Epoch: 116, Batch: 180, Training Loss: 0.037910256534814835, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:28, Epoch: 116, Batch: 190, Training Loss: 0.03507634066045284, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:29, Epoch: 116, Batch: 200, Training Loss: 0.03208551518619061, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:29, Epoch: 116, Batch: 210, Training Loss: 0.0380313366651535, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:30, Epoch: 116, Batch: 220, Training Loss: 0.04478265307843685, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:31, Epoch: 116, Batch: 230, Training Loss: 0.03981245867908001, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:32, Epoch: 116, Batch: 240, Training Loss: 0.041324459761381147, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:32, Epoch: 116, Batch: 250, Training Loss: 0.02687830850481987, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:33, Epoch: 116, Batch: 260, Training Loss: 0.042053528130054474, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:34, Epoch: 116, Batch: 270, Training Loss: 0.04487305469810963, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:34, Epoch: 116, Batch: 280, Training Loss: 0.04761254824697971, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:35, Epoch: 116, Batch: 290, Training Loss: 0.046282052621245386, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:36, Epoch: 116, Batch: 300, Training Loss: 0.05702618658542633, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:37, Epoch: 116, Batch: 310, Training Loss: 0.03494270518422127, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:37, Epoch: 116, Batch: 320, Training Loss: 0.03071650303900242, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:38, Epoch: 116, Batch: 330, Training Loss: 0.03189169652760029, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:39, Epoch: 116, Batch: 340, Training Loss: 0.04636047892272473, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:40, Epoch: 116, Batch: 350, Training Loss: 0.04353389069437981, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:40, Epoch: 116, Batch: 360, Training Loss: 0.03385675475001335, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:41, Epoch: 116, Batch: 370, Training Loss: 0.05175529085099697, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:42, Epoch: 116, Batch: 380, Training Loss: 0.04463324137032032, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:42, Epoch: 116, Batch: 390, Training Loss: 0.05562717653810978, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:43, Epoch: 116, Batch: 400, Training Loss: 0.034565390273928645, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:44, Epoch: 116, Batch: 410, Training Loss: 0.04214313551783562, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:45, Epoch: 116, Batch: 420, Training Loss: 0.04019709900021553, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:45, Epoch: 116, Batch: 430, Training Loss: 0.029371456056833268, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:46, Epoch: 116, Batch: 440, Training Loss: 0.05602300688624382, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:47, Epoch: 116, Batch: 450, Training Loss: 0.049247801303863525, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:48, Epoch: 116, Batch: 460, Training Loss: 0.04924686700105667, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:48, Epoch: 116, Batch: 470, Training Loss: 0.03033519461750984, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:49, Epoch: 116, Batch: 480, Training Loss: 0.03584915548563004, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:50, Epoch: 116, Batch: 490, Training Loss: 0.056534364074468615, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:51, Epoch: 116, Batch: 500, Training Loss: 0.038201794028282166, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:51, Epoch: 116, Batch: 510, Training Loss: 0.01921166330575943, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:52, Epoch: 116, Batch: 520, Training Loss: 0.034135502949357036, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:53, Epoch: 116, Batch: 530, Training Loss: 0.053446841239929196, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:54, Epoch: 116, Batch: 540, Training Loss: 0.031981299817562106, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:54, Epoch: 116, Batch: 550, Training Loss: 0.03589544296264648, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:55, Epoch: 116, Batch: 560, Training Loss: 0.037073947861790656, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:56, Epoch: 116, Batch: 570, Training Loss: 0.032107991725206377, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:57, Epoch: 116, Batch: 580, Training Loss: 0.03976198211312294, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:57, Epoch: 116, Batch: 590, Training Loss: 0.03573246076703072, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:58, Epoch: 116, Batch: 600, Training Loss: 0.045318521559238434, LR: 0.00010000000000000003
Time, 2019-01-01T20:13:59, Epoch: 116, Batch: 610, Training Loss: 0.03622110188007355, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:00, Epoch: 116, Batch: 620, Training Loss: 0.029262449219822885, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:00, Epoch: 116, Batch: 630, Training Loss: 0.029599811881780624, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:01, Epoch: 116, Batch: 640, Training Loss: 0.027107061818242073, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:02, Epoch: 116, Batch: 650, Training Loss: 0.0444774467498064, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:02, Epoch: 116, Batch: 660, Training Loss: 0.06173897683620453, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:03, Epoch: 116, Batch: 670, Training Loss: 0.03317074179649353, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:04, Epoch: 116, Batch: 680, Training Loss: 0.03978252112865448, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:05, Epoch: 116, Batch: 690, Training Loss: 0.045518628880381586, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:05, Epoch: 116, Batch: 700, Training Loss: 0.02242985963821411, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:06, Epoch: 116, Batch: 710, Training Loss: 0.03929863125085831, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:07, Epoch: 116, Batch: 720, Training Loss: 0.03172406330704689, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:08, Epoch: 116, Batch: 730, Training Loss: 0.028253429383039475, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:08, Epoch: 116, Batch: 740, Training Loss: 0.03500642478466034, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:09, Epoch: 116, Batch: 750, Training Loss: 0.05310304872691631, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:10, Epoch: 116, Batch: 760, Training Loss: 0.043631890416145326, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:11, Epoch: 116, Batch: 770, Training Loss: 0.057624871283769606, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:11, Epoch: 116, Batch: 780, Training Loss: 0.03111483305692673, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:12, Epoch: 116, Batch: 790, Training Loss: 0.04601966515183449, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:13, Epoch: 116, Batch: 800, Training Loss: 0.03574221767485142, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:13, Epoch: 116, Batch: 810, Training Loss: 0.03558031059801579, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:14, Epoch: 116, Batch: 820, Training Loss: 0.051326099410653116, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:15, Epoch: 116, Batch: 830, Training Loss: 0.04408430717885494, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:16, Epoch: 116, Batch: 840, Training Loss: 0.038828647881746295, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:16, Epoch: 116, Batch: 850, Training Loss: 0.040707853063941, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:17, Epoch: 116, Batch: 860, Training Loss: 0.03263053372502327, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:18, Epoch: 116, Batch: 870, Training Loss: 0.04828414097428322, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:18, Epoch: 116, Batch: 880, Training Loss: 0.03357287012040615, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:19, Epoch: 116, Batch: 890, Training Loss: 0.03485110886394978, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:20, Epoch: 116, Batch: 900, Training Loss: 0.05691530890762806, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:21, Epoch: 116, Batch: 910, Training Loss: 0.04688267037272453, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:21, Epoch: 116, Batch: 920, Training Loss: 0.03689966462552548, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:22, Epoch: 116, Batch: 930, Training Loss: 0.046887566521763804, LR: 0.00010000000000000003
Epoch: 116, Validation Top 1 acc: 98.88226318359375
Epoch: 116, Validation Top 5 acc: 99.99000549316406
Epoch: 116, Validation Set Loss: 0.04077765345573425
Start training epoch 117
Time, 2019-01-01T20:14:50, Epoch: 117, Batch: 10, Training Loss: 0.045918264985084535, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:50, Epoch: 117, Batch: 20, Training Loss: 0.03932748399674892, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:51, Epoch: 117, Batch: 30, Training Loss: 0.056412620842456816, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:52, Epoch: 117, Batch: 40, Training Loss: 0.04097231701016426, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:53, Epoch: 117, Batch: 50, Training Loss: 0.044006452336907385, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:53, Epoch: 117, Batch: 60, Training Loss: 0.036433075368404386, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:54, Epoch: 117, Batch: 70, Training Loss: 0.035977034270763396, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:55, Epoch: 117, Batch: 80, Training Loss: 0.04320884719491005, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:55, Epoch: 117, Batch: 90, Training Loss: 0.04239161759614944, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:56, Epoch: 117, Batch: 100, Training Loss: 0.03468451276421547, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:57, Epoch: 117, Batch: 110, Training Loss: 0.04126317910850048, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:58, Epoch: 117, Batch: 120, Training Loss: 0.03472844362258911, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:58, Epoch: 117, Batch: 130, Training Loss: 0.03667740747332573, LR: 0.00010000000000000003
Time, 2019-01-01T20:14:59, Epoch: 117, Batch: 140, Training Loss: 0.04151474609971047, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:00, Epoch: 117, Batch: 150, Training Loss: 0.05498259030282497, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:00, Epoch: 117, Batch: 160, Training Loss: 0.04862429723143578, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:01, Epoch: 117, Batch: 170, Training Loss: 0.05794952251017094, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:02, Epoch: 117, Batch: 180, Training Loss: 0.047511307150125505, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:03, Epoch: 117, Batch: 190, Training Loss: 0.03623901829123497, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:03, Epoch: 117, Batch: 200, Training Loss: 0.04109141603112221, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:04, Epoch: 117, Batch: 210, Training Loss: 0.043081474304199216, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:05, Epoch: 117, Batch: 220, Training Loss: 0.04576408453285694, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:06, Epoch: 117, Batch: 230, Training Loss: 0.02937019318342209, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:06, Epoch: 117, Batch: 240, Training Loss: 0.03546716161072254, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:07, Epoch: 117, Batch: 250, Training Loss: 0.05261891223490238, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:08, Epoch: 117, Batch: 260, Training Loss: 0.045556243509054184, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:08, Epoch: 117, Batch: 270, Training Loss: 0.035591240599751474, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:09, Epoch: 117, Batch: 280, Training Loss: 0.04125586338341236, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:10, Epoch: 117, Batch: 290, Training Loss: 0.02440527006983757, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:11, Epoch: 117, Batch: 300, Training Loss: 0.02720526307821274, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:11, Epoch: 117, Batch: 310, Training Loss: 0.04709675870835781, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:12, Epoch: 117, Batch: 320, Training Loss: 0.039591985195875166, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:13, Epoch: 117, Batch: 330, Training Loss: 0.02881375849246979, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:14, Epoch: 117, Batch: 340, Training Loss: 0.03867286294698715, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:14, Epoch: 117, Batch: 350, Training Loss: 0.024459435045719145, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:15, Epoch: 117, Batch: 360, Training Loss: 0.04047074057161808, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:16, Epoch: 117, Batch: 370, Training Loss: 0.05156360492110253, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:17, Epoch: 117, Batch: 380, Training Loss: 0.031481312960386275, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:17, Epoch: 117, Batch: 390, Training Loss: 0.03964406847953796, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:18, Epoch: 117, Batch: 400, Training Loss: 0.03897845782339573, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:19, Epoch: 117, Batch: 410, Training Loss: 0.03265220560133457, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:20, Epoch: 117, Batch: 420, Training Loss: 0.03628235161304474, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:20, Epoch: 117, Batch: 430, Training Loss: 0.03444480486214161, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:21, Epoch: 117, Batch: 440, Training Loss: 0.036301161721348764, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:22, Epoch: 117, Batch: 450, Training Loss: 0.049141842126846316, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:22, Epoch: 117, Batch: 460, Training Loss: 0.03982268236577511, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:23, Epoch: 117, Batch: 470, Training Loss: 0.05171361081302166, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:24, Epoch: 117, Batch: 480, Training Loss: 0.02985398322343826, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:25, Epoch: 117, Batch: 490, Training Loss: 0.036973844096064565, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:25, Epoch: 117, Batch: 500, Training Loss: 0.04107123762369156, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:26, Epoch: 117, Batch: 510, Training Loss: 0.049366043880581856, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:27, Epoch: 117, Batch: 520, Training Loss: 0.02921304851770401, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:27, Epoch: 117, Batch: 530, Training Loss: 0.055228932201862334, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:28, Epoch: 117, Batch: 540, Training Loss: 0.03822782784700394, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:29, Epoch: 117, Batch: 550, Training Loss: 0.051514987647533414, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:30, Epoch: 117, Batch: 560, Training Loss: 0.03189354613423347, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:30, Epoch: 117, Batch: 570, Training Loss: 0.04452277235686779, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:31, Epoch: 117, Batch: 580, Training Loss: 0.031279712170362475, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:32, Epoch: 117, Batch: 590, Training Loss: 0.04955210164189339, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:32, Epoch: 117, Batch: 600, Training Loss: 0.03401757925748825, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:33, Epoch: 117, Batch: 610, Training Loss: 0.04126881808042526, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:34, Epoch: 117, Batch: 620, Training Loss: 0.037061996012926104, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:35, Epoch: 117, Batch: 630, Training Loss: 0.026554619148373603, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:35, Epoch: 117, Batch: 640, Training Loss: 0.050085614621639254, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:36, Epoch: 117, Batch: 650, Training Loss: 0.033871890231966975, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:37, Epoch: 117, Batch: 660, Training Loss: 0.050982401520013806, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:37, Epoch: 117, Batch: 670, Training Loss: 0.02978871501982212, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:38, Epoch: 117, Batch: 680, Training Loss: 0.03652171939611435, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:39, Epoch: 117, Batch: 690, Training Loss: 0.03747408241033554, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:40, Epoch: 117, Batch: 700, Training Loss: 0.04326928220689297, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:40, Epoch: 117, Batch: 710, Training Loss: 0.04991176836192608, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:41, Epoch: 117, Batch: 720, Training Loss: 0.04953249804675579, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:42, Epoch: 117, Batch: 730, Training Loss: 0.03321092687547207, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:42, Epoch: 117, Batch: 740, Training Loss: 0.04378180131316185, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:43, Epoch: 117, Batch: 750, Training Loss: 0.05139190405607223, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:44, Epoch: 117, Batch: 760, Training Loss: 0.04000401683151722, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:45, Epoch: 117, Batch: 770, Training Loss: 0.026244597882032393, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:45, Epoch: 117, Batch: 780, Training Loss: 0.03328651450574398, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:46, Epoch: 117, Batch: 790, Training Loss: 0.05496655702590943, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:47, Epoch: 117, Batch: 800, Training Loss: 0.03234279453754425, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:47, Epoch: 117, Batch: 810, Training Loss: 0.04522216506302357, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:48, Epoch: 117, Batch: 820, Training Loss: 0.04079127795994282, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:49, Epoch: 117, Batch: 830, Training Loss: 0.03786005824804306, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:50, Epoch: 117, Batch: 840, Training Loss: 0.0464285597205162, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:50, Epoch: 117, Batch: 850, Training Loss: 0.041635046154260634, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:51, Epoch: 117, Batch: 860, Training Loss: 0.049805169180035594, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:52, Epoch: 117, Batch: 870, Training Loss: 0.047459641471505165, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:52, Epoch: 117, Batch: 880, Training Loss: 0.040437051653862, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:53, Epoch: 117, Batch: 890, Training Loss: 0.04748445749282837, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:54, Epoch: 117, Batch: 900, Training Loss: 0.052577226981520654, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:55, Epoch: 117, Batch: 910, Training Loss: 0.04455778077244758, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:55, Epoch: 117, Batch: 920, Training Loss: 0.04926871284842491, LR: 0.00010000000000000003
Time, 2019-01-01T20:15:56, Epoch: 117, Batch: 930, Training Loss: 0.04530240222811699, LR: 0.00010000000000000003
Epoch: 117, Validation Top 1 acc: 98.89225769042969
Epoch: 117, Validation Top 5 acc: 99.99166870117188
Epoch: 117, Validation Set Loss: 0.04074389487504959
Start training epoch 118
Time, 2019-01-01T20:16:23, Epoch: 118, Batch: 10, Training Loss: 0.05136956050992012, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:24, Epoch: 118, Batch: 20, Training Loss: 0.02524234801530838, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:25, Epoch: 118, Batch: 30, Training Loss: 0.030494510754942895, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:25, Epoch: 118, Batch: 40, Training Loss: 0.038948601111769676, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:26, Epoch: 118, Batch: 50, Training Loss: 0.030679035931825638, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:27, Epoch: 118, Batch: 60, Training Loss: 0.044759619235992434, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:28, Epoch: 118, Batch: 70, Training Loss: 0.04914550930261612, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:28, Epoch: 118, Batch: 80, Training Loss: 0.04208480641245842, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:29, Epoch: 118, Batch: 90, Training Loss: 0.046935342624783515, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:30, Epoch: 118, Batch: 100, Training Loss: 0.04565946497023106, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:30, Epoch: 118, Batch: 110, Training Loss: 0.05124122574925423, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:31, Epoch: 118, Batch: 120, Training Loss: 0.06785613596439362, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:32, Epoch: 118, Batch: 130, Training Loss: 0.05900918133556843, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:33, Epoch: 118, Batch: 140, Training Loss: 0.03396944403648376, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:33, Epoch: 118, Batch: 150, Training Loss: 0.049159877747297284, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:34, Epoch: 118, Batch: 160, Training Loss: 0.04729067794978618, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:35, Epoch: 118, Batch: 170, Training Loss: 0.030562162399291992, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:36, Epoch: 118, Batch: 180, Training Loss: 0.046124599128961566, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:36, Epoch: 118, Batch: 190, Training Loss: 0.038301202654838565, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:37, Epoch: 118, Batch: 200, Training Loss: 0.040315089374780656, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:38, Epoch: 118, Batch: 210, Training Loss: 0.050650833547115325, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:38, Epoch: 118, Batch: 220, Training Loss: 0.03734282627701759, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:39, Epoch: 118, Batch: 230, Training Loss: 0.05004052296280861, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:40, Epoch: 118, Batch: 240, Training Loss: 0.03637082427740097, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:41, Epoch: 118, Batch: 250, Training Loss: 0.04106922633945942, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:41, Epoch: 118, Batch: 260, Training Loss: 0.06602097637951373, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:42, Epoch: 118, Batch: 270, Training Loss: 0.04676991961896419, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:43, Epoch: 118, Batch: 280, Training Loss: 0.03312421031296253, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:44, Epoch: 118, Batch: 290, Training Loss: 0.03222226090729237, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:45, Epoch: 118, Batch: 300, Training Loss: 0.04234100952744484, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:46, Epoch: 118, Batch: 310, Training Loss: 0.049975369870662686, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:46, Epoch: 118, Batch: 320, Training Loss: 0.03016972616314888, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:47, Epoch: 118, Batch: 330, Training Loss: 0.059841899946331975, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:48, Epoch: 118, Batch: 340, Training Loss: 0.040984614938497546, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:49, Epoch: 118, Batch: 350, Training Loss: 0.023153626918792726, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:50, Epoch: 118, Batch: 360, Training Loss: 0.02887752577662468, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:51, Epoch: 118, Batch: 370, Training Loss: 0.03896847330033779, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:52, Epoch: 118, Batch: 380, Training Loss: 0.029145486280322076, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:52, Epoch: 118, Batch: 390, Training Loss: 0.04181841537356377, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:53, Epoch: 118, Batch: 400, Training Loss: 0.034166548401117325, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:54, Epoch: 118, Batch: 410, Training Loss: 0.04847036004066467, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:55, Epoch: 118, Batch: 420, Training Loss: 0.04026564583182335, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:56, Epoch: 118, Batch: 430, Training Loss: 0.03320121951401234, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:57, Epoch: 118, Batch: 440, Training Loss: 0.03883448764681816, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:57, Epoch: 118, Batch: 450, Training Loss: 0.02825739309191704, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:58, Epoch: 118, Batch: 460, Training Loss: 0.02800907418131828, LR: 0.00010000000000000003
Time, 2019-01-01T20:16:59, Epoch: 118, Batch: 470, Training Loss: 0.05055032894015312, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:00, Epoch: 118, Batch: 480, Training Loss: 0.047818959876894954, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:01, Epoch: 118, Batch: 490, Training Loss: 0.04341720603406429, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:02, Epoch: 118, Batch: 500, Training Loss: 0.041128892451524734, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:02, Epoch: 118, Batch: 510, Training Loss: 0.02770555391907692, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:03, Epoch: 118, Batch: 520, Training Loss: 0.04291748590767384, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:04, Epoch: 118, Batch: 530, Training Loss: 0.04678396955132484, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:05, Epoch: 118, Batch: 540, Training Loss: 0.04658735357224941, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:05, Epoch: 118, Batch: 550, Training Loss: 0.06282231733202934, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:06, Epoch: 118, Batch: 560, Training Loss: 0.03104659728705883, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:07, Epoch: 118, Batch: 570, Training Loss: 0.03305063806474209, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:08, Epoch: 118, Batch: 580, Training Loss: 0.04016541168093681, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:08, Epoch: 118, Batch: 590, Training Loss: 0.04754166454076767, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:09, Epoch: 118, Batch: 600, Training Loss: 0.035642708837985995, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:10, Epoch: 118, Batch: 610, Training Loss: 0.045090126991271975, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:11, Epoch: 118, Batch: 620, Training Loss: 0.0397968627512455, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:12, Epoch: 118, Batch: 630, Training Loss: 0.028247272595763206, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:12, Epoch: 118, Batch: 640, Training Loss: 0.05533154457807541, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:13, Epoch: 118, Batch: 650, Training Loss: 0.04028719291090965, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:14, Epoch: 118, Batch: 660, Training Loss: 0.04026701897382736, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:14, Epoch: 118, Batch: 670, Training Loss: 0.04702337458729744, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:15, Epoch: 118, Batch: 680, Training Loss: 0.03498177565634251, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:16, Epoch: 118, Batch: 690, Training Loss: 0.037994449213147166, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:17, Epoch: 118, Batch: 700, Training Loss: 0.04133947044610977, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:17, Epoch: 118, Batch: 710, Training Loss: 0.03219419457018376, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:18, Epoch: 118, Batch: 720, Training Loss: 0.04174893572926521, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:19, Epoch: 118, Batch: 730, Training Loss: 0.03314987123012543, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:19, Epoch: 118, Batch: 740, Training Loss: 0.040409935638308525, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:20, Epoch: 118, Batch: 750, Training Loss: 0.03805255740880966, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:21, Epoch: 118, Batch: 760, Training Loss: 0.040243246033787726, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:22, Epoch: 118, Batch: 770, Training Loss: 0.05474471338093281, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:22, Epoch: 118, Batch: 780, Training Loss: 0.04356499463319778, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:23, Epoch: 118, Batch: 790, Training Loss: 0.03640488833189011, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:24, Epoch: 118, Batch: 800, Training Loss: 0.025381511822342873, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:24, Epoch: 118, Batch: 810, Training Loss: 0.042370707541704175, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:25, Epoch: 118, Batch: 820, Training Loss: 0.04778463765978813, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:26, Epoch: 118, Batch: 830, Training Loss: 0.03643769919872284, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:27, Epoch: 118, Batch: 840, Training Loss: 0.041555067896842955, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:27, Epoch: 118, Batch: 850, Training Loss: 0.04619176797568798, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:28, Epoch: 118, Batch: 860, Training Loss: 0.05236186273396015, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:29, Epoch: 118, Batch: 870, Training Loss: 0.04205192923545838, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:29, Epoch: 118, Batch: 880, Training Loss: 0.035511980578303334, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:30, Epoch: 118, Batch: 890, Training Loss: 0.04247238449752331, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:31, Epoch: 118, Batch: 900, Training Loss: 0.03699280731379986, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:32, Epoch: 118, Batch: 910, Training Loss: 0.03661063797771931, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:32, Epoch: 118, Batch: 920, Training Loss: 0.031119440123438834, LR: 0.00010000000000000003
Time, 2019-01-01T20:17:33, Epoch: 118, Batch: 930, Training Loss: 0.03157990798354149, LR: 0.00010000000000000003
Epoch: 118, Validation Top 1 acc: 98.89725494384766
Epoch: 118, Validation Top 5 acc: 99.99000549316406
Epoch: 118, Validation Set Loss: 0.04075726494193077
Start training epoch 119
Time, 2019-01-01T20:18:02, Epoch: 119, Batch: 10, Training Loss: 0.026754750683903695, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:02, Epoch: 119, Batch: 20, Training Loss: 0.0426571324467659, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:03, Epoch: 119, Batch: 30, Training Loss: 0.03762809112668038, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:04, Epoch: 119, Batch: 40, Training Loss: 0.0375659354031086, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:05, Epoch: 119, Batch: 50, Training Loss: 0.02598264217376709, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:06, Epoch: 119, Batch: 60, Training Loss: 0.03306107148528099, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:07, Epoch: 119, Batch: 70, Training Loss: 0.031016302481293678, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:08, Epoch: 119, Batch: 80, Training Loss: 0.04486777186393738, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:09, Epoch: 119, Batch: 90, Training Loss: 0.052298673614859584, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:09, Epoch: 119, Batch: 100, Training Loss: 0.04045494645833969, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:10, Epoch: 119, Batch: 110, Training Loss: 0.02892204523086548, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:11, Epoch: 119, Batch: 120, Training Loss: 0.03304306343197823, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:12, Epoch: 119, Batch: 130, Training Loss: 0.030604436993598938, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:13, Epoch: 119, Batch: 140, Training Loss: 0.028994500264525415, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:14, Epoch: 119, Batch: 150, Training Loss: 0.04351416379213333, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:14, Epoch: 119, Batch: 160, Training Loss: 0.03955480791628361, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:15, Epoch: 119, Batch: 170, Training Loss: 0.03328461311757565, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:16, Epoch: 119, Batch: 180, Training Loss: 0.041935885697603224, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:17, Epoch: 119, Batch: 190, Training Loss: 0.061046280339360236, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:17, Epoch: 119, Batch: 200, Training Loss: 0.053143758699297904, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:18, Epoch: 119, Batch: 210, Training Loss: 0.04378705881536007, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:19, Epoch: 119, Batch: 220, Training Loss: 0.036975758895277974, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:19, Epoch: 119, Batch: 230, Training Loss: 0.04021434411406517, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:20, Epoch: 119, Batch: 240, Training Loss: 0.046545344218611714, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:21, Epoch: 119, Batch: 250, Training Loss: 0.047364100068807605, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:22, Epoch: 119, Batch: 260, Training Loss: 0.04385338798165321, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:22, Epoch: 119, Batch: 270, Training Loss: 0.048661010712385176, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:23, Epoch: 119, Batch: 280, Training Loss: 0.031039724498987196, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:24, Epoch: 119, Batch: 290, Training Loss: 0.029246576130390167, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:24, Epoch: 119, Batch: 300, Training Loss: 0.03196292147040367, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:25, Epoch: 119, Batch: 310, Training Loss: 0.04118291400372982, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:26, Epoch: 119, Batch: 320, Training Loss: 0.0339365117251873, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:27, Epoch: 119, Batch: 330, Training Loss: 0.04494786597788334, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:27, Epoch: 119, Batch: 340, Training Loss: 0.04188619889318943, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:28, Epoch: 119, Batch: 350, Training Loss: 0.036569146811962126, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:29, Epoch: 119, Batch: 360, Training Loss: 0.04309990108013153, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:29, Epoch: 119, Batch: 370, Training Loss: 0.03797466866672039, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:30, Epoch: 119, Batch: 380, Training Loss: 0.047003833949565886, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:31, Epoch: 119, Batch: 390, Training Loss: 0.05442083328962326, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:32, Epoch: 119, Batch: 400, Training Loss: 0.054174546897411344, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:32, Epoch: 119, Batch: 410, Training Loss: 0.038069769740104675, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:33, Epoch: 119, Batch: 420, Training Loss: 0.04724275395274162, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:34, Epoch: 119, Batch: 430, Training Loss: 0.048382996767759326, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:35, Epoch: 119, Batch: 440, Training Loss: 0.0330195214599371, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:36, Epoch: 119, Batch: 450, Training Loss: 0.04175257384777069, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:36, Epoch: 119, Batch: 460, Training Loss: 0.055891604721546174, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:37, Epoch: 119, Batch: 470, Training Loss: 0.04233711361885071, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:38, Epoch: 119, Batch: 480, Training Loss: 0.042516553029417994, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:38, Epoch: 119, Batch: 490, Training Loss: 0.06469875238835812, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:39, Epoch: 119, Batch: 500, Training Loss: 0.037631867453455925, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:40, Epoch: 119, Batch: 510, Training Loss: 0.03494189828634262, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:41, Epoch: 119, Batch: 520, Training Loss: 0.0468086414039135, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:41, Epoch: 119, Batch: 530, Training Loss: 0.03435085155069828, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:42, Epoch: 119, Batch: 540, Training Loss: 0.025536540150642394, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:43, Epoch: 119, Batch: 550, Training Loss: 0.04863134138286114, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:44, Epoch: 119, Batch: 560, Training Loss: 0.04765603542327881, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:44, Epoch: 119, Batch: 570, Training Loss: 0.04694051817059517, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:45, Epoch: 119, Batch: 580, Training Loss: 0.02855979688465595, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:46, Epoch: 119, Batch: 590, Training Loss: 0.045920441299676894, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:46, Epoch: 119, Batch: 600, Training Loss: 0.04538893215358257, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:47, Epoch: 119, Batch: 610, Training Loss: 0.03993071839213371, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:48, Epoch: 119, Batch: 620, Training Loss: 0.034908606484532353, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:49, Epoch: 119, Batch: 630, Training Loss: 0.03928233906626701, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:49, Epoch: 119, Batch: 640, Training Loss: 0.037166884541511534, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:50, Epoch: 119, Batch: 650, Training Loss: 0.041818706691265105, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:51, Epoch: 119, Batch: 660, Training Loss: 0.0373935267329216, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:52, Epoch: 119, Batch: 670, Training Loss: 0.033326685056090356, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:52, Epoch: 119, Batch: 680, Training Loss: 0.04138602614402771, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:53, Epoch: 119, Batch: 690, Training Loss: 0.041610265523195265, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:54, Epoch: 119, Batch: 700, Training Loss: 0.02554255947470665, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:54, Epoch: 119, Batch: 710, Training Loss: 0.055676556378602984, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:55, Epoch: 119, Batch: 720, Training Loss: 0.03098081760108471, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:56, Epoch: 119, Batch: 730, Training Loss: 0.05335612632334232, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:57, Epoch: 119, Batch: 740, Training Loss: 0.04557584039866924, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:57, Epoch: 119, Batch: 750, Training Loss: 0.03292654827237129, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:58, Epoch: 119, Batch: 760, Training Loss: 0.053301910310983656, LR: 0.00010000000000000003
Time, 2019-01-01T20:18:59, Epoch: 119, Batch: 770, Training Loss: 0.03679172843694687, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:00, Epoch: 119, Batch: 780, Training Loss: 0.0331327885389328, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:00, Epoch: 119, Batch: 790, Training Loss: 0.02937854826450348, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:01, Epoch: 119, Batch: 800, Training Loss: 0.03759119212627411, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:02, Epoch: 119, Batch: 810, Training Loss: 0.06384515836834907, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:02, Epoch: 119, Batch: 820, Training Loss: 0.04842144176363945, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:03, Epoch: 119, Batch: 830, Training Loss: 0.03573789782822132, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:04, Epoch: 119, Batch: 840, Training Loss: 0.03305414393544197, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:05, Epoch: 119, Batch: 850, Training Loss: 0.04738133661448955, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:05, Epoch: 119, Batch: 860, Training Loss: 0.055159220844507216, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:06, Epoch: 119, Batch: 870, Training Loss: 0.05365460254251957, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:07, Epoch: 119, Batch: 880, Training Loss: 0.05133276097476482, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:07, Epoch: 119, Batch: 890, Training Loss: 0.05103221014142036, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:08, Epoch: 119, Batch: 900, Training Loss: 0.034290964156389235, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:09, Epoch: 119, Batch: 910, Training Loss: 0.03616696260869503, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:10, Epoch: 119, Batch: 920, Training Loss: 0.03910192809998989, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:10, Epoch: 119, Batch: 930, Training Loss: 0.02552921287715435, LR: 0.00010000000000000003
Epoch: 119, Validation Top 1 acc: 98.90391540527344
Epoch: 119, Validation Top 5 acc: 99.99000549316406
Epoch: 119, Validation Set Loss: 0.040758226066827774
Start training epoch 120
Time, 2019-01-01T20:19:38, Epoch: 120, Batch: 10, Training Loss: 0.031134795770049094, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:39, Epoch: 120, Batch: 20, Training Loss: 0.04087427146732807, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:40, Epoch: 120, Batch: 30, Training Loss: 0.03805134147405624, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:41, Epoch: 120, Batch: 40, Training Loss: 0.03330951929092407, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:41, Epoch: 120, Batch: 50, Training Loss: 0.035542663931846616, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:42, Epoch: 120, Batch: 60, Training Loss: 0.041467639803886416, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:43, Epoch: 120, Batch: 70, Training Loss: 0.05238752253353596, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:43, Epoch: 120, Batch: 80, Training Loss: 0.05930163860321045, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:44, Epoch: 120, Batch: 90, Training Loss: 0.05402737334370613, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:45, Epoch: 120, Batch: 100, Training Loss: 0.046192479133605954, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:46, Epoch: 120, Batch: 110, Training Loss: 0.044174734875559805, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:46, Epoch: 120, Batch: 120, Training Loss: 0.029394232481718064, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:47, Epoch: 120, Batch: 130, Training Loss: 0.052785859629511836, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:48, Epoch: 120, Batch: 140, Training Loss: 0.027183112129569052, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:49, Epoch: 120, Batch: 150, Training Loss: 0.0525184340775013, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:49, Epoch: 120, Batch: 160, Training Loss: 0.02901226542890072, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:50, Epoch: 120, Batch: 170, Training Loss: 0.03310809880495071, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:51, Epoch: 120, Batch: 180, Training Loss: 0.03540680259466171, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:51, Epoch: 120, Batch: 190, Training Loss: 0.033612166345119474, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:52, Epoch: 120, Batch: 200, Training Loss: 0.05024294443428516, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:53, Epoch: 120, Batch: 210, Training Loss: 0.03482338935136795, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:54, Epoch: 120, Batch: 220, Training Loss: 0.049422469735145566, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:54, Epoch: 120, Batch: 230, Training Loss: 0.038947729766368865, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:55, Epoch: 120, Batch: 240, Training Loss: 0.05940157398581505, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:56, Epoch: 120, Batch: 250, Training Loss: 0.049552418291568756, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:56, Epoch: 120, Batch: 260, Training Loss: 0.03638928718864918, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:57, Epoch: 120, Batch: 270, Training Loss: 0.031220932304859162, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:58, Epoch: 120, Batch: 280, Training Loss: 0.07625024020671844, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:59, Epoch: 120, Batch: 290, Training Loss: 0.05686130300164223, LR: 0.00010000000000000003
Time, 2019-01-01T20:19:59, Epoch: 120, Batch: 300, Training Loss: 0.0374546155333519, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:00, Epoch: 120, Batch: 310, Training Loss: 0.036042249947786334, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:01, Epoch: 120, Batch: 320, Training Loss: 0.045835965499281886, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:02, Epoch: 120, Batch: 330, Training Loss: 0.03092159181833267, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:02, Epoch: 120, Batch: 340, Training Loss: 0.032577435672283175, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:03, Epoch: 120, Batch: 350, Training Loss: 0.03242946155369282, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:04, Epoch: 120, Batch: 360, Training Loss: 0.02828207239508629, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:05, Epoch: 120, Batch: 370, Training Loss: 0.02602378875017166, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:05, Epoch: 120, Batch: 380, Training Loss: 0.04277975670993328, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:06, Epoch: 120, Batch: 390, Training Loss: 0.05342698469758034, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:07, Epoch: 120, Batch: 400, Training Loss: 0.04076831229031086, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:07, Epoch: 120, Batch: 410, Training Loss: 0.038402621448040006, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:08, Epoch: 120, Batch: 420, Training Loss: 0.03480915501713753, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:09, Epoch: 120, Batch: 430, Training Loss: 0.04553847387433052, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:10, Epoch: 120, Batch: 440, Training Loss: 0.026261179894208907, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:10, Epoch: 120, Batch: 450, Training Loss: 0.02643674910068512, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:11, Epoch: 120, Batch: 460, Training Loss: 0.03421318456530571, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:12, Epoch: 120, Batch: 470, Training Loss: 0.026563523337244987, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:13, Epoch: 120, Batch: 480, Training Loss: 0.030954911559820174, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:13, Epoch: 120, Batch: 490, Training Loss: 0.04668691828846931, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:14, Epoch: 120, Batch: 500, Training Loss: 0.04479805454611778, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:15, Epoch: 120, Batch: 510, Training Loss: 0.04367116056382656, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:15, Epoch: 120, Batch: 520, Training Loss: 0.03575049042701721, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:16, Epoch: 120, Batch: 530, Training Loss: 0.04201709553599357, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:17, Epoch: 120, Batch: 540, Training Loss: 0.03485398106276989, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:18, Epoch: 120, Batch: 550, Training Loss: 0.0341033298522234, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:18, Epoch: 120, Batch: 560, Training Loss: 0.03722974956035614, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:19, Epoch: 120, Batch: 570, Training Loss: 0.03884119540452957, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:20, Epoch: 120, Batch: 580, Training Loss: 0.028584382310509682, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:20, Epoch: 120, Batch: 590, Training Loss: 0.04571826383471489, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:21, Epoch: 120, Batch: 600, Training Loss: 0.05497906133532524, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:22, Epoch: 120, Batch: 610, Training Loss: 0.034041034430265425, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:23, Epoch: 120, Batch: 620, Training Loss: 0.04072528555989265, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:23, Epoch: 120, Batch: 630, Training Loss: 0.052093614265322684, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:24, Epoch: 120, Batch: 640, Training Loss: 0.05579096674919128, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:25, Epoch: 120, Batch: 650, Training Loss: 0.040691176801919936, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:25, Epoch: 120, Batch: 660, Training Loss: 0.03770131878554821, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:26, Epoch: 120, Batch: 670, Training Loss: 0.04913730695843697, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:27, Epoch: 120, Batch: 680, Training Loss: 0.039905985817313194, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:28, Epoch: 120, Batch: 690, Training Loss: 0.04563391357660294, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:28, Epoch: 120, Batch: 700, Training Loss: 0.045058128982782365, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:29, Epoch: 120, Batch: 710, Training Loss: 0.044942157715559004, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:30, Epoch: 120, Batch: 720, Training Loss: 0.04816182591021061, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:30, Epoch: 120, Batch: 730, Training Loss: 0.03270406648516655, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:31, Epoch: 120, Batch: 740, Training Loss: 0.04543777890503407, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:32, Epoch: 120, Batch: 750, Training Loss: 0.049187805876135825, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:33, Epoch: 120, Batch: 760, Training Loss: 0.0252846647053957, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:33, Epoch: 120, Batch: 770, Training Loss: 0.05185530818998814, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:34, Epoch: 120, Batch: 780, Training Loss: 0.028836140781641005, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:35, Epoch: 120, Batch: 790, Training Loss: 0.043376839533448217, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:35, Epoch: 120, Batch: 800, Training Loss: 0.042620107531547546, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:36, Epoch: 120, Batch: 810, Training Loss: 0.05515829101204872, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:37, Epoch: 120, Batch: 820, Training Loss: 0.04925356693565845, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:38, Epoch: 120, Batch: 830, Training Loss: 0.04480369091033935, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:38, Epoch: 120, Batch: 840, Training Loss: 0.03961773552000523, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:39, Epoch: 120, Batch: 850, Training Loss: 0.037483426928520205, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:40, Epoch: 120, Batch: 860, Training Loss: 0.04097159616649151, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:40, Epoch: 120, Batch: 870, Training Loss: 0.041435974091291426, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:41, Epoch: 120, Batch: 880, Training Loss: 0.027570517361164094, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:42, Epoch: 120, Batch: 890, Training Loss: 0.03749777674674988, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:43, Epoch: 120, Batch: 900, Training Loss: 0.045019695162773134, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:43, Epoch: 120, Batch: 910, Training Loss: 0.043068628385663034, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:44, Epoch: 120, Batch: 920, Training Loss: 0.04657224975526333, LR: 0.00010000000000000003
Time, 2019-01-01T20:20:45, Epoch: 120, Batch: 930, Training Loss: 0.043503526225686075, LR: 0.00010000000000000003
Epoch: 120, Validation Top 1 acc: 98.90558624267578
Epoch: 120, Validation Top 5 acc: 99.99000549316406
Epoch: 120, Validation Set Loss: 0.04069310054183006
Start training epoch 121
Time, 2019-01-01T20:21:11, Epoch: 121, Batch: 10, Training Loss: 0.0321659579873085, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:12, Epoch: 121, Batch: 20, Training Loss: 0.045331578329205514, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:13, Epoch: 121, Batch: 30, Training Loss: 0.04166533797979355, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:13, Epoch: 121, Batch: 40, Training Loss: 0.048688041418790816, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:14, Epoch: 121, Batch: 50, Training Loss: 0.03995943404734135, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:15, Epoch: 121, Batch: 60, Training Loss: 0.027865302935242652, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:16, Epoch: 121, Batch: 70, Training Loss: 0.04697109162807465, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:16, Epoch: 121, Batch: 80, Training Loss: 0.04404868632555008, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:17, Epoch: 121, Batch: 90, Training Loss: 0.06975530833005905, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:18, Epoch: 121, Batch: 100, Training Loss: 0.04637597613036633, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:18, Epoch: 121, Batch: 110, Training Loss: 0.051233284175395966, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:19, Epoch: 121, Batch: 120, Training Loss: 0.034969163686037065, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:20, Epoch: 121, Batch: 130, Training Loss: 0.044029947742819785, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:21, Epoch: 121, Batch: 140, Training Loss: 0.025740252807736397, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:21, Epoch: 121, Batch: 150, Training Loss: 0.046101074665784836, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:22, Epoch: 121, Batch: 160, Training Loss: 0.02625914439558983, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:23, Epoch: 121, Batch: 170, Training Loss: 0.033005571365356444, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:23, Epoch: 121, Batch: 180, Training Loss: 0.04578244499862194, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:24, Epoch: 121, Batch: 190, Training Loss: 0.042306768521666524, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:25, Epoch: 121, Batch: 200, Training Loss: 0.04311299100518227, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:26, Epoch: 121, Batch: 210, Training Loss: 0.05415262207388878, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:26, Epoch: 121, Batch: 220, Training Loss: 0.06262561902403832, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:27, Epoch: 121, Batch: 230, Training Loss: 0.047940779104828836, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:28, Epoch: 121, Batch: 240, Training Loss: 0.04730150550603866, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:28, Epoch: 121, Batch: 250, Training Loss: 0.03232812099158764, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:29, Epoch: 121, Batch: 260, Training Loss: 0.044962751865386966, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:30, Epoch: 121, Batch: 270, Training Loss: 0.053313244879245755, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:31, Epoch: 121, Batch: 280, Training Loss: 0.04742863066494465, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:31, Epoch: 121, Batch: 290, Training Loss: 0.054321928322315215, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:32, Epoch: 121, Batch: 300, Training Loss: 0.03197557628154755, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:33, Epoch: 121, Batch: 310, Training Loss: 0.04465102329850197, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:33, Epoch: 121, Batch: 320, Training Loss: 0.04696915075182915, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:34, Epoch: 121, Batch: 330, Training Loss: 0.044539902731776236, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:35, Epoch: 121, Batch: 340, Training Loss: 0.05055761225521564, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:36, Epoch: 121, Batch: 350, Training Loss: 0.03541307188570499, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:36, Epoch: 121, Batch: 360, Training Loss: 0.04376359097659588, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:37, Epoch: 121, Batch: 370, Training Loss: 0.03157512731850147, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:38, Epoch: 121, Batch: 380, Training Loss: 0.02585550621151924, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:39, Epoch: 121, Batch: 390, Training Loss: 0.039258914440870284, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:39, Epoch: 121, Batch: 400, Training Loss: 0.02601955384016037, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:40, Epoch: 121, Batch: 410, Training Loss: 0.03656436987221241, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:41, Epoch: 121, Batch: 420, Training Loss: 0.04097274392843246, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:42, Epoch: 121, Batch: 430, Training Loss: 0.03761620111763477, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:42, Epoch: 121, Batch: 440, Training Loss: 0.05351810306310654, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:43, Epoch: 121, Batch: 450, Training Loss: 0.03401608392596245, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:44, Epoch: 121, Batch: 460, Training Loss: 0.04239761047065258, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:44, Epoch: 121, Batch: 470, Training Loss: 0.03849619813263416, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:45, Epoch: 121, Batch: 480, Training Loss: 0.040584509074687955, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:46, Epoch: 121, Batch: 490, Training Loss: 0.03510770127177239, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:47, Epoch: 121, Batch: 500, Training Loss: 0.05339868441224098, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:47, Epoch: 121, Batch: 510, Training Loss: 0.03286512792110443, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:48, Epoch: 121, Batch: 520, Training Loss: 0.03704764842987061, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:49, Epoch: 121, Batch: 530, Training Loss: 0.03573879972100258, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:49, Epoch: 121, Batch: 540, Training Loss: 0.027637775987386703, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:50, Epoch: 121, Batch: 550, Training Loss: 0.04755283817648888, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:51, Epoch: 121, Batch: 560, Training Loss: 0.049889614805579185, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:52, Epoch: 121, Batch: 570, Training Loss: 0.035997089743614194, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:52, Epoch: 121, Batch: 580, Training Loss: 0.039415834844112395, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:53, Epoch: 121, Batch: 590, Training Loss: 0.02887621447443962, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:54, Epoch: 121, Batch: 600, Training Loss: 0.03484064936637878, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:55, Epoch: 121, Batch: 610, Training Loss: 0.04612053409218788, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:55, Epoch: 121, Batch: 620, Training Loss: 0.03881707116961479, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:56, Epoch: 121, Batch: 630, Training Loss: 0.05573773048818111, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:57, Epoch: 121, Batch: 640, Training Loss: 0.03139517270028591, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:58, Epoch: 121, Batch: 650, Training Loss: 0.045524392277002335, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:58, Epoch: 121, Batch: 660, Training Loss: 0.04235592745244503, LR: 0.00010000000000000003
Time, 2019-01-01T20:21:59, Epoch: 121, Batch: 670, Training Loss: 0.025233279541134836, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:00, Epoch: 121, Batch: 680, Training Loss: 0.07202652357518673, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:00, Epoch: 121, Batch: 690, Training Loss: 0.037478496134281156, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:01, Epoch: 121, Batch: 700, Training Loss: 0.044741959869861604, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:02, Epoch: 121, Batch: 710, Training Loss: 0.05424435697495937, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:03, Epoch: 121, Batch: 720, Training Loss: 0.046158067509531976, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:03, Epoch: 121, Batch: 730, Training Loss: 0.04344647750258446, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:04, Epoch: 121, Batch: 740, Training Loss: 0.05184596963226795, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:05, Epoch: 121, Batch: 750, Training Loss: 0.02755977734923363, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:06, Epoch: 121, Batch: 760, Training Loss: 0.037230420485138895, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:06, Epoch: 121, Batch: 770, Training Loss: 0.034081483632326125, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:07, Epoch: 121, Batch: 780, Training Loss: 0.04224719516932964, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:08, Epoch: 121, Batch: 790, Training Loss: 0.03326656594872475, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:08, Epoch: 121, Batch: 800, Training Loss: 0.0378994632512331, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:09, Epoch: 121, Batch: 810, Training Loss: 0.028345344960689543, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:10, Epoch: 121, Batch: 820, Training Loss: 0.056233497336506844, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:11, Epoch: 121, Batch: 830, Training Loss: 0.044414354115724565, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:11, Epoch: 121, Batch: 840, Training Loss: 0.040280040353536606, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:12, Epoch: 121, Batch: 850, Training Loss: 0.02860911525785923, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:13, Epoch: 121, Batch: 860, Training Loss: 0.055797868967056276, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:14, Epoch: 121, Batch: 870, Training Loss: 0.032832345366477965, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:14, Epoch: 121, Batch: 880, Training Loss: 0.042954045534133914, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:15, Epoch: 121, Batch: 890, Training Loss: 0.03495439365506172, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:16, Epoch: 121, Batch: 900, Training Loss: 0.029442133754491805, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:16, Epoch: 121, Batch: 910, Training Loss: 0.02826327793300152, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:17, Epoch: 121, Batch: 920, Training Loss: 0.02641470357775688, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:18, Epoch: 121, Batch: 930, Training Loss: 0.024764837324619295, LR: 0.00010000000000000003
Epoch: 121, Validation Top 1 acc: 98.88392639160156
Epoch: 121, Validation Top 5 acc: 99.99166870117188
Epoch: 121, Validation Set Loss: 0.04082924872636795
Start training epoch 122
Time, 2019-01-01T20:22:45, Epoch: 122, Batch: 10, Training Loss: 0.03216736391186714, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:46, Epoch: 122, Batch: 20, Training Loss: 0.02906784079968929, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:47, Epoch: 122, Batch: 30, Training Loss: 0.03553502820432186, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:48, Epoch: 122, Batch: 40, Training Loss: 0.03736815489828586, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:48, Epoch: 122, Batch: 50, Training Loss: 0.034553893655538556, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:49, Epoch: 122, Batch: 60, Training Loss: 0.054719331115484236, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:50, Epoch: 122, Batch: 70, Training Loss: 0.03128744885325432, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:50, Epoch: 122, Batch: 80, Training Loss: 0.02761751189827919, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:51, Epoch: 122, Batch: 90, Training Loss: 0.07005458325147629, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:52, Epoch: 122, Batch: 100, Training Loss: 0.059270869195461276, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:53, Epoch: 122, Batch: 110, Training Loss: 0.07021706737577915, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:53, Epoch: 122, Batch: 120, Training Loss: 0.035624337196350095, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:54, Epoch: 122, Batch: 130, Training Loss: 0.025484827160835267, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:55, Epoch: 122, Batch: 140, Training Loss: 0.03399638421833515, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:56, Epoch: 122, Batch: 150, Training Loss: 0.042664797604084016, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:56, Epoch: 122, Batch: 160, Training Loss: 0.05981342941522598, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:57, Epoch: 122, Batch: 170, Training Loss: 0.043422972038388255, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:58, Epoch: 122, Batch: 180, Training Loss: 0.03620219007134438, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:59, Epoch: 122, Batch: 190, Training Loss: 0.05971972681581974, LR: 0.00010000000000000003
Time, 2019-01-01T20:22:59, Epoch: 122, Batch: 200, Training Loss: 0.05433556884527206, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:00, Epoch: 122, Batch: 210, Training Loss: 0.046396801620721816, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:01, Epoch: 122, Batch: 220, Training Loss: 0.06177638508379459, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:01, Epoch: 122, Batch: 230, Training Loss: 0.05493674837052822, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:02, Epoch: 122, Batch: 240, Training Loss: 0.03377197049558163, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:03, Epoch: 122, Batch: 250, Training Loss: 0.056916870176792145, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:04, Epoch: 122, Batch: 260, Training Loss: 0.04188408963382244, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:04, Epoch: 122, Batch: 270, Training Loss: 0.03816571161150932, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:05, Epoch: 122, Batch: 280, Training Loss: 0.050592361763119695, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:06, Epoch: 122, Batch: 290, Training Loss: 0.048301602527499196, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:07, Epoch: 122, Batch: 300, Training Loss: 0.03657874688506126, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:07, Epoch: 122, Batch: 310, Training Loss: 0.036922842636704443, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:08, Epoch: 122, Batch: 320, Training Loss: 0.04474130906164646, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:09, Epoch: 122, Batch: 330, Training Loss: 0.03216439038515091, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:09, Epoch: 122, Batch: 340, Training Loss: 0.048291254043579104, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:10, Epoch: 122, Batch: 350, Training Loss: 0.03793749064207077, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:11, Epoch: 122, Batch: 360, Training Loss: 0.030703289806842803, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:12, Epoch: 122, Batch: 370, Training Loss: 0.041256627440452574, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:12, Epoch: 122, Batch: 380, Training Loss: 0.042262285575270654, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:13, Epoch: 122, Batch: 390, Training Loss: 0.0550517950206995, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:14, Epoch: 122, Batch: 400, Training Loss: 0.0422777321189642, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:15, Epoch: 122, Batch: 410, Training Loss: 0.02546984553337097, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:15, Epoch: 122, Batch: 420, Training Loss: 0.02502681314945221, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:16, Epoch: 122, Batch: 430, Training Loss: 0.048886962607502936, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:17, Epoch: 122, Batch: 440, Training Loss: 0.04135516136884689, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:17, Epoch: 122, Batch: 450, Training Loss: 0.03182622380554676, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:18, Epoch: 122, Batch: 460, Training Loss: 0.032090863958001134, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:19, Epoch: 122, Batch: 470, Training Loss: 0.0413270752876997, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:20, Epoch: 122, Batch: 480, Training Loss: 0.050609583780169484, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:20, Epoch: 122, Batch: 490, Training Loss: 0.054153238981962205, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:21, Epoch: 122, Batch: 500, Training Loss: 0.02970249019563198, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:22, Epoch: 122, Batch: 510, Training Loss: 0.03570978567004204, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:23, Epoch: 122, Batch: 520, Training Loss: 0.043518538028001784, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:23, Epoch: 122, Batch: 530, Training Loss: 0.042369984835386273, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:24, Epoch: 122, Batch: 540, Training Loss: 0.031958944723010065, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:25, Epoch: 122, Batch: 550, Training Loss: 0.05409956090152264, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:25, Epoch: 122, Batch: 560, Training Loss: 0.04868115521967411, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:26, Epoch: 122, Batch: 570, Training Loss: 0.054964418336749074, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:27, Epoch: 122, Batch: 580, Training Loss: 0.03490898162126541, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:28, Epoch: 122, Batch: 590, Training Loss: 0.04040130451321602, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:28, Epoch: 122, Batch: 600, Training Loss: 0.03624257743358612, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:29, Epoch: 122, Batch: 610, Training Loss: 0.02782938927412033, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:30, Epoch: 122, Batch: 620, Training Loss: 0.03337362185120583, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:31, Epoch: 122, Batch: 630, Training Loss: 0.04505268856883049, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:31, Epoch: 122, Batch: 640, Training Loss: 0.03998599611222744, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:32, Epoch: 122, Batch: 650, Training Loss: 0.04002892486751079, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:33, Epoch: 122, Batch: 660, Training Loss: 0.0291880302131176, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:33, Epoch: 122, Batch: 670, Training Loss: 0.039746349304914476, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:34, Epoch: 122, Batch: 680, Training Loss: 0.044530516117811204, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:35, Epoch: 122, Batch: 690, Training Loss: 0.02764940932393074, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:35, Epoch: 122, Batch: 700, Training Loss: 0.032843416929244994, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:36, Epoch: 122, Batch: 710, Training Loss: 0.032561613619327544, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:37, Epoch: 122, Batch: 720, Training Loss: 0.022531090304255486, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:38, Epoch: 122, Batch: 730, Training Loss: 0.049881041049957275, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:38, Epoch: 122, Batch: 740, Training Loss: 0.03859119191765785, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:39, Epoch: 122, Batch: 750, Training Loss: 0.03650557659566402, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:40, Epoch: 122, Batch: 760, Training Loss: 0.03912702761590481, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:41, Epoch: 122, Batch: 770, Training Loss: 0.05594945251941681, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:41, Epoch: 122, Batch: 780, Training Loss: 0.029624880850315095, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:42, Epoch: 122, Batch: 790, Training Loss: 0.026276597008109092, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:43, Epoch: 122, Batch: 800, Training Loss: 0.04758612513542175, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:44, Epoch: 122, Batch: 810, Training Loss: 0.03828565627336502, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:44, Epoch: 122, Batch: 820, Training Loss: 0.04152693673968315, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:45, Epoch: 122, Batch: 830, Training Loss: 0.04576037637889385, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:46, Epoch: 122, Batch: 840, Training Loss: 0.030190593004226683, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:47, Epoch: 122, Batch: 850, Training Loss: 0.0169822096824646, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:47, Epoch: 122, Batch: 860, Training Loss: 0.05477468743920326, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:48, Epoch: 122, Batch: 870, Training Loss: 0.033459311723709105, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:49, Epoch: 122, Batch: 880, Training Loss: 0.04866456054151058, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:50, Epoch: 122, Batch: 890, Training Loss: 0.03020327351987362, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:51, Epoch: 122, Batch: 900, Training Loss: 0.03730713501572609, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:51, Epoch: 122, Batch: 910, Training Loss: 0.045610421895980836, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:52, Epoch: 122, Batch: 920, Training Loss: 0.03125525638461113, LR: 0.00010000000000000003
Time, 2019-01-01T20:23:53, Epoch: 122, Batch: 930, Training Loss: 0.05080152675509453, LR: 0.00010000000000000003
Epoch: 122, Validation Top 1 acc: 98.89559173583984
Epoch: 122, Validation Top 5 acc: 99.99166870117188
Epoch: 122, Validation Set Loss: 0.0406687967479229
Start training epoch 123
Time, 2019-01-01T20:24:23, Epoch: 123, Batch: 10, Training Loss: 0.047567731514573094, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:23, Epoch: 123, Batch: 20, Training Loss: 0.036058598384261134, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:24, Epoch: 123, Batch: 30, Training Loss: 0.04796022474765778, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:25, Epoch: 123, Batch: 40, Training Loss: 0.03552913554012775, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:26, Epoch: 123, Batch: 50, Training Loss: 0.03547806665301323, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:26, Epoch: 123, Batch: 60, Training Loss: 0.0550608366727829, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:27, Epoch: 123, Batch: 70, Training Loss: 0.0350412480533123, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:28, Epoch: 123, Batch: 80, Training Loss: 0.03883931152522564, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:28, Epoch: 123, Batch: 90, Training Loss: 0.06589161530137062, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:29, Epoch: 123, Batch: 100, Training Loss: 0.07609746754169464, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:30, Epoch: 123, Batch: 110, Training Loss: 0.03738655485212803, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:31, Epoch: 123, Batch: 120, Training Loss: 0.034503518417477605, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:31, Epoch: 123, Batch: 130, Training Loss: 0.047890900447964665, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:32, Epoch: 123, Batch: 140, Training Loss: 0.05887794494628906, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:33, Epoch: 123, Batch: 150, Training Loss: 0.04801755808293819, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:34, Epoch: 123, Batch: 160, Training Loss: 0.046968826279044154, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:34, Epoch: 123, Batch: 170, Training Loss: 0.04425781667232513, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:35, Epoch: 123, Batch: 180, Training Loss: 0.0486771147698164, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:36, Epoch: 123, Batch: 190, Training Loss: 0.030003614351153375, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:36, Epoch: 123, Batch: 200, Training Loss: 0.03859432265162468, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:37, Epoch: 123, Batch: 210, Training Loss: 0.028772870451211928, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:38, Epoch: 123, Batch: 220, Training Loss: 0.04174944460391998, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:39, Epoch: 123, Batch: 230, Training Loss: 0.032197297737002376, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:39, Epoch: 123, Batch: 240, Training Loss: 0.049238163605332376, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:40, Epoch: 123, Batch: 250, Training Loss: 0.026358097046613693, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:41, Epoch: 123, Batch: 260, Training Loss: 0.05208062753081322, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:42, Epoch: 123, Batch: 270, Training Loss: 0.023554164916276932, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:42, Epoch: 123, Batch: 280, Training Loss: 0.0604649443179369, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:43, Epoch: 123, Batch: 290, Training Loss: 0.0230385210365057, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:44, Epoch: 123, Batch: 300, Training Loss: 0.037281524389982224, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:45, Epoch: 123, Batch: 310, Training Loss: 0.0246206060051918, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:45, Epoch: 123, Batch: 320, Training Loss: 0.042358240112662315, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:46, Epoch: 123, Batch: 330, Training Loss: 0.046108411997556685, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:47, Epoch: 123, Batch: 340, Training Loss: 0.04235795401036739, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:47, Epoch: 123, Batch: 350, Training Loss: 0.03403808400034904, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:48, Epoch: 123, Batch: 360, Training Loss: 0.03158646523952484, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:49, Epoch: 123, Batch: 370, Training Loss: 0.05097696259617805, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:50, Epoch: 123, Batch: 380, Training Loss: 0.04030386283993721, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:50, Epoch: 123, Batch: 390, Training Loss: 0.03234280943870545, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:51, Epoch: 123, Batch: 400, Training Loss: 0.047415340691804884, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:52, Epoch: 123, Batch: 410, Training Loss: 0.03855220042169094, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:53, Epoch: 123, Batch: 420, Training Loss: 0.046478357166051865, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:54, Epoch: 123, Batch: 430, Training Loss: 0.04032937400043011, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:54, Epoch: 123, Batch: 440, Training Loss: 0.022842837497591972, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:55, Epoch: 123, Batch: 450, Training Loss: 0.03347305618226528, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:56, Epoch: 123, Batch: 460, Training Loss: 0.036582740023732185, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:57, Epoch: 123, Batch: 470, Training Loss: 0.023394964635372162, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:57, Epoch: 123, Batch: 480, Training Loss: 0.05437859296798706, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:58, Epoch: 123, Batch: 490, Training Loss: 0.03474439457058907, LR: 0.00010000000000000003
Time, 2019-01-01T20:24:59, Epoch: 123, Batch: 500, Training Loss: 0.030448931828141214, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:00, Epoch: 123, Batch: 510, Training Loss: 0.03624226711690426, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:00, Epoch: 123, Batch: 520, Training Loss: 0.03993968851864338, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:01, Epoch: 123, Batch: 530, Training Loss: 0.051130902767181394, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:02, Epoch: 123, Batch: 540, Training Loss: 0.03931513838469982, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:02, Epoch: 123, Batch: 550, Training Loss: 0.0462873563170433, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:03, Epoch: 123, Batch: 560, Training Loss: 0.03447946161031723, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:04, Epoch: 123, Batch: 570, Training Loss: 0.031006086245179178, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:05, Epoch: 123, Batch: 580, Training Loss: 0.030621089786291123, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:05, Epoch: 123, Batch: 590, Training Loss: 0.07058818265795708, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:06, Epoch: 123, Batch: 600, Training Loss: 0.03479280322790146, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:07, Epoch: 123, Batch: 610, Training Loss: 0.03816915489733219, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:08, Epoch: 123, Batch: 620, Training Loss: 0.03743424937129021, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:08, Epoch: 123, Batch: 630, Training Loss: 0.04220765419304371, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:09, Epoch: 123, Batch: 640, Training Loss: 0.03935164958238602, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:10, Epoch: 123, Batch: 650, Training Loss: 0.04243487380445003, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:11, Epoch: 123, Batch: 660, Training Loss: 0.03397369235754013, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:11, Epoch: 123, Batch: 670, Training Loss: 0.05829872004687786, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:12, Epoch: 123, Batch: 680, Training Loss: 0.029601048305630685, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:13, Epoch: 123, Batch: 690, Training Loss: 0.043115539103746416, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:14, Epoch: 123, Batch: 700, Training Loss: 0.03470561057329178, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:14, Epoch: 123, Batch: 710, Training Loss: 0.047939636558294293, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:15, Epoch: 123, Batch: 720, Training Loss: 0.051130592823028564, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:16, Epoch: 123, Batch: 730, Training Loss: 0.058268938958644864, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:16, Epoch: 123, Batch: 740, Training Loss: 0.046342646703124046, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:17, Epoch: 123, Batch: 750, Training Loss: 0.02631969079375267, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:18, Epoch: 123, Batch: 760, Training Loss: 0.03492727242410183, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:19, Epoch: 123, Batch: 770, Training Loss: 0.030203664675354958, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:19, Epoch: 123, Batch: 780, Training Loss: 0.036719806119799615, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:20, Epoch: 123, Batch: 790, Training Loss: 0.04918946288526058, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:21, Epoch: 123, Batch: 800, Training Loss: 0.03789051994681358, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:22, Epoch: 123, Batch: 810, Training Loss: 0.02281808331608772, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:22, Epoch: 123, Batch: 820, Training Loss: 0.02546239085495472, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:23, Epoch: 123, Batch: 830, Training Loss: 0.03745131865143776, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:24, Epoch: 123, Batch: 840, Training Loss: 0.03578982912003994, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:25, Epoch: 123, Batch: 850, Training Loss: 0.02879032604396343, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:25, Epoch: 123, Batch: 860, Training Loss: 0.06491695009171963, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:26, Epoch: 123, Batch: 870, Training Loss: 0.03332120329141617, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:27, Epoch: 123, Batch: 880, Training Loss: 0.051661473140120505, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:28, Epoch: 123, Batch: 890, Training Loss: 0.06798191331326961, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:28, Epoch: 123, Batch: 900, Training Loss: 0.036117450892925264, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:29, Epoch: 123, Batch: 910, Training Loss: 0.0494162168353796, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:30, Epoch: 123, Batch: 920, Training Loss: 0.03859950788319111, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:31, Epoch: 123, Batch: 930, Training Loss: 0.042855942621827126, LR: 0.00010000000000000003
Epoch: 123, Validation Top 1 acc: 98.90058898925781
Epoch: 123, Validation Top 5 acc: 99.99166870117188
Epoch: 123, Validation Set Loss: 0.040704887360334396
Start training epoch 124
Time, 2019-01-01T20:25:59, Epoch: 124, Batch: 10, Training Loss: 0.03800573460757732, LR: 0.00010000000000000003
Time, 2019-01-01T20:25:59, Epoch: 124, Batch: 20, Training Loss: 0.054376319795846936, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:00, Epoch: 124, Batch: 30, Training Loss: 0.03954771980643272, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:01, Epoch: 124, Batch: 40, Training Loss: 0.03360838443040848, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:01, Epoch: 124, Batch: 50, Training Loss: 0.04977044239640236, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:02, Epoch: 124, Batch: 60, Training Loss: 0.034735261276364324, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:03, Epoch: 124, Batch: 70, Training Loss: 0.031539336591959, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:04, Epoch: 124, Batch: 80, Training Loss: 0.0332677386701107, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:05, Epoch: 124, Batch: 90, Training Loss: 0.05044540613889694, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:05, Epoch: 124, Batch: 100, Training Loss: 0.03548567146062851, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:06, Epoch: 124, Batch: 110, Training Loss: 0.0315136332064867, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:07, Epoch: 124, Batch: 120, Training Loss: 0.03515845760703087, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:07, Epoch: 124, Batch: 130, Training Loss: 0.04707706086337567, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:08, Epoch: 124, Batch: 140, Training Loss: 0.04690246991813183, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:09, Epoch: 124, Batch: 150, Training Loss: 0.03926169164478779, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:10, Epoch: 124, Batch: 160, Training Loss: 0.059811022505164145, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:10, Epoch: 124, Batch: 170, Training Loss: 0.03756114020943642, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:11, Epoch: 124, Batch: 180, Training Loss: 0.040701049566268924, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:12, Epoch: 124, Batch: 190, Training Loss: 0.03714219778776169, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:12, Epoch: 124, Batch: 200, Training Loss: 0.048405947536230086, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:13, Epoch: 124, Batch: 210, Training Loss: 0.048118263483047485, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:14, Epoch: 124, Batch: 220, Training Loss: 0.037994037568569186, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:15, Epoch: 124, Batch: 230, Training Loss: 0.04690963290631771, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:15, Epoch: 124, Batch: 240, Training Loss: 0.039143846929073335, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:16, Epoch: 124, Batch: 250, Training Loss: 0.02900499291718006, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:17, Epoch: 124, Batch: 260, Training Loss: 0.034549984708428384, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:18, Epoch: 124, Batch: 270, Training Loss: 0.039197343215346334, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:18, Epoch: 124, Batch: 280, Training Loss: 0.04088882692158222, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:19, Epoch: 124, Batch: 290, Training Loss: 0.033187588304281236, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:20, Epoch: 124, Batch: 300, Training Loss: 0.062359002977609636, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:20, Epoch: 124, Batch: 310, Training Loss: 0.04642006829380989, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:21, Epoch: 124, Batch: 320, Training Loss: 0.02738189548254013, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:22, Epoch: 124, Batch: 330, Training Loss: 0.04571388997137547, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:23, Epoch: 124, Batch: 340, Training Loss: 0.04328019209206104, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:24, Epoch: 124, Batch: 350, Training Loss: 0.03914400413632393, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:24, Epoch: 124, Batch: 360, Training Loss: 0.04099749848246574, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:25, Epoch: 124, Batch: 370, Training Loss: 0.033163434639573094, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:26, Epoch: 124, Batch: 380, Training Loss: 0.04155920967459679, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:27, Epoch: 124, Batch: 390, Training Loss: 0.03305307477712631, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:27, Epoch: 124, Batch: 400, Training Loss: 0.048605906590819356, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:28, Epoch: 124, Batch: 410, Training Loss: 0.034933500736951825, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:29, Epoch: 124, Batch: 420, Training Loss: 0.03379372097551823, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:30, Epoch: 124, Batch: 430, Training Loss: 0.028608675673604013, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:30, Epoch: 124, Batch: 440, Training Loss: 0.049997588992118834, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:31, Epoch: 124, Batch: 450, Training Loss: 0.03824707008898258, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:32, Epoch: 124, Batch: 460, Training Loss: 0.026735441014170646, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:33, Epoch: 124, Batch: 470, Training Loss: 0.04968794137239456, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:33, Epoch: 124, Batch: 480, Training Loss: 0.05331995487213135, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:34, Epoch: 124, Batch: 490, Training Loss: 0.053007540106773374, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:35, Epoch: 124, Batch: 500, Training Loss: 0.0378552570939064, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:36, Epoch: 124, Batch: 510, Training Loss: 0.045047244429588316, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:36, Epoch: 124, Batch: 520, Training Loss: 0.038354336842894556, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:37, Epoch: 124, Batch: 530, Training Loss: 0.031648800522089, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:38, Epoch: 124, Batch: 540, Training Loss: 0.033547352999448776, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:38, Epoch: 124, Batch: 550, Training Loss: 0.03198113366961479, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:39, Epoch: 124, Batch: 560, Training Loss: 0.038125884532928464, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:40, Epoch: 124, Batch: 570, Training Loss: 0.02987418994307518, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:41, Epoch: 124, Batch: 580, Training Loss: 0.05198096223175526, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:41, Epoch: 124, Batch: 590, Training Loss: 0.04425811767578125, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:42, Epoch: 124, Batch: 600, Training Loss: 0.05739583075046539, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:43, Epoch: 124, Batch: 610, Training Loss: 0.04898576177656651, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:43, Epoch: 124, Batch: 620, Training Loss: 0.05442575886845589, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:44, Epoch: 124, Batch: 630, Training Loss: 0.035862033069133756, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:45, Epoch: 124, Batch: 640, Training Loss: 0.03607767373323441, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:46, Epoch: 124, Batch: 650, Training Loss: 0.03374748900532722, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:46, Epoch: 124, Batch: 660, Training Loss: 0.0315638329833746, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:47, Epoch: 124, Batch: 670, Training Loss: 0.04256755895912647, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:48, Epoch: 124, Batch: 680, Training Loss: 0.02651541233062744, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:49, Epoch: 124, Batch: 690, Training Loss: 0.04788567088544369, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:50, Epoch: 124, Batch: 700, Training Loss: 0.03781745173037052, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:50, Epoch: 124, Batch: 710, Training Loss: 0.03230386450886726, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:51, Epoch: 124, Batch: 720, Training Loss: 0.02632044106721878, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:52, Epoch: 124, Batch: 730, Training Loss: 0.04549493044614792, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:53, Epoch: 124, Batch: 740, Training Loss: 0.04398194774985313, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:53, Epoch: 124, Batch: 750, Training Loss: 0.04119912981986999, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:54, Epoch: 124, Batch: 760, Training Loss: 0.029714489355683327, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:55, Epoch: 124, Batch: 770, Training Loss: 0.043055927753448485, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:56, Epoch: 124, Batch: 780, Training Loss: 0.04082931242883205, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:57, Epoch: 124, Batch: 790, Training Loss: 0.03558057472109795, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:58, Epoch: 124, Batch: 800, Training Loss: 0.03438398838043213, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:58, Epoch: 124, Batch: 810, Training Loss: 0.05696406438946724, LR: 0.00010000000000000003
Time, 2019-01-01T20:26:59, Epoch: 124, Batch: 820, Training Loss: 0.039819780737161636, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:00, Epoch: 124, Batch: 830, Training Loss: 0.05234736204147339, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:01, Epoch: 124, Batch: 840, Training Loss: 0.04840608015656471, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:01, Epoch: 124, Batch: 850, Training Loss: 0.05571083873510361, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:02, Epoch: 124, Batch: 860, Training Loss: 0.04505319558084011, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:03, Epoch: 124, Batch: 870, Training Loss: 0.04735334292054176, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:04, Epoch: 124, Batch: 880, Training Loss: 0.04069236963987351, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:04, Epoch: 124, Batch: 890, Training Loss: 0.043331678956747055, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:05, Epoch: 124, Batch: 900, Training Loss: 0.034304005280137065, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:06, Epoch: 124, Batch: 910, Training Loss: 0.05798681415617466, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:07, Epoch: 124, Batch: 920, Training Loss: 0.029399433732032777, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:07, Epoch: 124, Batch: 930, Training Loss: 0.030144114792346955, LR: 0.00010000000000000003
Epoch: 124, Validation Top 1 acc: 98.89891815185547
Epoch: 124, Validation Top 5 acc: 99.99000549316406
Epoch: 124, Validation Set Loss: 0.040733009576797485
Start training epoch 125
Time, 2019-01-01T20:27:36, Epoch: 125, Batch: 10, Training Loss: 0.03404759243130684, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:36, Epoch: 125, Batch: 20, Training Loss: 0.041227874159812924, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:37, Epoch: 125, Batch: 30, Training Loss: 0.03243445344269276, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:38, Epoch: 125, Batch: 40, Training Loss: 0.05363151691854, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:39, Epoch: 125, Batch: 50, Training Loss: 0.04970168061554432, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:39, Epoch: 125, Batch: 60, Training Loss: 0.06925948932766915, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:40, Epoch: 125, Batch: 70, Training Loss: 0.03502485826611519, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:41, Epoch: 125, Batch: 80, Training Loss: 0.05062736496329308, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:42, Epoch: 125, Batch: 90, Training Loss: 0.04167711101472378, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:42, Epoch: 125, Batch: 100, Training Loss: 0.045174311846494675, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:43, Epoch: 125, Batch: 110, Training Loss: 0.04556318745017052, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:44, Epoch: 125, Batch: 120, Training Loss: 0.03203534334897995, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:45, Epoch: 125, Batch: 130, Training Loss: 0.03383049741387367, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:45, Epoch: 125, Batch: 140, Training Loss: 0.03533373586833477, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:46, Epoch: 125, Batch: 150, Training Loss: 0.04028233028948307, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:47, Epoch: 125, Batch: 160, Training Loss: 0.048114005103707315, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:48, Epoch: 125, Batch: 170, Training Loss: 0.03907120153307915, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:48, Epoch: 125, Batch: 180, Training Loss: 0.03861984498798847, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:49, Epoch: 125, Batch: 190, Training Loss: 0.040188844129443166, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:50, Epoch: 125, Batch: 200, Training Loss: 0.04843786470592022, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:51, Epoch: 125, Batch: 210, Training Loss: 0.03429364114999771, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:51, Epoch: 125, Batch: 220, Training Loss: 0.029145100712776185, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:52, Epoch: 125, Batch: 230, Training Loss: 0.057598885521292685, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:53, Epoch: 125, Batch: 240, Training Loss: 0.05402773134410381, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:53, Epoch: 125, Batch: 250, Training Loss: 0.04703459776937961, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:54, Epoch: 125, Batch: 260, Training Loss: 0.04145369455218315, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:55, Epoch: 125, Batch: 270, Training Loss: 0.05123170763254166, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:56, Epoch: 125, Batch: 280, Training Loss: 0.03985404334962368, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:56, Epoch: 125, Batch: 290, Training Loss: 0.04900346770882606, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:57, Epoch: 125, Batch: 300, Training Loss: 0.027586520090699194, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:58, Epoch: 125, Batch: 310, Training Loss: 0.028485018014907836, LR: 0.00010000000000000003
Time, 2019-01-01T20:27:59, Epoch: 125, Batch: 320, Training Loss: 0.029690103605389595, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:00, Epoch: 125, Batch: 330, Training Loss: 0.03540504015982151, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:00, Epoch: 125, Batch: 340, Training Loss: 0.037354695796966556, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:01, Epoch: 125, Batch: 350, Training Loss: 0.049951183050870894, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:02, Epoch: 125, Batch: 360, Training Loss: 0.042296845465898514, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:03, Epoch: 125, Batch: 370, Training Loss: 0.04206963032484055, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:04, Epoch: 125, Batch: 380, Training Loss: 0.03181469552218914, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:05, Epoch: 125, Batch: 390, Training Loss: 0.050395600497722626, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:05, Epoch: 125, Batch: 400, Training Loss: 0.03341509066522121, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:06, Epoch: 125, Batch: 410, Training Loss: 0.036615201830863954, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:07, Epoch: 125, Batch: 420, Training Loss: 0.038352879136800765, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:08, Epoch: 125, Batch: 430, Training Loss: 0.0233015701174736, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:08, Epoch: 125, Batch: 440, Training Loss: 0.0461837638169527, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:09, Epoch: 125, Batch: 450, Training Loss: 0.045565538480877875, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:10, Epoch: 125, Batch: 460, Training Loss: 0.03303731270134449, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:10, Epoch: 125, Batch: 470, Training Loss: 0.046641888096928596, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:11, Epoch: 125, Batch: 480, Training Loss: 0.041396234557032584, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:12, Epoch: 125, Batch: 490, Training Loss: 0.04252512753009796, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:13, Epoch: 125, Batch: 500, Training Loss: 0.04605275169014931, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:13, Epoch: 125, Batch: 510, Training Loss: 0.043958446383476256, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:14, Epoch: 125, Batch: 520, Training Loss: 0.031053638458251952, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:15, Epoch: 125, Batch: 530, Training Loss: 0.03217037282884121, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:16, Epoch: 125, Batch: 540, Training Loss: 0.03426578789949417, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:16, Epoch: 125, Batch: 550, Training Loss: 0.034159446507692336, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:17, Epoch: 125, Batch: 560, Training Loss: 0.03491439558565616, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:18, Epoch: 125, Batch: 570, Training Loss: 0.04949750229716301, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:19, Epoch: 125, Batch: 580, Training Loss: 0.02533820755779743, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:19, Epoch: 125, Batch: 590, Training Loss: 0.04264462478458882, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:20, Epoch: 125, Batch: 600, Training Loss: 0.0427103117108345, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:21, Epoch: 125, Batch: 610, Training Loss: 0.0416586771607399, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:21, Epoch: 125, Batch: 620, Training Loss: 0.041589776054024696, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:22, Epoch: 125, Batch: 630, Training Loss: 0.029653145000338554, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:23, Epoch: 125, Batch: 640, Training Loss: 0.04445246048271656, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:24, Epoch: 125, Batch: 650, Training Loss: 0.04472661204636097, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:24, Epoch: 125, Batch: 660, Training Loss: 0.01894112452864647, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:25, Epoch: 125, Batch: 670, Training Loss: 0.03616351708769798, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:26, Epoch: 125, Batch: 680, Training Loss: 0.029827937483787537, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:27, Epoch: 125, Batch: 690, Training Loss: 0.04662112817168236, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:27, Epoch: 125, Batch: 700, Training Loss: 0.03421754762530327, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:28, Epoch: 125, Batch: 710, Training Loss: 0.04228936173021793, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:29, Epoch: 125, Batch: 720, Training Loss: 0.045901944115757945, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:30, Epoch: 125, Batch: 730, Training Loss: 0.04949701651930809, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:30, Epoch: 125, Batch: 740, Training Loss: 0.04119130671024322, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:31, Epoch: 125, Batch: 750, Training Loss: 0.04774975441396236, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:32, Epoch: 125, Batch: 760, Training Loss: 0.04738064333796501, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:33, Epoch: 125, Batch: 770, Training Loss: 0.04021293818950653, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:33, Epoch: 125, Batch: 780, Training Loss: 0.0406104788184166, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:34, Epoch: 125, Batch: 790, Training Loss: 0.03261544667184353, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:35, Epoch: 125, Batch: 800, Training Loss: 0.03166314549744129, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:35, Epoch: 125, Batch: 810, Training Loss: 0.05019758641719818, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:36, Epoch: 125, Batch: 820, Training Loss: 0.032833484932780266, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:37, Epoch: 125, Batch: 830, Training Loss: 0.041972629725933075, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:38, Epoch: 125, Batch: 840, Training Loss: 0.04871757440268994, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:38, Epoch: 125, Batch: 850, Training Loss: 0.05283049196004867, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:39, Epoch: 125, Batch: 860, Training Loss: 0.040860050544142726, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:40, Epoch: 125, Batch: 870, Training Loss: 0.053588179498910905, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:40, Epoch: 125, Batch: 880, Training Loss: 0.0504808034747839, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:41, Epoch: 125, Batch: 890, Training Loss: 0.03770268969237804, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:42, Epoch: 125, Batch: 900, Training Loss: 0.0418733723461628, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:43, Epoch: 125, Batch: 910, Training Loss: 0.04391103684902191, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:43, Epoch: 125, Batch: 920, Training Loss: 0.04987293891608715, LR: 0.00010000000000000003
Time, 2019-01-01T20:28:44, Epoch: 125, Batch: 930, Training Loss: 0.03156945705413818, LR: 0.00010000000000000003
Epoch: 125, Validation Top 1 acc: 98.89059448242188
Epoch: 125, Validation Top 5 acc: 99.99000549316406
Epoch: 125, Validation Set Loss: 0.040699198842048645
Start training epoch 126
Time, 2019-01-01T20:29:12, Epoch: 126, Batch: 10, Training Loss: 0.02847371846437454, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:13, Epoch: 126, Batch: 20, Training Loss: 0.021004943922162057, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:13, Epoch: 126, Batch: 30, Training Loss: 0.027723900228738784, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:14, Epoch: 126, Batch: 40, Training Loss: 0.03660918511450291, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:15, Epoch: 126, Batch: 50, Training Loss: 0.028425085172057152, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:16, Epoch: 126, Batch: 60, Training Loss: 0.043285044655203816, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:17, Epoch: 126, Batch: 70, Training Loss: 0.022707296162843706, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:17, Epoch: 126, Batch: 80, Training Loss: 0.047701621800661086, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:18, Epoch: 126, Batch: 90, Training Loss: 0.02988972030580044, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:19, Epoch: 126, Batch: 100, Training Loss: 0.030072623491287233, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:20, Epoch: 126, Batch: 110, Training Loss: 0.03690332621335983, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:20, Epoch: 126, Batch: 120, Training Loss: 0.035871120914816854, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:21, Epoch: 126, Batch: 130, Training Loss: 0.037238695472478864, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:22, Epoch: 126, Batch: 140, Training Loss: 0.04461974464356899, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:23, Epoch: 126, Batch: 150, Training Loss: 0.05786807052791119, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:23, Epoch: 126, Batch: 160, Training Loss: 0.05863373056054115, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:24, Epoch: 126, Batch: 170, Training Loss: 0.04047856070101261, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:25, Epoch: 126, Batch: 180, Training Loss: 0.05049951858818531, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:26, Epoch: 126, Batch: 190, Training Loss: 0.042387866973876955, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:26, Epoch: 126, Batch: 200, Training Loss: 0.03556835167109966, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:27, Epoch: 126, Batch: 210, Training Loss: 0.05490560680627823, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:28, Epoch: 126, Batch: 220, Training Loss: 0.0482216015458107, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:29, Epoch: 126, Batch: 230, Training Loss: 0.05691960938274861, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:29, Epoch: 126, Batch: 240, Training Loss: 0.025388971716165543, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:30, Epoch: 126, Batch: 250, Training Loss: 0.05050331242382526, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:31, Epoch: 126, Batch: 260, Training Loss: 0.04262184575200081, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:32, Epoch: 126, Batch: 270, Training Loss: 0.028214847296476366, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:32, Epoch: 126, Batch: 280, Training Loss: 0.03521548621356487, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:33, Epoch: 126, Batch: 290, Training Loss: 0.03920404985547066, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:34, Epoch: 126, Batch: 300, Training Loss: 0.07862847819924354, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:34, Epoch: 126, Batch: 310, Training Loss: 0.03699558265507221, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:35, Epoch: 126, Batch: 320, Training Loss: 0.03831135332584381, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:36, Epoch: 126, Batch: 330, Training Loss: 0.03553230948746204, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:37, Epoch: 126, Batch: 340, Training Loss: 0.02909327521920204, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:37, Epoch: 126, Batch: 350, Training Loss: 0.033109357208013536, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:38, Epoch: 126, Batch: 360, Training Loss: 0.025797242671251296, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:39, Epoch: 126, Batch: 370, Training Loss: 0.04503061026334763, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:40, Epoch: 126, Batch: 380, Training Loss: 0.050034718587994576, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:40, Epoch: 126, Batch: 390, Training Loss: 0.050682507827878, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:41, Epoch: 126, Batch: 400, Training Loss: 0.05001248195767403, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:42, Epoch: 126, Batch: 410, Training Loss: 0.0271429605782032, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:43, Epoch: 126, Batch: 420, Training Loss: 0.04411114081740379, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:43, Epoch: 126, Batch: 430, Training Loss: 0.0350542601197958, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:44, Epoch: 126, Batch: 440, Training Loss: 0.02876208573579788, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:45, Epoch: 126, Batch: 450, Training Loss: 0.05686408206820488, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:46, Epoch: 126, Batch: 460, Training Loss: 0.02727552242577076, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:46, Epoch: 126, Batch: 470, Training Loss: 0.04318385012447834, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:47, Epoch: 126, Batch: 480, Training Loss: 0.04205152913928032, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:48, Epoch: 126, Batch: 490, Training Loss: 0.0366963554173708, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:48, Epoch: 126, Batch: 500, Training Loss: 0.042879366874694826, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:49, Epoch: 126, Batch: 510, Training Loss: 0.0251162126660347, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:50, Epoch: 126, Batch: 520, Training Loss: 0.04842401444911957, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:51, Epoch: 126, Batch: 530, Training Loss: 0.04128592796623707, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:51, Epoch: 126, Batch: 540, Training Loss: 0.03293679095804691, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:52, Epoch: 126, Batch: 550, Training Loss: 0.03202700838446617, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:53, Epoch: 126, Batch: 560, Training Loss: 0.04540475122630596, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:54, Epoch: 126, Batch: 570, Training Loss: 0.03617602623999119, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:55, Epoch: 126, Batch: 580, Training Loss: 0.06769338622689247, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:55, Epoch: 126, Batch: 590, Training Loss: 0.055214764177799226, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:56, Epoch: 126, Batch: 600, Training Loss: 0.03301575630903244, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:57, Epoch: 126, Batch: 610, Training Loss: 0.04157920889556408, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:57, Epoch: 126, Batch: 620, Training Loss: 0.04939096421003342, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:58, Epoch: 126, Batch: 630, Training Loss: 0.08878986164927483, LR: 0.00010000000000000003
Time, 2019-01-01T20:29:59, Epoch: 126, Batch: 640, Training Loss: 0.03646741658449173, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:00, Epoch: 126, Batch: 650, Training Loss: 0.018029539659619332, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:00, Epoch: 126, Batch: 660, Training Loss: 0.024468626454472543, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:01, Epoch: 126, Batch: 670, Training Loss: 0.02792411558330059, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:02, Epoch: 126, Batch: 680, Training Loss: 0.03444559946656227, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:03, Epoch: 126, Batch: 690, Training Loss: 0.038133995980024336, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:04, Epoch: 126, Batch: 700, Training Loss: 0.05744825191795826, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:04, Epoch: 126, Batch: 710, Training Loss: 0.03962331265211105, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:05, Epoch: 126, Batch: 720, Training Loss: 0.03647834211587906, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:06, Epoch: 126, Batch: 730, Training Loss: 0.025041764974594115, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:07, Epoch: 126, Batch: 740, Training Loss: 0.03987342342734337, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:08, Epoch: 126, Batch: 750, Training Loss: 0.02931942492723465, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:09, Epoch: 126, Batch: 760, Training Loss: 0.05167358107864857, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:10, Epoch: 126, Batch: 770, Training Loss: 0.03586409240961075, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:10, Epoch: 126, Batch: 780, Training Loss: 0.0322691161185503, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:11, Epoch: 126, Batch: 790, Training Loss: 0.03652062639594078, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:12, Epoch: 126, Batch: 800, Training Loss: 0.03336610645055771, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:12, Epoch: 126, Batch: 810, Training Loss: 0.03410112261772156, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:13, Epoch: 126, Batch: 820, Training Loss: 0.04571529552340507, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:14, Epoch: 126, Batch: 830, Training Loss: 0.04990873001515865, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:15, Epoch: 126, Batch: 840, Training Loss: 0.05393373966217041, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:15, Epoch: 126, Batch: 850, Training Loss: 0.045381690934300424, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:16, Epoch: 126, Batch: 860, Training Loss: 0.04825621470808983, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:17, Epoch: 126, Batch: 870, Training Loss: 0.05668330192565918, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:18, Epoch: 126, Batch: 880, Training Loss: 0.03557168133556843, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:18, Epoch: 126, Batch: 890, Training Loss: 0.04345606788992882, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:19, Epoch: 126, Batch: 900, Training Loss: 0.04585364162921905, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:20, Epoch: 126, Batch: 910, Training Loss: 0.046262618899345395, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:20, Epoch: 126, Batch: 920, Training Loss: 0.04695141538977623, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:21, Epoch: 126, Batch: 930, Training Loss: 0.038020306080579755, LR: 0.00010000000000000003
Epoch: 126, Validation Top 1 acc: 98.90225219726562
Epoch: 126, Validation Top 5 acc: 99.99333953857422
Epoch: 126, Validation Set Loss: 0.04069768637418747
Start training epoch 127
Time, 2019-01-01T20:30:49, Epoch: 127, Batch: 10, Training Loss: 0.01856934353709221, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:50, Epoch: 127, Batch: 20, Training Loss: 0.037252777814865114, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:51, Epoch: 127, Batch: 30, Training Loss: 0.03341507017612457, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:51, Epoch: 127, Batch: 40, Training Loss: 0.05439073592424393, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:52, Epoch: 127, Batch: 50, Training Loss: 0.04802728481590748, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:53, Epoch: 127, Batch: 60, Training Loss: 0.060129808634519576, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:54, Epoch: 127, Batch: 70, Training Loss: 0.05620053932070732, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:54, Epoch: 127, Batch: 80, Training Loss: 0.04663012623786926, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:55, Epoch: 127, Batch: 90, Training Loss: 0.04300284124910832, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:56, Epoch: 127, Batch: 100, Training Loss: 0.040176712721586225, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:57, Epoch: 127, Batch: 110, Training Loss: 0.04071411080658436, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:57, Epoch: 127, Batch: 120, Training Loss: 0.03798395320773125, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:58, Epoch: 127, Batch: 130, Training Loss: 0.05670670531690121, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:59, Epoch: 127, Batch: 140, Training Loss: 0.04207179583609104, LR: 0.00010000000000000003
Time, 2019-01-01T20:30:59, Epoch: 127, Batch: 150, Training Loss: 0.05484004244208336, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:00, Epoch: 127, Batch: 160, Training Loss: 0.047392642870545384, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:01, Epoch: 127, Batch: 170, Training Loss: 0.03196342661976814, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:02, Epoch: 127, Batch: 180, Training Loss: 0.03738593235611916, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:02, Epoch: 127, Batch: 190, Training Loss: 0.043137674778699876, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:03, Epoch: 127, Batch: 200, Training Loss: 0.038816841691732405, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:04, Epoch: 127, Batch: 210, Training Loss: 0.0447887659072876, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:05, Epoch: 127, Batch: 220, Training Loss: 0.029289649426937105, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:05, Epoch: 127, Batch: 230, Training Loss: 0.038110741227865216, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:06, Epoch: 127, Batch: 240, Training Loss: 0.04824333935976029, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:07, Epoch: 127, Batch: 250, Training Loss: 0.04546104073524475, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:07, Epoch: 127, Batch: 260, Training Loss: 0.03751727342605591, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:08, Epoch: 127, Batch: 270, Training Loss: 0.02627957835793495, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:09, Epoch: 127, Batch: 280, Training Loss: 0.03791492879390716, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:10, Epoch: 127, Batch: 290, Training Loss: 0.03430923782289028, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:10, Epoch: 127, Batch: 300, Training Loss: 0.04616949781775474, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:11, Epoch: 127, Batch: 310, Training Loss: 0.06393686681985855, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:12, Epoch: 127, Batch: 320, Training Loss: 0.04911432489752769, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:13, Epoch: 127, Batch: 330, Training Loss: 0.06338031962513924, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:13, Epoch: 127, Batch: 340, Training Loss: 0.04821346625685692, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:14, Epoch: 127, Batch: 350, Training Loss: 0.04879584200680256, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:15, Epoch: 127, Batch: 360, Training Loss: 0.024305279552936553, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:15, Epoch: 127, Batch: 370, Training Loss: 0.03957236930727959, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:16, Epoch: 127, Batch: 380, Training Loss: 0.027165261656045915, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:17, Epoch: 127, Batch: 390, Training Loss: 0.03523881733417511, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:18, Epoch: 127, Batch: 400, Training Loss: 0.03471725545823574, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:18, Epoch: 127, Batch: 410, Training Loss: 0.043623583018779756, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:19, Epoch: 127, Batch: 420, Training Loss: 0.03149537779390812, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:20, Epoch: 127, Batch: 430, Training Loss: 0.0386199876666069, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:20, Epoch: 127, Batch: 440, Training Loss: 0.03835282437503338, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:21, Epoch: 127, Batch: 450, Training Loss: 0.05047549270093441, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:22, Epoch: 127, Batch: 460, Training Loss: 0.035930754989385603, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:23, Epoch: 127, Batch: 470, Training Loss: 0.02835255116224289, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:23, Epoch: 127, Batch: 480, Training Loss: 0.03641847297549248, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:24, Epoch: 127, Batch: 490, Training Loss: 0.03837863206863403, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:25, Epoch: 127, Batch: 500, Training Loss: 0.04695289619266987, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:26, Epoch: 127, Batch: 510, Training Loss: 0.040359126031398775, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:26, Epoch: 127, Batch: 520, Training Loss: 0.04364150688052178, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:27, Epoch: 127, Batch: 530, Training Loss: 0.04200747795403004, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:28, Epoch: 127, Batch: 540, Training Loss: 0.06262452080845833, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:28, Epoch: 127, Batch: 550, Training Loss: 0.06797912418842315, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:29, Epoch: 127, Batch: 560, Training Loss: 0.04428529143333435, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:30, Epoch: 127, Batch: 570, Training Loss: 0.04374238736927509, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:31, Epoch: 127, Batch: 580, Training Loss: 0.04914194568991661, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:31, Epoch: 127, Batch: 590, Training Loss: 0.04557716958224774, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:32, Epoch: 127, Batch: 600, Training Loss: 0.030244127660989762, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:33, Epoch: 127, Batch: 610, Training Loss: 0.032861101999878885, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:33, Epoch: 127, Batch: 620, Training Loss: 0.027209196612238883, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:34, Epoch: 127, Batch: 630, Training Loss: 0.03276819810271263, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:35, Epoch: 127, Batch: 640, Training Loss: 0.04844948127865791, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:36, Epoch: 127, Batch: 650, Training Loss: 0.025595171749591826, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:36, Epoch: 127, Batch: 660, Training Loss: 0.036492889374494554, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:37, Epoch: 127, Batch: 670, Training Loss: 0.059631363674998286, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:38, Epoch: 127, Batch: 680, Training Loss: 0.037615378573536876, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:38, Epoch: 127, Batch: 690, Training Loss: 0.04568161442875862, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:39, Epoch: 127, Batch: 700, Training Loss: 0.03459021709859371, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:40, Epoch: 127, Batch: 710, Training Loss: 0.03540466427803039, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:41, Epoch: 127, Batch: 720, Training Loss: 0.03645996823906898, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:41, Epoch: 127, Batch: 730, Training Loss: 0.052757352218031885, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:42, Epoch: 127, Batch: 740, Training Loss: 0.027234215289354324, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:43, Epoch: 127, Batch: 750, Training Loss: 0.043753727525472644, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:43, Epoch: 127, Batch: 760, Training Loss: 0.03890262544155121, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:44, Epoch: 127, Batch: 770, Training Loss: 0.040247640758752826, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:45, Epoch: 127, Batch: 780, Training Loss: 0.03759326413273811, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:46, Epoch: 127, Batch: 790, Training Loss: 0.05757912322878837, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:46, Epoch: 127, Batch: 800, Training Loss: 0.016422315686941146, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:47, Epoch: 127, Batch: 810, Training Loss: 0.037941490113735196, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:48, Epoch: 127, Batch: 820, Training Loss: 0.03557867892086506, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:48, Epoch: 127, Batch: 830, Training Loss: 0.03956204503774643, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:49, Epoch: 127, Batch: 840, Training Loss: 0.04025452695786953, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:50, Epoch: 127, Batch: 850, Training Loss: 0.03746581822633743, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:51, Epoch: 127, Batch: 860, Training Loss: 0.045001907646656035, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:51, Epoch: 127, Batch: 870, Training Loss: 0.03506527356803417, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:52, Epoch: 127, Batch: 880, Training Loss: 0.036110447347164155, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:53, Epoch: 127, Batch: 890, Training Loss: 0.0354118749499321, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:54, Epoch: 127, Batch: 900, Training Loss: 0.03147073052823544, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:54, Epoch: 127, Batch: 910, Training Loss: 0.030054260045289993, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:55, Epoch: 127, Batch: 920, Training Loss: 0.04028107896447182, LR: 0.00010000000000000003
Time, 2019-01-01T20:31:56, Epoch: 127, Batch: 930, Training Loss: 0.04511383734643459, LR: 0.00010000000000000003
Epoch: 127, Validation Top 1 acc: 98.88392639160156
Epoch: 127, Validation Top 5 acc: 99.99000549316406
Epoch: 127, Validation Set Loss: 0.04069400578737259
Start training epoch 128
Time, 2019-01-01T20:32:24, Epoch: 128, Batch: 10, Training Loss: 0.042101823911070826, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:25, Epoch: 128, Batch: 20, Training Loss: 0.029599706083536147, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:26, Epoch: 128, Batch: 30, Training Loss: 0.03895454928278923, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:26, Epoch: 128, Batch: 40, Training Loss: 0.045145255699753764, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:27, Epoch: 128, Batch: 50, Training Loss: 0.028430696949362753, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:28, Epoch: 128, Batch: 60, Training Loss: 0.03792626187205315, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:29, Epoch: 128, Batch: 70, Training Loss: 0.039406916871666905, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:30, Epoch: 128, Batch: 80, Training Loss: 0.04132579639554024, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:30, Epoch: 128, Batch: 90, Training Loss: 0.0604098342359066, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:31, Epoch: 128, Batch: 100, Training Loss: 0.07503966130316257, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:32, Epoch: 128, Batch: 110, Training Loss: 0.03935663774609566, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:32, Epoch: 128, Batch: 120, Training Loss: 0.03299096710979939, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:33, Epoch: 128, Batch: 130, Training Loss: 0.061409467831254005, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:34, Epoch: 128, Batch: 140, Training Loss: 0.03610844723880291, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:35, Epoch: 128, Batch: 150, Training Loss: 0.04239013567566872, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:35, Epoch: 128, Batch: 160, Training Loss: 0.045859111845493315, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:36, Epoch: 128, Batch: 170, Training Loss: 0.030593233183026314, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:37, Epoch: 128, Batch: 180, Training Loss: 0.03060243874788284, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:38, Epoch: 128, Batch: 190, Training Loss: 0.03252141773700714, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:38, Epoch: 128, Batch: 200, Training Loss: 0.027956677600741386, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:39, Epoch: 128, Batch: 210, Training Loss: 0.028353162854909898, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:40, Epoch: 128, Batch: 220, Training Loss: 0.04937986209988594, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:41, Epoch: 128, Batch: 230, Training Loss: 0.04167235940694809, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:41, Epoch: 128, Batch: 240, Training Loss: 0.04326220788061619, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:42, Epoch: 128, Batch: 250, Training Loss: 0.051502865925431254, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:43, Epoch: 128, Batch: 260, Training Loss: 0.036545274779200554, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:43, Epoch: 128, Batch: 270, Training Loss: 0.051172679662704466, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:44, Epoch: 128, Batch: 280, Training Loss: 0.046921270340681075, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:45, Epoch: 128, Batch: 290, Training Loss: 0.03862358741462231, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:46, Epoch: 128, Batch: 300, Training Loss: 0.042127708718180654, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:46, Epoch: 128, Batch: 310, Training Loss: 0.03573216833174229, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:47, Epoch: 128, Batch: 320, Training Loss: 0.03560150489211082, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:48, Epoch: 128, Batch: 330, Training Loss: 0.03835306353867054, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:49, Epoch: 128, Batch: 340, Training Loss: 0.026942500844597816, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:49, Epoch: 128, Batch: 350, Training Loss: 0.04037830531597138, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:50, Epoch: 128, Batch: 360, Training Loss: 0.039187125489115716, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:51, Epoch: 128, Batch: 370, Training Loss: 0.03527606092393398, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:51, Epoch: 128, Batch: 380, Training Loss: 0.03149622119963169, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:52, Epoch: 128, Batch: 390, Training Loss: 0.04160460717976093, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:53, Epoch: 128, Batch: 400, Training Loss: 0.03166177943348884, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:54, Epoch: 128, Batch: 410, Training Loss: 0.03928052857518196, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:54, Epoch: 128, Batch: 420, Training Loss: 0.048639611899852754, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:55, Epoch: 128, Batch: 430, Training Loss: 0.051065173000097275, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:56, Epoch: 128, Batch: 440, Training Loss: 0.026752983778715135, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:56, Epoch: 128, Batch: 450, Training Loss: 0.04784757569432259, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:57, Epoch: 128, Batch: 460, Training Loss: 0.04078410938382149, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:58, Epoch: 128, Batch: 470, Training Loss: 0.039348480850458147, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:59, Epoch: 128, Batch: 480, Training Loss: 0.0573336273431778, LR: 0.00010000000000000003
Time, 2019-01-01T20:32:59, Epoch: 128, Batch: 490, Training Loss: 0.037005481868982316, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:00, Epoch: 128, Batch: 500, Training Loss: 0.045388394966721535, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:01, Epoch: 128, Batch: 510, Training Loss: 0.04388255439698696, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:01, Epoch: 128, Batch: 520, Training Loss: 0.03160249516367912, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:02, Epoch: 128, Batch: 530, Training Loss: 0.04001018069684505, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:03, Epoch: 128, Batch: 540, Training Loss: 0.04147457927465439, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:04, Epoch: 128, Batch: 550, Training Loss: 0.03479532189667225, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:04, Epoch: 128, Batch: 560, Training Loss: 0.04754161834716797, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:05, Epoch: 128, Batch: 570, Training Loss: 0.040744667127728465, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:06, Epoch: 128, Batch: 580, Training Loss: 0.05101204589009285, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:06, Epoch: 128, Batch: 590, Training Loss: 0.03134833946824074, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:07, Epoch: 128, Batch: 600, Training Loss: 0.04496922343969345, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:08, Epoch: 128, Batch: 610, Training Loss: 0.05999438613653183, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:09, Epoch: 128, Batch: 620, Training Loss: 0.0531425841152668, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:09, Epoch: 128, Batch: 630, Training Loss: 0.044627847895026204, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:10, Epoch: 128, Batch: 640, Training Loss: 0.03563646674156189, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:11, Epoch: 128, Batch: 650, Training Loss: 0.042699892073869705, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:11, Epoch: 128, Batch: 660, Training Loss: 0.0455888569355011, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:12, Epoch: 128, Batch: 670, Training Loss: 0.05658021531999111, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:13, Epoch: 128, Batch: 680, Training Loss: 0.04904819130897522, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:14, Epoch: 128, Batch: 690, Training Loss: 0.04903129227459431, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:14, Epoch: 128, Batch: 700, Training Loss: 0.04949214830994606, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:15, Epoch: 128, Batch: 710, Training Loss: 0.03438858389854431, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:16, Epoch: 128, Batch: 720, Training Loss: 0.028666330873966216, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:17, Epoch: 128, Batch: 730, Training Loss: 0.04792583584785461, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:17, Epoch: 128, Batch: 740, Training Loss: 0.036716265976428984, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:18, Epoch: 128, Batch: 750, Training Loss: 0.040501213446259496, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:19, Epoch: 128, Batch: 760, Training Loss: 0.04230904132127762, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:20, Epoch: 128, Batch: 770, Training Loss: 0.04146585837006569, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:20, Epoch: 128, Batch: 780, Training Loss: 0.04269218742847443, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:21, Epoch: 128, Batch: 790, Training Loss: 0.02985398583114147, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:22, Epoch: 128, Batch: 800, Training Loss: 0.0590825729072094, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:22, Epoch: 128, Batch: 810, Training Loss: 0.04228639602661133, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:23, Epoch: 128, Batch: 820, Training Loss: 0.02789827883243561, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:24, Epoch: 128, Batch: 830, Training Loss: 0.023696118593215944, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:25, Epoch: 128, Batch: 840, Training Loss: 0.03448019996285438, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:25, Epoch: 128, Batch: 850, Training Loss: 0.03707892522215843, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:26, Epoch: 128, Batch: 860, Training Loss: 0.02835473380982876, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:27, Epoch: 128, Batch: 870, Training Loss: 0.031224872916936874, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:28, Epoch: 128, Batch: 880, Training Loss: 0.025175948068499564, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:28, Epoch: 128, Batch: 890, Training Loss: 0.02354375496506691, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:29, Epoch: 128, Batch: 900, Training Loss: 0.0375129047781229, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:30, Epoch: 128, Batch: 910, Training Loss: 0.03006681464612484, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:31, Epoch: 128, Batch: 920, Training Loss: 0.07485701255500317, LR: 0.00010000000000000003
Time, 2019-01-01T20:33:31, Epoch: 128, Batch: 930, Training Loss: 0.02764270082116127, LR: 0.00010000000000000003
Epoch: 128, Validation Top 1 acc: 98.89891815185547
Epoch: 128, Validation Top 5 acc: 99.99000549316406
Epoch: 128, Validation Set Loss: 0.04059294983744621
Start training epoch 129
Time, 2019-01-01T20:34:00, Epoch: 129, Batch: 10, Training Loss: 0.05754005946218967, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:01, Epoch: 129, Batch: 20, Training Loss: 0.03745921403169632, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:02, Epoch: 129, Batch: 30, Training Loss: 0.0508538980036974, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:02, Epoch: 129, Batch: 40, Training Loss: 0.042580153048038485, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:03, Epoch: 129, Batch: 50, Training Loss: 0.0545831635594368, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:04, Epoch: 129, Batch: 60, Training Loss: 0.050350210070610045, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:04, Epoch: 129, Batch: 70, Training Loss: 0.036117513105273245, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:05, Epoch: 129, Batch: 80, Training Loss: 0.05538751594722271, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:06, Epoch: 129, Batch: 90, Training Loss: 0.034117691218853, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:07, Epoch: 129, Batch: 100, Training Loss: 0.05028325319290161, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:07, Epoch: 129, Batch: 110, Training Loss: 0.030536931380629538, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:08, Epoch: 129, Batch: 120, Training Loss: 0.05027227848768234, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:09, Epoch: 129, Batch: 130, Training Loss: 0.04596372544765472, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:09, Epoch: 129, Batch: 140, Training Loss: 0.030262255668640138, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:10, Epoch: 129, Batch: 150, Training Loss: 0.03503389954566956, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:11, Epoch: 129, Batch: 160, Training Loss: 0.03303714618086815, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:12, Epoch: 129, Batch: 170, Training Loss: 0.03303445130586624, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:12, Epoch: 129, Batch: 180, Training Loss: 0.04325020760297775, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:13, Epoch: 129, Batch: 190, Training Loss: 0.05284088514745235, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:14, Epoch: 129, Batch: 200, Training Loss: 0.03093880005180836, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:15, Epoch: 129, Batch: 210, Training Loss: 0.03845458701252937, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:15, Epoch: 129, Batch: 220, Training Loss: 0.05061740204691887, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:16, Epoch: 129, Batch: 230, Training Loss: 0.04611678719520569, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:17, Epoch: 129, Batch: 240, Training Loss: 0.05187541805207729, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:17, Epoch: 129, Batch: 250, Training Loss: 0.04568886384367943, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:18, Epoch: 129, Batch: 260, Training Loss: 0.025453557446599006, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:19, Epoch: 129, Batch: 270, Training Loss: 0.03170518688857556, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:20, Epoch: 129, Batch: 280, Training Loss: 0.04202001839876175, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:20, Epoch: 129, Batch: 290, Training Loss: 0.046984292566776276, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:21, Epoch: 129, Batch: 300, Training Loss: 0.057103922590613365, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:22, Epoch: 129, Batch: 310, Training Loss: 0.04355801567435265, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:22, Epoch: 129, Batch: 320, Training Loss: 0.028041163831949233, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:23, Epoch: 129, Batch: 330, Training Loss: 0.037935317307710645, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:24, Epoch: 129, Batch: 340, Training Loss: 0.04321276806294918, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:25, Epoch: 129, Batch: 350, Training Loss: 0.057051154971122744, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:25, Epoch: 129, Batch: 360, Training Loss: 0.03686278760433197, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:26, Epoch: 129, Batch: 370, Training Loss: 0.036125733703374865, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:27, Epoch: 129, Batch: 380, Training Loss: 0.03658984825015068, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:28, Epoch: 129, Batch: 390, Training Loss: 0.04989530630409718, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:28, Epoch: 129, Batch: 400, Training Loss: 0.04183500111103058, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:29, Epoch: 129, Batch: 410, Training Loss: 0.02870299555361271, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:30, Epoch: 129, Batch: 420, Training Loss: 0.04075670652091503, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:30, Epoch: 129, Batch: 430, Training Loss: 0.044346174225211143, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:31, Epoch: 129, Batch: 440, Training Loss: 0.05376680456101894, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:32, Epoch: 129, Batch: 450, Training Loss: 0.04033946618437767, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:33, Epoch: 129, Batch: 460, Training Loss: 0.037247995659708974, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:33, Epoch: 129, Batch: 470, Training Loss: 0.03751523420214653, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:34, Epoch: 129, Batch: 480, Training Loss: 0.04772178828716278, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:35, Epoch: 129, Batch: 490, Training Loss: 0.05025299526751041, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:35, Epoch: 129, Batch: 500, Training Loss: 0.03629156649112701, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:36, Epoch: 129, Batch: 510, Training Loss: 0.03951883837580681, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:37, Epoch: 129, Batch: 520, Training Loss: 0.04976038783788681, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:38, Epoch: 129, Batch: 530, Training Loss: 0.04592678621411324, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:38, Epoch: 129, Batch: 540, Training Loss: 0.03134378343820572, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:39, Epoch: 129, Batch: 550, Training Loss: 0.04481600970029831, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:40, Epoch: 129, Batch: 560, Training Loss: 0.04174713715910912, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:41, Epoch: 129, Batch: 570, Training Loss: 0.026207774877548218, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:41, Epoch: 129, Batch: 580, Training Loss: 0.0315464548766613, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:42, Epoch: 129, Batch: 590, Training Loss: 0.027128289639949798, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:43, Epoch: 129, Batch: 600, Training Loss: 0.04177515581250191, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:43, Epoch: 129, Batch: 610, Training Loss: 0.04420950710773468, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:44, Epoch: 129, Batch: 620, Training Loss: 0.026565920561552048, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:45, Epoch: 129, Batch: 630, Training Loss: 0.029693915322422982, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:46, Epoch: 129, Batch: 640, Training Loss: 0.04316493980586529, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:46, Epoch: 129, Batch: 650, Training Loss: 0.040703107789158824, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:47, Epoch: 129, Batch: 660, Training Loss: 0.035780494660139085, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:48, Epoch: 129, Batch: 670, Training Loss: 0.02813592106103897, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:49, Epoch: 129, Batch: 680, Training Loss: 0.033560780063271525, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:49, Epoch: 129, Batch: 690, Training Loss: 0.06429100930690765, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:50, Epoch: 129, Batch: 700, Training Loss: 0.03606823682785034, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:51, Epoch: 129, Batch: 710, Training Loss: 0.028702840209007263, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:51, Epoch: 129, Batch: 720, Training Loss: 0.04097859524190426, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:52, Epoch: 129, Batch: 730, Training Loss: 0.0419823806732893, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:53, Epoch: 129, Batch: 740, Training Loss: 0.02890757843852043, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:54, Epoch: 129, Batch: 750, Training Loss: 0.046868395432829855, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:54, Epoch: 129, Batch: 760, Training Loss: 0.045002774149179456, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:55, Epoch: 129, Batch: 770, Training Loss: 0.045577690750360486, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:56, Epoch: 129, Batch: 780, Training Loss: 0.054451731592416765, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:57, Epoch: 129, Batch: 790, Training Loss: 0.02741004414856434, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:57, Epoch: 129, Batch: 800, Training Loss: 0.037506724894046786, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:58, Epoch: 129, Batch: 810, Training Loss: 0.03531617596745491, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:59, Epoch: 129, Batch: 820, Training Loss: 0.04275843761861324, LR: 0.00010000000000000003
Time, 2019-01-01T20:34:59, Epoch: 129, Batch: 830, Training Loss: 0.035051696375012396, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:00, Epoch: 129, Batch: 840, Training Loss: 0.03873039595782757, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:01, Epoch: 129, Batch: 850, Training Loss: 0.035693814232945445, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:01, Epoch: 129, Batch: 860, Training Loss: 0.03662884905934334, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:02, Epoch: 129, Batch: 870, Training Loss: 0.0306320458650589, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:03, Epoch: 129, Batch: 880, Training Loss: 0.0318756066262722, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:04, Epoch: 129, Batch: 890, Training Loss: 0.041689804196357726, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:04, Epoch: 129, Batch: 900, Training Loss: 0.058946621417999265, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:05, Epoch: 129, Batch: 910, Training Loss: 0.06329186260700226, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:06, Epoch: 129, Batch: 920, Training Loss: 0.0455947645008564, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:06, Epoch: 129, Batch: 930, Training Loss: 0.0267569899559021, LR: 0.00010000000000000003
Epoch: 129, Validation Top 1 acc: 98.91058349609375
Epoch: 129, Validation Top 5 acc: 99.99000549316406
Epoch: 129, Validation Set Loss: 0.04069416970014572
Start training epoch 130
Time, 2019-01-01T20:35:34, Epoch: 130, Batch: 10, Training Loss: 0.03377151228487492, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:35, Epoch: 130, Batch: 20, Training Loss: 0.04279316961765289, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:36, Epoch: 130, Batch: 30, Training Loss: 0.029925017058849333, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:36, Epoch: 130, Batch: 40, Training Loss: 0.03899724222719669, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:37, Epoch: 130, Batch: 50, Training Loss: 0.03878930658102035, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:38, Epoch: 130, Batch: 60, Training Loss: 0.03123977854847908, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:39, Epoch: 130, Batch: 70, Training Loss: 0.04082740321755409, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:39, Epoch: 130, Batch: 80, Training Loss: 0.04267994500696659, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:40, Epoch: 130, Batch: 90, Training Loss: 0.043766708299517634, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:41, Epoch: 130, Batch: 100, Training Loss: 0.046669697389006615, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:41, Epoch: 130, Batch: 110, Training Loss: 0.028857598826289178, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:42, Epoch: 130, Batch: 120, Training Loss: 0.04170318841934204, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:43, Epoch: 130, Batch: 130, Training Loss: 0.025996815785765647, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:44, Epoch: 130, Batch: 140, Training Loss: 0.034030571952462195, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:44, Epoch: 130, Batch: 150, Training Loss: 0.04052428565919399, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:45, Epoch: 130, Batch: 160, Training Loss: 0.026101940497756004, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:46, Epoch: 130, Batch: 170, Training Loss: 0.054317834228277205, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:47, Epoch: 130, Batch: 180, Training Loss: 0.05437430888414383, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:47, Epoch: 130, Batch: 190, Training Loss: 0.057560624554753304, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:48, Epoch: 130, Batch: 200, Training Loss: 0.042214743047952655, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:49, Epoch: 130, Batch: 210, Training Loss: 0.0368101455271244, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:50, Epoch: 130, Batch: 220, Training Loss: 0.0324201438575983, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:51, Epoch: 130, Batch: 230, Training Loss: 0.04263067841529846, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:51, Epoch: 130, Batch: 240, Training Loss: 0.03480209819972515, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:52, Epoch: 130, Batch: 250, Training Loss: 0.031572068482637404, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:53, Epoch: 130, Batch: 260, Training Loss: 0.03188081309199333, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:54, Epoch: 130, Batch: 270, Training Loss: 0.04764498397707939, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:54, Epoch: 130, Batch: 280, Training Loss: 0.027919790148735045, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:55, Epoch: 130, Batch: 290, Training Loss: 0.04734091870486736, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:56, Epoch: 130, Batch: 300, Training Loss: 0.06526007242500782, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:56, Epoch: 130, Batch: 310, Training Loss: 0.04536110758781433, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:57, Epoch: 130, Batch: 320, Training Loss: 0.04598916172981262, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:58, Epoch: 130, Batch: 330, Training Loss: 0.04528353177011013, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:59, Epoch: 130, Batch: 340, Training Loss: 0.03989625908434391, LR: 0.00010000000000000003
Time, 2019-01-01T20:35:59, Epoch: 130, Batch: 350, Training Loss: 0.04859498105943203, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:00, Epoch: 130, Batch: 360, Training Loss: 0.04196180962026119, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:01, Epoch: 130, Batch: 370, Training Loss: 0.026670295372605325, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:02, Epoch: 130, Batch: 380, Training Loss: 0.0460466492921114, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:02, Epoch: 130, Batch: 390, Training Loss: 0.03573402911424637, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:03, Epoch: 130, Batch: 400, Training Loss: 0.0375680971890688, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:04, Epoch: 130, Batch: 410, Training Loss: 0.03235437199473381, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:04, Epoch: 130, Batch: 420, Training Loss: 0.054123179614543916, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:05, Epoch: 130, Batch: 430, Training Loss: 0.040943673253059386, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:06, Epoch: 130, Batch: 440, Training Loss: 0.04337710998952389, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:07, Epoch: 130, Batch: 450, Training Loss: 0.028635754063725472, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:07, Epoch: 130, Batch: 460, Training Loss: 0.024578270316123963, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:08, Epoch: 130, Batch: 470, Training Loss: 0.046339232474565506, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:09, Epoch: 130, Batch: 480, Training Loss: 0.03603672944009304, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:10, Epoch: 130, Batch: 490, Training Loss: 0.03429221026599407, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:10, Epoch: 130, Batch: 500, Training Loss: 0.059512360766530034, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:11, Epoch: 130, Batch: 510, Training Loss: 0.06026193797588349, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:12, Epoch: 130, Batch: 520, Training Loss: 0.05531964339315891, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:13, Epoch: 130, Batch: 530, Training Loss: 0.037611643224954604, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:13, Epoch: 130, Batch: 540, Training Loss: 0.04229169934988022, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:14, Epoch: 130, Batch: 550, Training Loss: 0.06349201537668706, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:15, Epoch: 130, Batch: 560, Training Loss: 0.03593095429241657, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:15, Epoch: 130, Batch: 570, Training Loss: 0.028566737473011018, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:16, Epoch: 130, Batch: 580, Training Loss: 0.050256003811955455, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:17, Epoch: 130, Batch: 590, Training Loss: 0.04974483884871006, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:18, Epoch: 130, Batch: 600, Training Loss: 0.03509813770651817, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:18, Epoch: 130, Batch: 610, Training Loss: 0.032320137694478036, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:19, Epoch: 130, Batch: 620, Training Loss: 0.05126987360417843, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:20, Epoch: 130, Batch: 630, Training Loss: 0.023049220070242883, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:21, Epoch: 130, Batch: 640, Training Loss: 0.04657584503293037, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:21, Epoch: 130, Batch: 650, Training Loss: 0.044807448610663415, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:22, Epoch: 130, Batch: 660, Training Loss: 0.03277990072965622, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:23, Epoch: 130, Batch: 670, Training Loss: 0.043412001058459285, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:23, Epoch: 130, Batch: 680, Training Loss: 0.03225967213511467, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:24, Epoch: 130, Batch: 690, Training Loss: 0.04768786169588566, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:25, Epoch: 130, Batch: 700, Training Loss: 0.043950673937797544, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:26, Epoch: 130, Batch: 710, Training Loss: 0.02335931099951267, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:26, Epoch: 130, Batch: 720, Training Loss: 0.046716343611478806, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:27, Epoch: 130, Batch: 730, Training Loss: 0.035704242438077925, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:28, Epoch: 130, Batch: 740, Training Loss: 0.04324219264090061, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:28, Epoch: 130, Batch: 750, Training Loss: 0.027579627558588983, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:29, Epoch: 130, Batch: 760, Training Loss: 0.033194833993911745, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:30, Epoch: 130, Batch: 770, Training Loss: 0.050982894375920296, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:31, Epoch: 130, Batch: 780, Training Loss: 0.05729841478168964, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:31, Epoch: 130, Batch: 790, Training Loss: 0.038797352835536, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:32, Epoch: 130, Batch: 800, Training Loss: 0.05104498937726021, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:33, Epoch: 130, Batch: 810, Training Loss: 0.030305464565753937, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:34, Epoch: 130, Batch: 820, Training Loss: 0.03816329389810562, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:34, Epoch: 130, Batch: 830, Training Loss: 0.05424754247069359, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:35, Epoch: 130, Batch: 840, Training Loss: 0.024427877739071846, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:36, Epoch: 130, Batch: 850, Training Loss: 0.02948480099439621, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:36, Epoch: 130, Batch: 860, Training Loss: 0.034297145903110504, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:37, Epoch: 130, Batch: 870, Training Loss: 0.05214219838380814, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:38, Epoch: 130, Batch: 880, Training Loss: 0.045199296250939366, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:39, Epoch: 130, Batch: 890, Training Loss: 0.033042538166046145, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:39, Epoch: 130, Batch: 900, Training Loss: 0.04976209700107574, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:40, Epoch: 130, Batch: 910, Training Loss: 0.04715111292898655, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:41, Epoch: 130, Batch: 920, Training Loss: 0.036248113587498666, LR: 0.00010000000000000003
Time, 2019-01-01T20:36:41, Epoch: 130, Batch: 930, Training Loss: 0.05197765752673149, LR: 0.00010000000000000003
Epoch: 130, Validation Top 1 acc: 98.90058898925781
Epoch: 130, Validation Top 5 acc: 99.99000549316406
Epoch: 130, Validation Set Loss: 0.04065811634063721
Start training epoch 131
Time, 2019-01-01T20:37:09, Epoch: 131, Batch: 10, Training Loss: 0.03262619003653526, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:10, Epoch: 131, Batch: 20, Training Loss: 0.056816352903842925, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:11, Epoch: 131, Batch: 30, Training Loss: 0.03732347339391708, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:11, Epoch: 131, Batch: 40, Training Loss: 0.04062162265181542, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:12, Epoch: 131, Batch: 50, Training Loss: 0.039367574453353885, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:13, Epoch: 131, Batch: 60, Training Loss: 0.038783357664942744, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:13, Epoch: 131, Batch: 70, Training Loss: 0.03931684009730816, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:14, Epoch: 131, Batch: 80, Training Loss: 0.036368533968925476, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:15, Epoch: 131, Batch: 90, Training Loss: 0.03754163831472397, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:16, Epoch: 131, Batch: 100, Training Loss: 0.04406941831111908, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:16, Epoch: 131, Batch: 110, Training Loss: 0.038783611729741096, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:17, Epoch: 131, Batch: 120, Training Loss: 0.05544922947883606, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:18, Epoch: 131, Batch: 130, Training Loss: 0.021257656067609786, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:18, Epoch: 131, Batch: 140, Training Loss: 0.038363969698548314, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:19, Epoch: 131, Batch: 150, Training Loss: 0.05979233682155609, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:20, Epoch: 131, Batch: 160, Training Loss: 0.03800696916878223, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:21, Epoch: 131, Batch: 170, Training Loss: 0.031362636387348174, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:21, Epoch: 131, Batch: 180, Training Loss: 0.05133703500032425, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:22, Epoch: 131, Batch: 190, Training Loss: 0.021503863483667375, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:23, Epoch: 131, Batch: 200, Training Loss: 0.04773305356502533, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:24, Epoch: 131, Batch: 210, Training Loss: 0.04758692011237144, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:25, Epoch: 131, Batch: 220, Training Loss: 0.036511585116386414, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:25, Epoch: 131, Batch: 230, Training Loss: 0.046571006253361705, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:26, Epoch: 131, Batch: 240, Training Loss: 0.06483603566884995, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:27, Epoch: 131, Batch: 250, Training Loss: 0.04392125457525253, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:28, Epoch: 131, Batch: 260, Training Loss: 0.035226161032915114, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:28, Epoch: 131, Batch: 270, Training Loss: 0.037210021167993546, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:29, Epoch: 131, Batch: 280, Training Loss: 0.037133946642279626, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:30, Epoch: 131, Batch: 290, Training Loss: 0.04186498075723648, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:31, Epoch: 131, Batch: 300, Training Loss: 0.04641497246921063, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:31, Epoch: 131, Batch: 310, Training Loss: 0.04638947509229183, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:32, Epoch: 131, Batch: 320, Training Loss: 0.03507196120917797, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:33, Epoch: 131, Batch: 330, Training Loss: 0.04611265249550343, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:33, Epoch: 131, Batch: 340, Training Loss: 0.029476303979754447, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:34, Epoch: 131, Batch: 350, Training Loss: 0.03365934379398823, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:35, Epoch: 131, Batch: 360, Training Loss: 0.03209497518837452, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:36, Epoch: 131, Batch: 370, Training Loss: 0.024359023198485374, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:36, Epoch: 131, Batch: 380, Training Loss: 0.02336387112736702, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:37, Epoch: 131, Batch: 390, Training Loss: 0.03950926847755909, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:38, Epoch: 131, Batch: 400, Training Loss: 0.025182478502392767, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:39, Epoch: 131, Batch: 410, Training Loss: 0.035425007343292236, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:39, Epoch: 131, Batch: 420, Training Loss: 0.028990673646330834, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:40, Epoch: 131, Batch: 430, Training Loss: 0.04746408201754093, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:41, Epoch: 131, Batch: 440, Training Loss: 0.04244396463036537, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:41, Epoch: 131, Batch: 450, Training Loss: 0.054052726924419404, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:42, Epoch: 131, Batch: 460, Training Loss: 0.03236310668289662, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:43, Epoch: 131, Batch: 470, Training Loss: 0.03354724533855915, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:44, Epoch: 131, Batch: 480, Training Loss: 0.04496327042579651, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:44, Epoch: 131, Batch: 490, Training Loss: 0.04713304489850998, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:45, Epoch: 131, Batch: 500, Training Loss: 0.036514494195580484, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:46, Epoch: 131, Batch: 510, Training Loss: 0.04229104891419411, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:47, Epoch: 131, Batch: 520, Training Loss: 0.04667914658784866, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:47, Epoch: 131, Batch: 530, Training Loss: 0.0606100220233202, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:48, Epoch: 131, Batch: 540, Training Loss: 0.04122706986963749, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:49, Epoch: 131, Batch: 550, Training Loss: 0.019437593594193458, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:49, Epoch: 131, Batch: 560, Training Loss: 0.058693480864167216, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:50, Epoch: 131, Batch: 570, Training Loss: 0.05192715041339398, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:51, Epoch: 131, Batch: 580, Training Loss: 0.03901011273264885, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:52, Epoch: 131, Batch: 590, Training Loss: 0.03570057600736618, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:52, Epoch: 131, Batch: 600, Training Loss: 0.05576752200722694, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:53, Epoch: 131, Batch: 610, Training Loss: 0.03874952495098114, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:54, Epoch: 131, Batch: 620, Training Loss: 0.03915024846792221, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:54, Epoch: 131, Batch: 630, Training Loss: 0.03214792869985104, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:55, Epoch: 131, Batch: 640, Training Loss: 0.05839634388685226, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:56, Epoch: 131, Batch: 650, Training Loss: 0.03356121741235256, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:57, Epoch: 131, Batch: 660, Training Loss: 0.03592793047428131, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:57, Epoch: 131, Batch: 670, Training Loss: 0.04239759035408497, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:58, Epoch: 131, Batch: 680, Training Loss: 0.04019537195563316, LR: 0.00010000000000000003
Time, 2019-01-01T20:37:59, Epoch: 131, Batch: 690, Training Loss: 0.039517713338136674, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:00, Epoch: 131, Batch: 700, Training Loss: 0.0330924317240715, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:00, Epoch: 131, Batch: 710, Training Loss: 0.03988338857889175, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:01, Epoch: 131, Batch: 720, Training Loss: 0.041612375527620316, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:02, Epoch: 131, Batch: 730, Training Loss: 0.053370629996061326, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:03, Epoch: 131, Batch: 740, Training Loss: 0.036391176655888556, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:03, Epoch: 131, Batch: 750, Training Loss: 0.024775850772857665, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:04, Epoch: 131, Batch: 760, Training Loss: 0.053426064923405646, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:05, Epoch: 131, Batch: 770, Training Loss: 0.03164815604686737, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:06, Epoch: 131, Batch: 780, Training Loss: 0.03380562886595726, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:06, Epoch: 131, Batch: 790, Training Loss: 0.034435346722602844, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:07, Epoch: 131, Batch: 800, Training Loss: 0.048017121851444244, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:08, Epoch: 131, Batch: 810, Training Loss: 0.055748581886291504, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:09, Epoch: 131, Batch: 820, Training Loss: 0.05061832927167416, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:09, Epoch: 131, Batch: 830, Training Loss: 0.05152270868420601, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:10, Epoch: 131, Batch: 840, Training Loss: 0.04538642354309559, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:11, Epoch: 131, Batch: 850, Training Loss: 0.03437232524156571, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:11, Epoch: 131, Batch: 860, Training Loss: 0.0403905738145113, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:12, Epoch: 131, Batch: 870, Training Loss: 0.06098347492516041, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:13, Epoch: 131, Batch: 880, Training Loss: 0.04787911549210548, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:14, Epoch: 131, Batch: 890, Training Loss: 0.036440227553248404, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:14, Epoch: 131, Batch: 900, Training Loss: 0.03052227236330509, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:15, Epoch: 131, Batch: 910, Training Loss: 0.03473619371652603, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:16, Epoch: 131, Batch: 920, Training Loss: 0.028387634456157683, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:17, Epoch: 131, Batch: 930, Training Loss: 0.04847083017230034, LR: 0.00010000000000000003
Epoch: 131, Validation Top 1 acc: 98.90558624267578
Epoch: 131, Validation Top 5 acc: 99.99166870117188
Epoch: 131, Validation Set Loss: 0.040608085691928864
Start training epoch 132
Time, 2019-01-01T20:38:44, Epoch: 132, Batch: 10, Training Loss: 0.053361570835113524, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:44, Epoch: 132, Batch: 20, Training Loss: 0.03282180726528168, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:45, Epoch: 132, Batch: 30, Training Loss: 0.05705433376133442, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:46, Epoch: 132, Batch: 40, Training Loss: 0.030295129865407944, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:47, Epoch: 132, Batch: 50, Training Loss: 0.0435462836176157, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:47, Epoch: 132, Batch: 60, Training Loss: 0.043749380856752396, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:48, Epoch: 132, Batch: 70, Training Loss: 0.043042140826582906, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:49, Epoch: 132, Batch: 80, Training Loss: 0.04654789976775646, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:49, Epoch: 132, Batch: 90, Training Loss: 0.0376142293214798, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:50, Epoch: 132, Batch: 100, Training Loss: 0.05223751589655876, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:51, Epoch: 132, Batch: 110, Training Loss: 0.02541741356253624, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:52, Epoch: 132, Batch: 120, Training Loss: 0.04610751643776893, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:52, Epoch: 132, Batch: 130, Training Loss: 0.03743505850434303, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:53, Epoch: 132, Batch: 140, Training Loss: 0.04403004013001919, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:54, Epoch: 132, Batch: 150, Training Loss: 0.04374101608991623, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:55, Epoch: 132, Batch: 160, Training Loss: 0.036732565611600876, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:55, Epoch: 132, Batch: 170, Training Loss: 0.04042552150785923, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:56, Epoch: 132, Batch: 180, Training Loss: 0.05485513210296631, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:57, Epoch: 132, Batch: 190, Training Loss: 0.03781094513833523, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:58, Epoch: 132, Batch: 200, Training Loss: 0.05460470728576183, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:58, Epoch: 132, Batch: 210, Training Loss: 0.03784872442483902, LR: 0.00010000000000000003
Time, 2019-01-01T20:38:59, Epoch: 132, Batch: 220, Training Loss: 0.023614019528031348, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:00, Epoch: 132, Batch: 230, Training Loss: 0.0419935330748558, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:00, Epoch: 132, Batch: 240, Training Loss: 0.04023138806223869, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:01, Epoch: 132, Batch: 250, Training Loss: 0.0267907090485096, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:02, Epoch: 132, Batch: 260, Training Loss: 0.031831751391291616, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:03, Epoch: 132, Batch: 270, Training Loss: 0.03907615654170513, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:03, Epoch: 132, Batch: 280, Training Loss: 0.024361632764339447, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:04, Epoch: 132, Batch: 290, Training Loss: 0.046836063265800476, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:05, Epoch: 132, Batch: 300, Training Loss: 0.04537213109433651, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:05, Epoch: 132, Batch: 310, Training Loss: 0.05279679521918297, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:06, Epoch: 132, Batch: 320, Training Loss: 0.026287445053458213, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:07, Epoch: 132, Batch: 330, Training Loss: 0.06177360527217388, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:08, Epoch: 132, Batch: 340, Training Loss: 0.02597094476222992, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:08, Epoch: 132, Batch: 350, Training Loss: 0.03232007697224617, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:09, Epoch: 132, Batch: 360, Training Loss: 0.0360330231487751, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:10, Epoch: 132, Batch: 370, Training Loss: 0.023689739033579828, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:10, Epoch: 132, Batch: 380, Training Loss: 0.03488406538963318, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:11, Epoch: 132, Batch: 390, Training Loss: 0.03177625462412834, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:12, Epoch: 132, Batch: 400, Training Loss: 0.03488873019814491, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:13, Epoch: 132, Batch: 410, Training Loss: 0.040934532135725024, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:13, Epoch: 132, Batch: 420, Training Loss: 0.02848733961582184, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:14, Epoch: 132, Batch: 430, Training Loss: 0.046766850352287295, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:15, Epoch: 132, Batch: 440, Training Loss: 0.04849663265049457, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:15, Epoch: 132, Batch: 450, Training Loss: 0.047994287312030794, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:16, Epoch: 132, Batch: 460, Training Loss: 0.038625751808285716, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:17, Epoch: 132, Batch: 470, Training Loss: 0.032932070270180705, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:18, Epoch: 132, Batch: 480, Training Loss: 0.04205925948917866, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:18, Epoch: 132, Batch: 490, Training Loss: 0.04275819510221481, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:19, Epoch: 132, Batch: 500, Training Loss: 0.04113379493355751, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:20, Epoch: 132, Batch: 510, Training Loss: 0.03011809252202511, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:20, Epoch: 132, Batch: 520, Training Loss: 0.024785029515624047, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:21, Epoch: 132, Batch: 530, Training Loss: 0.05185597017407417, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:22, Epoch: 132, Batch: 540, Training Loss: 0.03555786125361919, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:23, Epoch: 132, Batch: 550, Training Loss: 0.0369614589959383, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:23, Epoch: 132, Batch: 560, Training Loss: 0.05758482590317726, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:24, Epoch: 132, Batch: 570, Training Loss: 0.040104229003190994, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:25, Epoch: 132, Batch: 580, Training Loss: 0.04393217787146568, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:26, Epoch: 132, Batch: 590, Training Loss: 0.028036335855722426, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:26, Epoch: 132, Batch: 600, Training Loss: 0.03713923841714859, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:27, Epoch: 132, Batch: 610, Training Loss: 0.03567260876297951, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:28, Epoch: 132, Batch: 620, Training Loss: 0.03800087571144104, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:28, Epoch: 132, Batch: 630, Training Loss: 0.0598244521766901, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:29, Epoch: 132, Batch: 640, Training Loss: 0.03282378502190113, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:30, Epoch: 132, Batch: 650, Training Loss: 0.06108546666800976, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:31, Epoch: 132, Batch: 660, Training Loss: 0.04385203532874584, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:31, Epoch: 132, Batch: 670, Training Loss: 0.03854943513870239, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:32, Epoch: 132, Batch: 680, Training Loss: 0.04211176186800003, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:33, Epoch: 132, Batch: 690, Training Loss: 0.04144915156066418, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:34, Epoch: 132, Batch: 700, Training Loss: 0.04049446024000645, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:34, Epoch: 132, Batch: 710, Training Loss: 0.042624909058213235, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:35, Epoch: 132, Batch: 720, Training Loss: 0.05315105132758617, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:36, Epoch: 132, Batch: 730, Training Loss: 0.04995422214269638, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:36, Epoch: 132, Batch: 740, Training Loss: 0.03663543984293938, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:37, Epoch: 132, Batch: 750, Training Loss: 0.03939398378133774, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:38, Epoch: 132, Batch: 760, Training Loss: 0.03934074714779854, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:39, Epoch: 132, Batch: 770, Training Loss: 0.058703073859214784, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:39, Epoch: 132, Batch: 780, Training Loss: 0.039464961364865304, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:40, Epoch: 132, Batch: 790, Training Loss: 0.02961524426937103, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:41, Epoch: 132, Batch: 800, Training Loss: 0.041135454922914504, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:42, Epoch: 132, Batch: 810, Training Loss: 0.042322036996483806, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:42, Epoch: 132, Batch: 820, Training Loss: 0.026982322335243225, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:43, Epoch: 132, Batch: 830, Training Loss: 0.04246189296245575, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:44, Epoch: 132, Batch: 840, Training Loss: 0.05123349949717522, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:44, Epoch: 132, Batch: 850, Training Loss: 0.03600475639104843, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:45, Epoch: 132, Batch: 860, Training Loss: 0.04208410903811455, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:46, Epoch: 132, Batch: 870, Training Loss: 0.06482991203665733, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:47, Epoch: 132, Batch: 880, Training Loss: 0.04157602973282337, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:47, Epoch: 132, Batch: 890, Training Loss: 0.04328114837408066, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:48, Epoch: 132, Batch: 900, Training Loss: 0.037269216030836105, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:49, Epoch: 132, Batch: 910, Training Loss: 0.048070064187049864, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:50, Epoch: 132, Batch: 920, Training Loss: 0.04295142441987991, LR: 0.00010000000000000003
Time, 2019-01-01T20:39:50, Epoch: 132, Batch: 930, Training Loss: 0.03165217190980911, LR: 0.00010000000000000003
Epoch: 132, Validation Top 1 acc: 98.88226318359375
Epoch: 132, Validation Top 5 acc: 99.99000549316406
Epoch: 132, Validation Set Loss: 0.04066363722085953
Start training epoch 133
Time, 2019-01-01T20:40:17, Epoch: 133, Batch: 10, Training Loss: 0.03182820118963718, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:18, Epoch: 133, Batch: 20, Training Loss: 0.02615130729973316, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:19, Epoch: 133, Batch: 30, Training Loss: 0.04602773040533066, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:20, Epoch: 133, Batch: 40, Training Loss: 0.04595778807997704, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:20, Epoch: 133, Batch: 50, Training Loss: 0.03286109305918217, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:21, Epoch: 133, Batch: 60, Training Loss: 0.037443500384688376, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:22, Epoch: 133, Batch: 70, Training Loss: 0.05206665471196174, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:23, Epoch: 133, Batch: 80, Training Loss: 0.04343101978302002, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:23, Epoch: 133, Batch: 90, Training Loss: 0.03819942995905876, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:24, Epoch: 133, Batch: 100, Training Loss: 0.05374154858291149, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:25, Epoch: 133, Batch: 110, Training Loss: 0.0369393028318882, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:26, Epoch: 133, Batch: 120, Training Loss: 0.04270736798644066, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:26, Epoch: 133, Batch: 130, Training Loss: 0.03281800113618374, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:27, Epoch: 133, Batch: 140, Training Loss: 0.03200349807739258, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:28, Epoch: 133, Batch: 150, Training Loss: 0.05162119120359421, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:28, Epoch: 133, Batch: 160, Training Loss: 0.034006967395544055, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:29, Epoch: 133, Batch: 170, Training Loss: 0.05724739357829094, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:30, Epoch: 133, Batch: 180, Training Loss: 0.050477564334869385, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:31, Epoch: 133, Batch: 190, Training Loss: 0.028488825261592864, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:31, Epoch: 133, Batch: 200, Training Loss: 0.03311387374997139, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:32, Epoch: 133, Batch: 210, Training Loss: 0.0393803421407938, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:33, Epoch: 133, Batch: 220, Training Loss: 0.032625338062644005, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:34, Epoch: 133, Batch: 230, Training Loss: 0.0450254037976265, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:34, Epoch: 133, Batch: 240, Training Loss: 0.04842814430594444, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:35, Epoch: 133, Batch: 250, Training Loss: 0.03997391797602177, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:36, Epoch: 133, Batch: 260, Training Loss: 0.03125839382410049, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:37, Epoch: 133, Batch: 270, Training Loss: 0.031341776624321936, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:37, Epoch: 133, Batch: 280, Training Loss: 0.04445140399038792, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:38, Epoch: 133, Batch: 290, Training Loss: 0.030087329819798468, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:39, Epoch: 133, Batch: 300, Training Loss: 0.06674118861556053, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:40, Epoch: 133, Batch: 310, Training Loss: 0.055067310482263564, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:41, Epoch: 133, Batch: 320, Training Loss: 0.032889625430107115, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:41, Epoch: 133, Batch: 330, Training Loss: 0.04495364129543304, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:42, Epoch: 133, Batch: 340, Training Loss: 0.03443196378648281, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:43, Epoch: 133, Batch: 350, Training Loss: 0.04445507787168026, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:44, Epoch: 133, Batch: 360, Training Loss: 0.04500630274415016, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:45, Epoch: 133, Batch: 370, Training Loss: 0.03780970945954323, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:45, Epoch: 133, Batch: 380, Training Loss: 0.04110533073544502, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:46, Epoch: 133, Batch: 390, Training Loss: 0.04373663291335106, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:47, Epoch: 133, Batch: 400, Training Loss: 0.03642803207039833, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:48, Epoch: 133, Batch: 410, Training Loss: 0.038585498556494714, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:49, Epoch: 133, Batch: 420, Training Loss: 0.02389985918998718, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:50, Epoch: 133, Batch: 430, Training Loss: 0.04831850305199623, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:50, Epoch: 133, Batch: 440, Training Loss: 0.026124561205506325, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:51, Epoch: 133, Batch: 450, Training Loss: 0.04365729838609696, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:52, Epoch: 133, Batch: 460, Training Loss: 0.037181306630373, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:53, Epoch: 133, Batch: 470, Training Loss: 0.044593816995620726, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:53, Epoch: 133, Batch: 480, Training Loss: 0.048994938284158705, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:54, Epoch: 133, Batch: 490, Training Loss: 0.0446039792150259, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:55, Epoch: 133, Batch: 500, Training Loss: 0.03779871352016926, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:56, Epoch: 133, Batch: 510, Training Loss: 0.03248348794877529, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:57, Epoch: 133, Batch: 520, Training Loss: 0.03847251087427139, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:57, Epoch: 133, Batch: 530, Training Loss: 0.03572315834462643, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:58, Epoch: 133, Batch: 540, Training Loss: 0.03294928148388863, LR: 0.00010000000000000003
Time, 2019-01-01T20:40:59, Epoch: 133, Batch: 550, Training Loss: 0.027436119318008424, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:00, Epoch: 133, Batch: 560, Training Loss: 0.04955650418996811, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:01, Epoch: 133, Batch: 570, Training Loss: 0.036146174743771554, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:01, Epoch: 133, Batch: 580, Training Loss: 0.0314929373562336, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:02, Epoch: 133, Batch: 590, Training Loss: 0.04648839645087719, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:03, Epoch: 133, Batch: 600, Training Loss: 0.055420438200235365, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:04, Epoch: 133, Batch: 610, Training Loss: 0.04020384810864926, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:04, Epoch: 133, Batch: 620, Training Loss: 0.04701327830553055, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:05, Epoch: 133, Batch: 630, Training Loss: 0.035191486775875094, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:06, Epoch: 133, Batch: 640, Training Loss: 0.04797330256551504, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:07, Epoch: 133, Batch: 650, Training Loss: 0.05272785611450672, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:08, Epoch: 133, Batch: 660, Training Loss: 0.03985795825719833, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:08, Epoch: 133, Batch: 670, Training Loss: 0.05019174553453922, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:09, Epoch: 133, Batch: 680, Training Loss: 0.04720885157585144, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:10, Epoch: 133, Batch: 690, Training Loss: 0.05417227447032928, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:11, Epoch: 133, Batch: 700, Training Loss: 0.042028019577264784, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:12, Epoch: 133, Batch: 710, Training Loss: 0.04096861891448498, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:12, Epoch: 133, Batch: 720, Training Loss: 0.04698824919760227, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:13, Epoch: 133, Batch: 730, Training Loss: 0.040437906980514526, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:14, Epoch: 133, Batch: 740, Training Loss: 0.042538828775286676, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:15, Epoch: 133, Batch: 750, Training Loss: 0.04233337193727493, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:15, Epoch: 133, Batch: 760, Training Loss: 0.05280654765665531, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:16, Epoch: 133, Batch: 770, Training Loss: 0.050525461137294767, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:17, Epoch: 133, Batch: 780, Training Loss: 0.02716382294893265, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:18, Epoch: 133, Batch: 790, Training Loss: 0.0410269632935524, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:19, Epoch: 133, Batch: 800, Training Loss: 0.033488061279058456, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:20, Epoch: 133, Batch: 810, Training Loss: 0.03341640494763851, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:20, Epoch: 133, Batch: 820, Training Loss: 0.04264600947499275, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:22, Epoch: 133, Batch: 830, Training Loss: 0.03630032129585743, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:22, Epoch: 133, Batch: 840, Training Loss: 0.037624059990048406, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:23, Epoch: 133, Batch: 850, Training Loss: 0.028086864203214646, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:24, Epoch: 133, Batch: 860, Training Loss: 0.05286130905151367, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:25, Epoch: 133, Batch: 870, Training Loss: 0.03682026118040085, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:26, Epoch: 133, Batch: 880, Training Loss: 0.046528274193406105, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:27, Epoch: 133, Batch: 890, Training Loss: 0.032130875065922736, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:28, Epoch: 133, Batch: 900, Training Loss: 0.042769025266170504, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:28, Epoch: 133, Batch: 910, Training Loss: 0.035836323723196985, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:29, Epoch: 133, Batch: 920, Training Loss: 0.04454119578003883, LR: 0.00010000000000000003
Time, 2019-01-01T20:41:30, Epoch: 133, Batch: 930, Training Loss: 0.0440523412078619, LR: 0.00010000000000000003
Epoch: 133, Validation Top 1 acc: 98.89559173583984
Epoch: 133, Validation Top 5 acc: 99.99166870117188
Epoch: 133, Validation Set Loss: 0.04063441604375839
Start training epoch 134
Time, 2019-01-01T20:41:59, Epoch: 134, Batch: 10, Training Loss: 0.027580473199486734, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:00, Epoch: 134, Batch: 20, Training Loss: 0.03375332206487656, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:01, Epoch: 134, Batch: 30, Training Loss: 0.045648155733942986, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:02, Epoch: 134, Batch: 40, Training Loss: 0.06728502213954926, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:03, Epoch: 134, Batch: 50, Training Loss: 0.0479761827737093, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:03, Epoch: 134, Batch: 60, Training Loss: 0.03986610770225525, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:04, Epoch: 134, Batch: 70, Training Loss: 0.04217100441455841, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:05, Epoch: 134, Batch: 80, Training Loss: 0.03697285577654839, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:06, Epoch: 134, Batch: 90, Training Loss: 0.03510470390319824, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:07, Epoch: 134, Batch: 100, Training Loss: 0.028830492123961448, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:08, Epoch: 134, Batch: 110, Training Loss: 0.05387382321059704, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:09, Epoch: 134, Batch: 120, Training Loss: 0.05494007766246796, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:10, Epoch: 134, Batch: 130, Training Loss: 0.061823220923542976, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:11, Epoch: 134, Batch: 140, Training Loss: 0.02692255973815918, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:11, Epoch: 134, Batch: 150, Training Loss: 0.0613709919154644, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:12, Epoch: 134, Batch: 160, Training Loss: 0.053653116896748544, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:13, Epoch: 134, Batch: 170, Training Loss: 0.03771333731710911, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:14, Epoch: 134, Batch: 180, Training Loss: 0.05618651695549488, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:14, Epoch: 134, Batch: 190, Training Loss: 0.028170251101255418, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:15, Epoch: 134, Batch: 200, Training Loss: 0.032231346145272254, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:16, Epoch: 134, Batch: 210, Training Loss: 0.030787676200270654, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:17, Epoch: 134, Batch: 220, Training Loss: 0.05438384748995304, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:17, Epoch: 134, Batch: 230, Training Loss: 0.04923125021159649, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:18, Epoch: 134, Batch: 240, Training Loss: 0.03509818278253078, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:19, Epoch: 134, Batch: 250, Training Loss: 0.03648997396230698, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:20, Epoch: 134, Batch: 260, Training Loss: 0.0522901002317667, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:20, Epoch: 134, Batch: 270, Training Loss: 0.027611346915364265, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:21, Epoch: 134, Batch: 280, Training Loss: 0.040560302510857585, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:22, Epoch: 134, Batch: 290, Training Loss: 0.03788016475737095, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:22, Epoch: 134, Batch: 300, Training Loss: 0.03783211298286915, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:23, Epoch: 134, Batch: 310, Training Loss: 0.060995426028966904, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:24, Epoch: 134, Batch: 320, Training Loss: 0.03960963673889637, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:25, Epoch: 134, Batch: 330, Training Loss: 0.04785667397081852, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:25, Epoch: 134, Batch: 340, Training Loss: 0.03907187655568123, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:26, Epoch: 134, Batch: 350, Training Loss: 0.05110969133675099, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:27, Epoch: 134, Batch: 360, Training Loss: 0.04813666194677353, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:28, Epoch: 134, Batch: 370, Training Loss: 0.04316329136490822, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:28, Epoch: 134, Batch: 380, Training Loss: 0.02626645117998123, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:29, Epoch: 134, Batch: 390, Training Loss: 0.042340302094817164, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:30, Epoch: 134, Batch: 400, Training Loss: 0.030250638723373413, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:31, Epoch: 134, Batch: 410, Training Loss: 0.03879273161292076, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:31, Epoch: 134, Batch: 420, Training Loss: 0.044721896573901174, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:32, Epoch: 134, Batch: 430, Training Loss: 0.059620263427495955, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:33, Epoch: 134, Batch: 440, Training Loss: 0.03602309301495552, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:34, Epoch: 134, Batch: 450, Training Loss: 0.0313918549567461, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:34, Epoch: 134, Batch: 460, Training Loss: 0.024621670320630073, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:35, Epoch: 134, Batch: 470, Training Loss: 0.04837624877691269, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:36, Epoch: 134, Batch: 480, Training Loss: 0.049146789312362674, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:36, Epoch: 134, Batch: 490, Training Loss: 0.03284748345613479, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:37, Epoch: 134, Batch: 500, Training Loss: 0.04145719408988953, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:38, Epoch: 134, Batch: 510, Training Loss: 0.02947106808423996, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:39, Epoch: 134, Batch: 520, Training Loss: 0.05762982368469238, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:40, Epoch: 134, Batch: 530, Training Loss: 0.03805892430245876, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:41, Epoch: 134, Batch: 540, Training Loss: 0.04758302681148052, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:41, Epoch: 134, Batch: 550, Training Loss: 0.04200010560452938, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:42, Epoch: 134, Batch: 560, Training Loss: 0.024910468980669974, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:43, Epoch: 134, Batch: 570, Training Loss: 0.03749761953949928, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:44, Epoch: 134, Batch: 580, Training Loss: 0.03559868559241295, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:45, Epoch: 134, Batch: 590, Training Loss: 0.03625286445021629, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:46, Epoch: 134, Batch: 600, Training Loss: 0.0641030177474022, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:47, Epoch: 134, Batch: 610, Training Loss: 0.03270074650645256, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:48, Epoch: 134, Batch: 620, Training Loss: 0.032659699022769925, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:49, Epoch: 134, Batch: 630, Training Loss: 0.03650926239788532, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:50, Epoch: 134, Batch: 640, Training Loss: 0.04779987037181854, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:50, Epoch: 134, Batch: 650, Training Loss: 0.03636633008718491, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:51, Epoch: 134, Batch: 660, Training Loss: 0.03236699253320694, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:52, Epoch: 134, Batch: 670, Training Loss: 0.031445518508553504, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:53, Epoch: 134, Batch: 680, Training Loss: 0.040710701048374175, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:53, Epoch: 134, Batch: 690, Training Loss: 0.044802792742848395, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:54, Epoch: 134, Batch: 700, Training Loss: 0.07002954334020614, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:55, Epoch: 134, Batch: 710, Training Loss: 0.03663564547896385, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:56, Epoch: 134, Batch: 720, Training Loss: 0.027293440699577332, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:56, Epoch: 134, Batch: 730, Training Loss: 0.04126589559018612, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:57, Epoch: 134, Batch: 740, Training Loss: 0.025352227315306664, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:58, Epoch: 134, Batch: 750, Training Loss: 0.03244572766125202, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:59, Epoch: 134, Batch: 760, Training Loss: 0.042849531024694444, LR: 0.00010000000000000003
Time, 2019-01-01T20:42:59, Epoch: 134, Batch: 770, Training Loss: 0.03314856216311455, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:00, Epoch: 134, Batch: 780, Training Loss: 0.034493359923362735, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:01, Epoch: 134, Batch: 790, Training Loss: 0.039115381240844724, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:01, Epoch: 134, Batch: 800, Training Loss: 0.0355189360678196, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:02, Epoch: 134, Batch: 810, Training Loss: 0.046344148367643355, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:03, Epoch: 134, Batch: 820, Training Loss: 0.04260193072259426, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:04, Epoch: 134, Batch: 830, Training Loss: 0.03205134384334087, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:04, Epoch: 134, Batch: 840, Training Loss: 0.037267738953232765, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:05, Epoch: 134, Batch: 850, Training Loss: 0.04608361795544624, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:06, Epoch: 134, Batch: 860, Training Loss: 0.0547972347587347, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:06, Epoch: 134, Batch: 870, Training Loss: 0.031144116073846817, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:07, Epoch: 134, Batch: 880, Training Loss: 0.04099549725651741, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:08, Epoch: 134, Batch: 890, Training Loss: 0.04436920695006848, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:09, Epoch: 134, Batch: 900, Training Loss: 0.0307322908192873, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:09, Epoch: 134, Batch: 910, Training Loss: 0.03986811153590679, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:10, Epoch: 134, Batch: 920, Training Loss: 0.02985350973904133, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:11, Epoch: 134, Batch: 930, Training Loss: 0.029504019767045975, LR: 0.00010000000000000003
Epoch: 134, Validation Top 1 acc: 98.90391540527344
Epoch: 134, Validation Top 5 acc: 99.99166870117188
Epoch: 134, Validation Set Loss: 0.04055936262011528
Start training epoch 135
Time, 2019-01-01T20:43:38, Epoch: 135, Batch: 10, Training Loss: 0.0497405506670475, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:39, Epoch: 135, Batch: 20, Training Loss: 0.03261804543435574, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:40, Epoch: 135, Batch: 30, Training Loss: 0.02742716483771801, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:40, Epoch: 135, Batch: 40, Training Loss: 0.051190204173326495, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:41, Epoch: 135, Batch: 50, Training Loss: 0.03229704834520817, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:42, Epoch: 135, Batch: 60, Training Loss: 0.04528721310198307, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:42, Epoch: 135, Batch: 70, Training Loss: 0.022694553434848785, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:43, Epoch: 135, Batch: 80, Training Loss: 0.04467271007597447, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:44, Epoch: 135, Batch: 90, Training Loss: 0.05751407891511917, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:45, Epoch: 135, Batch: 100, Training Loss: 0.0495313510298729, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:45, Epoch: 135, Batch: 110, Training Loss: 0.0481478288769722, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:46, Epoch: 135, Batch: 120, Training Loss: 0.02664497122168541, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:47, Epoch: 135, Batch: 130, Training Loss: 0.04112569019198418, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:48, Epoch: 135, Batch: 140, Training Loss: 0.045340527594089505, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:48, Epoch: 135, Batch: 150, Training Loss: 0.028172297030687334, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:49, Epoch: 135, Batch: 160, Training Loss: 0.03889697790145874, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:50, Epoch: 135, Batch: 170, Training Loss: 0.039956551790237424, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:50, Epoch: 135, Batch: 180, Training Loss: 0.05472269207239151, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:51, Epoch: 135, Batch: 190, Training Loss: 0.05909855291247368, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:52, Epoch: 135, Batch: 200, Training Loss: 0.03408503122627735, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:53, Epoch: 135, Batch: 210, Training Loss: 0.04554603658616543, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:53, Epoch: 135, Batch: 220, Training Loss: 0.03536760024726391, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:54, Epoch: 135, Batch: 230, Training Loss: 0.043931341543793676, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:55, Epoch: 135, Batch: 240, Training Loss: 0.030318612605333327, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:56, Epoch: 135, Batch: 250, Training Loss: 0.03338228240609169, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:56, Epoch: 135, Batch: 260, Training Loss: 0.04450389929115772, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:57, Epoch: 135, Batch: 270, Training Loss: 0.03289306685328484, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:58, Epoch: 135, Batch: 280, Training Loss: 0.04520937018096447, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:58, Epoch: 135, Batch: 290, Training Loss: 0.04041009694337845, LR: 0.00010000000000000003
Time, 2019-01-01T20:43:59, Epoch: 135, Batch: 300, Training Loss: 0.03162130005657673, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:00, Epoch: 135, Batch: 310, Training Loss: 0.034785255789756775, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:01, Epoch: 135, Batch: 320, Training Loss: 0.047090183943510056, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:01, Epoch: 135, Batch: 330, Training Loss: 0.03912572488188744, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:02, Epoch: 135, Batch: 340, Training Loss: 0.05778851546347141, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:03, Epoch: 135, Batch: 350, Training Loss: 0.04807011298835277, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:04, Epoch: 135, Batch: 360, Training Loss: 0.02929827943444252, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:04, Epoch: 135, Batch: 370, Training Loss: 0.043782264739274976, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:05, Epoch: 135, Batch: 380, Training Loss: 0.041524820402264594, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:06, Epoch: 135, Batch: 390, Training Loss: 0.03984406180679798, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:06, Epoch: 135, Batch: 400, Training Loss: 0.03780767172574997, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:07, Epoch: 135, Batch: 410, Training Loss: 0.02922707498073578, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:08, Epoch: 135, Batch: 420, Training Loss: 0.05906679108738899, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:09, Epoch: 135, Batch: 430, Training Loss: 0.03884707987308502, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:09, Epoch: 135, Batch: 440, Training Loss: 0.029279616102576254, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:10, Epoch: 135, Batch: 450, Training Loss: 0.02671602591872215, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:11, Epoch: 135, Batch: 460, Training Loss: 0.03962454497814179, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:12, Epoch: 135, Batch: 470, Training Loss: 0.04426449164748192, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:12, Epoch: 135, Batch: 480, Training Loss: 0.033450519666075706, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:13, Epoch: 135, Batch: 490, Training Loss: 0.02775186561048031, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:14, Epoch: 135, Batch: 500, Training Loss: 0.03441072031855583, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:14, Epoch: 135, Batch: 510, Training Loss: 0.0435892216861248, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:15, Epoch: 135, Batch: 520, Training Loss: 0.032318269833922386, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:16, Epoch: 135, Batch: 530, Training Loss: 0.04586353823542595, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:17, Epoch: 135, Batch: 540, Training Loss: 0.048616790771484376, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:17, Epoch: 135, Batch: 550, Training Loss: 0.05779782980680466, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:18, Epoch: 135, Batch: 560, Training Loss: 0.05092235952615738, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:19, Epoch: 135, Batch: 570, Training Loss: 0.04579794406890869, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:19, Epoch: 135, Batch: 580, Training Loss: 0.04019050188362598, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:20, Epoch: 135, Batch: 590, Training Loss: 0.036566776037216184, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:21, Epoch: 135, Batch: 600, Training Loss: 0.02258574478328228, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:22, Epoch: 135, Batch: 610, Training Loss: 0.036673632264137265, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:22, Epoch: 135, Batch: 620, Training Loss: 0.02774687148630619, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:23, Epoch: 135, Batch: 630, Training Loss: 0.0412583202123642, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:24, Epoch: 135, Batch: 640, Training Loss: 0.03369674533605575, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:24, Epoch: 135, Batch: 650, Training Loss: 0.04097612127661705, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:25, Epoch: 135, Batch: 660, Training Loss: 0.05141620710492134, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:26, Epoch: 135, Batch: 670, Training Loss: 0.03709581792354584, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:27, Epoch: 135, Batch: 680, Training Loss: 0.04243252836167812, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:27, Epoch: 135, Batch: 690, Training Loss: 0.027206119149923325, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:28, Epoch: 135, Batch: 700, Training Loss: 0.046429277211427686, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:29, Epoch: 135, Batch: 710, Training Loss: 0.03596479929983616, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:30, Epoch: 135, Batch: 720, Training Loss: 0.03934391438961029, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:30, Epoch: 135, Batch: 730, Training Loss: 0.032277075573801994, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:31, Epoch: 135, Batch: 740, Training Loss: 0.05006277859210968, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:32, Epoch: 135, Batch: 750, Training Loss: 0.02891782335937023, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:32, Epoch: 135, Batch: 760, Training Loss: 0.06922695599496365, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:33, Epoch: 135, Batch: 770, Training Loss: 0.04739287868142128, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:34, Epoch: 135, Batch: 780, Training Loss: 0.04372398816049099, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:35, Epoch: 135, Batch: 790, Training Loss: 0.03512619771063328, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:35, Epoch: 135, Batch: 800, Training Loss: 0.034355178475379944, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:36, Epoch: 135, Batch: 810, Training Loss: 0.04419868737459183, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:37, Epoch: 135, Batch: 820, Training Loss: 0.04606023244559765, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:37, Epoch: 135, Batch: 830, Training Loss: 0.04197816178202629, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:38, Epoch: 135, Batch: 840, Training Loss: 0.036959557980299, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:39, Epoch: 135, Batch: 850, Training Loss: 0.039364966377615926, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:40, Epoch: 135, Batch: 860, Training Loss: 0.07874345183372497, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:40, Epoch: 135, Batch: 870, Training Loss: 0.0448825903236866, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:41, Epoch: 135, Batch: 880, Training Loss: 0.028812258318066598, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:42, Epoch: 135, Batch: 890, Training Loss: 0.04957809410989285, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:43, Epoch: 135, Batch: 900, Training Loss: 0.041978498175740245, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:43, Epoch: 135, Batch: 910, Training Loss: 0.038162101060152054, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:44, Epoch: 135, Batch: 920, Training Loss: 0.038481847196817395, LR: 0.00010000000000000003
Time, 2019-01-01T20:44:45, Epoch: 135, Batch: 930, Training Loss: 0.03303106278181076, LR: 0.00010000000000000003
Epoch: 135, Validation Top 1 acc: 98.90558624267578
Epoch: 135, Validation Top 5 acc: 99.99166870117188
Epoch: 135, Validation Set Loss: 0.04058344289660454
Start training epoch 136
Time, 2019-01-01T20:45:12, Epoch: 136, Batch: 10, Training Loss: 0.05861620306968689, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:13, Epoch: 136, Batch: 20, Training Loss: 0.04621935114264488, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:13, Epoch: 136, Batch: 30, Training Loss: 0.04547000452876091, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:14, Epoch: 136, Batch: 40, Training Loss: 0.04091209284961224, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:15, Epoch: 136, Batch: 50, Training Loss: 0.03255119994282722, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:15, Epoch: 136, Batch: 60, Training Loss: 0.059470510482788085, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:16, Epoch: 136, Batch: 70, Training Loss: 0.030910399556159974, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:17, Epoch: 136, Batch: 80, Training Loss: 0.04026375897228718, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:18, Epoch: 136, Batch: 90, Training Loss: 0.04902688339352608, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:18, Epoch: 136, Batch: 100, Training Loss: 0.04676102474331856, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:19, Epoch: 136, Batch: 110, Training Loss: 0.03926808536052704, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:20, Epoch: 136, Batch: 120, Training Loss: 0.0420793954282999, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:21, Epoch: 136, Batch: 130, Training Loss: 0.03351625017821789, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:21, Epoch: 136, Batch: 140, Training Loss: 0.025505523756146432, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:22, Epoch: 136, Batch: 150, Training Loss: 0.05336504466831684, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:23, Epoch: 136, Batch: 160, Training Loss: 0.03033348023891449, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:24, Epoch: 136, Batch: 170, Training Loss: 0.029488353058695793, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:24, Epoch: 136, Batch: 180, Training Loss: 0.042349474132061006, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:25, Epoch: 136, Batch: 190, Training Loss: 0.048801713064312933, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:26, Epoch: 136, Batch: 200, Training Loss: 0.024200160801410676, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:26, Epoch: 136, Batch: 210, Training Loss: 0.0330218993127346, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:27, Epoch: 136, Batch: 220, Training Loss: 0.044653921574354175, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:28, Epoch: 136, Batch: 230, Training Loss: 0.035096659511327746, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:29, Epoch: 136, Batch: 240, Training Loss: 0.022562861442565918, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:29, Epoch: 136, Batch: 250, Training Loss: 0.048440711200237276, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:30, Epoch: 136, Batch: 260, Training Loss: 0.05359453335404396, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:31, Epoch: 136, Batch: 270, Training Loss: 0.023810049146413804, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:32, Epoch: 136, Batch: 280, Training Loss: 0.04312038943171501, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:33, Epoch: 136, Batch: 290, Training Loss: 0.04069694466888905, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:33, Epoch: 136, Batch: 300, Training Loss: 0.023840131238102913, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:34, Epoch: 136, Batch: 310, Training Loss: 0.03741214498877525, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:35, Epoch: 136, Batch: 320, Training Loss: 0.040563003718853, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:36, Epoch: 136, Batch: 330, Training Loss: 0.04013150036334991, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:36, Epoch: 136, Batch: 340, Training Loss: 0.027519669383764267, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:37, Epoch: 136, Batch: 350, Training Loss: 0.038025180250406264, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:38, Epoch: 136, Batch: 360, Training Loss: 0.024043457210063936, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:39, Epoch: 136, Batch: 370, Training Loss: 0.04734955355525017, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:39, Epoch: 136, Batch: 380, Training Loss: 0.0464127704501152, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:40, Epoch: 136, Batch: 390, Training Loss: 0.04467628225684166, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:41, Epoch: 136, Batch: 400, Training Loss: 0.0357193622738123, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:42, Epoch: 136, Batch: 410, Training Loss: 0.0374203085899353, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:43, Epoch: 136, Batch: 420, Training Loss: 0.04880901798605919, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:45, Epoch: 136, Batch: 430, Training Loss: 0.04658070877194405, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:46, Epoch: 136, Batch: 440, Training Loss: 0.049621938914060595, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:47, Epoch: 136, Batch: 450, Training Loss: 0.0264105923473835, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:48, Epoch: 136, Batch: 460, Training Loss: 0.030170788615942003, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:50, Epoch: 136, Batch: 470, Training Loss: 0.04732879586517811, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:51, Epoch: 136, Batch: 480, Training Loss: 0.032266313582658766, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:52, Epoch: 136, Batch: 490, Training Loss: 0.026542878523468972, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:53, Epoch: 136, Batch: 500, Training Loss: 0.057051677256822586, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:53, Epoch: 136, Batch: 510, Training Loss: 0.04712907411158085, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:54, Epoch: 136, Batch: 520, Training Loss: 0.07898566536605359, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:55, Epoch: 136, Batch: 530, Training Loss: 0.04304623529314995, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:56, Epoch: 136, Batch: 540, Training Loss: 0.047751498967409135, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:56, Epoch: 136, Batch: 550, Training Loss: 0.03751237913966179, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:57, Epoch: 136, Batch: 560, Training Loss: 0.035830264538526536, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:58, Epoch: 136, Batch: 570, Training Loss: 0.02329215742647648, LR: 0.00010000000000000003
Time, 2019-01-01T20:45:59, Epoch: 136, Batch: 580, Training Loss: 0.049172687157988546, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:00, Epoch: 136, Batch: 590, Training Loss: 0.04996717087924481, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:01, Epoch: 136, Batch: 600, Training Loss: 0.03855784498155117, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:02, Epoch: 136, Batch: 610, Training Loss: 0.03311005681753158, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:02, Epoch: 136, Batch: 620, Training Loss: 0.047334370017051694, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:03, Epoch: 136, Batch: 630, Training Loss: 0.054285045340657236, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:04, Epoch: 136, Batch: 640, Training Loss: 0.04337622672319412, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:05, Epoch: 136, Batch: 650, Training Loss: 0.027271655201911927, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:05, Epoch: 136, Batch: 660, Training Loss: 0.04184619821608067, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:06, Epoch: 136, Batch: 670, Training Loss: 0.042369768396019934, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:07, Epoch: 136, Batch: 680, Training Loss: 0.04684669449925423, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:08, Epoch: 136, Batch: 690, Training Loss: 0.06134551018476486, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:09, Epoch: 136, Batch: 700, Training Loss: 0.04073458239436149, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:09, Epoch: 136, Batch: 710, Training Loss: 0.054890184104442595, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:10, Epoch: 136, Batch: 720, Training Loss: 0.042698008567094804, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:11, Epoch: 136, Batch: 730, Training Loss: 0.03167613595724106, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:12, Epoch: 136, Batch: 740, Training Loss: 0.03635777123272419, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:12, Epoch: 136, Batch: 750, Training Loss: 0.03633650578558445, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:13, Epoch: 136, Batch: 760, Training Loss: 0.025002500042319298, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:14, Epoch: 136, Batch: 770, Training Loss: 0.043191353976726535, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:15, Epoch: 136, Batch: 780, Training Loss: 0.038492701947689056, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:16, Epoch: 136, Batch: 790, Training Loss: 0.054320793598890305, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:16, Epoch: 136, Batch: 800, Training Loss: 0.02512574791908264, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:17, Epoch: 136, Batch: 810, Training Loss: 0.037627261877059934, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:18, Epoch: 136, Batch: 820, Training Loss: 0.02570173665881157, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:19, Epoch: 136, Batch: 830, Training Loss: 0.05216218903660774, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:19, Epoch: 136, Batch: 840, Training Loss: 0.04885580837726593, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:20, Epoch: 136, Batch: 850, Training Loss: 0.037523862347006796, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:21, Epoch: 136, Batch: 860, Training Loss: 0.056073158606886865, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:22, Epoch: 136, Batch: 870, Training Loss: 0.03235844634473324, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:22, Epoch: 136, Batch: 880, Training Loss: 0.03500525839626789, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:23, Epoch: 136, Batch: 890, Training Loss: 0.04161910451948643, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:24, Epoch: 136, Batch: 900, Training Loss: 0.03564840517938137, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:25, Epoch: 136, Batch: 910, Training Loss: 0.055423151701688766, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:25, Epoch: 136, Batch: 920, Training Loss: 0.044758700951933864, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:26, Epoch: 136, Batch: 930, Training Loss: 0.02683631405234337, LR: 0.00010000000000000003
Epoch: 136, Validation Top 1 acc: 98.90225219726562
Epoch: 136, Validation Top 5 acc: 99.99166870117188
Epoch: 136, Validation Set Loss: 0.040589604526758194
Start training epoch 137
Time, 2019-01-01T20:46:55, Epoch: 137, Batch: 10, Training Loss: 0.055907128378748894, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:56, Epoch: 137, Batch: 20, Training Loss: 0.043836666643619536, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:57, Epoch: 137, Batch: 30, Training Loss: 0.030191466584801673, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:58, Epoch: 137, Batch: 40, Training Loss: 0.03604218736290932, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:58, Epoch: 137, Batch: 50, Training Loss: 0.03178681768476963, LR: 0.00010000000000000003
Time, 2019-01-01T20:46:59, Epoch: 137, Batch: 60, Training Loss: 0.02732517533004284, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:00, Epoch: 137, Batch: 70, Training Loss: 0.03603440374135971, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:01, Epoch: 137, Batch: 80, Training Loss: 0.04561384655535221, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:02, Epoch: 137, Batch: 90, Training Loss: 0.04660854339599609, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:02, Epoch: 137, Batch: 100, Training Loss: 0.031184511631727217, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:03, Epoch: 137, Batch: 110, Training Loss: 0.034122254326939584, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:04, Epoch: 137, Batch: 120, Training Loss: 0.042272521927952764, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:05, Epoch: 137, Batch: 130, Training Loss: 0.06200633570551872, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:05, Epoch: 137, Batch: 140, Training Loss: 0.028628117963671686, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:06, Epoch: 137, Batch: 150, Training Loss: 0.05036594457924366, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:07, Epoch: 137, Batch: 160, Training Loss: 0.032321725785732267, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:08, Epoch: 137, Batch: 170, Training Loss: 0.03819979317486286, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:09, Epoch: 137, Batch: 180, Training Loss: 0.03448550514876843, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:09, Epoch: 137, Batch: 190, Training Loss: 0.0537741057574749, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:10, Epoch: 137, Batch: 200, Training Loss: 0.03250882476568222, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:11, Epoch: 137, Batch: 210, Training Loss: 0.05147856175899505, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:12, Epoch: 137, Batch: 220, Training Loss: 0.04269449673593044, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:12, Epoch: 137, Batch: 230, Training Loss: 0.02946961671113968, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:13, Epoch: 137, Batch: 240, Training Loss: 0.04214616417884827, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:14, Epoch: 137, Batch: 250, Training Loss: 0.031457582488656044, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:15, Epoch: 137, Batch: 260, Training Loss: 0.024514705687761307, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:15, Epoch: 137, Batch: 270, Training Loss: 0.025786831974983215, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:16, Epoch: 137, Batch: 280, Training Loss: 0.04404936209321022, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:17, Epoch: 137, Batch: 290, Training Loss: 0.05377314835786819, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:18, Epoch: 137, Batch: 300, Training Loss: 0.030777619406580924, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:18, Epoch: 137, Batch: 310, Training Loss: 0.022633302956819534, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:19, Epoch: 137, Batch: 320, Training Loss: 0.031260499358177186, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:20, Epoch: 137, Batch: 330, Training Loss: 0.03202196769416332, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:21, Epoch: 137, Batch: 340, Training Loss: 0.03617960587143898, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:21, Epoch: 137, Batch: 350, Training Loss: 0.035531822219491006, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:22, Epoch: 137, Batch: 360, Training Loss: 0.03307583257555961, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:23, Epoch: 137, Batch: 370, Training Loss: 0.05246311575174332, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:24, Epoch: 137, Batch: 380, Training Loss: 0.03225699067115784, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:24, Epoch: 137, Batch: 390, Training Loss: 0.04020668938755989, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:25, Epoch: 137, Batch: 400, Training Loss: 0.03014187142252922, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:26, Epoch: 137, Batch: 410, Training Loss: 0.03635554239153862, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:27, Epoch: 137, Batch: 420, Training Loss: 0.047809261828660965, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:27, Epoch: 137, Batch: 430, Training Loss: 0.04416895806789398, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:28, Epoch: 137, Batch: 440, Training Loss: 0.05098208636045456, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:29, Epoch: 137, Batch: 450, Training Loss: 0.044823767989873885, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:30, Epoch: 137, Batch: 460, Training Loss: 0.04084591865539551, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:30, Epoch: 137, Batch: 470, Training Loss: 0.0593834538012743, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:31, Epoch: 137, Batch: 480, Training Loss: 0.04164743423461914, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:32, Epoch: 137, Batch: 490, Training Loss: 0.030999912694096566, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:33, Epoch: 137, Batch: 500, Training Loss: 0.02637948878109455, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:34, Epoch: 137, Batch: 510, Training Loss: 0.024296916648745538, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:34, Epoch: 137, Batch: 520, Training Loss: 0.05601642243564129, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:35, Epoch: 137, Batch: 530, Training Loss: 0.022235959023237228, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:36, Epoch: 137, Batch: 540, Training Loss: 0.040117482841014865, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:37, Epoch: 137, Batch: 550, Training Loss: 0.054220028966665265, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:37, Epoch: 137, Batch: 560, Training Loss: 0.047257355973124505, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:38, Epoch: 137, Batch: 570, Training Loss: 0.04400584399700165, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:39, Epoch: 137, Batch: 580, Training Loss: 0.04229222759604454, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:40, Epoch: 137, Batch: 590, Training Loss: 0.039809877052903175, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:41, Epoch: 137, Batch: 600, Training Loss: 0.031034479290246962, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:41, Epoch: 137, Batch: 610, Training Loss: 0.05051042623817921, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:42, Epoch: 137, Batch: 620, Training Loss: 0.02404274195432663, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:43, Epoch: 137, Batch: 630, Training Loss: 0.04147835448384285, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:44, Epoch: 137, Batch: 640, Training Loss: 0.058409777283668515, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:45, Epoch: 137, Batch: 650, Training Loss: 0.035136837512254715, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:46, Epoch: 137, Batch: 660, Training Loss: 0.04185528382658958, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:46, Epoch: 137, Batch: 670, Training Loss: 0.04346091151237488, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:47, Epoch: 137, Batch: 680, Training Loss: 0.03529257215559482, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:48, Epoch: 137, Batch: 690, Training Loss: 0.047135628014802936, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:49, Epoch: 137, Batch: 700, Training Loss: 0.027320890873670577, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:49, Epoch: 137, Batch: 710, Training Loss: 0.03985016606748104, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:50, Epoch: 137, Batch: 720, Training Loss: 0.047956624627113344, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:51, Epoch: 137, Batch: 730, Training Loss: 0.03439346253871918, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:52, Epoch: 137, Batch: 740, Training Loss: 0.04724984206259251, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:52, Epoch: 137, Batch: 750, Training Loss: 0.04755324646830559, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:53, Epoch: 137, Batch: 760, Training Loss: 0.04855446666479111, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:54, Epoch: 137, Batch: 770, Training Loss: 0.03876777924597263, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:55, Epoch: 137, Batch: 780, Training Loss: 0.04392479434609413, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:55, Epoch: 137, Batch: 790, Training Loss: 0.050665054097771646, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:56, Epoch: 137, Batch: 800, Training Loss: 0.03960627168416977, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:57, Epoch: 137, Batch: 810, Training Loss: 0.030456317216157915, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:58, Epoch: 137, Batch: 820, Training Loss: 0.042155567184090616, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:58, Epoch: 137, Batch: 830, Training Loss: 0.06044279746711254, LR: 0.00010000000000000003
Time, 2019-01-01T20:47:59, Epoch: 137, Batch: 840, Training Loss: 0.040049771592020986, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:00, Epoch: 137, Batch: 850, Training Loss: 0.03766533210873604, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:01, Epoch: 137, Batch: 860, Training Loss: 0.05371967405080795, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:01, Epoch: 137, Batch: 870, Training Loss: 0.06585585102438926, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:02, Epoch: 137, Batch: 880, Training Loss: 0.041492652520537375, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:03, Epoch: 137, Batch: 890, Training Loss: 0.02810637801885605, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:04, Epoch: 137, Batch: 900, Training Loss: 0.0485489621758461, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:04, Epoch: 137, Batch: 910, Training Loss: 0.039929062873125074, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:05, Epoch: 137, Batch: 920, Training Loss: 0.03522140868008137, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:06, Epoch: 137, Batch: 930, Training Loss: 0.057557106018066406, LR: 0.00010000000000000003
Epoch: 137, Validation Top 1 acc: 98.91558074951172
Epoch: 137, Validation Top 5 acc: 99.99166870117188
Epoch: 137, Validation Set Loss: 0.040531378239393234
Start training epoch 138
Time, 2019-01-01T20:48:34, Epoch: 138, Batch: 10, Training Loss: 0.03807757943868637, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:35, Epoch: 138, Batch: 20, Training Loss: 0.03521166890859604, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:36, Epoch: 138, Batch: 30, Training Loss: 0.04042717590928078, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:37, Epoch: 138, Batch: 40, Training Loss: 0.03973674103617668, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:38, Epoch: 138, Batch: 50, Training Loss: 0.04521209597587585, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:39, Epoch: 138, Batch: 60, Training Loss: 0.03839260078966618, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:40, Epoch: 138, Batch: 70, Training Loss: 0.05573938600718975, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:40, Epoch: 138, Batch: 80, Training Loss: 0.04084058366715908, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:41, Epoch: 138, Batch: 90, Training Loss: 0.037474002316594125, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:42, Epoch: 138, Batch: 100, Training Loss: 0.03389255478978157, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:43, Epoch: 138, Batch: 110, Training Loss: 0.04914042502641678, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:43, Epoch: 138, Batch: 120, Training Loss: 0.04741562195122242, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:44, Epoch: 138, Batch: 130, Training Loss: 0.03866803534328937, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:45, Epoch: 138, Batch: 140, Training Loss: 0.04128449261188507, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:46, Epoch: 138, Batch: 150, Training Loss: 0.03518866300582886, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:46, Epoch: 138, Batch: 160, Training Loss: 0.03301093243062496, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:47, Epoch: 138, Batch: 170, Training Loss: 0.032166799157857896, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:48, Epoch: 138, Batch: 180, Training Loss: 0.03158320263028145, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:49, Epoch: 138, Batch: 190, Training Loss: 0.04396772123873234, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:49, Epoch: 138, Batch: 200, Training Loss: 0.04242192544043064, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:50, Epoch: 138, Batch: 210, Training Loss: 0.043224656209349635, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:51, Epoch: 138, Batch: 220, Training Loss: 0.0335780780762434, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:52, Epoch: 138, Batch: 230, Training Loss: 0.04015032574534416, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:52, Epoch: 138, Batch: 240, Training Loss: 0.02519019991159439, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:53, Epoch: 138, Batch: 250, Training Loss: 0.03610589802265167, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:54, Epoch: 138, Batch: 260, Training Loss: 0.04678473100066185, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:55, Epoch: 138, Batch: 270, Training Loss: 0.04809801205992699, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:55, Epoch: 138, Batch: 280, Training Loss: 0.03598968386650085, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:56, Epoch: 138, Batch: 290, Training Loss: 0.05419805273413658, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:57, Epoch: 138, Batch: 300, Training Loss: 0.04425707086920738, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:58, Epoch: 138, Batch: 310, Training Loss: 0.04892596192657948, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:58, Epoch: 138, Batch: 320, Training Loss: 0.03578279912471771, LR: 0.00010000000000000003
Time, 2019-01-01T20:48:59, Epoch: 138, Batch: 330, Training Loss: 0.04622706733644009, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:00, Epoch: 138, Batch: 340, Training Loss: 0.038738083094358444, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:01, Epoch: 138, Batch: 350, Training Loss: 0.04831403605639935, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:01, Epoch: 138, Batch: 360, Training Loss: 0.021343798935413362, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:02, Epoch: 138, Batch: 370, Training Loss: 0.027917267754673958, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:03, Epoch: 138, Batch: 380, Training Loss: 0.0396266583353281, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:04, Epoch: 138, Batch: 390, Training Loss: 0.0342483676970005, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:04, Epoch: 138, Batch: 400, Training Loss: 0.0367895383387804, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:05, Epoch: 138, Batch: 410, Training Loss: 0.02930716872215271, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:06, Epoch: 138, Batch: 420, Training Loss: 0.05165856666862965, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:07, Epoch: 138, Batch: 430, Training Loss: 0.03433297462761402, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:08, Epoch: 138, Batch: 440, Training Loss: 0.04787837006151676, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:08, Epoch: 138, Batch: 450, Training Loss: 0.02510281205177307, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:09, Epoch: 138, Batch: 460, Training Loss: 0.039507967233657834, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:10, Epoch: 138, Batch: 470, Training Loss: 0.0450561560690403, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:11, Epoch: 138, Batch: 480, Training Loss: 0.037332237139344214, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:11, Epoch: 138, Batch: 490, Training Loss: 0.04544639512896538, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:12, Epoch: 138, Batch: 500, Training Loss: 0.04286397770047188, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:13, Epoch: 138, Batch: 510, Training Loss: 0.057985374331474306, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:14, Epoch: 138, Batch: 520, Training Loss: 0.04668171219527721, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:14, Epoch: 138, Batch: 530, Training Loss: 0.0433301780372858, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:15, Epoch: 138, Batch: 540, Training Loss: 0.043648631870746614, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:16, Epoch: 138, Batch: 550, Training Loss: 0.05150307975709438, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:17, Epoch: 138, Batch: 560, Training Loss: 0.02426349073648453, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:17, Epoch: 138, Batch: 570, Training Loss: 0.06312573626637459, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:18, Epoch: 138, Batch: 580, Training Loss: 0.03517610281705856, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:19, Epoch: 138, Batch: 590, Training Loss: 0.03751248940825462, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:20, Epoch: 138, Batch: 600, Training Loss: 0.025348882749676706, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:20, Epoch: 138, Batch: 610, Training Loss: 0.05117326229810715, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:21, Epoch: 138, Batch: 620, Training Loss: 0.052858874574303626, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:22, Epoch: 138, Batch: 630, Training Loss: 0.06263132132589817, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:23, Epoch: 138, Batch: 640, Training Loss: 0.03765218481421471, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:23, Epoch: 138, Batch: 650, Training Loss: 0.06289803199470043, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:24, Epoch: 138, Batch: 660, Training Loss: 0.039898545295000074, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:25, Epoch: 138, Batch: 670, Training Loss: 0.05003386326134205, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:25, Epoch: 138, Batch: 680, Training Loss: 0.0463287204504013, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:26, Epoch: 138, Batch: 690, Training Loss: 0.03192623667418957, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:27, Epoch: 138, Batch: 700, Training Loss: 0.03834353610873222, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:28, Epoch: 138, Batch: 710, Training Loss: 0.03977664858102799, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:28, Epoch: 138, Batch: 720, Training Loss: 0.03510569930076599, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:29, Epoch: 138, Batch: 730, Training Loss: 0.027030115574598314, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:30, Epoch: 138, Batch: 740, Training Loss: 0.0707907073199749, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:31, Epoch: 138, Batch: 750, Training Loss: 0.04093636944890022, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:31, Epoch: 138, Batch: 760, Training Loss: 0.030766814202070236, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:32, Epoch: 138, Batch: 770, Training Loss: 0.035732222348451616, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:33, Epoch: 138, Batch: 780, Training Loss: 0.0324691191315651, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:34, Epoch: 138, Batch: 790, Training Loss: 0.045590676739811896, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:34, Epoch: 138, Batch: 800, Training Loss: 0.030604512989521028, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:35, Epoch: 138, Batch: 810, Training Loss: 0.03812143504619599, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:36, Epoch: 138, Batch: 820, Training Loss: 0.039192259311676025, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:36, Epoch: 138, Batch: 830, Training Loss: 0.029702122509479522, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:37, Epoch: 138, Batch: 840, Training Loss: 0.050767576694488524, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:38, Epoch: 138, Batch: 850, Training Loss: 0.033470557630062105, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:39, Epoch: 138, Batch: 860, Training Loss: 0.031658253818750384, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:39, Epoch: 138, Batch: 870, Training Loss: 0.03630927056074142, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:40, Epoch: 138, Batch: 880, Training Loss: 0.03815483525395393, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:41, Epoch: 138, Batch: 890, Training Loss: 0.047201495990157126, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:42, Epoch: 138, Batch: 900, Training Loss: 0.041513460129499434, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:42, Epoch: 138, Batch: 910, Training Loss: 0.02992783859372139, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:43, Epoch: 138, Batch: 920, Training Loss: 0.03818236961960793, LR: 0.00010000000000000003
Time, 2019-01-01T20:49:44, Epoch: 138, Batch: 930, Training Loss: 0.05445163697004318, LR: 0.00010000000000000003
Epoch: 138, Validation Top 1 acc: 98.90558624267578
Epoch: 138, Validation Top 5 acc: 99.99166870117188
Epoch: 138, Validation Set Loss: 0.04056413471698761
Start training epoch 139
Time, 2019-01-01T20:50:11, Epoch: 139, Batch: 10, Training Loss: 0.04187415950000286, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:12, Epoch: 139, Batch: 20, Training Loss: 0.044044701382517815, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:13, Epoch: 139, Batch: 30, Training Loss: 0.03608784899115562, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:14, Epoch: 139, Batch: 40, Training Loss: 0.03327089697122574, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:14, Epoch: 139, Batch: 50, Training Loss: 0.037542369961738584, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:15, Epoch: 139, Batch: 60, Training Loss: 0.03229124546051025, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:16, Epoch: 139, Batch: 70, Training Loss: 0.019716819748282433, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:17, Epoch: 139, Batch: 80, Training Loss: 0.04558907598257065, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:17, Epoch: 139, Batch: 90, Training Loss: 0.042664884775877, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:18, Epoch: 139, Batch: 100, Training Loss: 0.016435786336660384, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:19, Epoch: 139, Batch: 110, Training Loss: 0.031605453416705134, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:19, Epoch: 139, Batch: 120, Training Loss: 0.063950464874506, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:20, Epoch: 139, Batch: 130, Training Loss: 0.039843175932765006, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:21, Epoch: 139, Batch: 140, Training Loss: 0.05560858100652695, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:22, Epoch: 139, Batch: 150, Training Loss: 0.04983324185013771, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:22, Epoch: 139, Batch: 160, Training Loss: 0.03164489045739174, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:23, Epoch: 139, Batch: 170, Training Loss: 0.032737027108669284, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:24, Epoch: 139, Batch: 180, Training Loss: 0.037849053740501404, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:25, Epoch: 139, Batch: 190, Training Loss: 0.024113904684782028, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:25, Epoch: 139, Batch: 200, Training Loss: 0.04568954221904278, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:26, Epoch: 139, Batch: 210, Training Loss: 0.03864542432129383, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:27, Epoch: 139, Batch: 220, Training Loss: 0.02654290460050106, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:27, Epoch: 139, Batch: 230, Training Loss: 0.040897751972079274, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:28, Epoch: 139, Batch: 240, Training Loss: 0.06020178236067295, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:29, Epoch: 139, Batch: 250, Training Loss: 0.03425763137638569, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:30, Epoch: 139, Batch: 260, Training Loss: 0.03736356869339943, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:30, Epoch: 139, Batch: 270, Training Loss: 0.0541242741048336, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:31, Epoch: 139, Batch: 280, Training Loss: 0.04813060127198696, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:32, Epoch: 139, Batch: 290, Training Loss: 0.03479772359132767, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:33, Epoch: 139, Batch: 300, Training Loss: 0.06778685972094536, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:33, Epoch: 139, Batch: 310, Training Loss: 0.031098712235689163, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:34, Epoch: 139, Batch: 320, Training Loss: 0.04780920557677746, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:35, Epoch: 139, Batch: 330, Training Loss: 0.0322303906083107, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:36, Epoch: 139, Batch: 340, Training Loss: 0.03440943881869316, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:36, Epoch: 139, Batch: 350, Training Loss: 0.03311274461448192, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:37, Epoch: 139, Batch: 360, Training Loss: 0.04491322562098503, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:38, Epoch: 139, Batch: 370, Training Loss: 0.05082729198038578, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:38, Epoch: 139, Batch: 380, Training Loss: 0.043972867727279666, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:39, Epoch: 139, Batch: 390, Training Loss: 0.03879298120737076, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:40, Epoch: 139, Batch: 400, Training Loss: 0.03547910042107105, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:41, Epoch: 139, Batch: 410, Training Loss: 0.0507571741938591, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:41, Epoch: 139, Batch: 420, Training Loss: 0.04459549374878406, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:42, Epoch: 139, Batch: 430, Training Loss: 0.022425203025341033, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:43, Epoch: 139, Batch: 440, Training Loss: 0.03958234190940857, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:44, Epoch: 139, Batch: 450, Training Loss: 0.05336340181529522, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:45, Epoch: 139, Batch: 460, Training Loss: 0.03794330209493637, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:46, Epoch: 139, Batch: 470, Training Loss: 0.03795390613377094, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:46, Epoch: 139, Batch: 480, Training Loss: 0.06405854262411595, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:47, Epoch: 139, Batch: 490, Training Loss: 0.050937329232692716, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:48, Epoch: 139, Batch: 500, Training Loss: 0.05158889293670654, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:49, Epoch: 139, Batch: 510, Training Loss: 0.044429341331124306, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:50, Epoch: 139, Batch: 520, Training Loss: 0.024871542304754257, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:50, Epoch: 139, Batch: 530, Training Loss: 0.02333904691040516, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:51, Epoch: 139, Batch: 540, Training Loss: 0.04689850583672524, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:52, Epoch: 139, Batch: 550, Training Loss: 0.056543105840682985, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:53, Epoch: 139, Batch: 560, Training Loss: 0.031832560524344446, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:54, Epoch: 139, Batch: 570, Training Loss: 0.026058246195316315, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:55, Epoch: 139, Batch: 580, Training Loss: 0.04810913167893886, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:55, Epoch: 139, Batch: 590, Training Loss: 0.0407629955559969, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:56, Epoch: 139, Batch: 600, Training Loss: 0.032606564834713934, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:57, Epoch: 139, Batch: 610, Training Loss: 0.05336117148399353, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:58, Epoch: 139, Batch: 620, Training Loss: 0.03753402605652809, LR: 0.00010000000000000003
Time, 2019-01-01T20:50:59, Epoch: 139, Batch: 630, Training Loss: 0.05162563808262348, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:00, Epoch: 139, Batch: 640, Training Loss: 0.036766966059803965, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:00, Epoch: 139, Batch: 650, Training Loss: 0.04027042761445045, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:01, Epoch: 139, Batch: 660, Training Loss: 0.041358372569084166, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:02, Epoch: 139, Batch: 670, Training Loss: 0.04566891044378281, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:03, Epoch: 139, Batch: 680, Training Loss: 0.046339431405067445, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:04, Epoch: 139, Batch: 690, Training Loss: 0.05045489817857742, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:04, Epoch: 139, Batch: 700, Training Loss: 0.042530430108308794, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:05, Epoch: 139, Batch: 710, Training Loss: 0.036087403818964955, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:06, Epoch: 139, Batch: 720, Training Loss: 0.026965139806270598, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:07, Epoch: 139, Batch: 730, Training Loss: 0.039479374140501025, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:08, Epoch: 139, Batch: 740, Training Loss: 0.042263961583375934, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:08, Epoch: 139, Batch: 750, Training Loss: 0.03823572508990765, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:09, Epoch: 139, Batch: 760, Training Loss: 0.04353039376437664, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:10, Epoch: 139, Batch: 770, Training Loss: 0.03272799924015999, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:11, Epoch: 139, Batch: 780, Training Loss: 0.050152624398469924, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:12, Epoch: 139, Batch: 790, Training Loss: 0.03353940956294536, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:12, Epoch: 139, Batch: 800, Training Loss: 0.04518023282289505, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:13, Epoch: 139, Batch: 810, Training Loss: 0.04000325351953506, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:14, Epoch: 139, Batch: 820, Training Loss: 0.027260232344269754, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:15, Epoch: 139, Batch: 830, Training Loss: 0.03108162507414818, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:15, Epoch: 139, Batch: 840, Training Loss: 0.04177965074777603, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:16, Epoch: 139, Batch: 850, Training Loss: 0.03610193207859993, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:17, Epoch: 139, Batch: 860, Training Loss: 0.04251049496233463, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:18, Epoch: 139, Batch: 870, Training Loss: 0.032311012595891954, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:18, Epoch: 139, Batch: 880, Training Loss: 0.05550435520708561, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:19, Epoch: 139, Batch: 890, Training Loss: 0.03950949497520924, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:20, Epoch: 139, Batch: 900, Training Loss: 0.03012603521347046, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:21, Epoch: 139, Batch: 910, Training Loss: 0.05350095890462399, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:21, Epoch: 139, Batch: 920, Training Loss: 0.046490850299596785, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:22, Epoch: 139, Batch: 930, Training Loss: 0.05216133370995522, LR: 0.00010000000000000003
Epoch: 139, Validation Top 1 acc: 98.89559173583984
Epoch: 139, Validation Top 5 acc: 99.99000549316406
Epoch: 139, Validation Set Loss: 0.04064861312508583
Start training epoch 140
Time, 2019-01-01T20:51:51, Epoch: 140, Batch: 10, Training Loss: 0.04301951341331005, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:52, Epoch: 140, Batch: 20, Training Loss: 0.029352469369769096, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:52, Epoch: 140, Batch: 30, Training Loss: 0.03699750788509846, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:53, Epoch: 140, Batch: 40, Training Loss: 0.05253257229924202, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:54, Epoch: 140, Batch: 50, Training Loss: 0.03699849732220173, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:55, Epoch: 140, Batch: 60, Training Loss: 0.04166582338511944, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:56, Epoch: 140, Batch: 70, Training Loss: 0.02282099351286888, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:56, Epoch: 140, Batch: 80, Training Loss: 0.03355054669082165, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:57, Epoch: 140, Batch: 90, Training Loss: 0.05028122141957283, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:58, Epoch: 140, Batch: 100, Training Loss: 0.04426537975668907, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:59, Epoch: 140, Batch: 110, Training Loss: 0.044709115475416186, LR: 0.00010000000000000003
Time, 2019-01-01T20:51:59, Epoch: 140, Batch: 120, Training Loss: 0.02680276185274124, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:00, Epoch: 140, Batch: 130, Training Loss: 0.036523929238319396, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:01, Epoch: 140, Batch: 140, Training Loss: 0.016208554431796075, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:02, Epoch: 140, Batch: 150, Training Loss: 0.03418267145752907, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:03, Epoch: 140, Batch: 160, Training Loss: 0.04886619448661804, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:03, Epoch: 140, Batch: 170, Training Loss: 0.04243622682988644, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:04, Epoch: 140, Batch: 180, Training Loss: 0.044012099876999854, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:05, Epoch: 140, Batch: 190, Training Loss: 0.04507150314748287, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:06, Epoch: 140, Batch: 200, Training Loss: 0.04858944788575172, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:06, Epoch: 140, Batch: 210, Training Loss: 0.03155059851706028, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:07, Epoch: 140, Batch: 220, Training Loss: 0.028278813883662223, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:08, Epoch: 140, Batch: 230, Training Loss: 0.03877089507877827, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:09, Epoch: 140, Batch: 240, Training Loss: 0.03261132165789604, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:09, Epoch: 140, Batch: 250, Training Loss: 0.04798330329358578, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:10, Epoch: 140, Batch: 260, Training Loss: 0.03952101618051529, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:11, Epoch: 140, Batch: 270, Training Loss: 0.054368652403354645, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:12, Epoch: 140, Batch: 280, Training Loss: 0.04529575370252133, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:12, Epoch: 140, Batch: 290, Training Loss: 0.046530742570757865, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:13, Epoch: 140, Batch: 300, Training Loss: 0.04626146592199802, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:14, Epoch: 140, Batch: 310, Training Loss: 0.04200480580329895, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:15, Epoch: 140, Batch: 320, Training Loss: 0.04001443982124329, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:15, Epoch: 140, Batch: 330, Training Loss: 0.027411555871367455, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:16, Epoch: 140, Batch: 340, Training Loss: 0.04582988321781158, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:17, Epoch: 140, Batch: 350, Training Loss: 0.0347972609102726, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:18, Epoch: 140, Batch: 360, Training Loss: 0.047073894739151, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:18, Epoch: 140, Batch: 370, Training Loss: 0.04182347804307938, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:19, Epoch: 140, Batch: 380, Training Loss: 0.026126405596733092, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:20, Epoch: 140, Batch: 390, Training Loss: 0.04317987374961376, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:21, Epoch: 140, Batch: 400, Training Loss: 0.03402624912559986, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:21, Epoch: 140, Batch: 410, Training Loss: 0.03640580102801323, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:22, Epoch: 140, Batch: 420, Training Loss: 0.03700426407158375, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:23, Epoch: 140, Batch: 430, Training Loss: 0.041292660683393476, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:24, Epoch: 140, Batch: 440, Training Loss: 0.06412019543349742, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:24, Epoch: 140, Batch: 450, Training Loss: 0.03383700139820576, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:25, Epoch: 140, Batch: 460, Training Loss: 0.0433107253164053, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:26, Epoch: 140, Batch: 470, Training Loss: 0.06440259143710136, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:27, Epoch: 140, Batch: 480, Training Loss: 0.0550884734839201, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:27, Epoch: 140, Batch: 490, Training Loss: 0.04631536081433296, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:28, Epoch: 140, Batch: 500, Training Loss: 0.0279606644064188, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:29, Epoch: 140, Batch: 510, Training Loss: 0.028328590095043182, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:29, Epoch: 140, Batch: 520, Training Loss: 0.027381192520260812, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:30, Epoch: 140, Batch: 530, Training Loss: 0.045223652571439746, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:31, Epoch: 140, Batch: 540, Training Loss: 0.029814460128545762, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:32, Epoch: 140, Batch: 550, Training Loss: 0.026757054775953294, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:33, Epoch: 140, Batch: 560, Training Loss: 0.049528482183814046, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:33, Epoch: 140, Batch: 570, Training Loss: 0.030826357752084733, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:34, Epoch: 140, Batch: 580, Training Loss: 0.032193663716316226, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:35, Epoch: 140, Batch: 590, Training Loss: 0.048265916854143144, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:36, Epoch: 140, Batch: 600, Training Loss: 0.03883636109530926, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:36, Epoch: 140, Batch: 610, Training Loss: 0.053296200931072235, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:37, Epoch: 140, Batch: 620, Training Loss: 0.03664854466915131, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:38, Epoch: 140, Batch: 630, Training Loss: 0.04270035140216351, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:39, Epoch: 140, Batch: 640, Training Loss: 0.042386813834309575, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:39, Epoch: 140, Batch: 650, Training Loss: 0.03060336224734783, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:40, Epoch: 140, Batch: 660, Training Loss: 0.03407169990241528, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:41, Epoch: 140, Batch: 670, Training Loss: 0.030692245066165923, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:42, Epoch: 140, Batch: 680, Training Loss: 0.036483298242092135, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:42, Epoch: 140, Batch: 690, Training Loss: 0.06101345866918564, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:43, Epoch: 140, Batch: 700, Training Loss: 0.03813418708741665, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:44, Epoch: 140, Batch: 710, Training Loss: 0.056137054413557055, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:45, Epoch: 140, Batch: 720, Training Loss: 0.03513030596077442, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:45, Epoch: 140, Batch: 730, Training Loss: 0.04202231876552105, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:46, Epoch: 140, Batch: 740, Training Loss: 0.03605479076504707, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:47, Epoch: 140, Batch: 750, Training Loss: 0.041810093075037004, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:48, Epoch: 140, Batch: 760, Training Loss: 0.04173004440963268, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:48, Epoch: 140, Batch: 770, Training Loss: 0.021441706269979478, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:49, Epoch: 140, Batch: 780, Training Loss: 0.04743796847760677, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:50, Epoch: 140, Batch: 790, Training Loss: 0.03920362330973148, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:51, Epoch: 140, Batch: 800, Training Loss: 0.05370241701602936, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:51, Epoch: 140, Batch: 810, Training Loss: 0.040809216350317, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:52, Epoch: 140, Batch: 820, Training Loss: 0.04571840949356556, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:53, Epoch: 140, Batch: 830, Training Loss: 0.05160610675811768, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:54, Epoch: 140, Batch: 840, Training Loss: 0.04720231033861637, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:54, Epoch: 140, Batch: 850, Training Loss: 0.0374259926378727, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:55, Epoch: 140, Batch: 860, Training Loss: 0.05504840388894081, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:56, Epoch: 140, Batch: 870, Training Loss: 0.04521725215017795, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:57, Epoch: 140, Batch: 880, Training Loss: 0.036808451265096666, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:57, Epoch: 140, Batch: 890, Training Loss: 0.035343340039253233, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:58, Epoch: 140, Batch: 900, Training Loss: 0.05687611699104309, LR: 0.00010000000000000003
Time, 2019-01-01T20:52:59, Epoch: 140, Batch: 910, Training Loss: 0.03792263120412827, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:00, Epoch: 140, Batch: 920, Training Loss: 0.03557468019425869, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:00, Epoch: 140, Batch: 930, Training Loss: 0.06164292544126511, LR: 0.00010000000000000003
Epoch: 140, Validation Top 1 acc: 98.89225769042969
Epoch: 140, Validation Top 5 acc: 99.99166870117188
Epoch: 140, Validation Set Loss: 0.04053999483585358
Start training epoch 141
Time, 2019-01-01T20:53:29, Epoch: 141, Batch: 10, Training Loss: 0.044029679894447324, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:30, Epoch: 141, Batch: 20, Training Loss: 0.031106335669755937, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:30, Epoch: 141, Batch: 30, Training Loss: 0.043597311899065974, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:31, Epoch: 141, Batch: 40, Training Loss: 0.02907920442521572, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:32, Epoch: 141, Batch: 50, Training Loss: 0.04532370939850807, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:33, Epoch: 141, Batch: 60, Training Loss: 0.04611346125602722, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:33, Epoch: 141, Batch: 70, Training Loss: 0.051335687190294264, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:34, Epoch: 141, Batch: 80, Training Loss: 0.02681204751133919, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:35, Epoch: 141, Batch: 90, Training Loss: 0.03238840363919735, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:36, Epoch: 141, Batch: 100, Training Loss: 0.025049495697021484, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:36, Epoch: 141, Batch: 110, Training Loss: 0.04099261537194252, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:37, Epoch: 141, Batch: 120, Training Loss: 0.03937423080205917, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:38, Epoch: 141, Batch: 130, Training Loss: 0.04884362407028675, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:39, Epoch: 141, Batch: 140, Training Loss: 0.05089445561170578, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:39, Epoch: 141, Batch: 150, Training Loss: 0.038816820830106735, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:40, Epoch: 141, Batch: 160, Training Loss: 0.02768934443593025, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:41, Epoch: 141, Batch: 170, Training Loss: 0.0441892247647047, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:41, Epoch: 141, Batch: 180, Training Loss: 0.04450085610151291, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:42, Epoch: 141, Batch: 190, Training Loss: 0.047475066781044004, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:43, Epoch: 141, Batch: 200, Training Loss: 0.027244556695222855, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:44, Epoch: 141, Batch: 210, Training Loss: 0.03927918784320354, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:45, Epoch: 141, Batch: 220, Training Loss: 0.042034029588103296, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:45, Epoch: 141, Batch: 230, Training Loss: 0.035887304693460464, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:46, Epoch: 141, Batch: 240, Training Loss: 0.030438104271888734, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:47, Epoch: 141, Batch: 250, Training Loss: 0.03372817784547806, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:47, Epoch: 141, Batch: 260, Training Loss: 0.03321843147277832, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:48, Epoch: 141, Batch: 270, Training Loss: 0.05573457628488541, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:49, Epoch: 141, Batch: 280, Training Loss: 0.038493097946047786, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:50, Epoch: 141, Batch: 290, Training Loss: 0.058145031332969666, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:50, Epoch: 141, Batch: 300, Training Loss: 0.06036471612751484, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:51, Epoch: 141, Batch: 310, Training Loss: 0.042853983119130135, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:52, Epoch: 141, Batch: 320, Training Loss: 0.05935416705906391, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:53, Epoch: 141, Batch: 330, Training Loss: 0.03766018077731133, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:53, Epoch: 141, Batch: 340, Training Loss: 0.039717552810907365, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:54, Epoch: 141, Batch: 350, Training Loss: 0.05298937633633614, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:55, Epoch: 141, Batch: 360, Training Loss: 0.03019890859723091, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:56, Epoch: 141, Batch: 370, Training Loss: 0.029839108139276503, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:56, Epoch: 141, Batch: 380, Training Loss: 0.05173725560307503, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:57, Epoch: 141, Batch: 390, Training Loss: 0.04118272140622139, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:58, Epoch: 141, Batch: 400, Training Loss: 0.034490692615509036, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:59, Epoch: 141, Batch: 410, Training Loss: 0.03373463898897171, LR: 0.00010000000000000003
Time, 2019-01-01T20:53:59, Epoch: 141, Batch: 420, Training Loss: 0.034279819950461386, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:00, Epoch: 141, Batch: 430, Training Loss: 0.044931136071681976, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:01, Epoch: 141, Batch: 440, Training Loss: 0.04567265436053276, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:02, Epoch: 141, Batch: 450, Training Loss: 0.04690126106142998, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:02, Epoch: 141, Batch: 460, Training Loss: 0.031547529995441435, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:03, Epoch: 141, Batch: 470, Training Loss: 0.028831329196691513, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:04, Epoch: 141, Batch: 480, Training Loss: 0.021555543690919877, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:05, Epoch: 141, Batch: 490, Training Loss: 0.03402225486934185, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:05, Epoch: 141, Batch: 500, Training Loss: 0.03690559342503548, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:06, Epoch: 141, Batch: 510, Training Loss: 0.026216086000204086, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:07, Epoch: 141, Batch: 520, Training Loss: 0.04440127238631249, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:08, Epoch: 141, Batch: 530, Training Loss: 0.04324183948338032, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:08, Epoch: 141, Batch: 540, Training Loss: 0.035122306644916536, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:09, Epoch: 141, Batch: 550, Training Loss: 0.07056776508688926, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:10, Epoch: 141, Batch: 560, Training Loss: 0.04237237386405468, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:11, Epoch: 141, Batch: 570, Training Loss: 0.056157952174544334, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:11, Epoch: 141, Batch: 580, Training Loss: 0.04047293663024902, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:12, Epoch: 141, Batch: 590, Training Loss: 0.024898718670010568, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:13, Epoch: 141, Batch: 600, Training Loss: 0.033934491127729415, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:14, Epoch: 141, Batch: 610, Training Loss: 0.040751968324184415, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:14, Epoch: 141, Batch: 620, Training Loss: 0.043890655413269995, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:15, Epoch: 141, Batch: 630, Training Loss: 0.055404675751924516, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:16, Epoch: 141, Batch: 640, Training Loss: 0.03277380838990211, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:16, Epoch: 141, Batch: 650, Training Loss: 0.03372248597443104, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:17, Epoch: 141, Batch: 660, Training Loss: 0.028789452090859414, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:18, Epoch: 141, Batch: 670, Training Loss: 0.051298118010163304, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:19, Epoch: 141, Batch: 680, Training Loss: 0.041932118311524394, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:19, Epoch: 141, Batch: 690, Training Loss: 0.054916759952902794, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:20, Epoch: 141, Batch: 700, Training Loss: 0.05022920742630958, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:21, Epoch: 141, Batch: 710, Training Loss: 0.033703969419002534, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:22, Epoch: 141, Batch: 720, Training Loss: 0.03549630492925644, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:22, Epoch: 141, Batch: 730, Training Loss: 0.03999673128128052, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:23, Epoch: 141, Batch: 740, Training Loss: 0.035057536140084264, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:24, Epoch: 141, Batch: 750, Training Loss: 0.05634234994649887, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:25, Epoch: 141, Batch: 760, Training Loss: 0.02379953972995281, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:25, Epoch: 141, Batch: 770, Training Loss: 0.040755908191204074, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:26, Epoch: 141, Batch: 780, Training Loss: 0.05576428696513176, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:27, Epoch: 141, Batch: 790, Training Loss: 0.03725911304354668, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:28, Epoch: 141, Batch: 800, Training Loss: 0.04725114069879055, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:28, Epoch: 141, Batch: 810, Training Loss: 0.04311121441423893, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:29, Epoch: 141, Batch: 820, Training Loss: 0.04194861091673374, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:30, Epoch: 141, Batch: 830, Training Loss: 0.039760253578424457, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:31, Epoch: 141, Batch: 840, Training Loss: 0.035343341156840326, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:31, Epoch: 141, Batch: 850, Training Loss: 0.032099010050296785, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:32, Epoch: 141, Batch: 860, Training Loss: 0.03589679971337319, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:33, Epoch: 141, Batch: 870, Training Loss: 0.035848267003893854, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:33, Epoch: 141, Batch: 880, Training Loss: 0.042795171961188316, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:34, Epoch: 141, Batch: 890, Training Loss: 0.05894015356898308, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:35, Epoch: 141, Batch: 900, Training Loss: 0.05353899151086807, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:36, Epoch: 141, Batch: 910, Training Loss: 0.053581347316503526, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:36, Epoch: 141, Batch: 920, Training Loss: 0.03133184164762497, LR: 0.00010000000000000003
Time, 2019-01-01T20:54:37, Epoch: 141, Batch: 930, Training Loss: 0.0363165158778429, LR: 0.00010000000000000003
Epoch: 141, Validation Top 1 acc: 98.91224670410156
Epoch: 141, Validation Top 5 acc: 99.99166870117188
Epoch: 141, Validation Set Loss: 0.04048231616616249
Start training epoch 142
Time, 2019-01-01T20:55:06, Epoch: 142, Batch: 10, Training Loss: 0.042434244975447656, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:06, Epoch: 142, Batch: 20, Training Loss: 0.04836295507848263, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:07, Epoch: 142, Batch: 30, Training Loss: 0.031816592812538146, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:08, Epoch: 142, Batch: 40, Training Loss: 0.03249149136245251, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:09, Epoch: 142, Batch: 50, Training Loss: 0.048800742253661156, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:09, Epoch: 142, Batch: 60, Training Loss: 0.028644429892301558, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:10, Epoch: 142, Batch: 70, Training Loss: 0.035803715139627455, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:11, Epoch: 142, Batch: 80, Training Loss: 0.042280906811356544, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:12, Epoch: 142, Batch: 90, Training Loss: 0.04281769767403602, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:12, Epoch: 142, Batch: 100, Training Loss: 0.04590370953083038, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:13, Epoch: 142, Batch: 110, Training Loss: 0.040382052212953566, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:14, Epoch: 142, Batch: 120, Training Loss: 0.034743427857756616, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:14, Epoch: 142, Batch: 130, Training Loss: 0.02731667459011078, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:15, Epoch: 142, Batch: 140, Training Loss: 0.04554073773324489, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:16, Epoch: 142, Batch: 150, Training Loss: 0.03655291274189949, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:17, Epoch: 142, Batch: 160, Training Loss: 0.02722723111510277, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:17, Epoch: 142, Batch: 170, Training Loss: 0.03896989747881889, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:18, Epoch: 142, Batch: 180, Training Loss: 0.023516206443309783, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:19, Epoch: 142, Batch: 190, Training Loss: 0.03185581266880035, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:20, Epoch: 142, Batch: 200, Training Loss: 0.03866398632526398, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:20, Epoch: 142, Batch: 210, Training Loss: 0.0301762156188488, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:21, Epoch: 142, Batch: 220, Training Loss: 0.050779180228710176, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:22, Epoch: 142, Batch: 230, Training Loss: 0.028201397880911826, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:23, Epoch: 142, Batch: 240, Training Loss: 0.03573985919356346, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:23, Epoch: 142, Batch: 250, Training Loss: 0.04287874810397625, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:24, Epoch: 142, Batch: 260, Training Loss: 0.04582593776285648, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:25, Epoch: 142, Batch: 270, Training Loss: 0.039249882102012634, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:26, Epoch: 142, Batch: 280, Training Loss: 0.05134837627410889, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:26, Epoch: 142, Batch: 290, Training Loss: 0.03481800220906735, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:27, Epoch: 142, Batch: 300, Training Loss: 0.03737781904637814, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:28, Epoch: 142, Batch: 310, Training Loss: 0.050165364518761635, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:29, Epoch: 142, Batch: 320, Training Loss: 0.027169565111398696, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:29, Epoch: 142, Batch: 330, Training Loss: 0.0396507415920496, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:30, Epoch: 142, Batch: 340, Training Loss: 0.04807519093155861, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:31, Epoch: 142, Batch: 350, Training Loss: 0.049409717321395874, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:32, Epoch: 142, Batch: 360, Training Loss: 0.04805486984550953, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:32, Epoch: 142, Batch: 370, Training Loss: 0.04424783661961555, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:33, Epoch: 142, Batch: 380, Training Loss: 0.06125522367656231, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:34, Epoch: 142, Batch: 390, Training Loss: 0.032777252793312076, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:35, Epoch: 142, Batch: 400, Training Loss: 0.038748060911893846, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:35, Epoch: 142, Batch: 410, Training Loss: 0.028472431376576422, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:36, Epoch: 142, Batch: 420, Training Loss: 0.047803158313035964, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:37, Epoch: 142, Batch: 430, Training Loss: 0.02822994999587536, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:38, Epoch: 142, Batch: 440, Training Loss: 0.04045205824077129, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:38, Epoch: 142, Batch: 450, Training Loss: 0.025786832347512244, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:39, Epoch: 142, Batch: 460, Training Loss: 0.07377246022224426, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:40, Epoch: 142, Batch: 470, Training Loss: 0.04798877388238907, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:41, Epoch: 142, Batch: 480, Training Loss: 0.03877641819417477, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:41, Epoch: 142, Batch: 490, Training Loss: 0.04015357047319412, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:42, Epoch: 142, Batch: 500, Training Loss: 0.049295077472925185, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:43, Epoch: 142, Batch: 510, Training Loss: 0.06025892496109009, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:44, Epoch: 142, Batch: 520, Training Loss: 0.035135607793927194, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:44, Epoch: 142, Batch: 530, Training Loss: 0.04061940088868141, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:45, Epoch: 142, Batch: 540, Training Loss: 0.03819004371762276, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:46, Epoch: 142, Batch: 550, Training Loss: 0.028763900324702264, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:47, Epoch: 142, Batch: 560, Training Loss: 0.03198004700243473, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:47, Epoch: 142, Batch: 570, Training Loss: 0.04701775684952736, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:48, Epoch: 142, Batch: 580, Training Loss: 0.04460692219436169, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:49, Epoch: 142, Batch: 590, Training Loss: 0.042594951763749125, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:50, Epoch: 142, Batch: 600, Training Loss: 0.041316264495253566, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:50, Epoch: 142, Batch: 610, Training Loss: 0.04824364520609379, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:51, Epoch: 142, Batch: 620, Training Loss: 0.048128490895032884, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:52, Epoch: 142, Batch: 630, Training Loss: 0.04124172367155552, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:53, Epoch: 142, Batch: 640, Training Loss: 0.0510268434882164, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:53, Epoch: 142, Batch: 650, Training Loss: 0.041056247800588606, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:54, Epoch: 142, Batch: 660, Training Loss: 0.03504082299768925, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:55, Epoch: 142, Batch: 670, Training Loss: 0.04040520153939724, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:56, Epoch: 142, Batch: 680, Training Loss: 0.03875933811068535, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:56, Epoch: 142, Batch: 690, Training Loss: 0.03584799766540527, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:57, Epoch: 142, Batch: 700, Training Loss: 0.05332829877734184, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:58, Epoch: 142, Batch: 710, Training Loss: 0.037653419002890584, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:59, Epoch: 142, Batch: 720, Training Loss: 0.0406009666621685, LR: 0.00010000000000000003
Time, 2019-01-01T20:55:59, Epoch: 142, Batch: 730, Training Loss: 0.040756408125162125, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:00, Epoch: 142, Batch: 740, Training Loss: 0.04164510667324066, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:01, Epoch: 142, Batch: 750, Training Loss: 0.03817936182022095, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:02, Epoch: 142, Batch: 760, Training Loss: 0.04295626692473888, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:03, Epoch: 142, Batch: 770, Training Loss: 0.039350691437721255, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:03, Epoch: 142, Batch: 780, Training Loss: 0.0424736425280571, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:04, Epoch: 142, Batch: 790, Training Loss: 0.026559518277645112, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:05, Epoch: 142, Batch: 800, Training Loss: 0.030619623139500618, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:06, Epoch: 142, Batch: 810, Training Loss: 0.032754183560609815, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:06, Epoch: 142, Batch: 820, Training Loss: 0.02234278991818428, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:07, Epoch: 142, Batch: 830, Training Loss: 0.031882192566990854, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:08, Epoch: 142, Batch: 840, Training Loss: 0.039272763580083844, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:09, Epoch: 142, Batch: 850, Training Loss: 0.06814004108309746, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:09, Epoch: 142, Batch: 860, Training Loss: 0.04887198321521282, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:10, Epoch: 142, Batch: 870, Training Loss: 0.02978156618773937, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:11, Epoch: 142, Batch: 880, Training Loss: 0.043147557973861696, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:11, Epoch: 142, Batch: 890, Training Loss: 0.045352719724178314, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:12, Epoch: 142, Batch: 900, Training Loss: 0.03986332006752491, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:13, Epoch: 142, Batch: 910, Training Loss: 0.03916727416217327, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:14, Epoch: 142, Batch: 920, Training Loss: 0.04862413629889488, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:14, Epoch: 142, Batch: 930, Training Loss: 0.05191471911966801, LR: 0.00010000000000000003
Epoch: 142, Validation Top 1 acc: 98.89225769042969
Epoch: 142, Validation Top 5 acc: 99.99333953857422
Epoch: 142, Validation Set Loss: 0.04051535204052925
Start training epoch 143
Time, 2019-01-01T20:56:43, Epoch: 143, Batch: 10, Training Loss: 0.04194833487272263, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:44, Epoch: 143, Batch: 20, Training Loss: 0.02048783041536808, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:45, Epoch: 143, Batch: 30, Training Loss: 0.04392705298960209, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:46, Epoch: 143, Batch: 40, Training Loss: 0.04544556401669979, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:47, Epoch: 143, Batch: 50, Training Loss: 0.042199309915304184, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:47, Epoch: 143, Batch: 60, Training Loss: 0.03442283384501934, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:48, Epoch: 143, Batch: 70, Training Loss: 0.0425936333835125, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:49, Epoch: 143, Batch: 80, Training Loss: 0.030533209815621375, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:50, Epoch: 143, Batch: 90, Training Loss: 0.046170630306005475, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:50, Epoch: 143, Batch: 100, Training Loss: 0.022767090052366257, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:51, Epoch: 143, Batch: 110, Training Loss: 0.06616338416934013, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:52, Epoch: 143, Batch: 120, Training Loss: 0.04517203494906426, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:53, Epoch: 143, Batch: 130, Training Loss: 0.037944409623742105, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:53, Epoch: 143, Batch: 140, Training Loss: 0.04847152903676033, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:54, Epoch: 143, Batch: 150, Training Loss: 0.04390414170920849, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:55, Epoch: 143, Batch: 160, Training Loss: 0.050038063898682594, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:56, Epoch: 143, Batch: 170, Training Loss: 0.0458817970007658, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:56, Epoch: 143, Batch: 180, Training Loss: 0.03945907577872276, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:57, Epoch: 143, Batch: 190, Training Loss: 0.037316172197461125, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:58, Epoch: 143, Batch: 200, Training Loss: 0.04666351675987244, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:59, Epoch: 143, Batch: 210, Training Loss: 0.039897576346993445, LR: 0.00010000000000000003
Time, 2019-01-01T20:56:59, Epoch: 143, Batch: 220, Training Loss: 0.04174556322395802, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:00, Epoch: 143, Batch: 230, Training Loss: 0.0384761244058609, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:01, Epoch: 143, Batch: 240, Training Loss: 0.03012470304965973, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:02, Epoch: 143, Batch: 250, Training Loss: 0.03752823695540428, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:03, Epoch: 143, Batch: 260, Training Loss: 0.04294724352657795, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:03, Epoch: 143, Batch: 270, Training Loss: 0.03719056360423565, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:04, Epoch: 143, Batch: 280, Training Loss: 0.06754287146031857, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:05, Epoch: 143, Batch: 290, Training Loss: 0.02752908430993557, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:06, Epoch: 143, Batch: 300, Training Loss: 0.030304670333862305, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:06, Epoch: 143, Batch: 310, Training Loss: 0.03444049432873726, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:07, Epoch: 143, Batch: 320, Training Loss: 0.07170960530638695, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:08, Epoch: 143, Batch: 330, Training Loss: 0.05035804733633995, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:09, Epoch: 143, Batch: 340, Training Loss: 0.03612631969153881, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:10, Epoch: 143, Batch: 350, Training Loss: 0.047478648275136946, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:10, Epoch: 143, Batch: 360, Training Loss: 0.03516234010457993, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:11, Epoch: 143, Batch: 370, Training Loss: 0.034580066427588464, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:12, Epoch: 143, Batch: 380, Training Loss: 0.04303229972720146, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:13, Epoch: 143, Batch: 390, Training Loss: 0.020867840200662614, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:13, Epoch: 143, Batch: 400, Training Loss: 0.029796318709850313, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:14, Epoch: 143, Batch: 410, Training Loss: 0.03807398527860641, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:15, Epoch: 143, Batch: 420, Training Loss: 0.03066379688680172, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:16, Epoch: 143, Batch: 430, Training Loss: 0.03655707724392414, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:16, Epoch: 143, Batch: 440, Training Loss: 0.04556785337626934, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:17, Epoch: 143, Batch: 450, Training Loss: 0.04301836788654327, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:18, Epoch: 143, Batch: 460, Training Loss: 0.04632772989571095, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:19, Epoch: 143, Batch: 470, Training Loss: 0.03892171159386635, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:19, Epoch: 143, Batch: 480, Training Loss: 0.04118125103414059, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:20, Epoch: 143, Batch: 490, Training Loss: 0.07105850987136364, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:21, Epoch: 143, Batch: 500, Training Loss: 0.05578372851014137, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:22, Epoch: 143, Batch: 510, Training Loss: 0.04257153496146202, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:22, Epoch: 143, Batch: 520, Training Loss: 0.08094130605459213, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:23, Epoch: 143, Batch: 530, Training Loss: 0.02348458841443062, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:24, Epoch: 143, Batch: 540, Training Loss: 0.041918667033314705, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:25, Epoch: 143, Batch: 550, Training Loss: 0.03840302564203739, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:26, Epoch: 143, Batch: 560, Training Loss: 0.03767642378807068, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:26, Epoch: 143, Batch: 570, Training Loss: 0.026219800859689713, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:27, Epoch: 143, Batch: 580, Training Loss: 0.03206212520599365, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:28, Epoch: 143, Batch: 590, Training Loss: 0.05948484167456627, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:29, Epoch: 143, Batch: 600, Training Loss: 0.0401457317173481, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:29, Epoch: 143, Batch: 610, Training Loss: 0.052489259839057924, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:30, Epoch: 143, Batch: 620, Training Loss: 0.04455152153968811, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:31, Epoch: 143, Batch: 630, Training Loss: 0.03819999322295189, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:32, Epoch: 143, Batch: 640, Training Loss: 0.04185943268239498, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:33, Epoch: 143, Batch: 650, Training Loss: 0.052123107761144635, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:33, Epoch: 143, Batch: 660, Training Loss: 0.03191474005579949, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:34, Epoch: 143, Batch: 670, Training Loss: 0.046595876663923265, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:35, Epoch: 143, Batch: 680, Training Loss: 0.030827462673187256, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:36, Epoch: 143, Batch: 690, Training Loss: 0.03581194542348385, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:36, Epoch: 143, Batch: 700, Training Loss: 0.029582792520523073, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:37, Epoch: 143, Batch: 710, Training Loss: 0.041618794202804565, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:38, Epoch: 143, Batch: 720, Training Loss: 0.051343079656362534, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:39, Epoch: 143, Batch: 730, Training Loss: 0.02050159201025963, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:40, Epoch: 143, Batch: 740, Training Loss: 0.03142015263438225, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:40, Epoch: 143, Batch: 750, Training Loss: 0.03720078468322754, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:41, Epoch: 143, Batch: 760, Training Loss: 0.05583027452230453, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:42, Epoch: 143, Batch: 770, Training Loss: 0.03542834557592869, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:43, Epoch: 143, Batch: 780, Training Loss: 0.03780443891882897, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:43, Epoch: 143, Batch: 790, Training Loss: 0.03097690865397453, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:44, Epoch: 143, Batch: 800, Training Loss: 0.037911154329776764, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:45, Epoch: 143, Batch: 810, Training Loss: 0.033834191784262654, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:46, Epoch: 143, Batch: 820, Training Loss: 0.027988479286432267, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:46, Epoch: 143, Batch: 830, Training Loss: 0.028697597980499267, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:47, Epoch: 143, Batch: 840, Training Loss: 0.04401920437812805, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:48, Epoch: 143, Batch: 850, Training Loss: 0.040020523220300676, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:49, Epoch: 143, Batch: 860, Training Loss: 0.03346794284880161, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:49, Epoch: 143, Batch: 870, Training Loss: 0.03514362759888172, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:50, Epoch: 143, Batch: 880, Training Loss: 0.04151701480150223, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:51, Epoch: 143, Batch: 890, Training Loss: 0.042823601141572, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:52, Epoch: 143, Batch: 900, Training Loss: 0.03626075871288777, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:53, Epoch: 143, Batch: 910, Training Loss: 0.03998813033103943, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:53, Epoch: 143, Batch: 920, Training Loss: 0.03491982221603394, LR: 0.00010000000000000003
Time, 2019-01-01T20:57:54, Epoch: 143, Batch: 930, Training Loss: 0.047604134678840636, LR: 0.00010000000000000003
Epoch: 143, Validation Top 1 acc: 98.91058349609375
Epoch: 143, Validation Top 5 acc: 99.99166870117188
Epoch: 143, Validation Set Loss: 0.04045933112502098
Start training epoch 144
Time, 2019-01-01T20:58:23, Epoch: 144, Batch: 10, Training Loss: 0.052406226843595506, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:23, Epoch: 144, Batch: 20, Training Loss: 0.035168498009443286, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:24, Epoch: 144, Batch: 30, Training Loss: 0.046019406616687776, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:25, Epoch: 144, Batch: 40, Training Loss: 0.047130252420902255, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:26, Epoch: 144, Batch: 50, Training Loss: 0.053218674659729, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:26, Epoch: 144, Batch: 60, Training Loss: 0.037430234253406525, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:27, Epoch: 144, Batch: 70, Training Loss: 0.0294079951941967, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:28, Epoch: 144, Batch: 80, Training Loss: 0.03333359435200691, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:29, Epoch: 144, Batch: 90, Training Loss: 0.026905936747789384, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:29, Epoch: 144, Batch: 100, Training Loss: 0.045331158488988874, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:30, Epoch: 144, Batch: 110, Training Loss: 0.03567622676491737, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:31, Epoch: 144, Batch: 120, Training Loss: 0.04374429881572724, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:32, Epoch: 144, Batch: 130, Training Loss: 0.03059265688061714, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:32, Epoch: 144, Batch: 140, Training Loss: 0.034152160957455636, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:33, Epoch: 144, Batch: 150, Training Loss: 0.026673363894224165, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:34, Epoch: 144, Batch: 160, Training Loss: 0.04750928021967411, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:35, Epoch: 144, Batch: 170, Training Loss: 0.04694026932120323, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:36, Epoch: 144, Batch: 180, Training Loss: 0.03605043739080429, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:36, Epoch: 144, Batch: 190, Training Loss: 0.040966615453362464, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:37, Epoch: 144, Batch: 200, Training Loss: 0.03646254427731037, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:38, Epoch: 144, Batch: 210, Training Loss: 0.0219778623431921, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:39, Epoch: 144, Batch: 220, Training Loss: 0.040936703234910964, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:39, Epoch: 144, Batch: 230, Training Loss: 0.038449721038341524, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:40, Epoch: 144, Batch: 240, Training Loss: 0.026354246586561204, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:41, Epoch: 144, Batch: 250, Training Loss: 0.04403197690844536, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:42, Epoch: 144, Batch: 260, Training Loss: 0.03740867339074612, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:42, Epoch: 144, Batch: 270, Training Loss: 0.037950503081083296, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:43, Epoch: 144, Batch: 280, Training Loss: 0.05284711681306362, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:44, Epoch: 144, Batch: 290, Training Loss: 0.050686968863010405, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:45, Epoch: 144, Batch: 300, Training Loss: 0.030125474184751512, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:45, Epoch: 144, Batch: 310, Training Loss: 0.0421009860932827, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:46, Epoch: 144, Batch: 320, Training Loss: 0.03646169491112232, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:47, Epoch: 144, Batch: 330, Training Loss: 0.028855353966355322, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:48, Epoch: 144, Batch: 340, Training Loss: 0.050787001848220825, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:48, Epoch: 144, Batch: 350, Training Loss: 0.03989851139485836, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:49, Epoch: 144, Batch: 360, Training Loss: 0.04434652589261532, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:50, Epoch: 144, Batch: 370, Training Loss: 0.07171345762908458, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:51, Epoch: 144, Batch: 380, Training Loss: 0.03938533738255501, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:52, Epoch: 144, Batch: 390, Training Loss: 0.037347829341888426, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:52, Epoch: 144, Batch: 400, Training Loss: 0.03563382104039192, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:53, Epoch: 144, Batch: 410, Training Loss: 0.03713568486273289, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:54, Epoch: 144, Batch: 420, Training Loss: 0.06523976214230061, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:55, Epoch: 144, Batch: 430, Training Loss: 0.042498962953686714, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:55, Epoch: 144, Batch: 440, Training Loss: 0.043873350322246554, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:56, Epoch: 144, Batch: 450, Training Loss: 0.04379294626414776, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:57, Epoch: 144, Batch: 460, Training Loss: 0.036976663023233415, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:58, Epoch: 144, Batch: 470, Training Loss: 0.03091902881860733, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:58, Epoch: 144, Batch: 480, Training Loss: 0.03638763576745987, LR: 0.00010000000000000003
Time, 2019-01-01T20:58:59, Epoch: 144, Batch: 490, Training Loss: 0.03431613296270371, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:00, Epoch: 144, Batch: 500, Training Loss: 0.03656095564365387, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:01, Epoch: 144, Batch: 510, Training Loss: 0.042784498259425166, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:01, Epoch: 144, Batch: 520, Training Loss: 0.03377300463616848, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:02, Epoch: 144, Batch: 530, Training Loss: 0.064836785197258, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:03, Epoch: 144, Batch: 540, Training Loss: 0.05263288393616676, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:04, Epoch: 144, Batch: 550, Training Loss: 0.05028397440910339, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:04, Epoch: 144, Batch: 560, Training Loss: 0.026783686131238937, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:05, Epoch: 144, Batch: 570, Training Loss: 0.043444151431322096, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:06, Epoch: 144, Batch: 580, Training Loss: 0.04731613509356976, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:07, Epoch: 144, Batch: 590, Training Loss: 0.03957965970039368, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:07, Epoch: 144, Batch: 600, Training Loss: 0.04780204780399799, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:08, Epoch: 144, Batch: 610, Training Loss: 0.0361827127635479, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:09, Epoch: 144, Batch: 620, Training Loss: 0.029868915677070618, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:10, Epoch: 144, Batch: 630, Training Loss: 0.032882564887404445, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:10, Epoch: 144, Batch: 640, Training Loss: 0.0464908666908741, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:11, Epoch: 144, Batch: 650, Training Loss: 0.05073951706290245, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:12, Epoch: 144, Batch: 660, Training Loss: 0.03814074471592903, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:13, Epoch: 144, Batch: 670, Training Loss: 0.04231593869626522, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:13, Epoch: 144, Batch: 680, Training Loss: 0.03192854039371014, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:14, Epoch: 144, Batch: 690, Training Loss: 0.06424604021012784, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:15, Epoch: 144, Batch: 700, Training Loss: 0.04037972465157509, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:16, Epoch: 144, Batch: 710, Training Loss: 0.04597909599542618, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:16, Epoch: 144, Batch: 720, Training Loss: 0.03320927396416664, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:17, Epoch: 144, Batch: 730, Training Loss: 0.04331969730556011, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:18, Epoch: 144, Batch: 740, Training Loss: 0.034971856698393824, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:19, Epoch: 144, Batch: 750, Training Loss: 0.026833394542336464, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:19, Epoch: 144, Batch: 760, Training Loss: 0.04125705882906914, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:20, Epoch: 144, Batch: 770, Training Loss: 0.03438861966133118, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:21, Epoch: 144, Batch: 780, Training Loss: 0.058700092136859894, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:22, Epoch: 144, Batch: 790, Training Loss: 0.04716273918747902, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:22, Epoch: 144, Batch: 800, Training Loss: 0.02631172500550747, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:23, Epoch: 144, Batch: 810, Training Loss: 0.031606228277087214, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:24, Epoch: 144, Batch: 820, Training Loss: 0.037359775975346565, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:25, Epoch: 144, Batch: 830, Training Loss: 0.04244249425828457, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:25, Epoch: 144, Batch: 840, Training Loss: 0.05290899612009525, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:26, Epoch: 144, Batch: 850, Training Loss: 0.03929203227162361, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:27, Epoch: 144, Batch: 860, Training Loss: 0.03485498838126659, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:28, Epoch: 144, Batch: 870, Training Loss: 0.044847526028752326, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:28, Epoch: 144, Batch: 880, Training Loss: 0.03951432704925537, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:29, Epoch: 144, Batch: 890, Training Loss: 0.03187028057873249, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:30, Epoch: 144, Batch: 900, Training Loss: 0.04522216990590096, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:31, Epoch: 144, Batch: 910, Training Loss: 0.046744295954704286, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:31, Epoch: 144, Batch: 920, Training Loss: 0.032206039130687716, LR: 0.00010000000000000003
Time, 2019-01-01T20:59:32, Epoch: 144, Batch: 930, Training Loss: 0.04742584526538849, LR: 0.00010000000000000003
Epoch: 144, Validation Top 1 acc: 98.91390991210938
Epoch: 144, Validation Top 5 acc: 99.99166870117188
Epoch: 144, Validation Set Loss: 0.04046249017119408
Start training epoch 145
Time, 2019-01-01T21:00:04, Epoch: 145, Batch: 10, Training Loss: 0.04649510979652405, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:05, Epoch: 145, Batch: 20, Training Loss: 0.025757133215665817, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:05, Epoch: 145, Batch: 30, Training Loss: 0.027960223704576494, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:06, Epoch: 145, Batch: 40, Training Loss: 0.03879615589976311, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:07, Epoch: 145, Batch: 50, Training Loss: 0.03395899161696434, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:08, Epoch: 145, Batch: 60, Training Loss: 0.04458317011594772, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:08, Epoch: 145, Batch: 70, Training Loss: 0.046397728472948076, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:09, Epoch: 145, Batch: 80, Training Loss: 0.06558045111596585, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:10, Epoch: 145, Batch: 90, Training Loss: 0.044388101994991304, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:11, Epoch: 145, Batch: 100, Training Loss: 0.03311215341091156, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:11, Epoch: 145, Batch: 110, Training Loss: 0.03520728647708893, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:12, Epoch: 145, Batch: 120, Training Loss: 0.028558358177542688, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:13, Epoch: 145, Batch: 130, Training Loss: 0.030416970327496528, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:14, Epoch: 145, Batch: 140, Training Loss: 0.027454138174653052, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:14, Epoch: 145, Batch: 150, Training Loss: 0.03865837045013905, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:15, Epoch: 145, Batch: 160, Training Loss: 0.033817365393042566, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:16, Epoch: 145, Batch: 170, Training Loss: 0.03253358900547028, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:17, Epoch: 145, Batch: 180, Training Loss: 0.04654114693403244, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:17, Epoch: 145, Batch: 190, Training Loss: 0.04594919979572296, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:18, Epoch: 145, Batch: 200, Training Loss: 0.03980458416044712, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:19, Epoch: 145, Batch: 210, Training Loss: 0.04078680500388145, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:19, Epoch: 145, Batch: 220, Training Loss: 0.03579101301729679, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:20, Epoch: 145, Batch: 230, Training Loss: 0.03629773259162903, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:21, Epoch: 145, Batch: 240, Training Loss: 0.049831993877887726, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:22, Epoch: 145, Batch: 250, Training Loss: 0.03549312502145767, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:22, Epoch: 145, Batch: 260, Training Loss: 0.05544500984251499, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:23, Epoch: 145, Batch: 270, Training Loss: 0.06527533084154129, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:24, Epoch: 145, Batch: 280, Training Loss: 0.04611671455204487, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:25, Epoch: 145, Batch: 290, Training Loss: 0.025129224359989166, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:25, Epoch: 145, Batch: 300, Training Loss: 0.04280241206288338, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:26, Epoch: 145, Batch: 310, Training Loss: 0.03887079283595085, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:27, Epoch: 145, Batch: 320, Training Loss: 0.03522282168269157, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:28, Epoch: 145, Batch: 330, Training Loss: 0.047548094391822816, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:28, Epoch: 145, Batch: 340, Training Loss: 0.03714760653674602, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:29, Epoch: 145, Batch: 350, Training Loss: 0.02539304494857788, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:30, Epoch: 145, Batch: 360, Training Loss: 0.028296857699751855, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:30, Epoch: 145, Batch: 370, Training Loss: 0.06340850368142129, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:31, Epoch: 145, Batch: 380, Training Loss: 0.057695282995700835, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:32, Epoch: 145, Batch: 390, Training Loss: 0.0578071229159832, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:33, Epoch: 145, Batch: 400, Training Loss: 0.022421585395932198, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:33, Epoch: 145, Batch: 410, Training Loss: 0.05425243638455868, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:34, Epoch: 145, Batch: 420, Training Loss: 0.05185827277600765, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:35, Epoch: 145, Batch: 430, Training Loss: 0.07070045843720436, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:36, Epoch: 145, Batch: 440, Training Loss: 0.0284355778247118, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:36, Epoch: 145, Batch: 450, Training Loss: 0.057154403254389766, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:37, Epoch: 145, Batch: 460, Training Loss: 0.0502613790333271, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:38, Epoch: 145, Batch: 470, Training Loss: 0.03105596750974655, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:38, Epoch: 145, Batch: 480, Training Loss: 0.036544063687324525, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:39, Epoch: 145, Batch: 490, Training Loss: 0.03769470527768135, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:40, Epoch: 145, Batch: 500, Training Loss: 0.04630101881921291, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:41, Epoch: 145, Batch: 510, Training Loss: 0.033163584768772125, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:41, Epoch: 145, Batch: 520, Training Loss: 0.04151489436626434, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:42, Epoch: 145, Batch: 530, Training Loss: 0.03196263201534748, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:43, Epoch: 145, Batch: 540, Training Loss: 0.05502568855881691, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:44, Epoch: 145, Batch: 550, Training Loss: 0.03368813209235668, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:45, Epoch: 145, Batch: 560, Training Loss: 0.02924930490553379, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:45, Epoch: 145, Batch: 570, Training Loss: 0.05307219885289669, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:46, Epoch: 145, Batch: 580, Training Loss: 0.05461801774799824, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:47, Epoch: 145, Batch: 590, Training Loss: 0.026260916888713837, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:48, Epoch: 145, Batch: 600, Training Loss: 0.03499847128987312, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:49, Epoch: 145, Batch: 610, Training Loss: 0.04217637926340103, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:49, Epoch: 145, Batch: 620, Training Loss: 0.07426818683743477, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:50, Epoch: 145, Batch: 630, Training Loss: 0.033648164570331575, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:51, Epoch: 145, Batch: 640, Training Loss: 0.034028716012835504, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:52, Epoch: 145, Batch: 650, Training Loss: 0.0381335400044918, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:53, Epoch: 145, Batch: 660, Training Loss: 0.04022139608860016, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:53, Epoch: 145, Batch: 670, Training Loss: 0.03133039511740208, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:54, Epoch: 145, Batch: 680, Training Loss: 0.033790986984968185, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:55, Epoch: 145, Batch: 690, Training Loss: 0.03160117380321026, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:56, Epoch: 145, Batch: 700, Training Loss: 0.0405709195882082, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:56, Epoch: 145, Batch: 710, Training Loss: 0.058730177208781244, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:57, Epoch: 145, Batch: 720, Training Loss: 0.030907489731907843, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:58, Epoch: 145, Batch: 730, Training Loss: 0.025739739835262298, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:59, Epoch: 145, Batch: 740, Training Loss: 0.05980779379606247, LR: 0.00010000000000000003
Time, 2019-01-01T21:00:59, Epoch: 145, Batch: 750, Training Loss: 0.048073810338973996, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:00, Epoch: 145, Batch: 760, Training Loss: 0.02361268922686577, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:01, Epoch: 145, Batch: 770, Training Loss: 0.031001637876033782, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:02, Epoch: 145, Batch: 780, Training Loss: 0.03153783679008484, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:02, Epoch: 145, Batch: 790, Training Loss: 0.04173120707273483, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:03, Epoch: 145, Batch: 800, Training Loss: 0.027648819237947465, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:04, Epoch: 145, Batch: 810, Training Loss: 0.03374999165534973, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:04, Epoch: 145, Batch: 820, Training Loss: 0.05013418570160866, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:05, Epoch: 145, Batch: 830, Training Loss: 0.026332654058933258, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:06, Epoch: 145, Batch: 840, Training Loss: 0.03815607689321041, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:07, Epoch: 145, Batch: 850, Training Loss: 0.034503353759646416, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:07, Epoch: 145, Batch: 860, Training Loss: 0.040453947335481646, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:08, Epoch: 145, Batch: 870, Training Loss: 0.0324572429060936, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:09, Epoch: 145, Batch: 880, Training Loss: 0.03545457050204277, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:09, Epoch: 145, Batch: 890, Training Loss: 0.03928283303976059, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:10, Epoch: 145, Batch: 900, Training Loss: 0.03920264355838299, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:11, Epoch: 145, Batch: 910, Training Loss: 0.05969633348286152, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:12, Epoch: 145, Batch: 920, Training Loss: 0.046166520565748215, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:12, Epoch: 145, Batch: 930, Training Loss: 0.0409013744443655, LR: 0.00010000000000000003
Epoch: 145, Validation Top 1 acc: 98.9072494506836
Epoch: 145, Validation Top 5 acc: 99.99166870117188
Epoch: 145, Validation Set Loss: 0.040476925671100616
Start training epoch 146
Time, 2019-01-01T21:01:41, Epoch: 146, Batch: 10, Training Loss: 0.04649436064064503, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:42, Epoch: 146, Batch: 20, Training Loss: 0.035759904980659486, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:42, Epoch: 146, Batch: 30, Training Loss: 0.04629392437636852, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:43, Epoch: 146, Batch: 40, Training Loss: 0.0597218606621027, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:44, Epoch: 146, Batch: 50, Training Loss: 0.03299049288034439, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:44, Epoch: 146, Batch: 60, Training Loss: 0.02992408536374569, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:45, Epoch: 146, Batch: 70, Training Loss: 0.03467996567487717, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:46, Epoch: 146, Batch: 80, Training Loss: 0.04416404739022255, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:47, Epoch: 146, Batch: 90, Training Loss: 0.05183652751147747, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:47, Epoch: 146, Batch: 100, Training Loss: 0.05076717101037502, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:48, Epoch: 146, Batch: 110, Training Loss: 0.04297376051545143, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:49, Epoch: 146, Batch: 120, Training Loss: 0.04603532701730728, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:50, Epoch: 146, Batch: 130, Training Loss: 0.05058162361383438, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:50, Epoch: 146, Batch: 140, Training Loss: 0.05045938640832901, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:51, Epoch: 146, Batch: 150, Training Loss: 0.046948131173849106, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:52, Epoch: 146, Batch: 160, Training Loss: 0.04421971812844276, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:52, Epoch: 146, Batch: 170, Training Loss: 0.03298079147934914, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:53, Epoch: 146, Batch: 180, Training Loss: 0.033649659529328343, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:54, Epoch: 146, Batch: 190, Training Loss: 0.04421551674604416, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:55, Epoch: 146, Batch: 200, Training Loss: 0.04376647621393204, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:55, Epoch: 146, Batch: 210, Training Loss: 0.06449915021657944, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:56, Epoch: 146, Batch: 220, Training Loss: 0.04879078567028046, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:57, Epoch: 146, Batch: 230, Training Loss: 0.03896007314324379, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:57, Epoch: 146, Batch: 240, Training Loss: 0.041740912944078445, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:58, Epoch: 146, Batch: 250, Training Loss: 0.05142191648483276, LR: 0.00010000000000000003
Time, 2019-01-01T21:01:59, Epoch: 146, Batch: 260, Training Loss: 0.04078202247619629, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:00, Epoch: 146, Batch: 270, Training Loss: 0.03494361564517021, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:00, Epoch: 146, Batch: 280, Training Loss: 0.03725946471095085, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:01, Epoch: 146, Batch: 290, Training Loss: 0.04857946187257767, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:02, Epoch: 146, Batch: 300, Training Loss: 0.04997391849756241, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:03, Epoch: 146, Batch: 310, Training Loss: 0.02643769234418869, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:03, Epoch: 146, Batch: 320, Training Loss: 0.040127207338809964, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:04, Epoch: 146, Batch: 330, Training Loss: 0.04012389183044433, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:05, Epoch: 146, Batch: 340, Training Loss: 0.0570575937628746, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:05, Epoch: 146, Batch: 350, Training Loss: 0.029449078440666198, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:06, Epoch: 146, Batch: 360, Training Loss: 0.02800479345023632, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:07, Epoch: 146, Batch: 370, Training Loss: 0.04224299937486649, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:08, Epoch: 146, Batch: 380, Training Loss: 0.0498142771422863, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:08, Epoch: 146, Batch: 390, Training Loss: 0.03844016566872597, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:09, Epoch: 146, Batch: 400, Training Loss: 0.05142180733382702, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:10, Epoch: 146, Batch: 410, Training Loss: 0.03410051390528679, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:11, Epoch: 146, Batch: 420, Training Loss: 0.041625216603279114, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:11, Epoch: 146, Batch: 430, Training Loss: 0.04637051708996296, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:12, Epoch: 146, Batch: 440, Training Loss: 0.02138081192970276, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:13, Epoch: 146, Batch: 450, Training Loss: 0.04338606409728527, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:14, Epoch: 146, Batch: 460, Training Loss: 0.037043575942516324, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:14, Epoch: 146, Batch: 470, Training Loss: 0.05370272174477577, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:15, Epoch: 146, Batch: 480, Training Loss: 0.04065869897603989, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:16, Epoch: 146, Batch: 490, Training Loss: 0.04472314976155758, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:17, Epoch: 146, Batch: 500, Training Loss: 0.03835560195147991, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:17, Epoch: 146, Batch: 510, Training Loss: 0.042822464182972905, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:18, Epoch: 146, Batch: 520, Training Loss: 0.052008824050426485, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:19, Epoch: 146, Batch: 530, Training Loss: 0.042206718027591704, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:19, Epoch: 146, Batch: 540, Training Loss: 0.03100517801940441, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:20, Epoch: 146, Batch: 550, Training Loss: 0.0649093210697174, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:21, Epoch: 146, Batch: 560, Training Loss: 0.04200504161417484, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:22, Epoch: 146, Batch: 570, Training Loss: 0.03811362609267235, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:22, Epoch: 146, Batch: 580, Training Loss: 0.04451867453753948, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:23, Epoch: 146, Batch: 590, Training Loss: 0.03896484635770321, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:24, Epoch: 146, Batch: 600, Training Loss: 0.049865199625492095, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:25, Epoch: 146, Batch: 610, Training Loss: 0.040670350193977356, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:25, Epoch: 146, Batch: 620, Training Loss: 0.03235869891941547, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:26, Epoch: 146, Batch: 630, Training Loss: 0.030777570232748984, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:27, Epoch: 146, Batch: 640, Training Loss: 0.043617590144276616, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:28, Epoch: 146, Batch: 650, Training Loss: 0.03162961751222611, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:28, Epoch: 146, Batch: 660, Training Loss: 0.044627688080072406, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:29, Epoch: 146, Batch: 670, Training Loss: 0.023262973874807358, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:30, Epoch: 146, Batch: 680, Training Loss: 0.04740949422121048, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:31, Epoch: 146, Batch: 690, Training Loss: 0.021008986979722977, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:31, Epoch: 146, Batch: 700, Training Loss: 0.029825234413146974, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:32, Epoch: 146, Batch: 710, Training Loss: 0.03987848572432995, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:33, Epoch: 146, Batch: 720, Training Loss: 0.02439284473657608, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:33, Epoch: 146, Batch: 730, Training Loss: 0.05155956074595451, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:34, Epoch: 146, Batch: 740, Training Loss: 0.023503441363573074, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:35, Epoch: 146, Batch: 750, Training Loss: 0.041333454102277754, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:36, Epoch: 146, Batch: 760, Training Loss: 0.04567595086991787, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:36, Epoch: 146, Batch: 770, Training Loss: 0.04096703119575977, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:37, Epoch: 146, Batch: 780, Training Loss: 0.04941973313689232, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:38, Epoch: 146, Batch: 790, Training Loss: 0.03857760801911354, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:39, Epoch: 146, Batch: 800, Training Loss: 0.03259848654270172, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:39, Epoch: 146, Batch: 810, Training Loss: 0.030935968086123467, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:40, Epoch: 146, Batch: 820, Training Loss: 0.034090740233659746, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:41, Epoch: 146, Batch: 830, Training Loss: 0.02933245524764061, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:42, Epoch: 146, Batch: 840, Training Loss: 0.04741457626223564, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:42, Epoch: 146, Batch: 850, Training Loss: 0.04300711378455162, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:43, Epoch: 146, Batch: 860, Training Loss: 0.027091310918331148, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:44, Epoch: 146, Batch: 870, Training Loss: 0.051154775172472, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:45, Epoch: 146, Batch: 880, Training Loss: 0.04066637083888054, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:45, Epoch: 146, Batch: 890, Training Loss: 0.03640582375228405, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:46, Epoch: 146, Batch: 900, Training Loss: 0.041513364762067795, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:47, Epoch: 146, Batch: 910, Training Loss: 0.033205392956733706, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:47, Epoch: 146, Batch: 920, Training Loss: 0.03815062157809734, LR: 0.00010000000000000003
Time, 2019-01-01T21:02:48, Epoch: 146, Batch: 930, Training Loss: 0.021749018877744674, LR: 0.00010000000000000003
Epoch: 146, Validation Top 1 acc: 98.9222412109375
Epoch: 146, Validation Top 5 acc: 99.99166870117188
Epoch: 146, Validation Set Loss: 0.040449902415275574
Start training epoch 147
Time, 2019-01-01T21:03:15, Epoch: 147, Batch: 10, Training Loss: 0.04658331125974655, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:16, Epoch: 147, Batch: 20, Training Loss: 0.05011421293020248, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:17, Epoch: 147, Batch: 30, Training Loss: 0.031463686004281045, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:18, Epoch: 147, Batch: 40, Training Loss: 0.03177984096109867, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:18, Epoch: 147, Batch: 50, Training Loss: 0.03435355126857757, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:19, Epoch: 147, Batch: 60, Training Loss: 0.0338106781244278, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:20, Epoch: 147, Batch: 70, Training Loss: 0.041044744476675984, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:21, Epoch: 147, Batch: 80, Training Loss: 0.025893767178058625, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:21, Epoch: 147, Batch: 90, Training Loss: 0.05093616209924221, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:22, Epoch: 147, Batch: 100, Training Loss: 0.04062692373991013, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:23, Epoch: 147, Batch: 110, Training Loss: 0.03640234582126141, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:24, Epoch: 147, Batch: 120, Training Loss: 0.03437307216227055, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:24, Epoch: 147, Batch: 130, Training Loss: 0.030440539121627808, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:25, Epoch: 147, Batch: 140, Training Loss: 0.0449208851903677, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:26, Epoch: 147, Batch: 150, Training Loss: 0.03284580484032631, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:27, Epoch: 147, Batch: 160, Training Loss: 0.044527092948555946, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:27, Epoch: 147, Batch: 170, Training Loss: 0.03823523409664631, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:28, Epoch: 147, Batch: 180, Training Loss: 0.050058099627494815, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:29, Epoch: 147, Batch: 190, Training Loss: 0.03395097367465496, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:30, Epoch: 147, Batch: 200, Training Loss: 0.04224209785461426, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:30, Epoch: 147, Batch: 210, Training Loss: 0.03719006404280663, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:31, Epoch: 147, Batch: 220, Training Loss: 0.05115037709474564, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:32, Epoch: 147, Batch: 230, Training Loss: 0.032398543879389764, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:33, Epoch: 147, Batch: 240, Training Loss: 0.039951679110527036, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:33, Epoch: 147, Batch: 250, Training Loss: 0.036692443490028384, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:34, Epoch: 147, Batch: 260, Training Loss: 0.04949113875627518, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:35, Epoch: 147, Batch: 270, Training Loss: 0.05176381729543209, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:35, Epoch: 147, Batch: 280, Training Loss: 0.03958010748028755, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:36, Epoch: 147, Batch: 290, Training Loss: 0.04034247547388077, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:37, Epoch: 147, Batch: 300, Training Loss: 0.04274212084710598, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:38, Epoch: 147, Batch: 310, Training Loss: 0.04426371827721596, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:38, Epoch: 147, Batch: 320, Training Loss: 0.029354703426361085, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:39, Epoch: 147, Batch: 330, Training Loss: 0.034417996555566786, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:40, Epoch: 147, Batch: 340, Training Loss: 0.05382905676960945, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:40, Epoch: 147, Batch: 350, Training Loss: 0.02126513384282589, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:41, Epoch: 147, Batch: 360, Training Loss: 0.03419115804135799, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:42, Epoch: 147, Batch: 370, Training Loss: 0.02934713438153267, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:43, Epoch: 147, Batch: 380, Training Loss: 0.03856823816895485, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:43, Epoch: 147, Batch: 390, Training Loss: 0.04150227531790733, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:44, Epoch: 147, Batch: 400, Training Loss: 0.03440755233168602, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:45, Epoch: 147, Batch: 410, Training Loss: 0.04972030818462372, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:46, Epoch: 147, Batch: 420, Training Loss: 0.03911950290203094, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:46, Epoch: 147, Batch: 430, Training Loss: 0.03995873630046844, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:47, Epoch: 147, Batch: 440, Training Loss: 0.04890462346374989, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:48, Epoch: 147, Batch: 450, Training Loss: 0.028470445424318314, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:48, Epoch: 147, Batch: 460, Training Loss: 0.049114585667848584, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:49, Epoch: 147, Batch: 470, Training Loss: 0.04463747553527355, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:50, Epoch: 147, Batch: 480, Training Loss: 0.030780713260173797, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:51, Epoch: 147, Batch: 490, Training Loss: 0.038775448501110074, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:51, Epoch: 147, Batch: 500, Training Loss: 0.05925574973225593, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:52, Epoch: 147, Batch: 510, Training Loss: 0.027430129796266557, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:53, Epoch: 147, Batch: 520, Training Loss: 0.06391020268201827, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:53, Epoch: 147, Batch: 530, Training Loss: 0.02530251182615757, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:54, Epoch: 147, Batch: 540, Training Loss: 0.03876526318490505, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:55, Epoch: 147, Batch: 550, Training Loss: 0.043400832638144496, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:56, Epoch: 147, Batch: 560, Training Loss: 0.040038513392210005, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:57, Epoch: 147, Batch: 570, Training Loss: 0.050489998608827594, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:57, Epoch: 147, Batch: 580, Training Loss: 0.036521509289741516, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:58, Epoch: 147, Batch: 590, Training Loss: 0.04021743573248386, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:59, Epoch: 147, Batch: 600, Training Loss: 0.02745587155222893, LR: 0.00010000000000000003
Time, 2019-01-01T21:03:59, Epoch: 147, Batch: 610, Training Loss: 0.034810148924589154, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:00, Epoch: 147, Batch: 620, Training Loss: 0.03669289387762546, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:01, Epoch: 147, Batch: 630, Training Loss: 0.031500691547989845, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:02, Epoch: 147, Batch: 640, Training Loss: 0.0530196949839592, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:02, Epoch: 147, Batch: 650, Training Loss: 0.0420092049986124, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:03, Epoch: 147, Batch: 660, Training Loss: 0.04502792842686176, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:04, Epoch: 147, Batch: 670, Training Loss: 0.05181187018752098, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:04, Epoch: 147, Batch: 680, Training Loss: 0.034327616170048716, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:05, Epoch: 147, Batch: 690, Training Loss: 0.047867515310645106, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:06, Epoch: 147, Batch: 700, Training Loss: 0.040995153784751895, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:07, Epoch: 147, Batch: 710, Training Loss: 0.039498328045010564, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:07, Epoch: 147, Batch: 720, Training Loss: 0.04582803323864937, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:08, Epoch: 147, Batch: 730, Training Loss: 0.04955162033438683, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:09, Epoch: 147, Batch: 740, Training Loss: 0.043018925935029984, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:10, Epoch: 147, Batch: 750, Training Loss: 0.035188321769237516, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:10, Epoch: 147, Batch: 760, Training Loss: 0.048932034522295, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:11, Epoch: 147, Batch: 770, Training Loss: 0.05070181302726269, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:12, Epoch: 147, Batch: 780, Training Loss: 0.04481498412787914, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:13, Epoch: 147, Batch: 790, Training Loss: 0.04768240675330162, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:13, Epoch: 147, Batch: 800, Training Loss: 0.03627881705760956, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:14, Epoch: 147, Batch: 810, Training Loss: 0.05268668159842491, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:15, Epoch: 147, Batch: 820, Training Loss: 0.05118054673075676, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:15, Epoch: 147, Batch: 830, Training Loss: 0.04136374071240425, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:16, Epoch: 147, Batch: 840, Training Loss: 0.0530559629201889, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:17, Epoch: 147, Batch: 850, Training Loss: 0.03391921743750572, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:18, Epoch: 147, Batch: 860, Training Loss: 0.03658246137201786, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:18, Epoch: 147, Batch: 870, Training Loss: 0.04375508241355419, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:19, Epoch: 147, Batch: 880, Training Loss: 0.0551736269146204, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:20, Epoch: 147, Batch: 890, Training Loss: 0.022990015521645545, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:21, Epoch: 147, Batch: 900, Training Loss: 0.029787902906537055, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:21, Epoch: 147, Batch: 910, Training Loss: 0.04153415039181709, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:22, Epoch: 147, Batch: 920, Training Loss: 0.03017171248793602, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:23, Epoch: 147, Batch: 930, Training Loss: 0.054239104688167575, LR: 0.00010000000000000003
Epoch: 147, Validation Top 1 acc: 98.9089126586914
Epoch: 147, Validation Top 5 acc: 99.99166870117188
Epoch: 147, Validation Set Loss: 0.04044651985168457
Start training epoch 148
Time, 2019-01-01T21:04:51, Epoch: 148, Batch: 10, Training Loss: 0.03684041649103165, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:52, Epoch: 148, Batch: 20, Training Loss: 0.026400598883628845, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:53, Epoch: 148, Batch: 30, Training Loss: 0.037002738937735556, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:53, Epoch: 148, Batch: 40, Training Loss: 0.0552742213010788, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:54, Epoch: 148, Batch: 50, Training Loss: 0.04395600631833076, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:55, Epoch: 148, Batch: 60, Training Loss: 0.04253768846392632, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:56, Epoch: 148, Batch: 70, Training Loss: 0.04658394865691662, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:56, Epoch: 148, Batch: 80, Training Loss: 0.030163082107901574, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:57, Epoch: 148, Batch: 90, Training Loss: 0.03261302337050438, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:58, Epoch: 148, Batch: 100, Training Loss: 0.043255395442247394, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:58, Epoch: 148, Batch: 110, Training Loss: 0.025094897672533988, LR: 0.00010000000000000003
Time, 2019-01-01T21:04:59, Epoch: 148, Batch: 120, Training Loss: 0.029215342923998834, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:00, Epoch: 148, Batch: 130, Training Loss: 0.039624278992414476, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:01, Epoch: 148, Batch: 140, Training Loss: 0.05615519993007183, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:01, Epoch: 148, Batch: 150, Training Loss: 0.029354354366660118, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:02, Epoch: 148, Batch: 160, Training Loss: 0.038535138592123985, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:03, Epoch: 148, Batch: 170, Training Loss: 0.05462557785212994, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:04, Epoch: 148, Batch: 180, Training Loss: 0.034132364019751546, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:04, Epoch: 148, Batch: 190, Training Loss: 0.03781571127474308, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:05, Epoch: 148, Batch: 200, Training Loss: 0.029644165933132172, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:06, Epoch: 148, Batch: 210, Training Loss: 0.04310764074325561, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:06, Epoch: 148, Batch: 220, Training Loss: 0.0420513603836298, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:07, Epoch: 148, Batch: 230, Training Loss: 0.029539817199110984, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:08, Epoch: 148, Batch: 240, Training Loss: 0.03638892620801926, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:09, Epoch: 148, Batch: 250, Training Loss: 0.046300165355205536, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:09, Epoch: 148, Batch: 260, Training Loss: 0.03697355203330517, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:10, Epoch: 148, Batch: 270, Training Loss: 0.027359789609909056, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:11, Epoch: 148, Batch: 280, Training Loss: 0.03803179860115051, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:11, Epoch: 148, Batch: 290, Training Loss: 0.052459622547030446, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:12, Epoch: 148, Batch: 300, Training Loss: 0.03145973160862923, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:13, Epoch: 148, Batch: 310, Training Loss: 0.0390154018998146, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:14, Epoch: 148, Batch: 320, Training Loss: 0.040804363787174225, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:14, Epoch: 148, Batch: 330, Training Loss: 0.0393974743783474, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:15, Epoch: 148, Batch: 340, Training Loss: 0.03808705322444439, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:16, Epoch: 148, Batch: 350, Training Loss: 0.047406529635190965, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:17, Epoch: 148, Batch: 360, Training Loss: 0.04469969496130943, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:17, Epoch: 148, Batch: 370, Training Loss: 0.03414993770420551, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:18, Epoch: 148, Batch: 380, Training Loss: 0.05901047475636005, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:19, Epoch: 148, Batch: 390, Training Loss: 0.03426040709018707, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:19, Epoch: 148, Batch: 400, Training Loss: 0.03407403156161308, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:20, Epoch: 148, Batch: 410, Training Loss: 0.04400765299797058, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:21, Epoch: 148, Batch: 420, Training Loss: 0.04569574072957039, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:22, Epoch: 148, Batch: 430, Training Loss: 0.03652733415365219, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:22, Epoch: 148, Batch: 440, Training Loss: 0.05092822387814522, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:23, Epoch: 148, Batch: 450, Training Loss: 0.04300112724304199, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:24, Epoch: 148, Batch: 460, Training Loss: 0.04202986769378185, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:24, Epoch: 148, Batch: 470, Training Loss: 0.028859426826238634, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:25, Epoch: 148, Batch: 480, Training Loss: 0.049165469408035276, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:26, Epoch: 148, Batch: 490, Training Loss: 0.03426709733903408, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:27, Epoch: 148, Batch: 500, Training Loss: 0.035529954358935356, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:27, Epoch: 148, Batch: 510, Training Loss: 0.04189058877527714, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:28, Epoch: 148, Batch: 520, Training Loss: 0.031872375309467314, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:29, Epoch: 148, Batch: 530, Training Loss: 0.02461499720811844, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:29, Epoch: 148, Batch: 540, Training Loss: 0.025381435826420783, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:30, Epoch: 148, Batch: 550, Training Loss: 0.06558731533586978, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:31, Epoch: 148, Batch: 560, Training Loss: 0.03907992914319038, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:32, Epoch: 148, Batch: 570, Training Loss: 0.049188417941331865, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:32, Epoch: 148, Batch: 580, Training Loss: 0.048214130103588104, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:33, Epoch: 148, Batch: 590, Training Loss: 0.05582100972533226, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:34, Epoch: 148, Batch: 600, Training Loss: 0.037274419143795964, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:35, Epoch: 148, Batch: 610, Training Loss: 0.04367045760154724, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:35, Epoch: 148, Batch: 620, Training Loss: 0.04602925181388855, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:36, Epoch: 148, Batch: 630, Training Loss: 0.026608032733201982, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:37, Epoch: 148, Batch: 640, Training Loss: 0.029295409843325616, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:37, Epoch: 148, Batch: 650, Training Loss: 0.028054943308234215, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:38, Epoch: 148, Batch: 660, Training Loss: 0.04391331784427166, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:39, Epoch: 148, Batch: 670, Training Loss: 0.025832734256982803, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:40, Epoch: 148, Batch: 680, Training Loss: 0.03724256530404091, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:40, Epoch: 148, Batch: 690, Training Loss: 0.03954916410148144, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:41, Epoch: 148, Batch: 700, Training Loss: 0.03732654526829719, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:42, Epoch: 148, Batch: 710, Training Loss: 0.04042700082063675, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:43, Epoch: 148, Batch: 720, Training Loss: 0.04501878060400486, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:43, Epoch: 148, Batch: 730, Training Loss: 0.06835914105176925, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:44, Epoch: 148, Batch: 740, Training Loss: 0.05100064277648926, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:45, Epoch: 148, Batch: 750, Training Loss: 0.04251324534416199, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:46, Epoch: 148, Batch: 760, Training Loss: 0.04156969040632248, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:46, Epoch: 148, Batch: 770, Training Loss: 0.032795262336730954, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:47, Epoch: 148, Batch: 780, Training Loss: 0.04168776385486126, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:48, Epoch: 148, Batch: 790, Training Loss: 0.03835140988230705, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:49, Epoch: 148, Batch: 800, Training Loss: 0.05233401954174042, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:49, Epoch: 148, Batch: 810, Training Loss: 0.03768930248916149, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:50, Epoch: 148, Batch: 820, Training Loss: 0.058788612484931946, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:51, Epoch: 148, Batch: 830, Training Loss: 0.0335509791970253, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:51, Epoch: 148, Batch: 840, Training Loss: 0.044375306367874144, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:52, Epoch: 148, Batch: 850, Training Loss: 0.04709553681313992, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:53, Epoch: 148, Batch: 860, Training Loss: 0.039716554805636406, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:54, Epoch: 148, Batch: 870, Training Loss: 0.04830914624035358, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:54, Epoch: 148, Batch: 880, Training Loss: 0.05789032578468323, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:55, Epoch: 148, Batch: 890, Training Loss: 0.02580456845462322, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:56, Epoch: 148, Batch: 900, Training Loss: 0.04944970682263374, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:56, Epoch: 148, Batch: 910, Training Loss: 0.05398843586444855, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:57, Epoch: 148, Batch: 920, Training Loss: 0.057664474844932555, LR: 0.00010000000000000003
Time, 2019-01-01T21:05:58, Epoch: 148, Batch: 930, Training Loss: 0.02546084448695183, LR: 0.00010000000000000003
Epoch: 148, Validation Top 1 acc: 98.90558624267578
Epoch: 148, Validation Top 5 acc: 99.99000549316406
Epoch: 148, Validation Set Loss: 0.04060190171003342
Start training epoch 149
Time, 2019-01-01T21:06:25, Epoch: 149, Batch: 10, Training Loss: 0.037056078389287, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:26, Epoch: 149, Batch: 20, Training Loss: 0.036269073560833934, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:27, Epoch: 149, Batch: 30, Training Loss: 0.03012871965765953, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:27, Epoch: 149, Batch: 40, Training Loss: 0.04000329561531544, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:28, Epoch: 149, Batch: 50, Training Loss: 0.045764772966504094, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:29, Epoch: 149, Batch: 60, Training Loss: 0.03269570767879486, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:30, Epoch: 149, Batch: 70, Training Loss: 0.03358806967735291, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:30, Epoch: 149, Batch: 80, Training Loss: 0.033217582106590274, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:31, Epoch: 149, Batch: 90, Training Loss: 0.056882985308766364, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:32, Epoch: 149, Batch: 100, Training Loss: 0.03927582018077373, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:33, Epoch: 149, Batch: 110, Training Loss: 0.02299945689737797, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:33, Epoch: 149, Batch: 120, Training Loss: 0.041212116554379466, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:34, Epoch: 149, Batch: 130, Training Loss: 0.02753884457051754, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:35, Epoch: 149, Batch: 140, Training Loss: 0.03065223731100559, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:36, Epoch: 149, Batch: 150, Training Loss: 0.04584007114171982, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:37, Epoch: 149, Batch: 160, Training Loss: 0.03637758977711201, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:37, Epoch: 149, Batch: 170, Training Loss: 0.05450463853776455, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:38, Epoch: 149, Batch: 180, Training Loss: 0.03665522113442421, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:39, Epoch: 149, Batch: 190, Training Loss: 0.03798574209213257, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:40, Epoch: 149, Batch: 200, Training Loss: 0.029417216777801514, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:41, Epoch: 149, Batch: 210, Training Loss: 0.04084762446582317, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:41, Epoch: 149, Batch: 220, Training Loss: 0.03521058782935142, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:42, Epoch: 149, Batch: 230, Training Loss: 0.036577914282679556, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:43, Epoch: 149, Batch: 240, Training Loss: 0.030753551051020622, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:44, Epoch: 149, Batch: 250, Training Loss: 0.04011563807725906, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:45, Epoch: 149, Batch: 260, Training Loss: 0.033113067597150804, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:45, Epoch: 149, Batch: 270, Training Loss: 0.043569544702768324, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:46, Epoch: 149, Batch: 280, Training Loss: 0.04136958867311478, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:47, Epoch: 149, Batch: 290, Training Loss: 0.04229392632842064, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:48, Epoch: 149, Batch: 300, Training Loss: 0.05427968055009842, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:48, Epoch: 149, Batch: 310, Training Loss: 0.04087040722370148, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:49, Epoch: 149, Batch: 320, Training Loss: 0.03676296472549438, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:50, Epoch: 149, Batch: 330, Training Loss: 0.037343259900808334, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:50, Epoch: 149, Batch: 340, Training Loss: 0.035995999723672865, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:51, Epoch: 149, Batch: 350, Training Loss: 0.047576982900500296, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:52, Epoch: 149, Batch: 360, Training Loss: 0.050871292129158974, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:53, Epoch: 149, Batch: 370, Training Loss: 0.03945405371487141, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:53, Epoch: 149, Batch: 380, Training Loss: 0.03687860295176506, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:54, Epoch: 149, Batch: 390, Training Loss: 0.039585572108626364, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:55, Epoch: 149, Batch: 400, Training Loss: 0.046245189383625984, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:56, Epoch: 149, Batch: 410, Training Loss: 0.037778792157769206, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:56, Epoch: 149, Batch: 420, Training Loss: 0.04190208651125431, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:57, Epoch: 149, Batch: 430, Training Loss: 0.028701844811439513, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:58, Epoch: 149, Batch: 440, Training Loss: 0.05299150198698044, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:59, Epoch: 149, Batch: 450, Training Loss: 0.0699351578950882, LR: 0.00010000000000000003
Time, 2019-01-01T21:06:59, Epoch: 149, Batch: 460, Training Loss: 0.035836993530392644, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:00, Epoch: 149, Batch: 470, Training Loss: 0.06179150566458702, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:01, Epoch: 149, Batch: 480, Training Loss: 0.020503779873251915, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:02, Epoch: 149, Batch: 490, Training Loss: 0.04274323359131813, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:03, Epoch: 149, Batch: 500, Training Loss: 0.03616520538926125, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:03, Epoch: 149, Batch: 510, Training Loss: 0.053254494443535805, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:04, Epoch: 149, Batch: 520, Training Loss: 0.03639410771429539, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:05, Epoch: 149, Batch: 530, Training Loss: 0.037121666595339775, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:06, Epoch: 149, Batch: 540, Training Loss: 0.057915487140417096, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:06, Epoch: 149, Batch: 550, Training Loss: 0.030634979158639906, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:07, Epoch: 149, Batch: 560, Training Loss: 0.050115180388093, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:08, Epoch: 149, Batch: 570, Training Loss: 0.031684108451008795, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:09, Epoch: 149, Batch: 580, Training Loss: 0.04147990718483925, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:10, Epoch: 149, Batch: 590, Training Loss: 0.05034881643950939, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:10, Epoch: 149, Batch: 600, Training Loss: 0.03396563716232777, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:11, Epoch: 149, Batch: 610, Training Loss: 0.04259520694613457, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:12, Epoch: 149, Batch: 620, Training Loss: 0.04527952969074249, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:13, Epoch: 149, Batch: 630, Training Loss: 0.05114884190261364, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:13, Epoch: 149, Batch: 640, Training Loss: 0.0299478892236948, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:14, Epoch: 149, Batch: 650, Training Loss: 0.040912316739559175, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:15, Epoch: 149, Batch: 660, Training Loss: 0.04148438684642315, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:16, Epoch: 149, Batch: 670, Training Loss: 0.05343073569238186, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:16, Epoch: 149, Batch: 680, Training Loss: 0.04431007578969002, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:17, Epoch: 149, Batch: 690, Training Loss: 0.037613142654299735, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:18, Epoch: 149, Batch: 700, Training Loss: 0.06288159489631653, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:18, Epoch: 149, Batch: 710, Training Loss: 0.05444154553115368, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:19, Epoch: 149, Batch: 720, Training Loss: 0.029138316959142686, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:20, Epoch: 149, Batch: 730, Training Loss: 0.03568791225552559, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:21, Epoch: 149, Batch: 740, Training Loss: 0.0296344093978405, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:21, Epoch: 149, Batch: 750, Training Loss: 0.027077529951930045, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:22, Epoch: 149, Batch: 760, Training Loss: 0.06896304450929165, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:23, Epoch: 149, Batch: 770, Training Loss: 0.03473245427012443, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:24, Epoch: 149, Batch: 780, Training Loss: 0.03848538994789123, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:24, Epoch: 149, Batch: 790, Training Loss: 0.042083850130438805, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:25, Epoch: 149, Batch: 800, Training Loss: 0.0465567696839571, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:26, Epoch: 149, Batch: 810, Training Loss: 0.028726801648736, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:26, Epoch: 149, Batch: 820, Training Loss: 0.05268171429634094, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:27, Epoch: 149, Batch: 830, Training Loss: 0.042893382161855696, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:28, Epoch: 149, Batch: 840, Training Loss: 0.0313785009086132, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:29, Epoch: 149, Batch: 850, Training Loss: 0.05060304813086987, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:29, Epoch: 149, Batch: 860, Training Loss: 0.03231922909617424, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:30, Epoch: 149, Batch: 870, Training Loss: 0.056052929162979125, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:31, Epoch: 149, Batch: 880, Training Loss: 0.03984091803431511, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:31, Epoch: 149, Batch: 890, Training Loss: 0.0509966678917408, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:32, Epoch: 149, Batch: 900, Training Loss: 0.04044435583055019, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:33, Epoch: 149, Batch: 910, Training Loss: 0.02894728034734726, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:34, Epoch: 149, Batch: 920, Training Loss: 0.027653786167502404, LR: 0.00010000000000000003
Time, 2019-01-01T21:07:34, Epoch: 149, Batch: 930, Training Loss: 0.04346728771924972, LR: 0.00010000000000000003
Epoch: 149, Validation Top 1 acc: 98.91390991210938
Epoch: 149, Validation Top 5 acc: 99.99166870117188
Epoch: 149, Validation Set Loss: 0.04046650230884552
Start training epoch 150
Time, 2019-01-01T21:08:02, Epoch: 150, Batch: 10, Training Loss: 0.028497137874364854, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:02, Epoch: 150, Batch: 20, Training Loss: 0.034289564937353134, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:03, Epoch: 150, Batch: 30, Training Loss: 0.04945770502090454, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:04, Epoch: 150, Batch: 40, Training Loss: 0.0410986416041851, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:04, Epoch: 150, Batch: 50, Training Loss: 0.04536979719996452, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:05, Epoch: 150, Batch: 60, Training Loss: 0.053818858414888385, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:06, Epoch: 150, Batch: 70, Training Loss: 0.045659666508436204, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:07, Epoch: 150, Batch: 80, Training Loss: 0.03462974354624748, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:07, Epoch: 150, Batch: 90, Training Loss: 0.047592146694660185, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:08, Epoch: 150, Batch: 100, Training Loss: 0.03838544338941574, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:09, Epoch: 150, Batch: 110, Training Loss: 0.04479339085519314, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:10, Epoch: 150, Batch: 120, Training Loss: 0.042127673700451854, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:10, Epoch: 150, Batch: 130, Training Loss: 0.03149196431040764, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:11, Epoch: 150, Batch: 140, Training Loss: 0.044286738708615304, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:12, Epoch: 150, Batch: 150, Training Loss: 0.03809152245521545, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:12, Epoch: 150, Batch: 160, Training Loss: 0.05198143683373928, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:13, Epoch: 150, Batch: 170, Training Loss: 0.03398973047733307, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:14, Epoch: 150, Batch: 180, Training Loss: 0.04317861944437027, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:15, Epoch: 150, Batch: 190, Training Loss: 0.04177714474499226, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:15, Epoch: 150, Batch: 200, Training Loss: 0.035074660927057265, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:16, Epoch: 150, Batch: 210, Training Loss: 0.055570407584309575, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:17, Epoch: 150, Batch: 220, Training Loss: 0.047332408279180525, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:18, Epoch: 150, Batch: 230, Training Loss: 0.06223895214498043, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:18, Epoch: 150, Batch: 240, Training Loss: 0.040287939459085466, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:19, Epoch: 150, Batch: 250, Training Loss: 0.047238214686512944, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:20, Epoch: 150, Batch: 260, Training Loss: 0.03243274763226509, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:20, Epoch: 150, Batch: 270, Training Loss: 0.04479341357946396, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:21, Epoch: 150, Batch: 280, Training Loss: 0.03703343868255615, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:22, Epoch: 150, Batch: 290, Training Loss: 0.026556023210287095, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:23, Epoch: 150, Batch: 300, Training Loss: 0.025486080348491667, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:23, Epoch: 150, Batch: 310, Training Loss: 0.034933201223611834, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:24, Epoch: 150, Batch: 320, Training Loss: 0.0423737920820713, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:25, Epoch: 150, Batch: 330, Training Loss: 0.050891827791929245, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:26, Epoch: 150, Batch: 340, Training Loss: 0.032530909031629564, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:26, Epoch: 150, Batch: 350, Training Loss: 0.030439749360084534, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:27, Epoch: 150, Batch: 360, Training Loss: 0.04365459941327572, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:28, Epoch: 150, Batch: 370, Training Loss: 0.05887629203498364, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:29, Epoch: 150, Batch: 380, Training Loss: 0.05693364106118679, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:29, Epoch: 150, Batch: 390, Training Loss: 0.025338965654373168, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:30, Epoch: 150, Batch: 400, Training Loss: 0.02651807777583599, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:31, Epoch: 150, Batch: 410, Training Loss: 0.04461078979074955, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:31, Epoch: 150, Batch: 420, Training Loss: 0.03079032301902771, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:32, Epoch: 150, Batch: 430, Training Loss: 0.039694105833768846, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:33, Epoch: 150, Batch: 440, Training Loss: 0.040166210383176804, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:34, Epoch: 150, Batch: 450, Training Loss: 0.03556778207421303, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:34, Epoch: 150, Batch: 460, Training Loss: 0.047795720398426056, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:35, Epoch: 150, Batch: 470, Training Loss: 0.05343425199389458, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:36, Epoch: 150, Batch: 480, Training Loss: 0.04021890051662922, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:37, Epoch: 150, Batch: 490, Training Loss: 0.04152146354317665, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:37, Epoch: 150, Batch: 500, Training Loss: 0.040481049194931985, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:38, Epoch: 150, Batch: 510, Training Loss: 0.032129062712192534, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:39, Epoch: 150, Batch: 520, Training Loss: 0.07099149450659752, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:40, Epoch: 150, Batch: 530, Training Loss: 0.03104865998029709, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:40, Epoch: 150, Batch: 540, Training Loss: 0.0336307093501091, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:41, Epoch: 150, Batch: 550, Training Loss: 0.041790463775396344, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:42, Epoch: 150, Batch: 560, Training Loss: 0.043684767186641695, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:43, Epoch: 150, Batch: 570, Training Loss: 0.04667169414460659, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:43, Epoch: 150, Batch: 580, Training Loss: 0.04421299025416374, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:44, Epoch: 150, Batch: 590, Training Loss: 0.03660090863704681, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:45, Epoch: 150, Batch: 600, Training Loss: 0.0287765521556139, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:45, Epoch: 150, Batch: 610, Training Loss: 0.04218747094273567, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:46, Epoch: 150, Batch: 620, Training Loss: 0.04447498880326748, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:47, Epoch: 150, Batch: 630, Training Loss: 0.030482318252325058, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:48, Epoch: 150, Batch: 640, Training Loss: 0.03166873790323734, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:48, Epoch: 150, Batch: 650, Training Loss: 0.04370767176151276, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:49, Epoch: 150, Batch: 660, Training Loss: 0.04243003390729427, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:50, Epoch: 150, Batch: 670, Training Loss: 0.03265733830630779, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:50, Epoch: 150, Batch: 680, Training Loss: 0.03171210885047913, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:51, Epoch: 150, Batch: 690, Training Loss: 0.03262143880128861, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:52, Epoch: 150, Batch: 700, Training Loss: 0.031171193346381187, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:53, Epoch: 150, Batch: 710, Training Loss: 0.0503114715218544, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:53, Epoch: 150, Batch: 720, Training Loss: 0.03747177720069885, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:54, Epoch: 150, Batch: 730, Training Loss: 0.0456224337220192, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:55, Epoch: 150, Batch: 740, Training Loss: 0.035236214846372606, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:55, Epoch: 150, Batch: 750, Training Loss: 0.0409617904573679, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:56, Epoch: 150, Batch: 760, Training Loss: 0.036054473370313644, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:57, Epoch: 150, Batch: 770, Training Loss: 0.042819726094603536, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:58, Epoch: 150, Batch: 780, Training Loss: 0.04657861217856407, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:58, Epoch: 150, Batch: 790, Training Loss: 0.024372726678848267, LR: 0.00010000000000000003
Time, 2019-01-01T21:08:59, Epoch: 150, Batch: 800, Training Loss: 0.043100540339946744, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:00, Epoch: 150, Batch: 810, Training Loss: 0.043917106464505196, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:01, Epoch: 150, Batch: 820, Training Loss: 0.050810137391090394, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:01, Epoch: 150, Batch: 830, Training Loss: 0.040313249081373216, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:02, Epoch: 150, Batch: 840, Training Loss: 0.041327894851565364, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:03, Epoch: 150, Batch: 850, Training Loss: 0.044029374793171885, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:03, Epoch: 150, Batch: 860, Training Loss: 0.05079680979251862, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:04, Epoch: 150, Batch: 870, Training Loss: 0.039345163851976395, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:05, Epoch: 150, Batch: 880, Training Loss: 0.046818458288908, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:06, Epoch: 150, Batch: 890, Training Loss: 0.03513379767537117, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:06, Epoch: 150, Batch: 900, Training Loss: 0.04359058253467083, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:07, Epoch: 150, Batch: 910, Training Loss: 0.04343054704368114, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:08, Epoch: 150, Batch: 920, Training Loss: 0.03407642170786858, LR: 0.00010000000000000003
Time, 2019-01-01T21:09:09, Epoch: 150, Batch: 930, Training Loss: 0.03616808131337166, LR: 0.00010000000000000003
Epoch: 150, Validation Top 1 acc: 98.89891815185547
Epoch: 150, Validation Top 5 acc: 99.99166870117188
Epoch: 150, Validation Set Loss: 0.04045877978205681
Start training epoch 151
Time, 2019-01-01T21:20:54, Epoch: 151, Batch: 10, Training Loss: 0.04563365578651428, LR: 0.00010000000000000003
Time, 2019-01-01T21:20:55, Epoch: 151, Batch: 20, Training Loss: 0.045276199281215665, LR: 0.00010000000000000003
Time, 2019-01-01T21:20:56, Epoch: 151, Batch: 30, Training Loss: 0.03785809017717838, LR: 0.00010000000000000003
Time, 2019-01-01T21:20:56, Epoch: 151, Batch: 40, Training Loss: 0.034943850338459016, LR: 0.00010000000000000003
Time, 2019-01-01T21:20:57, Epoch: 151, Batch: 50, Training Loss: 0.04402618259191513, LR: 0.00010000000000000003
Time, 2019-01-01T21:20:58, Epoch: 151, Batch: 60, Training Loss: 0.03663347661495209, LR: 0.00010000000000000003
Time, 2019-01-01T21:20:59, Epoch: 151, Batch: 70, Training Loss: 0.04210595227777958, LR: 0.00010000000000000003
Time, 2019-01-01T21:20:59, Epoch: 151, Batch: 80, Training Loss: 0.04137486517429352, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:00, Epoch: 151, Batch: 90, Training Loss: 0.049192268401384354, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:01, Epoch: 151, Batch: 100, Training Loss: 0.04254130125045776, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:02, Epoch: 151, Batch: 110, Training Loss: 0.038491323590278625, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:03, Epoch: 151, Batch: 120, Training Loss: 0.042265857383608815, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:03, Epoch: 151, Batch: 130, Training Loss: 0.04890741109848022, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:04, Epoch: 151, Batch: 140, Training Loss: 0.051776503771543504, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:05, Epoch: 151, Batch: 150, Training Loss: 0.03845912218093872, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:06, Epoch: 151, Batch: 160, Training Loss: 0.039201436564326286, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:06, Epoch: 151, Batch: 170, Training Loss: 0.04454450234770775, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:07, Epoch: 151, Batch: 180, Training Loss: 0.032186617329716685, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:08, Epoch: 151, Batch: 190, Training Loss: 0.04968388713896275, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:09, Epoch: 151, Batch: 200, Training Loss: 0.054443908110260966, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:10, Epoch: 151, Batch: 210, Training Loss: 0.040964608639478685, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:10, Epoch: 151, Batch: 220, Training Loss: 0.037594632059335706, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:11, Epoch: 151, Batch: 230, Training Loss: 0.04443184956908226, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:12, Epoch: 151, Batch: 240, Training Loss: 0.03345029465854168, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:13, Epoch: 151, Batch: 250, Training Loss: 0.028811386972665786, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:14, Epoch: 151, Batch: 260, Training Loss: 0.03487885780632496, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:14, Epoch: 151, Batch: 270, Training Loss: 0.05268312729895115, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:15, Epoch: 151, Batch: 280, Training Loss: 0.033387931808829305, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:16, Epoch: 151, Batch: 290, Training Loss: 0.031717078387737276, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:17, Epoch: 151, Batch: 300, Training Loss: 0.03004614859819412, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:17, Epoch: 151, Batch: 310, Training Loss: 0.05232328027486801, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:18, Epoch: 151, Batch: 320, Training Loss: 0.03572053462266922, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:19, Epoch: 151, Batch: 330, Training Loss: 0.035570957511663434, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:20, Epoch: 151, Batch: 340, Training Loss: 0.03355436995625496, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:21, Epoch: 151, Batch: 350, Training Loss: 0.029973560571670534, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:21, Epoch: 151, Batch: 360, Training Loss: 0.041397207602858546, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:22, Epoch: 151, Batch: 370, Training Loss: 0.043349362909793854, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:23, Epoch: 151, Batch: 380, Training Loss: 0.02999250628054142, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:24, Epoch: 151, Batch: 390, Training Loss: 0.033949428051710126, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:25, Epoch: 151, Batch: 400, Training Loss: 0.04457387179136276, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:25, Epoch: 151, Batch: 410, Training Loss: 0.035653142258524895, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:26, Epoch: 151, Batch: 420, Training Loss: 0.034507011994719505, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:27, Epoch: 151, Batch: 430, Training Loss: 0.027250827103853226, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:28, Epoch: 151, Batch: 440, Training Loss: 0.03498268127441406, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:28, Epoch: 151, Batch: 450, Training Loss: 0.038117269054055214, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:29, Epoch: 151, Batch: 460, Training Loss: 0.058887722343206404, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:30, Epoch: 151, Batch: 470, Training Loss: 0.047772341221570966, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:31, Epoch: 151, Batch: 480, Training Loss: 0.04797774814069271, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:31, Epoch: 151, Batch: 490, Training Loss: 0.041899102926254275, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:32, Epoch: 151, Batch: 500, Training Loss: 0.03700469918549061, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:33, Epoch: 151, Batch: 510, Training Loss: 0.031548908725380895, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:34, Epoch: 151, Batch: 520, Training Loss: 0.05170328728854656, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:35, Epoch: 151, Batch: 530, Training Loss: 0.04517281092703342, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:35, Epoch: 151, Batch: 540, Training Loss: 0.026564735919237137, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:36, Epoch: 151, Batch: 550, Training Loss: 0.04761419296264648, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:37, Epoch: 151, Batch: 560, Training Loss: 0.04024171233177185, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:38, Epoch: 151, Batch: 570, Training Loss: 0.028967392817139625, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:38, Epoch: 151, Batch: 580, Training Loss: 0.049758315458893775, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:39, Epoch: 151, Batch: 590, Training Loss: 0.029438380151987076, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:40, Epoch: 151, Batch: 600, Training Loss: 0.030394261702895164, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:41, Epoch: 151, Batch: 610, Training Loss: 0.05913983210921288, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:42, Epoch: 151, Batch: 620, Training Loss: 0.04241306371986866, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:42, Epoch: 151, Batch: 630, Training Loss: 0.04247276820242405, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:43, Epoch: 151, Batch: 640, Training Loss: 0.05431682281196117, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:44, Epoch: 151, Batch: 650, Training Loss: 0.031981130689382554, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:45, Epoch: 151, Batch: 660, Training Loss: 0.03998354524374008, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:45, Epoch: 151, Batch: 670, Training Loss: 0.04991166442632675, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:46, Epoch: 151, Batch: 680, Training Loss: 0.04372535720467567, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:47, Epoch: 151, Batch: 690, Training Loss: 0.02656768634915352, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:48, Epoch: 151, Batch: 700, Training Loss: 0.036986881867051125, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:48, Epoch: 151, Batch: 710, Training Loss: 0.048529833927750586, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:49, Epoch: 151, Batch: 720, Training Loss: 0.0425642479211092, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:50, Epoch: 151, Batch: 730, Training Loss: 0.044734416902065276, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:51, Epoch: 151, Batch: 740, Training Loss: 0.053662759438157084, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:51, Epoch: 151, Batch: 750, Training Loss: 0.023948049917817116, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:52, Epoch: 151, Batch: 760, Training Loss: 0.03781392201781273, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:53, Epoch: 151, Batch: 770, Training Loss: 0.04522967338562012, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:54, Epoch: 151, Batch: 780, Training Loss: 0.060874132066965105, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:54, Epoch: 151, Batch: 790, Training Loss: 0.04340667463839054, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:55, Epoch: 151, Batch: 800, Training Loss: 0.03531293720006943, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:56, Epoch: 151, Batch: 810, Training Loss: 0.045287984609603885, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:57, Epoch: 151, Batch: 820, Training Loss: 0.03745003491640091, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:57, Epoch: 151, Batch: 830, Training Loss: 0.03333640769124031, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:58, Epoch: 151, Batch: 840, Training Loss: 0.051200602948665616, LR: 0.00010000000000000003
Time, 2019-01-01T21:21:59, Epoch: 151, Batch: 850, Training Loss: 0.03296405263245106, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:00, Epoch: 151, Batch: 860, Training Loss: 0.031094752624630927, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:00, Epoch: 151, Batch: 870, Training Loss: 0.043088006973266604, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:01, Epoch: 151, Batch: 880, Training Loss: 0.05807430222630501, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:02, Epoch: 151, Batch: 890, Training Loss: 0.03845697119832039, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:03, Epoch: 151, Batch: 900, Training Loss: 0.04159046597778797, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:03, Epoch: 151, Batch: 910, Training Loss: 0.03099822849035263, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:04, Epoch: 151, Batch: 920, Training Loss: 0.04675249382853508, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:05, Epoch: 151, Batch: 930, Training Loss: 0.03161981143057346, LR: 0.00010000000000000003
Epoch: 151, Validation Top 1 acc: 98.9089126586914
Epoch: 151, Validation Top 5 acc: 99.99166870117188
Epoch: 151, Validation Set Loss: 0.04041442275047302
Start training epoch 152
Time, 2019-01-01T21:22:33, Epoch: 152, Batch: 10, Training Loss: 0.028528710082173347, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:34, Epoch: 152, Batch: 20, Training Loss: 0.05316676013171673, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:35, Epoch: 152, Batch: 30, Training Loss: 0.034098105132579805, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:35, Epoch: 152, Batch: 40, Training Loss: 0.03886484652757645, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:36, Epoch: 152, Batch: 50, Training Loss: 0.039814645797014235, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:37, Epoch: 152, Batch: 60, Training Loss: 0.051533349230885504, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:38, Epoch: 152, Batch: 70, Training Loss: 0.03412589840590954, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:38, Epoch: 152, Batch: 80, Training Loss: 0.05035539157688618, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:39, Epoch: 152, Batch: 90, Training Loss: 0.04718523621559143, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:40, Epoch: 152, Batch: 100, Training Loss: 0.06131657958030701, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:40, Epoch: 152, Batch: 110, Training Loss: 0.061679623648524286, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:41, Epoch: 152, Batch: 120, Training Loss: 0.0374364759773016, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:42, Epoch: 152, Batch: 130, Training Loss: 0.05722122862935066, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:43, Epoch: 152, Batch: 140, Training Loss: 0.039995094388723375, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:43, Epoch: 152, Batch: 150, Training Loss: 0.050394580513238904, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:44, Epoch: 152, Batch: 160, Training Loss: 0.03363240733742714, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:45, Epoch: 152, Batch: 170, Training Loss: 0.03370470181107521, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:45, Epoch: 152, Batch: 180, Training Loss: 0.04431123062968254, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:46, Epoch: 152, Batch: 190, Training Loss: 0.04122998863458634, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:47, Epoch: 152, Batch: 200, Training Loss: 0.038684215396642685, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:48, Epoch: 152, Batch: 210, Training Loss: 0.05209602750837803, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:48, Epoch: 152, Batch: 220, Training Loss: 0.040275825560092925, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:49, Epoch: 152, Batch: 230, Training Loss: 0.029836903139948846, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:50, Epoch: 152, Batch: 240, Training Loss: 0.04119837433099747, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:51, Epoch: 152, Batch: 250, Training Loss: 0.05890398770570755, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:51, Epoch: 152, Batch: 260, Training Loss: 0.03977532908320427, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:52, Epoch: 152, Batch: 270, Training Loss: 0.02842230647802353, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:53, Epoch: 152, Batch: 280, Training Loss: 0.045243335887789726, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:53, Epoch: 152, Batch: 290, Training Loss: 0.03860256373882294, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:54, Epoch: 152, Batch: 300, Training Loss: 0.0423246156424284, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:55, Epoch: 152, Batch: 310, Training Loss: 0.02960026226937771, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:56, Epoch: 152, Batch: 320, Training Loss: 0.030945568904280664, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:56, Epoch: 152, Batch: 330, Training Loss: 0.026582739129662514, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:57, Epoch: 152, Batch: 340, Training Loss: 0.04704671390354633, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:58, Epoch: 152, Batch: 350, Training Loss: 0.051387841999530795, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:59, Epoch: 152, Batch: 360, Training Loss: 0.043775564432144164, LR: 0.00010000000000000003
Time, 2019-01-01T21:22:59, Epoch: 152, Batch: 370, Training Loss: 0.03924061357975006, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:00, Epoch: 152, Batch: 380, Training Loss: 0.025014341250061987, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:01, Epoch: 152, Batch: 390, Training Loss: 0.047473470121622084, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:02, Epoch: 152, Batch: 400, Training Loss: 0.038467176258563995, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:02, Epoch: 152, Batch: 410, Training Loss: 0.04951705187559128, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:03, Epoch: 152, Batch: 420, Training Loss: 0.03860866911709308, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:04, Epoch: 152, Batch: 430, Training Loss: 0.028088749572634696, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:05, Epoch: 152, Batch: 440, Training Loss: 0.04391989894211292, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:05, Epoch: 152, Batch: 450, Training Loss: 0.04383977055549622, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:06, Epoch: 152, Batch: 460, Training Loss: 0.036755966022610664, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:07, Epoch: 152, Batch: 470, Training Loss: 0.03288092687726021, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:08, Epoch: 152, Batch: 480, Training Loss: 0.05581263788044453, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:08, Epoch: 152, Batch: 490, Training Loss: 0.03626288250088692, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:09, Epoch: 152, Batch: 500, Training Loss: 0.041786929592490196, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:10, Epoch: 152, Batch: 510, Training Loss: 0.048936644196510316, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:10, Epoch: 152, Batch: 520, Training Loss: 0.02906886860728264, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:11, Epoch: 152, Batch: 530, Training Loss: 0.02599974423646927, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:12, Epoch: 152, Batch: 540, Training Loss: 0.03309759870171547, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:13, Epoch: 152, Batch: 550, Training Loss: 0.0400314450263977, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:13, Epoch: 152, Batch: 560, Training Loss: 0.03962291404604912, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:14, Epoch: 152, Batch: 570, Training Loss: 0.03250319920480251, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:15, Epoch: 152, Batch: 580, Training Loss: 0.037148427590727805, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:16, Epoch: 152, Batch: 590, Training Loss: 0.03743056133389473, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:16, Epoch: 152, Batch: 600, Training Loss: 0.034219682961702344, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:17, Epoch: 152, Batch: 610, Training Loss: 0.02989342100918293, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:18, Epoch: 152, Batch: 620, Training Loss: 0.0358954019844532, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:18, Epoch: 152, Batch: 630, Training Loss: 0.036915674805641174, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:19, Epoch: 152, Batch: 640, Training Loss: 0.030250856280326845, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:20, Epoch: 152, Batch: 650, Training Loss: 0.03498232513666153, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:21, Epoch: 152, Batch: 660, Training Loss: 0.03901474177837372, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:21, Epoch: 152, Batch: 670, Training Loss: 0.06413541473448277, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:22, Epoch: 152, Batch: 680, Training Loss: 0.05302685908973217, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:23, Epoch: 152, Batch: 690, Training Loss: 0.03460480198264122, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:23, Epoch: 152, Batch: 700, Training Loss: 0.03322219401597977, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:24, Epoch: 152, Batch: 710, Training Loss: 0.0360275749117136, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:25, Epoch: 152, Batch: 720, Training Loss: 0.06692942306399345, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:26, Epoch: 152, Batch: 730, Training Loss: 0.047971824929118154, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:26, Epoch: 152, Batch: 740, Training Loss: 0.042460649833083154, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:27, Epoch: 152, Batch: 750, Training Loss: 0.049208573251962665, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:28, Epoch: 152, Batch: 760, Training Loss: 0.03271219544112682, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:28, Epoch: 152, Batch: 770, Training Loss: 0.04450623244047165, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:29, Epoch: 152, Batch: 780, Training Loss: 0.051362020522356035, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:30, Epoch: 152, Batch: 790, Training Loss: 0.03313291184604168, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:31, Epoch: 152, Batch: 800, Training Loss: 0.04013445191085339, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:31, Epoch: 152, Batch: 810, Training Loss: 0.056785456091165545, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:32, Epoch: 152, Batch: 820, Training Loss: 0.052128642052412036, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:33, Epoch: 152, Batch: 830, Training Loss: 0.03748736456036568, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:33, Epoch: 152, Batch: 840, Training Loss: 0.022209306061267853, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:34, Epoch: 152, Batch: 850, Training Loss: 0.0481841716915369, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:35, Epoch: 152, Batch: 860, Training Loss: 0.03977627083659172, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:36, Epoch: 152, Batch: 870, Training Loss: 0.04410912580788136, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:36, Epoch: 152, Batch: 880, Training Loss: 0.02649080418050289, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:37, Epoch: 152, Batch: 890, Training Loss: 0.038030318915843964, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:38, Epoch: 152, Batch: 900, Training Loss: 0.028752340748906136, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:38, Epoch: 152, Batch: 910, Training Loss: 0.026683570444583894, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:39, Epoch: 152, Batch: 920, Training Loss: 0.04518863335251808, LR: 0.00010000000000000003
Time, 2019-01-01T21:23:40, Epoch: 152, Batch: 930, Training Loss: 0.03701205253601074, LR: 0.00010000000000000003
Epoch: 152, Validation Top 1 acc: 98.91558074951172
Epoch: 152, Validation Top 5 acc: 99.99000549316406
Epoch: 152, Validation Set Loss: 0.04049644246697426
Start training epoch 153
Time, 2019-01-01T21:24:09, Epoch: 153, Batch: 10, Training Loss: 0.04659200496971607, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:09, Epoch: 153, Batch: 20, Training Loss: 0.05381196327507496, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:10, Epoch: 153, Batch: 30, Training Loss: 0.06435247734189034, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:11, Epoch: 153, Batch: 40, Training Loss: 0.047327028959989546, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:12, Epoch: 153, Batch: 50, Training Loss: 0.04114034548401833, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:12, Epoch: 153, Batch: 60, Training Loss: 0.03495225235819817, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:13, Epoch: 153, Batch: 70, Training Loss: 0.028816384449601173, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:14, Epoch: 153, Batch: 80, Training Loss: 0.04105313494801521, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:15, Epoch: 153, Batch: 90, Training Loss: 0.04188499264419079, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:16, Epoch: 153, Batch: 100, Training Loss: 0.029671278223395346, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:17, Epoch: 153, Batch: 110, Training Loss: 0.028623942658305167, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:17, Epoch: 153, Batch: 120, Training Loss: 0.04186203181743622, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:18, Epoch: 153, Batch: 130, Training Loss: 0.03299763649702072, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:19, Epoch: 153, Batch: 140, Training Loss: 0.027700489014387132, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:20, Epoch: 153, Batch: 150, Training Loss: 0.04060130417346954, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:20, Epoch: 153, Batch: 160, Training Loss: 0.02650485560297966, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:21, Epoch: 153, Batch: 170, Training Loss: 0.03217974379658699, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:22, Epoch: 153, Batch: 180, Training Loss: 0.04503338970243931, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:22, Epoch: 153, Batch: 190, Training Loss: 0.04172713570296764, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:23, Epoch: 153, Batch: 200, Training Loss: 0.03275144286453724, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:24, Epoch: 153, Batch: 210, Training Loss: 0.03492172248661518, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:25, Epoch: 153, Batch: 220, Training Loss: 0.045026345551013945, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:25, Epoch: 153, Batch: 230, Training Loss: 0.049896682426333425, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:26, Epoch: 153, Batch: 240, Training Loss: 0.05152766294777393, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:27, Epoch: 153, Batch: 250, Training Loss: 0.04638337716460228, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:28, Epoch: 153, Batch: 260, Training Loss: 0.03565296530723572, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:28, Epoch: 153, Batch: 270, Training Loss: 0.0511581651866436, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:29, Epoch: 153, Batch: 280, Training Loss: 0.04770496934652328, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:30, Epoch: 153, Batch: 290, Training Loss: 0.033149002492427825, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:31, Epoch: 153, Batch: 300, Training Loss: 0.0430148996412754, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:31, Epoch: 153, Batch: 310, Training Loss: 0.05100028552114964, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:32, Epoch: 153, Batch: 320, Training Loss: 0.03892158344388008, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:33, Epoch: 153, Batch: 330, Training Loss: 0.047722576558589934, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:34, Epoch: 153, Batch: 340, Training Loss: 0.03838118836283684, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:34, Epoch: 153, Batch: 350, Training Loss: 0.028727751597762106, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:35, Epoch: 153, Batch: 360, Training Loss: 0.030143699049949645, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:36, Epoch: 153, Batch: 370, Training Loss: 0.031404634565114976, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:37, Epoch: 153, Batch: 380, Training Loss: 0.05522548630833626, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:37, Epoch: 153, Batch: 390, Training Loss: 0.0502091683447361, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:38, Epoch: 153, Batch: 400, Training Loss: 0.03685397431254387, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:39, Epoch: 153, Batch: 410, Training Loss: 0.0243070051074028, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:40, Epoch: 153, Batch: 420, Training Loss: 0.039595789089798925, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:40, Epoch: 153, Batch: 430, Training Loss: 0.02458108440041542, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:41, Epoch: 153, Batch: 440, Training Loss: 0.04275140874087811, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:42, Epoch: 153, Batch: 450, Training Loss: 0.03233682587742805, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:42, Epoch: 153, Batch: 460, Training Loss: 0.05576226897537708, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:43, Epoch: 153, Batch: 470, Training Loss: 0.0317367821931839, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:44, Epoch: 153, Batch: 480, Training Loss: 0.0320907823741436, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:45, Epoch: 153, Batch: 490, Training Loss: 0.05278630703687668, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:45, Epoch: 153, Batch: 500, Training Loss: 0.033539269119501114, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:46, Epoch: 153, Batch: 510, Training Loss: 0.037717010080814364, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:47, Epoch: 153, Batch: 520, Training Loss: 0.045945975184440616, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:48, Epoch: 153, Batch: 530, Training Loss: 0.031017789617180824, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:48, Epoch: 153, Batch: 540, Training Loss: 0.029787852242588998, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:49, Epoch: 153, Batch: 550, Training Loss: 0.032323754578828814, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:50, Epoch: 153, Batch: 560, Training Loss: 0.06457055509090423, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:51, Epoch: 153, Batch: 570, Training Loss: 0.037498437240719794, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:51, Epoch: 153, Batch: 580, Training Loss: 0.02867990955710411, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:52, Epoch: 153, Batch: 590, Training Loss: 0.03862309455871582, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:53, Epoch: 153, Batch: 600, Training Loss: 0.04394511580467224, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:54, Epoch: 153, Batch: 610, Training Loss: 0.03712436556816101, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:54, Epoch: 153, Batch: 620, Training Loss: 0.031211329996585845, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:55, Epoch: 153, Batch: 630, Training Loss: 0.04157809466123581, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:56, Epoch: 153, Batch: 640, Training Loss: 0.042726929485797885, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:57, Epoch: 153, Batch: 650, Training Loss: 0.03710538297891617, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:57, Epoch: 153, Batch: 660, Training Loss: 0.044977547228336336, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:58, Epoch: 153, Batch: 670, Training Loss: 0.060539230704307556, LR: 0.00010000000000000003
Time, 2019-01-01T21:24:59, Epoch: 153, Batch: 680, Training Loss: 0.06623631343245506, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:00, Epoch: 153, Batch: 690, Training Loss: 0.06560841500759125, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:00, Epoch: 153, Batch: 700, Training Loss: 0.047403079271316526, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:01, Epoch: 153, Batch: 710, Training Loss: 0.035255680605769155, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:02, Epoch: 153, Batch: 720, Training Loss: 0.031636086851358415, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:03, Epoch: 153, Batch: 730, Training Loss: 0.03502728566527367, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:03, Epoch: 153, Batch: 740, Training Loss: 0.03892691917717457, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:04, Epoch: 153, Batch: 750, Training Loss: 0.028968528658151627, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:05, Epoch: 153, Batch: 760, Training Loss: 0.0451750747859478, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:06, Epoch: 153, Batch: 770, Training Loss: 0.035137438029050824, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:06, Epoch: 153, Batch: 780, Training Loss: 0.03789304457604885, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:07, Epoch: 153, Batch: 790, Training Loss: 0.055931493639945984, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:08, Epoch: 153, Batch: 800, Training Loss: 0.04862789548933506, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:08, Epoch: 153, Batch: 810, Training Loss: 0.0407086431980133, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:09, Epoch: 153, Batch: 820, Training Loss: 0.03387124128639698, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:10, Epoch: 153, Batch: 830, Training Loss: 0.030902956426143647, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:11, Epoch: 153, Batch: 840, Training Loss: 0.039949516952037814, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:11, Epoch: 153, Batch: 850, Training Loss: 0.045202203840017316, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:12, Epoch: 153, Batch: 860, Training Loss: 0.03619353249669075, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:13, Epoch: 153, Batch: 870, Training Loss: 0.04788896404206753, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:14, Epoch: 153, Batch: 880, Training Loss: 0.05054678693413735, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:14, Epoch: 153, Batch: 890, Training Loss: 0.036129212751984593, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:15, Epoch: 153, Batch: 900, Training Loss: 0.03264769464731217, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:16, Epoch: 153, Batch: 910, Training Loss: 0.04468953534960747, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:16, Epoch: 153, Batch: 920, Training Loss: 0.04503804892301559, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:17, Epoch: 153, Batch: 930, Training Loss: 0.044918882101774214, LR: 0.00010000000000000003
Epoch: 153, Validation Top 1 acc: 98.91058349609375
Epoch: 153, Validation Top 5 acc: 99.99166870117188
Epoch: 153, Validation Set Loss: 0.040463127195835114
Start training epoch 154
Time, 2019-01-01T21:25:47, Epoch: 154, Batch: 10, Training Loss: 0.037364103645086286, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:48, Epoch: 154, Batch: 20, Training Loss: 0.03612480275332928, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:49, Epoch: 154, Batch: 30, Training Loss: 0.0453720286488533, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:50, Epoch: 154, Batch: 40, Training Loss: 0.05118880718946457, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:50, Epoch: 154, Batch: 50, Training Loss: 0.031223056837916374, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:51, Epoch: 154, Batch: 60, Training Loss: 0.02856651656329632, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:52, Epoch: 154, Batch: 70, Training Loss: 0.042373863607645036, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:53, Epoch: 154, Batch: 80, Training Loss: 0.036608514934778215, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:53, Epoch: 154, Batch: 90, Training Loss: 0.04836429730057716, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:54, Epoch: 154, Batch: 100, Training Loss: 0.040042205899953845, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:55, Epoch: 154, Batch: 110, Training Loss: 0.0446172870695591, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:55, Epoch: 154, Batch: 120, Training Loss: 0.02962680086493492, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:56, Epoch: 154, Batch: 130, Training Loss: 0.04619724825024605, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:57, Epoch: 154, Batch: 140, Training Loss: 0.05294562391936779, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:58, Epoch: 154, Batch: 150, Training Loss: 0.04409418702125549, LR: 0.00010000000000000003
Time, 2019-01-01T21:25:59, Epoch: 154, Batch: 160, Training Loss: 0.031799943372607234, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:00, Epoch: 154, Batch: 170, Training Loss: 0.05446605309844017, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:01, Epoch: 154, Batch: 180, Training Loss: 0.037842852622270585, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:02, Epoch: 154, Batch: 190, Training Loss: 0.03870094306766987, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:02, Epoch: 154, Batch: 200, Training Loss: 0.044828581437468526, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:03, Epoch: 154, Batch: 210, Training Loss: 0.03663590997457504, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:04, Epoch: 154, Batch: 220, Training Loss: 0.04533548392355442, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:04, Epoch: 154, Batch: 230, Training Loss: 0.03280298225581646, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:05, Epoch: 154, Batch: 240, Training Loss: 0.039346878230571744, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:06, Epoch: 154, Batch: 250, Training Loss: 0.05372971445322037, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:07, Epoch: 154, Batch: 260, Training Loss: 0.05641246624290943, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:07, Epoch: 154, Batch: 270, Training Loss: 0.029756296798586844, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:08, Epoch: 154, Batch: 280, Training Loss: 0.04182858727872372, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:09, Epoch: 154, Batch: 290, Training Loss: 0.03858153633773327, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:10, Epoch: 154, Batch: 300, Training Loss: 0.05340892896056175, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:10, Epoch: 154, Batch: 310, Training Loss: 0.04502258822321892, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:11, Epoch: 154, Batch: 320, Training Loss: 0.04306831881403923, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:12, Epoch: 154, Batch: 330, Training Loss: 0.055964280292391776, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:13, Epoch: 154, Batch: 340, Training Loss: 0.04435574784874916, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:14, Epoch: 154, Batch: 350, Training Loss: 0.03646377921104431, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:15, Epoch: 154, Batch: 360, Training Loss: 0.038667697086930276, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:15, Epoch: 154, Batch: 370, Training Loss: 0.04265095591545105, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:16, Epoch: 154, Batch: 380, Training Loss: 0.043065615743398664, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:17, Epoch: 154, Batch: 390, Training Loss: 0.04999222047626972, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:18, Epoch: 154, Batch: 400, Training Loss: 0.042528802528977394, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:19, Epoch: 154, Batch: 410, Training Loss: 0.04443354830145836, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:19, Epoch: 154, Batch: 420, Training Loss: 0.03488232046365738, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:20, Epoch: 154, Batch: 430, Training Loss: 0.03479311540722847, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:21, Epoch: 154, Batch: 440, Training Loss: 0.032354867458343504, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:22, Epoch: 154, Batch: 450, Training Loss: 0.02726869359612465, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:22, Epoch: 154, Batch: 460, Training Loss: 0.03423121199011803, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:23, Epoch: 154, Batch: 470, Training Loss: 0.041055941581726076, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:24, Epoch: 154, Batch: 480, Training Loss: 0.04078937843441963, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:25, Epoch: 154, Batch: 490, Training Loss: 0.038693641871213914, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:25, Epoch: 154, Batch: 500, Training Loss: 0.04176783040165901, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:26, Epoch: 154, Batch: 510, Training Loss: 0.032597743719816205, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:27, Epoch: 154, Batch: 520, Training Loss: 0.05306938514113426, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:28, Epoch: 154, Batch: 530, Training Loss: 0.034571827948093416, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:29, Epoch: 154, Batch: 540, Training Loss: 0.035062506794929504, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:30, Epoch: 154, Batch: 550, Training Loss: 0.039904259517788886, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:31, Epoch: 154, Batch: 560, Training Loss: 0.03309635780751705, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:32, Epoch: 154, Batch: 570, Training Loss: 0.04085392057895661, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:33, Epoch: 154, Batch: 580, Training Loss: 0.04846511520445347, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:33, Epoch: 154, Batch: 590, Training Loss: 0.030654096603393556, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:34, Epoch: 154, Batch: 600, Training Loss: 0.04490813314914703, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:35, Epoch: 154, Batch: 610, Training Loss: 0.053595485910773275, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:36, Epoch: 154, Batch: 620, Training Loss: 0.03551550954580307, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:36, Epoch: 154, Batch: 630, Training Loss: 0.047212503477931024, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:37, Epoch: 154, Batch: 640, Training Loss: 0.022714066505432128, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:38, Epoch: 154, Batch: 650, Training Loss: 0.05324367098510265, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:39, Epoch: 154, Batch: 660, Training Loss: 0.048554117977619174, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:39, Epoch: 154, Batch: 670, Training Loss: 0.030614452064037324, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:40, Epoch: 154, Batch: 680, Training Loss: 0.056383041664958, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:41, Epoch: 154, Batch: 690, Training Loss: 0.047202978655695914, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:42, Epoch: 154, Batch: 700, Training Loss: 0.03348546288907528, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:42, Epoch: 154, Batch: 710, Training Loss: 0.034421733021736144, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:43, Epoch: 154, Batch: 720, Training Loss: 0.035811954736709596, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:44, Epoch: 154, Batch: 730, Training Loss: 0.0451748613268137, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:45, Epoch: 154, Batch: 740, Training Loss: 0.04088066890835762, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:45, Epoch: 154, Batch: 750, Training Loss: 0.04417358934879303, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:46, Epoch: 154, Batch: 760, Training Loss: 0.02819327563047409, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:47, Epoch: 154, Batch: 770, Training Loss: 0.0377097375690937, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:48, Epoch: 154, Batch: 780, Training Loss: 0.04322753734886646, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:49, Epoch: 154, Batch: 790, Training Loss: 0.042443306371569636, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:49, Epoch: 154, Batch: 800, Training Loss: 0.036669214442372325, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:50, Epoch: 154, Batch: 810, Training Loss: 0.038637261092662814, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:51, Epoch: 154, Batch: 820, Training Loss: 0.023777035623788835, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:52, Epoch: 154, Batch: 830, Training Loss: 0.04729307368397713, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:52, Epoch: 154, Batch: 840, Training Loss: 0.04486081413924694, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:53, Epoch: 154, Batch: 850, Training Loss: 0.03998926319181919, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:54, Epoch: 154, Batch: 860, Training Loss: 0.044299249351024625, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:55, Epoch: 154, Batch: 870, Training Loss: 0.043061552196741106, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:56, Epoch: 154, Batch: 880, Training Loss: 0.02987976111471653, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:56, Epoch: 154, Batch: 890, Training Loss: 0.03559603914618492, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:57, Epoch: 154, Batch: 900, Training Loss: 0.03254953734576702, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:58, Epoch: 154, Batch: 910, Training Loss: 0.039728694409132, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:59, Epoch: 154, Batch: 920, Training Loss: 0.03005271889269352, LR: 0.00010000000000000003
Time, 2019-01-01T21:26:59, Epoch: 154, Batch: 930, Training Loss: 0.05135537795722485, LR: 0.00010000000000000003
Epoch: 154, Validation Top 1 acc: 98.90391540527344
Epoch: 154, Validation Top 5 acc: 99.99000549316406
Epoch: 154, Validation Set Loss: 0.040496960282325745
Start training epoch 155
Time, 2019-01-01T21:27:27, Epoch: 155, Batch: 10, Training Loss: 0.036628997698426244, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:28, Epoch: 155, Batch: 20, Training Loss: 0.05233884863555431, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:29, Epoch: 155, Batch: 30, Training Loss: 0.04430155754089356, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:29, Epoch: 155, Batch: 40, Training Loss: 0.035171762481331824, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:30, Epoch: 155, Batch: 50, Training Loss: 0.04411372169852257, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:31, Epoch: 155, Batch: 60, Training Loss: 0.035210036858916285, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:31, Epoch: 155, Batch: 70, Training Loss: 0.03918497711420059, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:32, Epoch: 155, Batch: 80, Training Loss: 0.03413873389363289, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:33, Epoch: 155, Batch: 90, Training Loss: 0.03942613676190376, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:34, Epoch: 155, Batch: 100, Training Loss: 0.047823548316955566, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:34, Epoch: 155, Batch: 110, Training Loss: 0.044874048605561254, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:35, Epoch: 155, Batch: 120, Training Loss: 0.04787263385951519, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:36, Epoch: 155, Batch: 130, Training Loss: 0.029884686693549156, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:36, Epoch: 155, Batch: 140, Training Loss: 0.035351771861314774, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:37, Epoch: 155, Batch: 150, Training Loss: 0.03381836414337158, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:38, Epoch: 155, Batch: 160, Training Loss: 0.057642412930727006, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:38, Epoch: 155, Batch: 170, Training Loss: 0.04610191956162453, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:39, Epoch: 155, Batch: 180, Training Loss: 0.0621166467666626, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:40, Epoch: 155, Batch: 190, Training Loss: 0.04437099546194077, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:41, Epoch: 155, Batch: 200, Training Loss: 0.03915592730045318, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:41, Epoch: 155, Batch: 210, Training Loss: 0.022619976475834847, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:42, Epoch: 155, Batch: 220, Training Loss: 0.039126220345497134, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:43, Epoch: 155, Batch: 230, Training Loss: 0.04570887871086597, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:43, Epoch: 155, Batch: 240, Training Loss: 0.06115095093846321, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:44, Epoch: 155, Batch: 250, Training Loss: 0.046579180657863616, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:45, Epoch: 155, Batch: 260, Training Loss: 0.0482541099190712, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:46, Epoch: 155, Batch: 270, Training Loss: 0.03415824994444847, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:46, Epoch: 155, Batch: 280, Training Loss: 0.058402632176876065, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:47, Epoch: 155, Batch: 290, Training Loss: 0.04265354610979557, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:48, Epoch: 155, Batch: 300, Training Loss: 0.031195272132754324, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:49, Epoch: 155, Batch: 310, Training Loss: 0.046577959507703784, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:49, Epoch: 155, Batch: 320, Training Loss: 0.04210372045636177, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:50, Epoch: 155, Batch: 330, Training Loss: 0.0283783208578825, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:51, Epoch: 155, Batch: 340, Training Loss: 0.044671455025672914, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:51, Epoch: 155, Batch: 350, Training Loss: 0.039320258051157, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:52, Epoch: 155, Batch: 360, Training Loss: 0.045289003476500514, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:53, Epoch: 155, Batch: 370, Training Loss: 0.040185898169875146, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:54, Epoch: 155, Batch: 380, Training Loss: 0.030933847650885582, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:55, Epoch: 155, Batch: 390, Training Loss: 0.0630061574280262, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:55, Epoch: 155, Batch: 400, Training Loss: 0.04523092359304428, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:56, Epoch: 155, Batch: 410, Training Loss: 0.032645370438694955, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:57, Epoch: 155, Batch: 420, Training Loss: 0.01970341205596924, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:58, Epoch: 155, Batch: 430, Training Loss: 0.06754195541143418, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:58, Epoch: 155, Batch: 440, Training Loss: 0.04922818131744862, LR: 0.00010000000000000003
Time, 2019-01-01T21:27:59, Epoch: 155, Batch: 450, Training Loss: 0.030670664459466934, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:00, Epoch: 155, Batch: 460, Training Loss: 0.027106715366244316, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:00, Epoch: 155, Batch: 470, Training Loss: 0.030153321102261543, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:01, Epoch: 155, Batch: 480, Training Loss: 0.027623242139816283, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:02, Epoch: 155, Batch: 490, Training Loss: 0.02879837602376938, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:03, Epoch: 155, Batch: 500, Training Loss: 0.03568167164921761, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:03, Epoch: 155, Batch: 510, Training Loss: 0.032920371368527414, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:04, Epoch: 155, Batch: 520, Training Loss: 0.03812154680490494, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:05, Epoch: 155, Batch: 530, Training Loss: 0.042775148153305055, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:06, Epoch: 155, Batch: 540, Training Loss: 0.03789349049329758, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:06, Epoch: 155, Batch: 550, Training Loss: 0.030700766295194627, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:07, Epoch: 155, Batch: 560, Training Loss: 0.038097473606467244, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:08, Epoch: 155, Batch: 570, Training Loss: 0.032151835039258, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:08, Epoch: 155, Batch: 580, Training Loss: 0.059896206855773924, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:09, Epoch: 155, Batch: 590, Training Loss: 0.030538979545235635, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:10, Epoch: 155, Batch: 600, Training Loss: 0.040823265165090564, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:11, Epoch: 155, Batch: 610, Training Loss: 0.027894971892237663, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:11, Epoch: 155, Batch: 620, Training Loss: 0.041724496707320216, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:12, Epoch: 155, Batch: 630, Training Loss: 0.05143288597464561, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:13, Epoch: 155, Batch: 640, Training Loss: 0.0408196747303009, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:14, Epoch: 155, Batch: 650, Training Loss: 0.034805386513471606, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:14, Epoch: 155, Batch: 660, Training Loss: 0.045035170763731, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:15, Epoch: 155, Batch: 670, Training Loss: 0.03614317998290062, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:16, Epoch: 155, Batch: 680, Training Loss: 0.039792663604021075, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:16, Epoch: 155, Batch: 690, Training Loss: 0.03052782006561756, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:17, Epoch: 155, Batch: 700, Training Loss: 0.04019966870546341, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:18, Epoch: 155, Batch: 710, Training Loss: 0.043419180810451506, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:19, Epoch: 155, Batch: 720, Training Loss: 0.027132924646139145, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:19, Epoch: 155, Batch: 730, Training Loss: 0.032868964970111846, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:20, Epoch: 155, Batch: 740, Training Loss: 0.043966830149292944, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:21, Epoch: 155, Batch: 750, Training Loss: 0.05157511569559574, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:22, Epoch: 155, Batch: 760, Training Loss: 0.03666809611022472, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:22, Epoch: 155, Batch: 770, Training Loss: 0.04059064649045467, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:23, Epoch: 155, Batch: 780, Training Loss: 0.041902405023574826, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:24, Epoch: 155, Batch: 790, Training Loss: 0.02488972097635269, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:25, Epoch: 155, Batch: 800, Training Loss: 0.03727159798145294, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:25, Epoch: 155, Batch: 810, Training Loss: 0.02450094521045685, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:26, Epoch: 155, Batch: 820, Training Loss: 0.055014684051275256, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:27, Epoch: 155, Batch: 830, Training Loss: 0.04501238502562046, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:28, Epoch: 155, Batch: 840, Training Loss: 0.035411881655454634, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:28, Epoch: 155, Batch: 850, Training Loss: 0.042292213439941405, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:29, Epoch: 155, Batch: 860, Training Loss: 0.05406774990260601, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:30, Epoch: 155, Batch: 870, Training Loss: 0.033570239692926405, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:31, Epoch: 155, Batch: 880, Training Loss: 0.05976181328296661, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:31, Epoch: 155, Batch: 890, Training Loss: 0.06281657293438911, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:32, Epoch: 155, Batch: 900, Training Loss: 0.03158736452460289, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:33, Epoch: 155, Batch: 910, Training Loss: 0.03605074323713779, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:34, Epoch: 155, Batch: 920, Training Loss: 0.03907742090523243, LR: 0.00010000000000000003
Time, 2019-01-01T21:28:34, Epoch: 155, Batch: 930, Training Loss: 0.038490789011120796, LR: 0.00010000000000000003
Epoch: 155, Validation Top 1 acc: 98.90558624267578
Epoch: 155, Validation Top 5 acc: 99.99166870117188
Epoch: 155, Validation Set Loss: 0.040410127490758896
Start training epoch 156
Time, 2019-01-01T21:29:03, Epoch: 156, Batch: 10, Training Loss: 0.024840685352683068, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:04, Epoch: 156, Batch: 20, Training Loss: 0.04599569961428642, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:04, Epoch: 156, Batch: 30, Training Loss: 0.04990164861083031, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:05, Epoch: 156, Batch: 40, Training Loss: 0.027588439732789995, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:06, Epoch: 156, Batch: 50, Training Loss: 0.04111583419144153, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:06, Epoch: 156, Batch: 60, Training Loss: 0.04925034269690513, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:07, Epoch: 156, Batch: 70, Training Loss: 0.029846609383821488, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:08, Epoch: 156, Batch: 80, Training Loss: 0.036622614786028865, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:09, Epoch: 156, Batch: 90, Training Loss: 0.05419429130852223, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:09, Epoch: 156, Batch: 100, Training Loss: 0.037944828346371653, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:10, Epoch: 156, Batch: 110, Training Loss: 0.0396993700414896, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:11, Epoch: 156, Batch: 120, Training Loss: 0.04120749086141586, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:12, Epoch: 156, Batch: 130, Training Loss: 0.05289905369281769, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:12, Epoch: 156, Batch: 140, Training Loss: 0.04571260809898377, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:13, Epoch: 156, Batch: 150, Training Loss: 0.04247366264462471, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:14, Epoch: 156, Batch: 160, Training Loss: 0.034842369705438615, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:14, Epoch: 156, Batch: 170, Training Loss: 0.03183335363864899, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:15, Epoch: 156, Batch: 180, Training Loss: 0.03600063621997833, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:16, Epoch: 156, Batch: 190, Training Loss: 0.02395997494459152, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:17, Epoch: 156, Batch: 200, Training Loss: 0.041221195831894875, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:17, Epoch: 156, Batch: 210, Training Loss: 0.05477844178676605, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:18, Epoch: 156, Batch: 220, Training Loss: 0.058394980058073996, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:19, Epoch: 156, Batch: 230, Training Loss: 0.029554858431220056, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:19, Epoch: 156, Batch: 240, Training Loss: 0.027073683589696883, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:20, Epoch: 156, Batch: 250, Training Loss: 0.05690336599946022, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:21, Epoch: 156, Batch: 260, Training Loss: 0.04469096213579178, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:22, Epoch: 156, Batch: 270, Training Loss: 0.029520874097943305, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:22, Epoch: 156, Batch: 280, Training Loss: 0.02265956960618496, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:23, Epoch: 156, Batch: 290, Training Loss: 0.03997289687395096, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:24, Epoch: 156, Batch: 300, Training Loss: 0.036751463636755945, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:25, Epoch: 156, Batch: 310, Training Loss: 0.035934672132134436, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:25, Epoch: 156, Batch: 320, Training Loss: 0.03969601318240166, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:26, Epoch: 156, Batch: 330, Training Loss: 0.03996754810214043, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:27, Epoch: 156, Batch: 340, Training Loss: 0.031209506466984747, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:28, Epoch: 156, Batch: 350, Training Loss: 0.036470353975892066, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:28, Epoch: 156, Batch: 360, Training Loss: 0.0427121102809906, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:29, Epoch: 156, Batch: 370, Training Loss: 0.03891024701297283, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:30, Epoch: 156, Batch: 380, Training Loss: 0.02960423640906811, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:30, Epoch: 156, Batch: 390, Training Loss: 0.05055614188313484, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:31, Epoch: 156, Batch: 400, Training Loss: 0.046019389480352405, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:32, Epoch: 156, Batch: 410, Training Loss: 0.029782801866531372, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:33, Epoch: 156, Batch: 420, Training Loss: 0.035517163947224616, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:34, Epoch: 156, Batch: 430, Training Loss: 0.05302300676703453, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:34, Epoch: 156, Batch: 440, Training Loss: 0.043754040449857715, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:35, Epoch: 156, Batch: 450, Training Loss: 0.06635967381298542, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:36, Epoch: 156, Batch: 460, Training Loss: 0.05567828193306923, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:37, Epoch: 156, Batch: 470, Training Loss: 0.036984550207853316, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:37, Epoch: 156, Batch: 480, Training Loss: 0.03160013929009438, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:38, Epoch: 156, Batch: 490, Training Loss: 0.04147269017994404, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:39, Epoch: 156, Batch: 500, Training Loss: 0.05648768953979015, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:40, Epoch: 156, Batch: 510, Training Loss: 0.02944502905011177, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:40, Epoch: 156, Batch: 520, Training Loss: 0.04074610210955143, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:41, Epoch: 156, Batch: 530, Training Loss: 0.03625412136316299, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:42, Epoch: 156, Batch: 540, Training Loss: 0.025439101457595825, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:43, Epoch: 156, Batch: 550, Training Loss: 0.042214245349168775, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:43, Epoch: 156, Batch: 560, Training Loss: 0.04291446246206761, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:44, Epoch: 156, Batch: 570, Training Loss: 0.05395512543618679, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:45, Epoch: 156, Batch: 580, Training Loss: 0.039797044917941095, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:46, Epoch: 156, Batch: 590, Training Loss: 0.03526613861322403, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:47, Epoch: 156, Batch: 600, Training Loss: 0.0362014889717102, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:47, Epoch: 156, Batch: 610, Training Loss: 0.025688819214701654, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:48, Epoch: 156, Batch: 620, Training Loss: 0.04632908031344414, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:49, Epoch: 156, Batch: 630, Training Loss: 0.04985918626189232, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:50, Epoch: 156, Batch: 640, Training Loss: 0.04629590585827827, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:50, Epoch: 156, Batch: 650, Training Loss: 0.036033326759934425, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:51, Epoch: 156, Batch: 660, Training Loss: 0.04932662770152092, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:52, Epoch: 156, Batch: 670, Training Loss: 0.05801231563091278, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:53, Epoch: 156, Batch: 680, Training Loss: 0.038638722896575925, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:53, Epoch: 156, Batch: 690, Training Loss: 0.038133838772773744, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:54, Epoch: 156, Batch: 700, Training Loss: 0.03296004608273506, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:55, Epoch: 156, Batch: 710, Training Loss: 0.028908992931246758, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:56, Epoch: 156, Batch: 720, Training Loss: 0.049095486849546434, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:57, Epoch: 156, Batch: 730, Training Loss: 0.05168137177824974, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:57, Epoch: 156, Batch: 740, Training Loss: 0.04921126365661621, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:58, Epoch: 156, Batch: 750, Training Loss: 0.04334617927670479, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:59, Epoch: 156, Batch: 760, Training Loss: 0.021826621145009995, LR: 0.00010000000000000003
Time, 2019-01-01T21:29:59, Epoch: 156, Batch: 770, Training Loss: 0.039004524052143094, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:00, Epoch: 156, Batch: 780, Training Loss: 0.0318877175450325, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:01, Epoch: 156, Batch: 790, Training Loss: 0.04073797613382339, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:02, Epoch: 156, Batch: 800, Training Loss: 0.04754549413919449, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:02, Epoch: 156, Batch: 810, Training Loss: 0.048000576347112654, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:03, Epoch: 156, Batch: 820, Training Loss: 0.040592454746365546, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:04, Epoch: 156, Batch: 830, Training Loss: 0.037254832684993744, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:05, Epoch: 156, Batch: 840, Training Loss: 0.05684988833963871, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:05, Epoch: 156, Batch: 850, Training Loss: 0.044234463945031166, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:06, Epoch: 156, Batch: 860, Training Loss: 0.05129745751619339, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:07, Epoch: 156, Batch: 870, Training Loss: 0.04042558297514916, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:07, Epoch: 156, Batch: 880, Training Loss: 0.04616196937859059, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:08, Epoch: 156, Batch: 890, Training Loss: 0.041581489518284796, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:09, Epoch: 156, Batch: 900, Training Loss: 0.03359425887465477, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:10, Epoch: 156, Batch: 910, Training Loss: 0.033945903182029724, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:10, Epoch: 156, Batch: 920, Training Loss: 0.033438117057085034, LR: 0.00010000000000000003
Time, 2019-01-01T21:30:11, Epoch: 156, Batch: 930, Training Loss: 0.02980959676206112, LR: 0.00010000000000000003
Epoch: 156, Validation Top 1 acc: 98.92057800292969
Epoch: 156, Validation Top 5 acc: 99.99166870117188
Epoch: 156, Validation Set Loss: 0.040387798100709915