----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 6, 28, 28]             156
              Tanh-2            [-1, 6, 28, 28]               0
         AvgPool2d-3            [-1, 6, 14, 14]               0
              Tanh-4            [-1, 6, 14, 14]               0
            Conv2d-5           [-1, 16, 10, 10]           2,416
              Tanh-6           [-1, 16, 10, 10]               0
         AvgPool2d-7             [-1, 16, 5, 5]               0
              Tanh-8             [-1, 16, 5, 5]               0
            Conv2d-9            [-1, 120, 1, 1]          48,120
             Tanh-10            [-1, 120, 1, 1]               0
           Linear-11                   [-1, 84]          10,164
             Tanh-12                   [-1, 84]               0
           Linear-13                   [-1, 10]             850
================================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.12
Params size (MB): 0.24
Estimated Total Size (MB): 0.36
----------------------------------------------------------------
Epoch: 0, Validation Top 1 acc: 10.718550682067871
Epoch: 0, Validation Top 5 acc: 55.11544418334961
Epoch: 0, Validation Set Loss: 2.2943458557128906
Start training epoch 1
Time, 2019-01-07T01:29:48, Epoch: 1, Batch: 10, Training Loss: 2.1330564260482787, LR: 0.001
Time, 2019-01-07T01:29:49, Epoch: 1, Batch: 20, Training Loss: 1.5609509825706482, LR: 0.001
Time, 2019-01-07T01:29:50, Epoch: 1, Batch: 30, Training Loss: 1.083481878042221, LR: 0.001
Time, 2019-01-07T01:29:51, Epoch: 1, Batch: 40, Training Loss: 0.8853368401527405, LR: 0.001
Time, 2019-01-07T01:29:53, Epoch: 1, Batch: 50, Training Loss: 0.6854679465293885, LR: 0.001
Time, 2019-01-07T01:29:54, Epoch: 1, Batch: 60, Training Loss: 0.6054766416549683, LR: 0.001
Time, 2019-01-07T01:29:55, Epoch: 1, Batch: 70, Training Loss: 0.5268504947423935, LR: 0.001
Time, 2019-01-07T01:29:56, Epoch: 1, Batch: 80, Training Loss: 0.48691863715648653, LR: 0.001
Time, 2019-01-07T01:29:58, Epoch: 1, Batch: 90, Training Loss: 0.4294642835855484, LR: 0.001
Time, 2019-01-07T01:29:59, Epoch: 1, Batch: 100, Training Loss: 0.40676910281181333, LR: 0.001
Time, 2019-01-07T01:30:00, Epoch: 1, Batch: 110, Training Loss: 0.3688649073243141, LR: 0.001
Time, 2019-01-07T01:30:02, Epoch: 1, Batch: 120, Training Loss: 0.30944238901138305, LR: 0.001
Time, 2019-01-07T01:30:03, Epoch: 1, Batch: 130, Training Loss: 0.30089755952358244, LR: 0.001
Time, 2019-01-07T01:30:04, Epoch: 1, Batch: 140, Training Loss: 0.2665337473154068, LR: 0.001
Time, 2019-01-07T01:30:05, Epoch: 1, Batch: 150, Training Loss: 0.3012449875473976, LR: 0.001
Time, 2019-01-07T01:30:07, Epoch: 1, Batch: 160, Training Loss: 0.2683669999241829, LR: 0.001
Time, 2019-01-07T01:30:08, Epoch: 1, Batch: 170, Training Loss: 0.2636556699872017, LR: 0.001
Time, 2019-01-07T01:30:09, Epoch: 1, Batch: 180, Training Loss: 0.2873569384217262, LR: 0.001
Time, 2019-01-07T01:30:10, Epoch: 1, Batch: 190, Training Loss: 0.19578439220786095, LR: 0.001
Time, 2019-01-07T01:30:11, Epoch: 1, Batch: 200, Training Loss: 0.23121825829148293, LR: 0.001
Time, 2019-01-07T01:30:13, Epoch: 1, Batch: 210, Training Loss: 0.1809037908911705, LR: 0.001
Time, 2019-01-07T01:30:14, Epoch: 1, Batch: 220, Training Loss: 0.2161002963781357, LR: 0.001
Time, 2019-01-07T01:30:15, Epoch: 1, Batch: 230, Training Loss: 0.21935095340013505, LR: 0.001
Time, 2019-01-07T01:30:17, Epoch: 1, Batch: 240, Training Loss: 0.2034095674753189, LR: 0.001
Time, 2019-01-07T01:30:18, Epoch: 1, Batch: 250, Training Loss: 0.17292148768901824, LR: 0.001
Time, 2019-01-07T01:30:20, Epoch: 1, Batch: 260, Training Loss: 0.1689014256000519, LR: 0.001
Time, 2019-01-07T01:30:21, Epoch: 1, Batch: 270, Training Loss: 0.1680861733853817, LR: 0.001
Time, 2019-01-07T01:30:22, Epoch: 1, Batch: 280, Training Loss: 0.14467762485146524, LR: 0.001
Time, 2019-01-07T01:30:24, Epoch: 1, Batch: 290, Training Loss: 0.1675448551774025, LR: 0.001
Time, 2019-01-07T01:30:25, Epoch: 1, Batch: 300, Training Loss: 0.1470084995031357, LR: 0.001
Time, 2019-01-07T01:30:26, Epoch: 1, Batch: 310, Training Loss: 0.16541306339204312, LR: 0.001
Time, 2019-01-07T01:30:27, Epoch: 1, Batch: 320, Training Loss: 0.16745035499334335, LR: 0.001
Time, 2019-01-07T01:30:28, Epoch: 1, Batch: 330, Training Loss: 0.18714680448174476, LR: 0.001
Time, 2019-01-07T01:30:29, Epoch: 1, Batch: 340, Training Loss: 0.16918399557471275, LR: 0.001
Time, 2019-01-07T01:30:30, Epoch: 1, Batch: 350, Training Loss: 0.14098236709833145, LR: 0.001
Time, 2019-01-07T01:30:32, Epoch: 1, Batch: 360, Training Loss: 0.15406671911478043, LR: 0.001
Time, 2019-01-07T01:30:33, Epoch: 1, Batch: 370, Training Loss: 0.1399175100028515, LR: 0.001
Time, 2019-01-07T01:30:35, Epoch: 1, Batch: 380, Training Loss: 0.12438278421759605, LR: 0.001
Time, 2019-01-07T01:30:36, Epoch: 1, Batch: 390, Training Loss: 0.13452157750725746, LR: 0.001
Time, 2019-01-07T01:30:37, Epoch: 1, Batch: 400, Training Loss: 0.14830161184072493, LR: 0.001
Time, 2019-01-07T01:30:39, Epoch: 1, Batch: 410, Training Loss: 0.11412447579205036, LR: 0.001
Time, 2019-01-07T01:30:40, Epoch: 1, Batch: 420, Training Loss: 0.11606258675456047, LR: 0.001
Time, 2019-01-07T01:30:41, Epoch: 1, Batch: 430, Training Loss: 0.11575647667050362, LR: 0.001
Time, 2019-01-07T01:30:42, Epoch: 1, Batch: 440, Training Loss: 0.12054388895630837, LR: 0.001
Time, 2019-01-07T01:30:44, Epoch: 1, Batch: 450, Training Loss: 0.12351100333034992, LR: 0.001
Time, 2019-01-07T01:30:45, Epoch: 1, Batch: 460, Training Loss: 0.12992169037461282, LR: 0.001
Time, 2019-01-07T01:30:46, Epoch: 1, Batch: 470, Training Loss: 0.14060833379626275, LR: 0.001
Time, 2019-01-07T01:30:47, Epoch: 1, Batch: 480, Training Loss: 0.10633007250726223, LR: 0.001
Time, 2019-01-07T01:30:49, Epoch: 1, Batch: 490, Training Loss: 0.11787304803729057, LR: 0.001
Time, 2019-01-07T01:30:50, Epoch: 1, Batch: 500, Training Loss: 0.0866930328309536, LR: 0.001
Time, 2019-01-07T01:30:51, Epoch: 1, Batch: 510, Training Loss: 0.14029234275221825, LR: 0.001
Time, 2019-01-07T01:30:53, Epoch: 1, Batch: 520, Training Loss: 0.15067963302135468, LR: 0.001
Time, 2019-01-07T01:30:54, Epoch: 1, Batch: 530, Training Loss: 0.13586689233779908, LR: 0.001
Time, 2019-01-07T01:30:55, Epoch: 1, Batch: 540, Training Loss: 0.11521290987730026, LR: 0.001
Time, 2019-01-07T01:30:57, Epoch: 1, Batch: 550, Training Loss: 0.08340171948075295, LR: 0.001
Time, 2019-01-07T01:30:58, Epoch: 1, Batch: 560, Training Loss: 0.11367204524576664, LR: 0.001
Time, 2019-01-07T01:30:59, Epoch: 1, Batch: 570, Training Loss: 0.11105857044458389, LR: 0.001
Time, 2019-01-07T01:31:00, Epoch: 1, Batch: 580, Training Loss: 0.07617815211415291, LR: 0.001
Time, 2019-01-07T01:31:01, Epoch: 1, Batch: 590, Training Loss: 0.11591092459857463, LR: 0.001
Time, 2019-01-07T01:31:02, Epoch: 1, Batch: 600, Training Loss: 0.11440263465046882, LR: 0.001
Time, 2019-01-07T01:31:04, Epoch: 1, Batch: 610, Training Loss: 0.06997515931725502, LR: 0.001
Time, 2019-01-07T01:31:05, Epoch: 1, Batch: 620, Training Loss: 0.0942816086113453, LR: 0.001
Time, 2019-01-07T01:31:07, Epoch: 1, Batch: 630, Training Loss: 0.13007145524024963, LR: 0.001
Time, 2019-01-07T01:31:08, Epoch: 1, Batch: 640, Training Loss: 0.15819969847798349, LR: 0.001
Time, 2019-01-07T01:31:10, Epoch: 1, Batch: 650, Training Loss: 0.09419598653912545, LR: 0.001
Time, 2019-01-07T01:31:11, Epoch: 1, Batch: 660, Training Loss: 0.125142689794302, LR: 0.001
Time, 2019-01-07T01:31:13, Epoch: 1, Batch: 670, Training Loss: 0.10877405293285847, LR: 0.001
Time, 2019-01-07T01:31:14, Epoch: 1, Batch: 680, Training Loss: 0.07795679941773415, LR: 0.001
Time, 2019-01-07T01:31:16, Epoch: 1, Batch: 690, Training Loss: 0.11138135045766831, LR: 0.001
Time, 2019-01-07T01:31:17, Epoch: 1, Batch: 700, Training Loss: 0.07882338799536229, LR: 0.001
Time, 2019-01-07T01:31:19, Epoch: 1, Batch: 710, Training Loss: 0.0879601113498211, LR: 0.001
Time, 2019-01-07T01:31:20, Epoch: 1, Batch: 720, Training Loss: 0.09335624575614929, LR: 0.001
Time, 2019-01-07T01:31:21, Epoch: 1, Batch: 730, Training Loss: 0.06427465677261353, LR: 0.001
Time, 2019-01-07T01:31:23, Epoch: 1, Batch: 740, Training Loss: 0.08226473182439804, LR: 0.001
Time, 2019-01-07T01:31:24, Epoch: 1, Batch: 750, Training Loss: 0.08379165418446063, LR: 0.001
Time, 2019-01-07T01:31:26, Epoch: 1, Batch: 760, Training Loss: 0.11566778272390366, LR: 0.001
Time, 2019-01-07T01:31:27, Epoch: 1, Batch: 770, Training Loss: 0.06898775398731231, LR: 0.001
Time, 2019-01-07T01:31:29, Epoch: 1, Batch: 780, Training Loss: 0.09269575998187066, LR: 0.001
Time, 2019-01-07T01:31:30, Epoch: 1, Batch: 790, Training Loss: 0.08125635161995888, LR: 0.001
Time, 2019-01-07T01:31:32, Epoch: 1, Batch: 800, Training Loss: 0.076289352029562, LR: 0.001
Time, 2019-01-07T01:31:33, Epoch: 1, Batch: 810, Training Loss: 0.07619512639939785, LR: 0.001
Time, 2019-01-07T01:31:34, Epoch: 1, Batch: 820, Training Loss: 0.11469531580805778, LR: 0.001
Time, 2019-01-07T01:31:36, Epoch: 1, Batch: 830, Training Loss: 0.05501989722251892, LR: 0.001
Time, 2019-01-07T01:31:37, Epoch: 1, Batch: 840, Training Loss: 0.11295639313757419, LR: 0.001
Time, 2019-01-07T01:31:39, Epoch: 1, Batch: 850, Training Loss: 0.0992390587925911, LR: 0.001
Time, 2019-01-07T01:31:40, Epoch: 1, Batch: 860, Training Loss: 0.06709806546568871, LR: 0.001
Time, 2019-01-07T01:31:41, Epoch: 1, Batch: 870, Training Loss: 0.09647068977355958, LR: 0.001
Time, 2019-01-07T01:31:43, Epoch: 1, Batch: 880, Training Loss: 0.0994114838540554, LR: 0.001
Time, 2019-01-07T01:31:44, Epoch: 1, Batch: 890, Training Loss: 0.13221840113401412, LR: 0.001
Time, 2019-01-07T01:31:45, Epoch: 1, Batch: 900, Training Loss: 0.14363978728652, LR: 0.001
Time, 2019-01-07T01:31:46, Epoch: 1, Batch: 910, Training Loss: 0.11438620127737523, LR: 0.001
Time, 2019-01-07T01:31:48, Epoch: 1, Batch: 920, Training Loss: 0.07205015160143376, LR: 0.001
Time, 2019-01-07T01:31:49, Epoch: 1, Batch: 930, Training Loss: 0.09670360349118709, LR: 0.001
Epoch: 1, Validation Top 1 acc: 97.60150909423828
Epoch: 1, Validation Top 5 acc: 99.95024108886719
Epoch: 1, Validation Set Loss: 0.07985444366931915
Start training epoch 2
Time, 2019-01-07T01:31:56, Epoch: 2, Batch: 10, Training Loss: 0.05205878317356109, LR: 0.001
Time, 2019-01-07T01:31:57, Epoch: 2, Batch: 20, Training Loss: 0.06993498504161835, LR: 0.001
Time, 2019-01-07T01:31:59, Epoch: 2, Batch: 30, Training Loss: 0.0919552031904459, LR: 0.001
Time, 2019-01-07T01:32:00, Epoch: 2, Batch: 40, Training Loss: 0.09306403920054436, LR: 0.001
Time, 2019-01-07T01:32:01, Epoch: 2, Batch: 50, Training Loss: 0.10181202217936516, LR: 0.001
Time, 2019-01-07T01:32:03, Epoch: 2, Batch: 60, Training Loss: 0.0887346014380455, LR: 0.001
Time, 2019-01-07T01:32:04, Epoch: 2, Batch: 70, Training Loss: 0.0863435048609972, LR: 0.001
Time, 2019-01-07T01:32:05, Epoch: 2, Batch: 80, Training Loss: 0.07714024744927883, LR: 0.001
Time, 2019-01-07T01:32:07, Epoch: 2, Batch: 90, Training Loss: 0.06332738921046258, LR: 0.001
Time, 2019-01-07T01:32:08, Epoch: 2, Batch: 100, Training Loss: 0.06540968492627144, LR: 0.001
Time, 2019-01-07T01:32:09, Epoch: 2, Batch: 110, Training Loss: 0.08617509305477142, LR: 0.001
Time, 2019-01-07T01:32:10, Epoch: 2, Batch: 120, Training Loss: 0.05821733102202416, LR: 0.001
Time, 2019-01-07T01:32:12, Epoch: 2, Batch: 130, Training Loss: 0.07944176532328129, LR: 0.001
Time, 2019-01-07T01:32:13, Epoch: 2, Batch: 140, Training Loss: 0.06797978021204472, LR: 0.001
Time, 2019-01-07T01:32:15, Epoch: 2, Batch: 150, Training Loss: 0.057584499940276146, LR: 0.001
Time, 2019-01-07T01:32:16, Epoch: 2, Batch: 160, Training Loss: 0.09320751689374447, LR: 0.001
Time, 2019-01-07T01:32:17, Epoch: 2, Batch: 170, Training Loss: 0.0698550682514906, LR: 0.001
Time, 2019-01-07T01:32:18, Epoch: 2, Batch: 180, Training Loss: 0.07021092996001244, LR: 0.001
Time, 2019-01-07T01:32:20, Epoch: 2, Batch: 190, Training Loss: 0.11398296244442463, LR: 0.001
Time, 2019-01-07T01:32:21, Epoch: 2, Batch: 200, Training Loss: 0.11850223094224929, LR: 0.001
Time, 2019-01-07T01:32:23, Epoch: 2, Batch: 210, Training Loss: 0.08417964428663254, LR: 0.001
Time, 2019-01-07T01:32:24, Epoch: 2, Batch: 220, Training Loss: 0.11113092489540577, LR: 0.001
Time, 2019-01-07T01:32:25, Epoch: 2, Batch: 230, Training Loss: 0.0865546390414238, LR: 0.001
Time, 2019-01-07T01:32:26, Epoch: 2, Batch: 240, Training Loss: 0.06161295212805271, LR: 0.001
Time, 2019-01-07T01:32:28, Epoch: 2, Batch: 250, Training Loss: 0.08370052054524421, LR: 0.001
Time, 2019-01-07T01:32:29, Epoch: 2, Batch: 260, Training Loss: 0.0704419795423746, LR: 0.001
Time, 2019-01-07T01:32:31, Epoch: 2, Batch: 270, Training Loss: 0.07120465710759163, LR: 0.001
Time, 2019-01-07T01:32:32, Epoch: 2, Batch: 280, Training Loss: 0.08220590017735958, LR: 0.001
Time, 2019-01-07T01:32:33, Epoch: 2, Batch: 290, Training Loss: 0.052391945570707324, LR: 0.001
Time, 2019-01-07T01:32:35, Epoch: 2, Batch: 300, Training Loss: 0.059262415021657945, LR: 0.001
Time, 2019-01-07T01:32:36, Epoch: 2, Batch: 310, Training Loss: 0.09799587056040764, LR: 0.001
Time, 2019-01-07T01:32:37, Epoch: 2, Batch: 320, Training Loss: 0.07782242074608803, LR: 0.001
Time, 2019-01-07T01:32:38, Epoch: 2, Batch: 330, Training Loss: 0.07281119786202908, LR: 0.001
Time, 2019-01-07T01:32:40, Epoch: 2, Batch: 340, Training Loss: 0.08080573603510857, LR: 0.001
Time, 2019-01-07T01:32:41, Epoch: 2, Batch: 350, Training Loss: 0.08791057541966438, LR: 0.001
Time, 2019-01-07T01:32:42, Epoch: 2, Batch: 360, Training Loss: 0.08627500385046005, LR: 0.001
Time, 2019-01-07T01:32:44, Epoch: 2, Batch: 370, Training Loss: 0.04533844217658043, LR: 0.001
Time, 2019-01-07T01:32:45, Epoch: 2, Batch: 380, Training Loss: 0.05461052991449833, LR: 0.001
Time, 2019-01-07T01:32:46, Epoch: 2, Batch: 390, Training Loss: 0.06952544450759887, LR: 0.001
Time, 2019-01-07T01:32:48, Epoch: 2, Batch: 400, Training Loss: 0.07245870754122734, LR: 0.001
Time, 2019-01-07T01:32:49, Epoch: 2, Batch: 410, Training Loss: 0.07092640325427055, LR: 0.001
Time, 2019-01-07T01:32:50, Epoch: 2, Batch: 420, Training Loss: 0.11941531673073769, LR: 0.001
Time, 2019-01-07T01:32:51, Epoch: 2, Batch: 430, Training Loss: 0.07947047539055348, LR: 0.001
Time, 2019-01-07T01:32:53, Epoch: 2, Batch: 440, Training Loss: 0.07215692698955536, LR: 0.001
Time, 2019-01-07T01:32:54, Epoch: 2, Batch: 450, Training Loss: 0.11091189160943031, LR: 0.001
Time, 2019-01-07T01:32:56, Epoch: 2, Batch: 460, Training Loss: 0.046546507999300955, LR: 0.001
Time, 2019-01-07T01:32:57, Epoch: 2, Batch: 470, Training Loss: 0.05670273751020431, LR: 0.001
Time, 2019-01-07T01:32:58, Epoch: 2, Batch: 480, Training Loss: 0.08473964929580688, LR: 0.001
Time, 2019-01-07T01:33:00, Epoch: 2, Batch: 490, Training Loss: 0.05731913968920708, LR: 0.001
Time, 2019-01-07T01:33:01, Epoch: 2, Batch: 500, Training Loss: 0.054112395644187926, LR: 0.001
Time, 2019-01-07T01:33:02, Epoch: 2, Batch: 510, Training Loss: 0.06395387053489685, LR: 0.001
Time, 2019-01-07T01:33:03, Epoch: 2, Batch: 520, Training Loss: 0.06154689155519009, LR: 0.001
Time, 2019-01-07T01:33:05, Epoch: 2, Batch: 530, Training Loss: 0.06766395941376686, LR: 0.001
Time, 2019-01-07T01:33:06, Epoch: 2, Batch: 540, Training Loss: 0.049460627138614655, LR: 0.001
Time, 2019-01-07T01:33:07, Epoch: 2, Batch: 550, Training Loss: 0.07069406732916832, LR: 0.001
Time, 2019-01-07T01:33:09, Epoch: 2, Batch: 560, Training Loss: 0.06884745322167873, LR: 0.001
Time, 2019-01-07T01:33:10, Epoch: 2, Batch: 570, Training Loss: 0.06784346923232079, LR: 0.001
Time, 2019-01-07T01:33:11, Epoch: 2, Batch: 580, Training Loss: 0.09249146431684493, LR: 0.001
Time, 2019-01-07T01:33:13, Epoch: 2, Batch: 590, Training Loss: 0.09510979950428008, LR: 0.001
Time, 2019-01-07T01:33:14, Epoch: 2, Batch: 600, Training Loss: 0.062419714778661727, LR: 0.001
Time, 2019-01-07T01:33:15, Epoch: 2, Batch: 610, Training Loss: 0.04766579754650593, LR: 0.001
Time, 2019-01-07T01:33:17, Epoch: 2, Batch: 620, Training Loss: 0.05968342199921608, LR: 0.001
Time, 2019-01-07T01:33:18, Epoch: 2, Batch: 630, Training Loss: 0.06912343800067902, LR: 0.001
Time, 2019-01-07T01:33:20, Epoch: 2, Batch: 640, Training Loss: 0.08685155510902405, LR: 0.001
Time, 2019-01-07T01:33:21, Epoch: 2, Batch: 650, Training Loss: 0.07907310724258423, LR: 0.001
Time, 2019-01-07T01:33:22, Epoch: 2, Batch: 660, Training Loss: 0.04267159104347229, LR: 0.001
Time, 2019-01-07T01:33:24, Epoch: 2, Batch: 670, Training Loss: 0.04988042525947094, LR: 0.001
Time, 2019-01-07T01:33:25, Epoch: 2, Batch: 680, Training Loss: 0.08440381512045861, LR: 0.001
Time, 2019-01-07T01:33:27, Epoch: 2, Batch: 690, Training Loss: 0.0508247971534729, LR: 0.001
Time, 2019-01-07T01:33:28, Epoch: 2, Batch: 700, Training Loss: 0.04678445234894753, LR: 0.001
Time, 2019-01-07T01:33:30, Epoch: 2, Batch: 710, Training Loss: 0.07856674045324326, LR: 0.001
Time, 2019-01-07T01:33:32, Epoch: 2, Batch: 720, Training Loss: 0.07027129158377647, LR: 0.001
Time, 2019-01-07T01:33:33, Epoch: 2, Batch: 730, Training Loss: 0.06290644258260727, LR: 0.001
Time, 2019-01-07T01:33:34, Epoch: 2, Batch: 740, Training Loss: 0.04770185239613056, LR: 0.001
Time, 2019-01-07T01:33:36, Epoch: 2, Batch: 750, Training Loss: 0.06474848054349422, LR: 0.001
Time, 2019-01-07T01:33:38, Epoch: 2, Batch: 760, Training Loss: 0.061939452216029166, LR: 0.001
Time, 2019-01-07T01:33:39, Epoch: 2, Batch: 770, Training Loss: 0.042448690533638, LR: 0.001
Time, 2019-01-07T01:33:40, Epoch: 2, Batch: 780, Training Loss: 0.07027954943478107, LR: 0.001
Time, 2019-01-07T01:33:41, Epoch: 2, Batch: 790, Training Loss: 0.10920759439468383, LR: 0.001
Time, 2019-01-07T01:33:43, Epoch: 2, Batch: 800, Training Loss: 0.08020025715231896, LR: 0.001
Time, 2019-01-07T01:33:44, Epoch: 2, Batch: 810, Training Loss: 0.060221309587359426, LR: 0.001
Time, 2019-01-07T01:33:45, Epoch: 2, Batch: 820, Training Loss: 0.0653492521494627, LR: 0.001
Time, 2019-01-07T01:33:47, Epoch: 2, Batch: 830, Training Loss: 0.05196639522910118, LR: 0.001
Time, 2019-01-07T01:33:48, Epoch: 2, Batch: 840, Training Loss: 0.07858693823218346, LR: 0.001
Time, 2019-01-07T01:33:49, Epoch: 2, Batch: 850, Training Loss: 0.039432968199253085, LR: 0.001
Time, 2019-01-07T01:33:50, Epoch: 2, Batch: 860, Training Loss: 0.036929060518741605, LR: 0.001
Time, 2019-01-07T01:33:52, Epoch: 2, Batch: 870, Training Loss: 0.0757547102868557, LR: 0.001
Time, 2019-01-07T01:33:53, Epoch: 2, Batch: 880, Training Loss: 0.06452714502811432, LR: 0.001
Time, 2019-01-07T01:33:54, Epoch: 2, Batch: 890, Training Loss: 0.05879363603889942, LR: 0.001
Time, 2019-01-07T01:33:55, Epoch: 2, Batch: 900, Training Loss: 0.07959491088986397, LR: 0.001
Time, 2019-01-07T01:33:57, Epoch: 2, Batch: 910, Training Loss: 0.07300023473799229, LR: 0.001
Time, 2019-01-07T01:33:58, Epoch: 2, Batch: 920, Training Loss: 0.07854583114385605, LR: 0.001
Time, 2019-01-07T01:34:00, Epoch: 2, Batch: 930, Training Loss: 0.08973031491041183, LR: 0.001
Epoch: 2, Validation Top 1 acc: 97.5119400024414
Epoch: 2, Validation Top 5 acc: 99.97014617919922
Epoch: 2, Validation Set Loss: 0.0742325410246849
Start training epoch 3
Time, 2019-01-07T01:34:07, Epoch: 3, Batch: 10, Training Loss: 0.04868466556072235, LR: 0.001
Time, 2019-01-07T01:34:09, Epoch: 3, Batch: 20, Training Loss: 0.060641147941350934, LR: 0.001
Time, 2019-01-07T01:34:10, Epoch: 3, Batch: 30, Training Loss: 0.08110039643943309, LR: 0.001
Time, 2019-01-07T01:34:11, Epoch: 3, Batch: 40, Training Loss: 0.06033925786614418, LR: 0.001
Time, 2019-01-07T01:34:13, Epoch: 3, Batch: 50, Training Loss: 0.04625260345637798, LR: 0.001
Time, 2019-01-07T01:34:14, Epoch: 3, Batch: 60, Training Loss: 0.07077772654592991, LR: 0.001
Time, 2019-01-07T01:34:15, Epoch: 3, Batch: 70, Training Loss: 0.04609301686286926, LR: 0.001
Time, 2019-01-07T01:34:16, Epoch: 3, Batch: 80, Training Loss: 0.05453018993139267, LR: 0.001
Time, 2019-01-07T01:34:18, Epoch: 3, Batch: 90, Training Loss: 0.07267324887216091, LR: 0.001
Time, 2019-01-07T01:34:20, Epoch: 3, Batch: 100, Training Loss: 0.06703870296478272, LR: 0.001
Time, 2019-01-07T01:34:21, Epoch: 3, Batch: 110, Training Loss: 0.05989702939987183, LR: 0.001
Time, 2019-01-07T01:34:22, Epoch: 3, Batch: 120, Training Loss: 0.06819926425814629, LR: 0.001
Time, 2019-01-07T01:34:23, Epoch: 3, Batch: 130, Training Loss: 0.05873174481093883, LR: 0.001
Time, 2019-01-07T01:34:25, Epoch: 3, Batch: 140, Training Loss: 0.09744443148374557, LR: 0.001
Time, 2019-01-07T01:34:26, Epoch: 3, Batch: 150, Training Loss: 0.06781560070812702, LR: 0.001
Time, 2019-01-07T01:34:28, Epoch: 3, Batch: 160, Training Loss: 0.05869835987687111, LR: 0.001
Time, 2019-01-07T01:34:29, Epoch: 3, Batch: 170, Training Loss: 0.06297318786382675, LR: 0.001
Time, 2019-01-07T01:34:31, Epoch: 3, Batch: 180, Training Loss: 0.04259612783789635, LR: 0.001
Time, 2019-01-07T01:34:32, Epoch: 3, Batch: 190, Training Loss: 0.07239556834101676, LR: 0.001
Time, 2019-01-07T01:34:33, Epoch: 3, Batch: 200, Training Loss: 0.049791159480810164, LR: 0.001
Time, 2019-01-07T01:34:35, Epoch: 3, Batch: 210, Training Loss: 0.04769967719912529, LR: 0.001
Time, 2019-01-07T01:34:36, Epoch: 3, Batch: 220, Training Loss: 0.05729357190430164, LR: 0.001
Time, 2019-01-07T01:34:37, Epoch: 3, Batch: 230, Training Loss: 0.06523993462324143, LR: 0.001
Time, 2019-01-07T01:34:38, Epoch: 3, Batch: 240, Training Loss: 0.09565386027097703, LR: 0.001
Time, 2019-01-07T01:34:39, Epoch: 3, Batch: 250, Training Loss: 0.053816686198115346, LR: 0.001
Time, 2019-01-07T01:34:40, Epoch: 3, Batch: 260, Training Loss: 0.048809072375297545, LR: 0.001
Time, 2019-01-07T01:34:42, Epoch: 3, Batch: 270, Training Loss: 0.04368058741092682, LR: 0.001
Time, 2019-01-07T01:34:43, Epoch: 3, Batch: 280, Training Loss: 0.058499318361282346, LR: 0.001
Time, 2019-01-07T01:34:44, Epoch: 3, Batch: 290, Training Loss: 0.0482029989361763, LR: 0.001
Time, 2019-01-07T01:34:45, Epoch: 3, Batch: 300, Training Loss: 0.052846790105104444, LR: 0.001
Time, 2019-01-07T01:34:47, Epoch: 3, Batch: 310, Training Loss: 0.05248216800391674, LR: 0.001
Time, 2019-01-07T01:34:48, Epoch: 3, Batch: 320, Training Loss: 0.07522489950060844, LR: 0.001
Time, 2019-01-07T01:34:49, Epoch: 3, Batch: 330, Training Loss: 0.046040669083595276, LR: 0.001
Time, 2019-01-07T01:34:50, Epoch: 3, Batch: 340, Training Loss: 0.052081213891506196, LR: 0.001
Time, 2019-01-07T01:34:51, Epoch: 3, Batch: 350, Training Loss: 0.030736271664500235, LR: 0.001
Time, 2019-01-07T01:34:52, Epoch: 3, Batch: 360, Training Loss: 0.059030992537736894, LR: 0.001
Time, 2019-01-07T01:34:53, Epoch: 3, Batch: 370, Training Loss: 0.046355141699314116, LR: 0.001
Time, 2019-01-07T01:34:55, Epoch: 3, Batch: 380, Training Loss: 0.0650533739477396, LR: 0.001
Time, 2019-01-07T01:34:56, Epoch: 3, Batch: 390, Training Loss: 0.033685334399342536, LR: 0.001
Time, 2019-01-07T01:34:57, Epoch: 3, Batch: 400, Training Loss: 0.04107075557112694, LR: 0.001
Time, 2019-01-07T01:34:58, Epoch: 3, Batch: 410, Training Loss: 0.03588889837265015, LR: 0.001
Time, 2019-01-07T01:35:00, Epoch: 3, Batch: 420, Training Loss: 0.04030896574258804, LR: 0.001
Time, 2019-01-07T01:35:01, Epoch: 3, Batch: 430, Training Loss: 0.02484481781721115, LR: 0.001
Time, 2019-01-07T01:35:02, Epoch: 3, Batch: 440, Training Loss: 0.05201470777392388, LR: 0.001
Time, 2019-01-07T01:35:04, Epoch: 3, Batch: 450, Training Loss: 0.03729100339114666, LR: 0.001
Time, 2019-01-07T01:35:05, Epoch: 3, Batch: 460, Training Loss: 0.04589777179062367, LR: 0.001
Time, 2019-01-07T01:35:06, Epoch: 3, Batch: 470, Training Loss: 0.04993940517306328, LR: 0.001
Time, 2019-01-07T01:35:07, Epoch: 3, Batch: 480, Training Loss: 0.05871227756142616, LR: 0.001
Time, 2019-01-07T01:35:09, Epoch: 3, Batch: 490, Training Loss: 0.05328650176525116, LR: 0.001
Time, 2019-01-07T01:35:10, Epoch: 3, Batch: 500, Training Loss: 0.06371458023786544, LR: 0.001
Time, 2019-01-07T01:35:11, Epoch: 3, Batch: 510, Training Loss: 0.06721380613744259, LR: 0.001
Time, 2019-01-07T01:35:13, Epoch: 3, Batch: 520, Training Loss: 0.01971217505633831, LR: 0.001
Time, 2019-01-07T01:35:14, Epoch: 3, Batch: 530, Training Loss: 0.046428651362657544, LR: 0.001
Time, 2019-01-07T01:35:15, Epoch: 3, Batch: 540, Training Loss: 0.05579913780093193, LR: 0.001
Time, 2019-01-07T01:35:16, Epoch: 3, Batch: 550, Training Loss: 0.04915921688079834, LR: 0.001
Time, 2019-01-07T01:35:17, Epoch: 3, Batch: 560, Training Loss: 0.05339144244790077, LR: 0.001
Time, 2019-01-07T01:35:18, Epoch: 3, Batch: 570, Training Loss: 0.06833862364292145, LR: 0.001
Time, 2019-01-07T01:35:19, Epoch: 3, Batch: 580, Training Loss: 0.05099312625825405, LR: 0.001
Time, 2019-01-07T01:35:21, Epoch: 3, Batch: 590, Training Loss: 0.06174550279974937, LR: 0.001
Time, 2019-01-07T01:35:22, Epoch: 3, Batch: 600, Training Loss: 0.06649550572037696, LR: 0.001
Time, 2019-01-07T01:35:23, Epoch: 3, Batch: 610, Training Loss: 0.05238759629428387, LR: 0.001
Time, 2019-01-07T01:35:24, Epoch: 3, Batch: 620, Training Loss: 0.046059253811836245, LR: 0.001
Time, 2019-01-07T01:35:25, Epoch: 3, Batch: 630, Training Loss: 0.06333058923482895, LR: 0.001
Time, 2019-01-07T01:35:27, Epoch: 3, Batch: 640, Training Loss: 0.047248289361596106, LR: 0.001
Time, 2019-01-07T01:35:28, Epoch: 3, Batch: 650, Training Loss: 0.04672204703092575, LR: 0.001
Time, 2019-01-07T01:35:29, Epoch: 3, Batch: 660, Training Loss: 0.09101062119007111, LR: 0.001
Time, 2019-01-07T01:35:30, Epoch: 3, Batch: 670, Training Loss: 0.044128171354532245, LR: 0.001
Time, 2019-01-07T01:35:31, Epoch: 3, Batch: 680, Training Loss: 0.03846108354628086, LR: 0.001
Time, 2019-01-07T01:35:32, Epoch: 3, Batch: 690, Training Loss: 0.05023079663515091, LR: 0.001
Time, 2019-01-07T01:35:33, Epoch: 3, Batch: 700, Training Loss: 0.06965245455503463, LR: 0.001
Time, 2019-01-07T01:35:34, Epoch: 3, Batch: 710, Training Loss: 0.07403545081615448, LR: 0.001
Time, 2019-01-07T01:35:36, Epoch: 3, Batch: 720, Training Loss: 0.07232568636536599, LR: 0.001
Time, 2019-01-07T01:35:37, Epoch: 3, Batch: 730, Training Loss: 0.06241599023342133, LR: 0.001
Time, 2019-01-07T01:35:38, Epoch: 3, Batch: 740, Training Loss: 0.048270948231220245, LR: 0.001
Time, 2019-01-07T01:35:39, Epoch: 3, Batch: 750, Training Loss: 0.04732324220240116, LR: 0.001
Time, 2019-01-07T01:35:40, Epoch: 3, Batch: 760, Training Loss: 0.03979021050035954, LR: 0.001
Time, 2019-01-07T01:35:41, Epoch: 3, Batch: 770, Training Loss: 0.05554435960948467, LR: 0.001
Time, 2019-01-07T01:35:43, Epoch: 3, Batch: 780, Training Loss: 0.05009578093886376, LR: 0.001
Time, 2019-01-07T01:35:44, Epoch: 3, Batch: 790, Training Loss: 0.04721065051853657, LR: 0.001
Time, 2019-01-07T01:35:45, Epoch: 3, Batch: 800, Training Loss: 0.05512369498610496, LR: 0.001
Time, 2019-01-07T01:35:46, Epoch: 3, Batch: 810, Training Loss: 0.05054500848054886, LR: 0.001
Time, 2019-01-07T01:35:47, Epoch: 3, Batch: 820, Training Loss: 0.048140039294958116, LR: 0.001
Time, 2019-01-07T01:35:48, Epoch: 3, Batch: 830, Training Loss: 0.03734902888536453, LR: 0.001
Time, 2019-01-07T01:35:50, Epoch: 3, Batch: 840, Training Loss: 0.037491829693317415, LR: 0.001
Time, 2019-01-07T01:35:51, Epoch: 3, Batch: 850, Training Loss: 0.05315031781792641, LR: 0.001
Time, 2019-01-07T01:35:52, Epoch: 3, Batch: 860, Training Loss: 0.05012927502393723, LR: 0.001
Time, 2019-01-07T01:35:53, Epoch: 3, Batch: 870, Training Loss: 0.062226056680083276, LR: 0.001
Time, 2019-01-07T01:35:54, Epoch: 3, Batch: 880, Training Loss: 0.06231708787381649, LR: 0.001
Time, 2019-01-07T01:35:56, Epoch: 3, Batch: 890, Training Loss: 0.071649856492877, LR: 0.001
Time, 2019-01-07T01:35:57, Epoch: 3, Batch: 900, Training Loss: 0.057490124553442004, LR: 0.001
Time, 2019-01-07T01:35:58, Epoch: 3, Batch: 910, Training Loss: 0.07353932484984398, LR: 0.001
Time, 2019-01-07T01:35:59, Epoch: 3, Batch: 920, Training Loss: 0.08616993837058544, LR: 0.001
Time, 2019-01-07T01:36:00, Epoch: 3, Batch: 930, Training Loss: 0.05662268176674843, LR: 0.001
Epoch: 3, Validation Top 1 acc: 98.29817199707031
Epoch: 3, Validation Top 5 acc: 100.0
Epoch: 3, Validation Set Loss: 0.052674103528261185
Start training epoch 4
Time, 2019-01-07T01:36:07, Epoch: 4, Batch: 10, Training Loss: 0.0470931489020586, LR: 0.001
Time, 2019-01-07T01:36:08, Epoch: 4, Batch: 20, Training Loss: 0.031536328792572024, LR: 0.001
Time, 2019-01-07T01:36:09, Epoch: 4, Batch: 30, Training Loss: 0.037101052701473236, LR: 0.001
Time, 2019-01-07T01:36:10, Epoch: 4, Batch: 40, Training Loss: 0.034278185665607454, LR: 0.001
Time, 2019-01-07T01:36:12, Epoch: 4, Batch: 50, Training Loss: 0.06064036972820759, LR: 0.001
Time, 2019-01-07T01:36:13, Epoch: 4, Batch: 60, Training Loss: 0.045979319512844084, LR: 0.001
Time, 2019-01-07T01:36:14, Epoch: 4, Batch: 70, Training Loss: 0.048877511918544767, LR: 0.001
Time, 2019-01-07T01:36:15, Epoch: 4, Batch: 80, Training Loss: 0.04429318644106388, LR: 0.001
Time, 2019-01-07T01:36:16, Epoch: 4, Batch: 90, Training Loss: 0.04948275238275528, LR: 0.001
Time, 2019-01-07T01:36:18, Epoch: 4, Batch: 100, Training Loss: 0.06206979900598526, LR: 0.001
Time, 2019-01-07T01:36:19, Epoch: 4, Batch: 110, Training Loss: 0.028669243305921556, LR: 0.001
Time, 2019-01-07T01:36:20, Epoch: 4, Batch: 120, Training Loss: 0.01835707537829876, LR: 0.001
Time, 2019-01-07T01:36:21, Epoch: 4, Batch: 130, Training Loss: 0.03264254629611969, LR: 0.001
Time, 2019-01-07T01:36:22, Epoch: 4, Batch: 140, Training Loss: 0.03601979464292526, LR: 0.001
Time, 2019-01-07T01:36:23, Epoch: 4, Batch: 150, Training Loss: 0.031202353537082672, LR: 0.001
Time, 2019-01-07T01:36:25, Epoch: 4, Batch: 160, Training Loss: 0.052027595788240434, LR: 0.001
Time, 2019-01-07T01:36:26, Epoch: 4, Batch: 170, Training Loss: 0.021911239251494408, LR: 0.001
Time, 2019-01-07T01:36:27, Epoch: 4, Batch: 180, Training Loss: 0.022787950187921523, LR: 0.001
Time, 2019-01-07T01:36:28, Epoch: 4, Batch: 190, Training Loss: 0.029470121115446092, LR: 0.001
Time, 2019-01-07T01:36:29, Epoch: 4, Batch: 200, Training Loss: 0.058937860280275346, LR: 0.001
Time, 2019-01-07T01:36:31, Epoch: 4, Batch: 210, Training Loss: 0.04502750486135483, LR: 0.001
Time, 2019-01-07T01:36:32, Epoch: 4, Batch: 220, Training Loss: 0.08210824206471443, LR: 0.001
Time, 2019-01-07T01:36:33, Epoch: 4, Batch: 230, Training Loss: 0.039629319310188295, LR: 0.001
Time, 2019-01-07T01:36:34, Epoch: 4, Batch: 240, Training Loss: 0.047676156461238864, LR: 0.001
Time, 2019-01-07T01:36:35, Epoch: 4, Batch: 250, Training Loss: 0.059257755428552626, LR: 0.001
Time, 2019-01-07T01:36:37, Epoch: 4, Batch: 260, Training Loss: 0.041769275814294814, LR: 0.001
Time, 2019-01-07T01:36:38, Epoch: 4, Batch: 270, Training Loss: 0.028491324931383132, LR: 0.001
Time, 2019-01-07T01:36:39, Epoch: 4, Batch: 280, Training Loss: 0.04044438228011131, LR: 0.001
Time, 2019-01-07T01:36:40, Epoch: 4, Batch: 290, Training Loss: 0.061070195958018304, LR: 0.001
Time, 2019-01-07T01:36:43, Epoch: 4, Batch: 300, Training Loss: 0.04555027112364769, LR: 0.001
Time, 2019-01-07T01:36:44, Epoch: 4, Batch: 310, Training Loss: 0.03970426470041275, LR: 0.001
Time, 2019-01-07T01:36:46, Epoch: 4, Batch: 320, Training Loss: 0.049591992795467374, LR: 0.001
Time, 2019-01-07T01:36:47, Epoch: 4, Batch: 330, Training Loss: 0.04904760122299194, LR: 0.001
Time, 2019-01-07T01:36:49, Epoch: 4, Batch: 340, Training Loss: 0.05996644236147404, LR: 0.001
Time, 2019-01-07T01:36:50, Epoch: 4, Batch: 350, Training Loss: 0.052441464737057686, LR: 0.001
Time, 2019-01-07T01:36:51, Epoch: 4, Batch: 360, Training Loss: 0.04439219124615192, LR: 0.001
Time, 2019-01-07T01:36:53, Epoch: 4, Batch: 370, Training Loss: 0.046752683073282245, LR: 0.001
Time, 2019-01-07T01:36:54, Epoch: 4, Batch: 380, Training Loss: 0.046541308611631395, LR: 0.001
Time, 2019-01-07T01:36:55, Epoch: 4, Batch: 390, Training Loss: 0.05006144195795059, LR: 0.001
Time, 2019-01-07T01:36:56, Epoch: 4, Batch: 400, Training Loss: 0.05655408129096031, LR: 0.001
Time, 2019-01-07T01:36:58, Epoch: 4, Batch: 410, Training Loss: 0.045113309472799304, LR: 0.001
Time, 2019-01-07T01:36:59, Epoch: 4, Batch: 420, Training Loss: 0.03495549261569977, LR: 0.001
Time, 2019-01-07T01:37:00, Epoch: 4, Batch: 430, Training Loss: 0.02612149454653263, LR: 0.001
Time, 2019-01-07T01:37:02, Epoch: 4, Batch: 440, Training Loss: 0.03270675837993622, LR: 0.001
Time, 2019-01-07T01:37:03, Epoch: 4, Batch: 450, Training Loss: 0.04703441709280014, LR: 0.001
Time, 2019-01-07T01:37:04, Epoch: 4, Batch: 460, Training Loss: 0.05608819201588631, LR: 0.001
Time, 2019-01-07T01:37:06, Epoch: 4, Batch: 470, Training Loss: 0.030466260761022566, LR: 0.001
Time, 2019-01-07T01:37:07, Epoch: 4, Batch: 480, Training Loss: 0.04282038398087025, LR: 0.001
Time, 2019-01-07T01:37:08, Epoch: 4, Batch: 490, Training Loss: 0.031069990992546082, LR: 0.001
Time, 2019-01-07T01:37:10, Epoch: 4, Batch: 500, Training Loss: 0.04082994386553764, LR: 0.001
Time, 2019-01-07T01:37:11, Epoch: 4, Batch: 510, Training Loss: 0.05187605731189251, LR: 0.001
Time, 2019-01-07T01:37:12, Epoch: 4, Batch: 520, Training Loss: 0.053807984292507174, LR: 0.001
Time, 2019-01-07T01:37:14, Epoch: 4, Batch: 530, Training Loss: 0.053325232863426206, LR: 0.001
Time, 2019-01-07T01:37:15, Epoch: 4, Batch: 540, Training Loss: 0.034272891283035276, LR: 0.001
Time, 2019-01-07T01:37:16, Epoch: 4, Batch: 550, Training Loss: 0.04224973469972611, LR: 0.001
Time, 2019-01-07T01:37:17, Epoch: 4, Batch: 560, Training Loss: 0.041492660343647, LR: 0.001
Time, 2019-01-07T01:37:19, Epoch: 4, Batch: 570, Training Loss: 0.03376183547079563, LR: 0.001
Time, 2019-01-07T01:37:20, Epoch: 4, Batch: 580, Training Loss: 0.041455937922000884, LR: 0.001
Time, 2019-01-07T01:37:21, Epoch: 4, Batch: 590, Training Loss: 0.03936659097671509, LR: 0.001
Time, 2019-01-07T01:37:23, Epoch: 4, Batch: 600, Training Loss: 0.051445110142230986, LR: 0.001
Time, 2019-01-07T01:37:24, Epoch: 4, Batch: 610, Training Loss: 0.036722705513238904, LR: 0.001
Time, 2019-01-07T01:37:25, Epoch: 4, Batch: 620, Training Loss: 0.03940295688807964, LR: 0.001
Time, 2019-01-07T01:37:27, Epoch: 4, Batch: 630, Training Loss: 0.02695445269346237, LR: 0.001
Time, 2019-01-07T01:37:28, Epoch: 4, Batch: 640, Training Loss: 0.06182647496461868, LR: 0.001
Time, 2019-01-07T01:37:29, Epoch: 4, Batch: 650, Training Loss: 0.046820586547255516, LR: 0.001
Time, 2019-01-07T01:37:31, Epoch: 4, Batch: 660, Training Loss: 0.05438507422804832, LR: 0.001
Time, 2019-01-07T01:37:32, Epoch: 4, Batch: 670, Training Loss: 0.04316607229411602, LR: 0.001
Time, 2019-01-07T01:37:34, Epoch: 4, Batch: 680, Training Loss: 0.03916005939245224, LR: 0.001
Time, 2019-01-07T01:37:35, Epoch: 4, Batch: 690, Training Loss: 0.04147126525640488, LR: 0.001
Time, 2019-01-07T01:37:36, Epoch: 4, Batch: 700, Training Loss: 0.029821711778640746, LR: 0.001
Time, 2019-01-07T01:37:38, Epoch: 4, Batch: 710, Training Loss: 0.052329523116350175, LR: 0.001
Time, 2019-01-07T01:37:39, Epoch: 4, Batch: 720, Training Loss: 0.055072211846709254, LR: 0.001
Time, 2019-01-07T01:37:40, Epoch: 4, Batch: 730, Training Loss: 0.04289684966206551, LR: 0.001
Time, 2019-01-07T01:37:42, Epoch: 4, Batch: 740, Training Loss: 0.051835454627871515, LR: 0.001
Time, 2019-01-07T01:37:43, Epoch: 4, Batch: 750, Training Loss: 0.0462163433432579, LR: 0.001
Time, 2019-01-07T01:37:44, Epoch: 4, Batch: 760, Training Loss: 0.059427670389413836, LR: 0.001
Time, 2019-01-07T01:37:45, Epoch: 4, Batch: 770, Training Loss: 0.031684015318751334, LR: 0.001
Time, 2019-01-07T01:37:46, Epoch: 4, Batch: 780, Training Loss: 0.04952250234782696, LR: 0.001
Time, 2019-01-07T01:37:48, Epoch: 4, Batch: 790, Training Loss: 0.034273849427700044, LR: 0.001
Time, 2019-01-07T01:37:49, Epoch: 4, Batch: 800, Training Loss: 0.056152204424142836, LR: 0.001
Time, 2019-01-07T01:37:50, Epoch: 4, Batch: 810, Training Loss: 0.05786243677139282, LR: 0.001
Time, 2019-01-07T01:37:51, Epoch: 4, Batch: 820, Training Loss: 0.03864232152700424, LR: 0.001
Time, 2019-01-07T01:37:53, Epoch: 4, Batch: 830, Training Loss: 0.030291135981678964, LR: 0.001
Time, 2019-01-07T01:37:54, Epoch: 4, Batch: 840, Training Loss: 0.06506555676460266, LR: 0.001
Time, 2019-01-07T01:37:55, Epoch: 4, Batch: 850, Training Loss: 0.04745758697390556, LR: 0.001
Time, 2019-01-07T01:37:56, Epoch: 4, Batch: 860, Training Loss: 0.0655148558318615, LR: 0.001
Time, 2019-01-07T01:37:57, Epoch: 4, Batch: 870, Training Loss: 0.03751706779003143, LR: 0.001
Time, 2019-01-07T01:37:59, Epoch: 4, Batch: 880, Training Loss: 0.04542900733649731, LR: 0.001
Time, 2019-01-07T01:38:00, Epoch: 4, Batch: 890, Training Loss: 0.026223517209291457, LR: 0.001
Time, 2019-01-07T01:38:01, Epoch: 4, Batch: 900, Training Loss: 0.04296161234378815, LR: 0.001
Time, 2019-01-07T01:38:02, Epoch: 4, Batch: 910, Training Loss: 0.050675299018621445, LR: 0.001
Time, 2019-01-07T01:38:04, Epoch: 4, Batch: 920, Training Loss: 0.04091640301048756, LR: 0.001
Time, 2019-01-07T01:38:05, Epoch: 4, Batch: 930, Training Loss: 0.06865932270884514, LR: 0.001
Epoch: 4, Validation Top 1 acc: 98.45740509033203
Epoch: 4, Validation Top 5 acc: 99.99005126953125
Epoch: 4, Validation Set Loss: 0.05135500803589821
Start training epoch 5
Time, 2019-01-07T01:38:14, Epoch: 5, Batch: 10, Training Loss: 0.039914161711931226, LR: 0.001
Time, 2019-01-07T01:38:15, Epoch: 5, Batch: 20, Training Loss: 0.04505550600588322, LR: 0.001
Time, 2019-01-07T01:38:16, Epoch: 5, Batch: 30, Training Loss: 0.029184488207101823, LR: 0.001
Time, 2019-01-07T01:38:18, Epoch: 5, Batch: 40, Training Loss: 0.04540487006306648, LR: 0.001
Time, 2019-01-07T01:38:19, Epoch: 5, Batch: 50, Training Loss: 0.04611566960811615, LR: 0.001
Time, 2019-01-07T01:38:20, Epoch: 5, Batch: 60, Training Loss: 0.029244241863489152, LR: 0.001
Time, 2019-01-07T01:38:21, Epoch: 5, Batch: 70, Training Loss: 0.043392162024974826, LR: 0.001
Time, 2019-01-07T01:38:22, Epoch: 5, Batch: 80, Training Loss: 0.042601637169718744, LR: 0.001
Time, 2019-01-07T01:38:23, Epoch: 5, Batch: 90, Training Loss: 0.02795269265770912, LR: 0.001
Time, 2019-01-07T01:38:25, Epoch: 5, Batch: 100, Training Loss: 0.03591894656419754, LR: 0.001
Time, 2019-01-07T01:38:26, Epoch: 5, Batch: 110, Training Loss: 0.05014023631811142, LR: 0.001
Time, 2019-01-07T01:38:27, Epoch: 5, Batch: 120, Training Loss: 0.02729901820421219, LR: 0.001
Time, 2019-01-07T01:38:28, Epoch: 5, Batch: 130, Training Loss: 0.033496546745300296, LR: 0.001
Time, 2019-01-07T01:38:29, Epoch: 5, Batch: 140, Training Loss: 0.04315678328275681, LR: 0.001
Time, 2019-01-07T01:38:30, Epoch: 5, Batch: 150, Training Loss: 0.04003444314002991, LR: 0.001
Time, 2019-01-07T01:38:32, Epoch: 5, Batch: 160, Training Loss: 0.04181291684508324, LR: 0.001
Time, 2019-01-07T01:38:33, Epoch: 5, Batch: 170, Training Loss: 0.04199143573641777, LR: 0.001
Time, 2019-01-07T01:38:34, Epoch: 5, Batch: 180, Training Loss: 0.044112421572208405, LR: 0.001
Time, 2019-01-07T01:38:35, Epoch: 5, Batch: 190, Training Loss: 0.03323128521442413, LR: 0.001
Time, 2019-01-07T01:38:36, Epoch: 5, Batch: 200, Training Loss: 0.02582988664507866, LR: 0.001
Time, 2019-01-07T01:38:37, Epoch: 5, Batch: 210, Training Loss: 0.02156445011496544, LR: 0.001
Time, 2019-01-07T01:38:39, Epoch: 5, Batch: 220, Training Loss: 0.020412657409906387, LR: 0.001
Time, 2019-01-07T01:38:40, Epoch: 5, Batch: 230, Training Loss: 0.03649036809802055, LR: 0.001
Time, 2019-01-07T01:38:41, Epoch: 5, Batch: 240, Training Loss: 0.014115981757640839, LR: 0.001
Time, 2019-01-07T01:38:42, Epoch: 5, Batch: 250, Training Loss: 0.03485014364123344, LR: 0.001
Time, 2019-01-07T01:38:43, Epoch: 5, Batch: 260, Training Loss: 0.07174036800861358, LR: 0.001
Time, 2019-01-07T01:38:45, Epoch: 5, Batch: 270, Training Loss: 0.033570966124534606, LR: 0.001
Time, 2019-01-07T01:38:46, Epoch: 5, Batch: 280, Training Loss: 0.039877559244632724, LR: 0.001
Time, 2019-01-07T01:38:47, Epoch: 5, Batch: 290, Training Loss: 0.05389177352190018, LR: 0.001
Time, 2019-01-07T01:38:48, Epoch: 5, Batch: 300, Training Loss: 0.04243084117770195, LR: 0.001
Time, 2019-01-07T01:38:49, Epoch: 5, Batch: 310, Training Loss: 0.034329844638705254, LR: 0.001
Time, 2019-01-07T01:38:50, Epoch: 5, Batch: 320, Training Loss: 0.05448072254657745, LR: 0.001
Time, 2019-01-07T01:38:52, Epoch: 5, Batch: 330, Training Loss: 0.03157945163547993, LR: 0.001
Time, 2019-01-07T01:38:53, Epoch: 5, Batch: 340, Training Loss: 0.02304416224360466, LR: 0.001
Time, 2019-01-07T01:38:54, Epoch: 5, Batch: 350, Training Loss: 0.032898256182670595, LR: 0.001
Time, 2019-01-07T01:38:55, Epoch: 5, Batch: 360, Training Loss: 0.042539699375629424, LR: 0.001
Time, 2019-01-07T01:38:56, Epoch: 5, Batch: 370, Training Loss: 0.04412792958319187, LR: 0.001
Time, 2019-01-07T01:38:57, Epoch: 5, Batch: 380, Training Loss: 0.026268049702048303, LR: 0.001
Time, 2019-01-07T01:38:58, Epoch: 5, Batch: 390, Training Loss: 0.018491635099053383, LR: 0.001
Time, 2019-01-07T01:39:00, Epoch: 5, Batch: 400, Training Loss: 0.040822622179985044, LR: 0.001
Time, 2019-01-07T01:39:01, Epoch: 5, Batch: 410, Training Loss: 0.0372750997543335, LR: 0.001
Time, 2019-01-07T01:39:02, Epoch: 5, Batch: 420, Training Loss: 0.03584342896938324, LR: 0.001
Time, 2019-01-07T01:39:03, Epoch: 5, Batch: 430, Training Loss: 0.04891879931092262, LR: 0.001
Time, 2019-01-07T01:39:04, Epoch: 5, Batch: 440, Training Loss: 0.026923276484012604, LR: 0.001
Time, 2019-01-07T01:39:06, Epoch: 5, Batch: 450, Training Loss: 0.04108831509947777, LR: 0.001
Time, 2019-01-07T01:39:07, Epoch: 5, Batch: 460, Training Loss: 0.031403465569019316, LR: 0.001
Time, 2019-01-07T01:39:08, Epoch: 5, Batch: 470, Training Loss: 0.024015124514698984, LR: 0.001
Time, 2019-01-07T01:39:09, Epoch: 5, Batch: 480, Training Loss: 0.04804857783019543, LR: 0.001
Time, 2019-01-07T01:39:10, Epoch: 5, Batch: 490, Training Loss: 0.025017108023166656, LR: 0.001
Time, 2019-01-07T01:39:11, Epoch: 5, Batch: 500, Training Loss: 0.0220082625746727, LR: 0.001
Time, 2019-01-07T01:39:12, Epoch: 5, Batch: 510, Training Loss: 0.014002111554145814, LR: 0.001
Time, 2019-01-07T01:39:14, Epoch: 5, Batch: 520, Training Loss: 0.02099120430648327, LR: 0.001
Time, 2019-01-07T01:39:15, Epoch: 5, Batch: 530, Training Loss: 0.05274387449026108, LR: 0.001
Time, 2019-01-07T01:39:17, Epoch: 5, Batch: 540, Training Loss: 0.03185444362461567, LR: 0.001
Time, 2019-01-07T01:39:18, Epoch: 5, Batch: 550, Training Loss: 0.04263643771409988, LR: 0.001
Time, 2019-01-07T01:39:20, Epoch: 5, Batch: 560, Training Loss: 0.030630625784397125, LR: 0.001
Time, 2019-01-07T01:39:21, Epoch: 5, Batch: 570, Training Loss: 0.021632400900125505, LR: 0.001
Time, 2019-01-07T01:39:23, Epoch: 5, Batch: 580, Training Loss: 0.04031476676464081, LR: 0.001
Time, 2019-01-07T01:39:25, Epoch: 5, Batch: 590, Training Loss: 0.03858666121959686, LR: 0.001
Time, 2019-01-07T01:39:27, Epoch: 5, Batch: 600, Training Loss: 0.030579525604844095, LR: 0.001
Time, 2019-01-07T01:39:28, Epoch: 5, Batch: 610, Training Loss: 0.04105096161365509, LR: 0.001
Time, 2019-01-07T01:39:30, Epoch: 5, Batch: 620, Training Loss: 0.05365344062447548, LR: 0.001
Time, 2019-01-07T01:39:31, Epoch: 5, Batch: 630, Training Loss: 0.04631186574697495, LR: 0.001
Time, 2019-01-07T01:39:32, Epoch: 5, Batch: 640, Training Loss: 0.04308820813894272, LR: 0.001
Time, 2019-01-07T01:39:34, Epoch: 5, Batch: 650, Training Loss: 0.02572808861732483, LR: 0.001
Time, 2019-01-07T01:39:35, Epoch: 5, Batch: 660, Training Loss: 0.05328095033764839, LR: 0.001
Time, 2019-01-07T01:39:36, Epoch: 5, Batch: 670, Training Loss: 0.05066108554601669, LR: 0.001
Time, 2019-01-07T01:39:37, Epoch: 5, Batch: 680, Training Loss: 0.034310225397348404, LR: 0.001
Time, 2019-01-07T01:39:39, Epoch: 5, Batch: 690, Training Loss: 0.033826658874750136, LR: 0.001
Time, 2019-01-07T01:39:40, Epoch: 5, Batch: 700, Training Loss: 0.040722303092479706, LR: 0.001
Time, 2019-01-07T01:39:41, Epoch: 5, Batch: 710, Training Loss: 0.024341555684804915, LR: 0.001
Time, 2019-01-07T01:39:42, Epoch: 5, Batch: 720, Training Loss: 0.0303610660135746, LR: 0.001
Time, 2019-01-07T01:39:43, Epoch: 5, Batch: 730, Training Loss: 0.04973919615149498, LR: 0.001
Time, 2019-01-07T01:39:44, Epoch: 5, Batch: 740, Training Loss: 0.030328723043203352, LR: 0.001
Time, 2019-01-07T01:39:45, Epoch: 5, Batch: 750, Training Loss: 0.04182036034762859, LR: 0.001
Time, 2019-01-07T01:39:46, Epoch: 5, Batch: 760, Training Loss: 0.03408761769533157, LR: 0.001
Time, 2019-01-07T01:39:48, Epoch: 5, Batch: 770, Training Loss: 0.0440490897744894, LR: 0.001
Time, 2019-01-07T01:39:49, Epoch: 5, Batch: 780, Training Loss: 0.04189857542514801, LR: 0.001
Time, 2019-01-07T01:39:50, Epoch: 5, Batch: 790, Training Loss: 0.034288770705461505, LR: 0.001
Time, 2019-01-07T01:39:51, Epoch: 5, Batch: 800, Training Loss: 0.040742647275328635, LR: 0.001
Time, 2019-01-07T01:39:52, Epoch: 5, Batch: 810, Training Loss: 0.042866593971848485, LR: 0.001
Time, 2019-01-07T01:39:53, Epoch: 5, Batch: 820, Training Loss: 0.03632461689412594, LR: 0.001
Time, 2019-01-07T01:39:55, Epoch: 5, Batch: 830, Training Loss: 0.03740103617310524, LR: 0.001
Time, 2019-01-07T01:39:56, Epoch: 5, Batch: 840, Training Loss: 0.03165539391338825, LR: 0.001
Time, 2019-01-07T01:39:57, Epoch: 5, Batch: 850, Training Loss: 0.0319292426109314, LR: 0.001
Time, 2019-01-07T01:39:58, Epoch: 5, Batch: 860, Training Loss: 0.0609383836388588, LR: 0.001
Time, 2019-01-07T01:40:00, Epoch: 5, Batch: 870, Training Loss: 0.029568929225206375, LR: 0.001
Time, 2019-01-07T01:40:01, Epoch: 5, Batch: 880, Training Loss: 0.043394874036312106, LR: 0.001
Time, 2019-01-07T01:40:02, Epoch: 5, Batch: 890, Training Loss: 0.03500889763236046, LR: 0.001
Time, 2019-01-07T01:40:03, Epoch: 5, Batch: 900, Training Loss: 0.05796041637659073, LR: 0.001
Time, 2019-01-07T01:40:05, Epoch: 5, Batch: 910, Training Loss: 0.04103492796421051, LR: 0.001
Time, 2019-01-07T01:40:06, Epoch: 5, Batch: 920, Training Loss: 0.035220614075660704, LR: 0.001
Time, 2019-01-07T01:40:07, Epoch: 5, Batch: 930, Training Loss: 0.03587495982646942, LR: 0.001
Epoch: 5, Validation Top 1 acc: 98.48725891113281
Epoch: 5, Validation Top 5 acc: 100.0
Epoch: 5, Validation Set Loss: 0.04489491134881973
Start training epoch 6
Time, 2019-01-07T01:40:16, Epoch: 6, Batch: 10, Training Loss: 0.0442201092839241, LR: 0.001
Time, 2019-01-07T01:40:17, Epoch: 6, Batch: 20, Training Loss: 0.025210017710924147, LR: 0.001
Time, 2019-01-07T01:40:19, Epoch: 6, Batch: 30, Training Loss: 0.05107554122805595, LR: 0.001
Time, 2019-01-07T01:40:20, Epoch: 6, Batch: 40, Training Loss: 0.03065902441740036, LR: 0.001
Time, 2019-01-07T01:40:22, Epoch: 6, Batch: 50, Training Loss: 0.02626323252916336, LR: 0.001
Time, 2019-01-07T01:40:23, Epoch: 6, Batch: 60, Training Loss: 0.036808771640062334, LR: 0.001
Time, 2019-01-07T01:40:25, Epoch: 6, Batch: 70, Training Loss: 0.03321705423295498, LR: 0.001
Time, 2019-01-07T01:40:26, Epoch: 6, Batch: 80, Training Loss: 0.025489206612110137, LR: 0.001
Time, 2019-01-07T01:40:27, Epoch: 6, Batch: 90, Training Loss: 0.012712564319372177, LR: 0.001
Time, 2019-01-07T01:40:29, Epoch: 6, Batch: 100, Training Loss: 0.033512157201766965, LR: 0.001
Time, 2019-01-07T01:40:30, Epoch: 6, Batch: 110, Training Loss: 0.014218330383300781, LR: 0.001
Time, 2019-01-07T01:40:31, Epoch: 6, Batch: 120, Training Loss: 0.03213155269622803, LR: 0.001
Time, 2019-01-07T01:40:33, Epoch: 6, Batch: 130, Training Loss: 0.02614656426012516, LR: 0.001
Time, 2019-01-07T01:40:34, Epoch: 6, Batch: 140, Training Loss: 0.024144041538238525, LR: 0.001
Time, 2019-01-07T01:40:36, Epoch: 6, Batch: 150, Training Loss: 0.019340844452381135, LR: 0.001
Time, 2019-01-07T01:40:38, Epoch: 6, Batch: 160, Training Loss: 0.05515442341566086, LR: 0.001
Time, 2019-01-07T01:40:39, Epoch: 6, Batch: 170, Training Loss: 0.032449682056903836, LR: 0.001
Time, 2019-01-07T01:40:40, Epoch: 6, Batch: 180, Training Loss: 0.04191613644361496, LR: 0.001
Time, 2019-01-07T01:40:41, Epoch: 6, Batch: 190, Training Loss: 0.038033677637577055, LR: 0.001
Time, 2019-01-07T01:40:42, Epoch: 6, Batch: 200, Training Loss: 0.027457190677523613, LR: 0.001
Time, 2019-01-07T01:40:44, Epoch: 6, Batch: 210, Training Loss: 0.035349490493535994, LR: 0.001
Time, 2019-01-07T01:40:46, Epoch: 6, Batch: 220, Training Loss: 0.033068031817674634, LR: 0.001
Time, 2019-01-07T01:40:47, Epoch: 6, Batch: 230, Training Loss: 0.0314695380628109, LR: 0.001
Time, 2019-01-07T01:40:48, Epoch: 6, Batch: 240, Training Loss: 0.03948831222951412, LR: 0.001
Time, 2019-01-07T01:40:51, Epoch: 6, Batch: 250, Training Loss: 0.05194883905351162, LR: 0.001
Time, 2019-01-07T01:40:52, Epoch: 6, Batch: 260, Training Loss: 0.04997894391417503, LR: 0.001
Time, 2019-01-07T01:40:54, Epoch: 6, Batch: 270, Training Loss: 0.06736168935894966, LR: 0.001
Time, 2019-01-07T01:40:55, Epoch: 6, Batch: 280, Training Loss: 0.025841113179922104, LR: 0.001
Time, 2019-01-07T01:40:56, Epoch: 6, Batch: 290, Training Loss: 0.03286644741892815, LR: 0.001
Time, 2019-01-07T01:40:58, Epoch: 6, Batch: 300, Training Loss: 0.0389462236315012, LR: 0.001
Time, 2019-01-07T01:40:59, Epoch: 6, Batch: 310, Training Loss: 0.028473281860351564, LR: 0.001
Time, 2019-01-07T01:41:00, Epoch: 6, Batch: 320, Training Loss: 0.043018242716789244, LR: 0.001
Time, 2019-01-07T01:41:02, Epoch: 6, Batch: 330, Training Loss: 0.02422553300857544, LR: 0.001
Time, 2019-01-07T01:41:04, Epoch: 6, Batch: 340, Training Loss: 0.03756890967488289, LR: 0.001
Time, 2019-01-07T01:41:05, Epoch: 6, Batch: 350, Training Loss: 0.04166808649897576, LR: 0.001
Time, 2019-01-07T01:41:07, Epoch: 6, Batch: 360, Training Loss: 0.014484509080648422, LR: 0.001
Time, 2019-01-07T01:41:08, Epoch: 6, Batch: 370, Training Loss: 0.017376069724559785, LR: 0.001
Time, 2019-01-07T01:41:09, Epoch: 6, Batch: 380, Training Loss: 0.035783087462186815, LR: 0.001
Time, 2019-01-07T01:41:11, Epoch: 6, Batch: 390, Training Loss: 0.031121239066123962, LR: 0.001
Time, 2019-01-07T01:41:13, Epoch: 6, Batch: 400, Training Loss: 0.023674370348453523, LR: 0.001
Time, 2019-01-07T01:41:14, Epoch: 6, Batch: 410, Training Loss: 0.0246607206761837, LR: 0.001
Time, 2019-01-07T01:41:15, Epoch: 6, Batch: 420, Training Loss: 0.03149902373552323, LR: 0.001
Time, 2019-01-07T01:41:17, Epoch: 6, Batch: 430, Training Loss: 0.011799589544534684, LR: 0.001
Time, 2019-01-07T01:41:19, Epoch: 6, Batch: 440, Training Loss: 0.04145418517291546, LR: 0.001
Time, 2019-01-07T01:41:20, Epoch: 6, Batch: 450, Training Loss: 0.03802956379950047, LR: 0.001
Time, 2019-01-07T01:41:21, Epoch: 6, Batch: 460, Training Loss: 0.04273238182067871, LR: 0.001
Time, 2019-01-07T01:41:23, Epoch: 6, Batch: 470, Training Loss: 0.031112870573997496, LR: 0.001
Time, 2019-01-07T01:41:24, Epoch: 6, Batch: 480, Training Loss: 0.031131109222769737, LR: 0.001
Time, 2019-01-07T01:41:25, Epoch: 6, Batch: 490, Training Loss: 0.028481577709317207, LR: 0.001
Time, 2019-01-07T01:41:27, Epoch: 6, Batch: 500, Training Loss: 0.02466244325041771, LR: 0.001
Time, 2019-01-07T01:41:28, Epoch: 6, Batch: 510, Training Loss: 0.027605726197361947, LR: 0.001
Time, 2019-01-07T01:41:30, Epoch: 6, Batch: 520, Training Loss: 0.03586046323180199, LR: 0.001
Time, 2019-01-07T01:41:31, Epoch: 6, Batch: 530, Training Loss: 0.04040573537349701, LR: 0.001
Time, 2019-01-07T01:41:32, Epoch: 6, Batch: 540, Training Loss: 0.026339244842529298, LR: 0.001
Time, 2019-01-07T01:41:34, Epoch: 6, Batch: 550, Training Loss: 0.030243875831365584, LR: 0.001
Time, 2019-01-07T01:41:35, Epoch: 6, Batch: 560, Training Loss: 0.04238715022802353, LR: 0.001
Time, 2019-01-07T01:41:36, Epoch: 6, Batch: 570, Training Loss: 0.021846189349889755, LR: 0.001
Time, 2019-01-07T01:41:38, Epoch: 6, Batch: 580, Training Loss: 0.0406467504799366, LR: 0.001
Time, 2019-01-07T01:41:39, Epoch: 6, Batch: 590, Training Loss: 0.020763881504535675, LR: 0.001
Time, 2019-01-07T01:41:41, Epoch: 6, Batch: 600, Training Loss: 0.02433324009180069, LR: 0.001
Time, 2019-01-07T01:41:42, Epoch: 6, Batch: 610, Training Loss: 0.03298571854829788, LR: 0.001
Time, 2019-01-07T01:41:43, Epoch: 6, Batch: 620, Training Loss: 0.027193085849285127, LR: 0.001
Time, 2019-01-07T01:41:45, Epoch: 6, Batch: 630, Training Loss: 0.045734024792909625, LR: 0.001
Time, 2019-01-07T01:41:47, Epoch: 6, Batch: 640, Training Loss: 0.01766565814614296, LR: 0.001
Time, 2019-01-07T01:41:48, Epoch: 6, Batch: 650, Training Loss: 0.033216755092144015, LR: 0.001
Time, 2019-01-07T01:41:50, Epoch: 6, Batch: 660, Training Loss: 0.01892971470952034, LR: 0.001
Time, 2019-01-07T01:41:51, Epoch: 6, Batch: 670, Training Loss: 0.029774516075849532, LR: 0.001
Time, 2019-01-07T01:41:52, Epoch: 6, Batch: 680, Training Loss: 0.037064360827207564, LR: 0.001
Time, 2019-01-07T01:41:53, Epoch: 6, Batch: 690, Training Loss: 0.03917650356888771, LR: 0.001
Time, 2019-01-07T01:41:54, Epoch: 6, Batch: 700, Training Loss: 0.0633293479681015, LR: 0.001
Time, 2019-01-07T01:41:56, Epoch: 6, Batch: 710, Training Loss: 0.024811507016420365, LR: 0.001
Time, 2019-01-07T01:41:57, Epoch: 6, Batch: 720, Training Loss: 0.038354671001434325, LR: 0.001
Time, 2019-01-07T01:41:58, Epoch: 6, Batch: 730, Training Loss: 0.05431553274393082, LR: 0.001
Time, 2019-01-07T01:41:59, Epoch: 6, Batch: 740, Training Loss: 0.026368851959705352, LR: 0.001
Time, 2019-01-07T01:42:01, Epoch: 6, Batch: 750, Training Loss: 0.042507512122392656, LR: 0.001
Time, 2019-01-07T01:42:02, Epoch: 6, Batch: 760, Training Loss: 0.04080891236662865, LR: 0.001
Time, 2019-01-07T01:42:03, Epoch: 6, Batch: 770, Training Loss: 0.050438099354505536, LR: 0.001
Time, 2019-01-07T01:42:05, Epoch: 6, Batch: 780, Training Loss: 0.046595515310764314, LR: 0.001
Time, 2019-01-07T01:42:07, Epoch: 6, Batch: 790, Training Loss: 0.04353732988238335, LR: 0.001
Time, 2019-01-07T01:42:09, Epoch: 6, Batch: 800, Training Loss: 0.04216503500938416, LR: 0.001
Time, 2019-01-07T01:42:10, Epoch: 6, Batch: 810, Training Loss: 0.036405058950185774, LR: 0.001
Time, 2019-01-07T01:42:11, Epoch: 6, Batch: 820, Training Loss: 0.031063295155763625, LR: 0.001
Time, 2019-01-07T01:42:12, Epoch: 6, Batch: 830, Training Loss: 0.0470543771982193, LR: 0.001
Time, 2019-01-07T01:42:13, Epoch: 6, Batch: 840, Training Loss: 0.027102117240428925, LR: 0.001
Time, 2019-01-07T01:42:15, Epoch: 6, Batch: 850, Training Loss: 0.021479927748441697, LR: 0.001
Time, 2019-01-07T01:42:16, Epoch: 6, Batch: 860, Training Loss: 0.02846355885267258, LR: 0.001
Time, 2019-01-07T01:42:17, Epoch: 6, Batch: 870, Training Loss: 0.04945816621184349, LR: 0.001
Time, 2019-01-07T01:42:19, Epoch: 6, Batch: 880, Training Loss: 0.04832859113812447, LR: 0.001
Time, 2019-01-07T01:42:20, Epoch: 6, Batch: 890, Training Loss: 0.024191907420754433, LR: 0.001
Time, 2019-01-07T01:42:21, Epoch: 6, Batch: 900, Training Loss: 0.027830979228019713, LR: 0.001
Time, 2019-01-07T01:42:22, Epoch: 6, Batch: 910, Training Loss: 0.040206486359238625, LR: 0.001
Time, 2019-01-07T01:42:23, Epoch: 6, Batch: 920, Training Loss: 0.02124688997864723, LR: 0.001
Time, 2019-01-07T01:42:25, Epoch: 6, Batch: 930, Training Loss: 0.03979832082986832, LR: 0.001
Epoch: 6, Validation Top 1 acc: 98.62659454345703
Epoch: 6, Validation Top 5 acc: 99.99005126953125
Epoch: 6, Validation Set Loss: 0.04572491720318794
Start training epoch 7
Time, 2019-01-07T01:42:34, Epoch: 7, Batch: 10, Training Loss: 0.051590695977211, LR: 0.001
Time, 2019-01-07T01:42:35, Epoch: 7, Batch: 20, Training Loss: 0.08079857528209686, LR: 0.001
Time, 2019-01-07T01:42:36, Epoch: 7, Batch: 30, Training Loss: 0.045614130795001984, LR: 0.001
Time, 2019-01-07T01:42:38, Epoch: 7, Batch: 40, Training Loss: 0.036506551504135135, LR: 0.001
Time, 2019-01-07T01:42:39, Epoch: 7, Batch: 50, Training Loss: 0.02201073318719864, LR: 0.001
Time, 2019-01-07T01:42:41, Epoch: 7, Batch: 60, Training Loss: 0.03279552236199379, LR: 0.001
Time, 2019-01-07T01:42:42, Epoch: 7, Batch: 70, Training Loss: 0.023771192878484726, LR: 0.001
Time, 2019-01-07T01:42:44, Epoch: 7, Batch: 80, Training Loss: 0.02057339921593666, LR: 0.001
Time, 2019-01-07T01:42:45, Epoch: 7, Batch: 90, Training Loss: 0.03639711514115333, LR: 0.001
Time, 2019-01-07T01:42:46, Epoch: 7, Batch: 100, Training Loss: 0.025676653534173966, LR: 0.001
Time, 2019-01-07T01:42:47, Epoch: 7, Batch: 110, Training Loss: 0.02732246443629265, LR: 0.001
Time, 2019-01-07T01:42:49, Epoch: 7, Batch: 120, Training Loss: 0.015911924839019775, LR: 0.001
Time, 2019-01-07T01:42:50, Epoch: 7, Batch: 130, Training Loss: 0.044980581104755404, LR: 0.001
Time, 2019-01-07T01:42:51, Epoch: 7, Batch: 140, Training Loss: 0.04047750309109688, LR: 0.001
Time, 2019-01-07T01:42:52, Epoch: 7, Batch: 150, Training Loss: 0.05148762054741383, LR: 0.001
Time, 2019-01-07T01:42:53, Epoch: 7, Batch: 160, Training Loss: 0.04873244352638721, LR: 0.001
Time, 2019-01-07T01:42:55, Epoch: 7, Batch: 170, Training Loss: 0.02704046554863453, LR: 0.001
Time, 2019-01-07T01:42:56, Epoch: 7, Batch: 180, Training Loss: 0.018044276535511015, LR: 0.001
Time, 2019-01-07T01:42:57, Epoch: 7, Batch: 190, Training Loss: 0.019131183996796607, LR: 0.001
Time, 2019-01-07T01:42:59, Epoch: 7, Batch: 200, Training Loss: 0.033710266649723056, LR: 0.001
Time, 2019-01-07T01:43:00, Epoch: 7, Batch: 210, Training Loss: 0.03400886282324791, LR: 0.001
Time, 2019-01-07T01:43:01, Epoch: 7, Batch: 220, Training Loss: 0.02922610379755497, LR: 0.001
Time, 2019-01-07T01:43:03, Epoch: 7, Batch: 230, Training Loss: 0.03331146612763405, LR: 0.001
Time, 2019-01-07T01:43:04, Epoch: 7, Batch: 240, Training Loss: 0.024259207397699357, LR: 0.001
Time, 2019-01-07T01:43:05, Epoch: 7, Batch: 250, Training Loss: 0.02710815966129303, LR: 0.001
Time, 2019-01-07T01:43:06, Epoch: 7, Batch: 260, Training Loss: 0.053243833035230635, LR: 0.001
Time, 2019-01-07T01:43:07, Epoch: 7, Batch: 270, Training Loss: 0.02951066792011261, LR: 0.001
Time, 2019-01-07T01:43:09, Epoch: 7, Batch: 280, Training Loss: 0.04924279898405075, LR: 0.001
Time, 2019-01-07T01:43:10, Epoch: 7, Batch: 290, Training Loss: 0.01134573332965374, LR: 0.001
Time, 2019-01-07T01:43:11, Epoch: 7, Batch: 300, Training Loss: 0.0239043191075325, LR: 0.001
Time, 2019-01-07T01:43:12, Epoch: 7, Batch: 310, Training Loss: 0.03143688663840294, LR: 0.001
Time, 2019-01-07T01:43:13, Epoch: 7, Batch: 320, Training Loss: 0.03548827990889549, LR: 0.001
Time, 2019-01-07T01:43:15, Epoch: 7, Batch: 330, Training Loss: 0.02741943746805191, LR: 0.001
Time, 2019-01-07T01:43:16, Epoch: 7, Batch: 340, Training Loss: 0.04097783453762531, LR: 0.001
Time, 2019-01-07T01:43:17, Epoch: 7, Batch: 350, Training Loss: 0.030223411321640015, LR: 0.001
Time, 2019-01-07T01:43:18, Epoch: 7, Batch: 360, Training Loss: 0.020386553183197976, LR: 0.001
Time, 2019-01-07T01:43:19, Epoch: 7, Batch: 370, Training Loss: 0.028181207180023194, LR: 0.001
Time, 2019-01-07T01:43:21, Epoch: 7, Batch: 380, Training Loss: 0.024394112452864648, LR: 0.001
Time, 2019-01-07T01:43:22, Epoch: 7, Batch: 390, Training Loss: 0.028783094137907028, LR: 0.001
Time, 2019-01-07T01:43:24, Epoch: 7, Batch: 400, Training Loss: 0.02919185906648636, LR: 0.001
Time, 2019-01-07T01:43:25, Epoch: 7, Batch: 410, Training Loss: 0.01726432964205742, LR: 0.001
Time, 2019-01-07T01:43:27, Epoch: 7, Batch: 420, Training Loss: 0.029007542133331298, LR: 0.001
Time, 2019-01-07T01:43:28, Epoch: 7, Batch: 430, Training Loss: 0.030718071013689043, LR: 0.001
Time, 2019-01-07T01:43:30, Epoch: 7, Batch: 440, Training Loss: 0.024883380532264708, LR: 0.001
Time, 2019-01-07T01:43:31, Epoch: 7, Batch: 450, Training Loss: 0.03020591028034687, LR: 0.001
Time, 2019-01-07T01:43:33, Epoch: 7, Batch: 460, Training Loss: 0.027378123626112937, LR: 0.001
Time, 2019-01-07T01:43:34, Epoch: 7, Batch: 470, Training Loss: 0.016807713359594346, LR: 0.001
Time, 2019-01-07T01:43:35, Epoch: 7, Batch: 480, Training Loss: 0.029825763404369356, LR: 0.001
Time, 2019-01-07T01:43:37, Epoch: 7, Batch: 490, Training Loss: 0.031389070302248, LR: 0.001
Time, 2019-01-07T01:43:39, Epoch: 7, Batch: 500, Training Loss: 0.040591941028833387, LR: 0.001
Time, 2019-01-07T01:43:41, Epoch: 7, Batch: 510, Training Loss: 0.030490076914429665, LR: 0.001
Time, 2019-01-07T01:43:42, Epoch: 7, Batch: 520, Training Loss: 0.03396379351615906, LR: 0.001
Time, 2019-01-07T01:43:44, Epoch: 7, Batch: 530, Training Loss: 0.04793278798460961, LR: 0.001
Time, 2019-01-07T01:43:45, Epoch: 7, Batch: 540, Training Loss: 0.01593332663178444, LR: 0.001
Time, 2019-01-07T01:43:47, Epoch: 7, Batch: 550, Training Loss: 0.012747564166784287, LR: 0.001
Time, 2019-01-07T01:43:49, Epoch: 7, Batch: 560, Training Loss: 0.033417101949453354, LR: 0.001
Time, 2019-01-07T01:43:50, Epoch: 7, Batch: 570, Training Loss: 0.01999455541372299, LR: 0.001
Time, 2019-01-07T01:43:51, Epoch: 7, Batch: 580, Training Loss: 0.025415372103452682, LR: 0.001
Time, 2019-01-07T01:43:53, Epoch: 7, Batch: 590, Training Loss: 0.02180939242243767, LR: 0.001
Time, 2019-01-07T01:43:54, Epoch: 7, Batch: 600, Training Loss: 0.01799812987446785, LR: 0.001
Time, 2019-01-07T01:43:56, Epoch: 7, Batch: 610, Training Loss: 0.028373721241950988, LR: 0.001
Time, 2019-01-07T01:43:58, Epoch: 7, Batch: 620, Training Loss: 0.041381184756755826, LR: 0.001
Time, 2019-01-07T01:44:00, Epoch: 7, Batch: 630, Training Loss: 0.03380503058433533, LR: 0.001
Time, 2019-01-07T01:44:01, Epoch: 7, Batch: 640, Training Loss: 0.04653957486152649, LR: 0.001
Time, 2019-01-07T01:44:03, Epoch: 7, Batch: 650, Training Loss: 0.0333185575902462, LR: 0.001
Time, 2019-01-07T01:44:04, Epoch: 7, Batch: 660, Training Loss: 0.0433011956512928, LR: 0.001
Time, 2019-01-07T01:44:05, Epoch: 7, Batch: 670, Training Loss: 0.025088170170783998, LR: 0.001
Time, 2019-01-07T01:44:06, Epoch: 7, Batch: 680, Training Loss: 0.031338848918676374, LR: 0.001
Time, 2019-01-07T01:44:08, Epoch: 7, Batch: 690, Training Loss: 0.015593387559056282, LR: 0.001
Time, 2019-01-07T01:44:09, Epoch: 7, Batch: 700, Training Loss: 0.016058534383773804, LR: 0.001
Time, 2019-01-07T01:44:10, Epoch: 7, Batch: 710, Training Loss: 0.015069375932216644, LR: 0.001
Time, 2019-01-07T01:44:11, Epoch: 7, Batch: 720, Training Loss: 0.019979999959468843, LR: 0.001
Time, 2019-01-07T01:44:13, Epoch: 7, Batch: 730, Training Loss: 0.030014394968748092, LR: 0.001
Time, 2019-01-07T01:44:14, Epoch: 7, Batch: 740, Training Loss: 0.026520036906003953, LR: 0.001
Time, 2019-01-07T01:44:16, Epoch: 7, Batch: 750, Training Loss: 0.019770118594169616, LR: 0.001
Time, 2019-01-07T01:44:17, Epoch: 7, Batch: 760, Training Loss: 0.031338462978601454, LR: 0.001
Time, 2019-01-07T01:44:19, Epoch: 7, Batch: 770, Training Loss: 0.037493279203772545, LR: 0.001
Time, 2019-01-07T01:44:20, Epoch: 7, Batch: 780, Training Loss: 0.02358669862151146, LR: 0.001
Time, 2019-01-07T01:44:21, Epoch: 7, Batch: 790, Training Loss: 0.019887610897421838, LR: 0.001
Time, 2019-01-07T01:44:22, Epoch: 7, Batch: 800, Training Loss: 0.039655955880880354, LR: 0.001
Time, 2019-01-07T01:44:24, Epoch: 7, Batch: 810, Training Loss: 0.026310159265995024, LR: 0.001
Time, 2019-01-07T01:44:26, Epoch: 7, Batch: 820, Training Loss: 0.01500968188047409, LR: 0.001
Time, 2019-01-07T01:44:27, Epoch: 7, Batch: 830, Training Loss: 0.019751469045877455, LR: 0.001
Time, 2019-01-07T01:44:29, Epoch: 7, Batch: 840, Training Loss: 0.030199836194515228, LR: 0.001
Time, 2019-01-07T01:44:30, Epoch: 7, Batch: 850, Training Loss: 0.04003826379776001, LR: 0.001
Time, 2019-01-07T01:44:32, Epoch: 7, Batch: 860, Training Loss: 0.019434231519699096, LR: 0.001
Time, 2019-01-07T01:44:33, Epoch: 7, Batch: 870, Training Loss: 0.022188615053892136, LR: 0.001
Time, 2019-01-07T01:44:35, Epoch: 7, Batch: 880, Training Loss: 0.013459399342536926, LR: 0.001
Time, 2019-01-07T01:44:36, Epoch: 7, Batch: 890, Training Loss: 0.040155014395713805, LR: 0.001
Time, 2019-01-07T01:44:38, Epoch: 7, Batch: 900, Training Loss: 0.02781699001789093, LR: 0.001
Time, 2019-01-07T01:44:41, Epoch: 7, Batch: 910, Training Loss: 0.0204820416867733, LR: 0.001
Time, 2019-01-07T01:44:43, Epoch: 7, Batch: 920, Training Loss: 0.029103434085845946, LR: 0.001
Time, 2019-01-07T01:44:44, Epoch: 7, Batch: 930, Training Loss: 0.043535680323839185, LR: 0.001
Epoch: 7, Validation Top 1 acc: 98.81568145751953
Epoch: 7, Validation Top 5 acc: 99.99005126953125
Epoch: 7, Validation Set Loss: 0.039532650262117386
Start training epoch 8
Time, 2019-01-07T01:44:52, Epoch: 8, Batch: 10, Training Loss: 0.01703254207968712, LR: 0.001
Time, 2019-01-07T01:44:53, Epoch: 8, Batch: 20, Training Loss: 0.015504302084445953, LR: 0.001
Time, 2019-01-07T01:44:55, Epoch: 8, Batch: 30, Training Loss: 0.03842403143644333, LR: 0.001
Time, 2019-01-07T01:44:56, Epoch: 8, Batch: 40, Training Loss: 0.013411392271518708, LR: 0.001
Time, 2019-01-07T01:44:57, Epoch: 8, Batch: 50, Training Loss: 0.009145242720842361, LR: 0.001
Time, 2019-01-07T01:44:58, Epoch: 8, Batch: 60, Training Loss: 0.025353823974728584, LR: 0.001
Time, 2019-01-07T01:44:59, Epoch: 8, Batch: 70, Training Loss: 0.025533074885606764, LR: 0.001
Time, 2019-01-07T01:45:00, Epoch: 8, Batch: 80, Training Loss: 0.03338749036192894, LR: 0.001
Time, 2019-01-07T01:45:02, Epoch: 8, Batch: 90, Training Loss: 0.025192293524742126, LR: 0.001
Time, 2019-01-07T01:45:03, Epoch: 8, Batch: 100, Training Loss: 0.023423349112272264, LR: 0.001
Time, 2019-01-07T01:45:04, Epoch: 8, Batch: 110, Training Loss: 0.029497284442186356, LR: 0.001
Time, 2019-01-07T01:45:05, Epoch: 8, Batch: 120, Training Loss: 0.023866099119186402, LR: 0.001
Time, 2019-01-07T01:45:07, Epoch: 8, Batch: 130, Training Loss: 0.021441824734210968, LR: 0.001
Time, 2019-01-07T01:45:08, Epoch: 8, Batch: 140, Training Loss: 0.02821434587240219, LR: 0.001
Time, 2019-01-07T01:45:09, Epoch: 8, Batch: 150, Training Loss: 0.024212723970413207, LR: 0.001
Time, 2019-01-07T01:45:10, Epoch: 8, Batch: 160, Training Loss: 0.013124325126409531, LR: 0.001
Time, 2019-01-07T01:45:12, Epoch: 8, Batch: 170, Training Loss: 0.019940883666276932, LR: 0.001
Time, 2019-01-07T01:45:13, Epoch: 8, Batch: 180, Training Loss: 0.0241928368806839, LR: 0.001
Time, 2019-01-07T01:45:14, Epoch: 8, Batch: 190, Training Loss: 0.02144695967435837, LR: 0.001
Time, 2019-01-07T01:45:15, Epoch: 8, Batch: 200, Training Loss: 0.008825061470270157, LR: 0.001
Time, 2019-01-07T01:45:17, Epoch: 8, Batch: 210, Training Loss: 0.042599426954984664, LR: 0.001
Time, 2019-01-07T01:45:18, Epoch: 8, Batch: 220, Training Loss: 0.015696457773447036, LR: 0.001
Time, 2019-01-07T01:45:19, Epoch: 8, Batch: 230, Training Loss: 0.016196829080581666, LR: 0.001
Time, 2019-01-07T01:45:21, Epoch: 8, Batch: 240, Training Loss: 0.01741204783320427, LR: 0.001
Time, 2019-01-07T01:45:22, Epoch: 8, Batch: 250, Training Loss: 0.027149175107479096, LR: 0.001
Time, 2019-01-07T01:45:23, Epoch: 8, Batch: 260, Training Loss: 0.02147434800863266, LR: 0.001
Time, 2019-01-07T01:45:25, Epoch: 8, Batch: 270, Training Loss: 0.035414820164442064, LR: 0.001
Time, 2019-01-07T01:45:26, Epoch: 8, Batch: 280, Training Loss: 0.01757011339068413, LR: 0.001
Time, 2019-01-07T01:45:27, Epoch: 8, Batch: 290, Training Loss: 0.02397567704319954, LR: 0.001
Time, 2019-01-07T01:45:29, Epoch: 8, Batch: 300, Training Loss: 0.015622558444738388, LR: 0.001
Time, 2019-01-07T01:45:30, Epoch: 8, Batch: 310, Training Loss: 0.02557310611009598, LR: 0.001
Time, 2019-01-07T01:45:32, Epoch: 8, Batch: 320, Training Loss: 0.05223284810781479, LR: 0.001
Time, 2019-01-07T01:45:33, Epoch: 8, Batch: 330, Training Loss: 0.025495601072907447, LR: 0.001
Time, 2019-01-07T01:45:34, Epoch: 8, Batch: 340, Training Loss: 0.04456832632422447, LR: 0.001
Time, 2019-01-07T01:45:35, Epoch: 8, Batch: 350, Training Loss: 0.013017483055591583, LR: 0.001
Time, 2019-01-07T01:45:36, Epoch: 8, Batch: 360, Training Loss: 0.043408625200390814, LR: 0.001
Time, 2019-01-07T01:45:36, Epoch: 8, Batch: 370, Training Loss: 0.030117587745189668, LR: 0.001
Time, 2019-01-07T01:45:37, Epoch: 8, Batch: 380, Training Loss: 0.02493097148835659, LR: 0.001
Time, 2019-01-07T01:45:38, Epoch: 8, Batch: 390, Training Loss: 0.027253658324480057, LR: 0.001
Time, 2019-01-07T01:45:38, Epoch: 8, Batch: 400, Training Loss: 0.019354839250445365, LR: 0.001
Time, 2019-01-07T01:45:39, Epoch: 8, Batch: 410, Training Loss: 0.029424026608467102, LR: 0.001
Time, 2019-01-07T01:45:40, Epoch: 8, Batch: 420, Training Loss: 0.013169119134545326, LR: 0.001
Time, 2019-01-07T01:45:40, Epoch: 8, Batch: 430, Training Loss: 0.02975298687815666, LR: 0.001
Time, 2019-01-07T01:45:41, Epoch: 8, Batch: 440, Training Loss: 0.01990440711379051, LR: 0.001
Time, 2019-01-07T01:45:42, Epoch: 8, Batch: 450, Training Loss: 0.019171987473964692, LR: 0.001
Time, 2019-01-07T01:45:43, Epoch: 8, Batch: 460, Training Loss: 0.021030636876821517, LR: 0.001
Time, 2019-01-07T01:45:43, Epoch: 8, Batch: 470, Training Loss: 0.015414417535066605, LR: 0.001
Time, 2019-01-07T01:45:44, Epoch: 8, Batch: 480, Training Loss: 0.015104832500219345, LR: 0.001
Time, 2019-01-07T01:45:45, Epoch: 8, Batch: 490, Training Loss: 0.0211426742374897, LR: 0.001
Time, 2019-01-07T01:45:45, Epoch: 8, Batch: 500, Training Loss: 0.03149597942829132, LR: 0.001
Time, 2019-01-07T01:45:46, Epoch: 8, Batch: 510, Training Loss: 0.024029050022363663, LR: 0.001
Time, 2019-01-07T01:45:47, Epoch: 8, Batch: 520, Training Loss: 0.014454769343137741, LR: 0.001
Time, 2019-01-07T01:45:47, Epoch: 8, Batch: 530, Training Loss: 0.01910187378525734, LR: 0.001
Time, 2019-01-07T01:45:48, Epoch: 8, Batch: 540, Training Loss: 0.026104527711868285, LR: 0.001
Time, 2019-01-07T01:45:49, Epoch: 8, Batch: 550, Training Loss: 0.02524009793996811, LR: 0.001
Time, 2019-01-07T01:45:49, Epoch: 8, Batch: 560, Training Loss: 0.01940673924982548, LR: 0.001
Time, 2019-01-07T01:45:50, Epoch: 8, Batch: 570, Training Loss: 0.016181672737002373, LR: 0.001
Time, 2019-01-07T01:45:51, Epoch: 8, Batch: 580, Training Loss: 0.02487057149410248, LR: 0.001
Time, 2019-01-07T01:45:52, Epoch: 8, Batch: 590, Training Loss: 0.035276158899068835, LR: 0.001
Time, 2019-01-07T01:45:52, Epoch: 8, Batch: 600, Training Loss: 0.01369449645280838, LR: 0.001
Time, 2019-01-07T01:45:53, Epoch: 8, Batch: 610, Training Loss: 0.030225738137960433, LR: 0.001
Time, 2019-01-07T01:45:53, Epoch: 8, Batch: 620, Training Loss: 0.010403981059789657, LR: 0.001
Time, 2019-01-07T01:45:54, Epoch: 8, Batch: 630, Training Loss: 0.01239015907049179, LR: 0.001
Time, 2019-01-07T01:45:55, Epoch: 8, Batch: 640, Training Loss: 0.019170741736888885, LR: 0.001
Time, 2019-01-07T01:45:55, Epoch: 8, Batch: 650, Training Loss: 0.028773263841867448, LR: 0.001
Time, 2019-01-07T01:45:56, Epoch: 8, Batch: 660, Training Loss: 0.023457663506269454, LR: 0.001
Time, 2019-01-07T01:45:57, Epoch: 8, Batch: 670, Training Loss: 0.01853100061416626, LR: 0.001
Time, 2019-01-07T01:45:57, Epoch: 8, Batch: 680, Training Loss: 0.022669255360960962, LR: 0.001
Time, 2019-01-07T01:45:58, Epoch: 8, Batch: 690, Training Loss: 0.01652459315955639, LR: 0.001
Time, 2019-01-07T01:45:59, Epoch: 8, Batch: 700, Training Loss: 0.042436741292476654, LR: 0.001
Time, 2019-01-07T01:46:00, Epoch: 8, Batch: 710, Training Loss: 0.014247473329305649, LR: 0.001
Time, 2019-01-07T01:46:00, Epoch: 8, Batch: 720, Training Loss: 0.01673860028386116, LR: 0.001
Time, 2019-01-07T01:46:01, Epoch: 8, Batch: 730, Training Loss: 0.0418044276535511, LR: 0.001
Time, 2019-01-07T01:46:02, Epoch: 8, Batch: 740, Training Loss: 0.019465597718954085, LR: 0.001
Time, 2019-01-07T01:46:02, Epoch: 8, Batch: 750, Training Loss: 0.01668654978275299, LR: 0.001
Time, 2019-01-07T01:46:03, Epoch: 8, Batch: 760, Training Loss: 0.022464578598737718, LR: 0.001
Time, 2019-01-07T01:46:04, Epoch: 8, Batch: 770, Training Loss: 0.023290865868330003, LR: 0.001
Time, 2019-01-07T01:46:04, Epoch: 8, Batch: 780, Training Loss: 0.007289945334196091, LR: 0.001
Time, 2019-01-07T01:46:05, Epoch: 8, Batch: 790, Training Loss: 0.015187004208564758, LR: 0.001
Time, 2019-01-07T01:46:06, Epoch: 8, Batch: 800, Training Loss: 0.02845626026391983, LR: 0.001
Time, 2019-01-07T01:46:06, Epoch: 8, Batch: 810, Training Loss: 0.020179328322410584, LR: 0.001
Time, 2019-01-07T01:46:07, Epoch: 8, Batch: 820, Training Loss: 0.03311105519533157, LR: 0.001
Time, 2019-01-07T01:46:08, Epoch: 8, Batch: 830, Training Loss: 0.027404148876667023, LR: 0.001
Time, 2019-01-07T01:46:08, Epoch: 8, Batch: 840, Training Loss: 0.027931854501366614, LR: 0.001
Time, 2019-01-07T01:46:09, Epoch: 8, Batch: 850, Training Loss: 0.024723389744758607, LR: 0.001
Time, 2019-01-07T01:46:10, Epoch: 8, Batch: 860, Training Loss: 0.012795976549386977, LR: 0.001
Time, 2019-01-07T01:46:10, Epoch: 8, Batch: 870, Training Loss: 0.04311150275170803, LR: 0.001
Time, 2019-01-07T01:46:11, Epoch: 8, Batch: 880, Training Loss: 0.05273366719484329, LR: 0.001
Time, 2019-01-07T01:46:12, Epoch: 8, Batch: 890, Training Loss: 0.02438509464263916, LR: 0.001
Time, 2019-01-07T01:46:12, Epoch: 8, Batch: 900, Training Loss: 0.03851120993494987, LR: 0.001
Time, 2019-01-07T01:46:13, Epoch: 8, Batch: 910, Training Loss: 0.027996163815259933, LR: 0.001
Time, 2019-01-07T01:46:14, Epoch: 8, Batch: 920, Training Loss: 0.027746286243200302, LR: 0.001
Time, 2019-01-07T01:46:14, Epoch: 8, Batch: 930, Training Loss: 0.04088960662484169, LR: 0.001
Epoch: 8, Validation Top 1 acc: 98.42755126953125
Epoch: 8, Validation Top 5 acc: 100.0
Epoch: 8, Validation Set Loss: 0.049568310379981995
Start training epoch 9
Time, 2019-01-07T01:46:17, Epoch: 9, Batch: 10, Training Loss: 0.0339804545044899, LR: 0.001
Time, 2019-01-07T01:46:18, Epoch: 9, Batch: 20, Training Loss: 0.018437417596578597, LR: 0.001
Time, 2019-01-07T01:46:19, Epoch: 9, Batch: 30, Training Loss: 0.005522295832633972, LR: 0.001
Time, 2019-01-07T01:46:19, Epoch: 9, Batch: 40, Training Loss: 0.006393180042505264, LR: 0.001
Time, 2019-01-07T01:46:20, Epoch: 9, Batch: 50, Training Loss: 0.016627445816993713, LR: 0.001
Time, 2019-01-07T01:46:21, Epoch: 9, Batch: 60, Training Loss: 0.015528123080730438, LR: 0.001
Time, 2019-01-07T01:46:21, Epoch: 9, Batch: 70, Training Loss: 0.024542887508869172, LR: 0.001
Time, 2019-01-07T01:46:22, Epoch: 9, Batch: 80, Training Loss: 0.034163308516144755, LR: 0.001
Time, 2019-01-07T01:46:23, Epoch: 9, Batch: 90, Training Loss: 0.01355358138680458, LR: 0.001
Time, 2019-01-07T01:46:24, Epoch: 9, Batch: 100, Training Loss: 0.03183259405195713, LR: 0.001
Time, 2019-01-07T01:46:24, Epoch: 9, Batch: 110, Training Loss: 0.05437229052186012, LR: 0.001
Time, 2019-01-07T01:46:25, Epoch: 9, Batch: 120, Training Loss: 0.026143403351306917, LR: 0.001
Time, 2019-01-07T01:46:26, Epoch: 9, Batch: 130, Training Loss: 0.009331028163433074, LR: 0.001
Time, 2019-01-07T01:46:26, Epoch: 9, Batch: 140, Training Loss: 0.05309392809867859, LR: 0.001
Time, 2019-01-07T01:46:27, Epoch: 9, Batch: 150, Training Loss: 0.016395801678299904, LR: 0.001
Time, 2019-01-07T01:46:28, Epoch: 9, Batch: 160, Training Loss: 0.02377288565039635, LR: 0.001
Time, 2019-01-07T01:46:28, Epoch: 9, Batch: 170, Training Loss: 0.013437538594007491, LR: 0.001
Time, 2019-01-07T01:46:29, Epoch: 9, Batch: 180, Training Loss: 0.007697625458240509, LR: 0.001
Time, 2019-01-07T01:46:30, Epoch: 9, Batch: 190, Training Loss: 0.012151975929737092, LR: 0.001
Time, 2019-01-07T01:46:30, Epoch: 9, Batch: 200, Training Loss: 0.007956965267658234, LR: 0.001
Time, 2019-01-07T01:46:31, Epoch: 9, Batch: 210, Training Loss: 0.016498369723558427, LR: 0.001
Time, 2019-01-07T01:46:32, Epoch: 9, Batch: 220, Training Loss: 0.016700638085603715, LR: 0.001
Time, 2019-01-07T01:46:32, Epoch: 9, Batch: 230, Training Loss: 0.023164847493171693, LR: 0.001
Time, 2019-01-07T01:46:33, Epoch: 9, Batch: 240, Training Loss: 0.017365565150976182, LR: 0.001
Time, 2019-01-07T01:46:34, Epoch: 9, Batch: 250, Training Loss: 0.02445101588964462, LR: 0.001
Time, 2019-01-07T01:46:34, Epoch: 9, Batch: 260, Training Loss: 0.013392852991819382, LR: 0.001
Time, 2019-01-07T01:46:35, Epoch: 9, Batch: 270, Training Loss: 0.01742984801530838, LR: 0.001
Time, 2019-01-07T01:46:36, Epoch: 9, Batch: 280, Training Loss: 0.00890946164727211, LR: 0.001
Time, 2019-01-07T01:46:37, Epoch: 9, Batch: 290, Training Loss: 0.017231851816177368, LR: 0.001
Time, 2019-01-07T01:46:37, Epoch: 9, Batch: 300, Training Loss: 0.021686540544033052, LR: 0.001
Time, 2019-01-07T01:46:38, Epoch: 9, Batch: 310, Training Loss: 0.011888301372528077, LR: 0.001
Time, 2019-01-07T01:46:39, Epoch: 9, Batch: 320, Training Loss: 0.020218563824892045, LR: 0.001
Time, 2019-01-07T01:46:39, Epoch: 9, Batch: 330, Training Loss: 0.00962454527616501, LR: 0.001
Time, 2019-01-07T01:46:40, Epoch: 9, Batch: 340, Training Loss: 0.027086850255727768, LR: 0.001
Time, 2019-01-07T01:46:41, Epoch: 9, Batch: 350, Training Loss: 0.02613794356584549, LR: 0.001
Time, 2019-01-07T01:46:41, Epoch: 9, Batch: 360, Training Loss: 0.01277347058057785, LR: 0.001
Time, 2019-01-07T01:46:42, Epoch: 9, Batch: 370, Training Loss: 0.0315858356654644, LR: 0.001
Time, 2019-01-07T01:46:43, Epoch: 9, Batch: 380, Training Loss: 0.01010173261165619, LR: 0.001
Time, 2019-01-07T01:46:43, Epoch: 9, Batch: 390, Training Loss: 0.03721936717629433, LR: 0.001
Time, 2019-01-07T01:46:44, Epoch: 9, Batch: 400, Training Loss: 0.029468893259763717, LR: 0.001
Time, 2019-01-07T01:46:45, Epoch: 9, Batch: 410, Training Loss: 0.021030739694833756, LR: 0.001
Time, 2019-01-07T01:46:45, Epoch: 9, Batch: 420, Training Loss: 0.017942766100168227, LR: 0.001
Time, 2019-01-07T01:46:46, Epoch: 9, Batch: 430, Training Loss: 0.01143483817577362, LR: 0.001
Time, 2019-01-07T01:46:47, Epoch: 9, Batch: 440, Training Loss: 0.027024508267641068, LR: 0.001
Time, 2019-01-07T01:46:47, Epoch: 9, Batch: 450, Training Loss: 0.04588151648640633, LR: 0.001
Time, 2019-01-07T01:46:48, Epoch: 9, Batch: 460, Training Loss: 0.030653731524944307, LR: 0.001
Time, 2019-01-07T01:46:49, Epoch: 9, Batch: 470, Training Loss: 0.02684728093445301, LR: 0.001
Time, 2019-01-07T01:46:50, Epoch: 9, Batch: 480, Training Loss: 0.03259751945734024, LR: 0.001
Time, 2019-01-07T01:46:50, Epoch: 9, Batch: 490, Training Loss: 0.010571867972612382, LR: 0.001
Time, 2019-01-07T01:46:51, Epoch: 9, Batch: 500, Training Loss: 0.026541304588317872, LR: 0.001
Time, 2019-01-07T01:46:52, Epoch: 9, Batch: 510, Training Loss: 0.0209576390683651, LR: 0.001
Time, 2019-01-07T01:46:52, Epoch: 9, Batch: 520, Training Loss: 0.014576952904462814, LR: 0.001
Time, 2019-01-07T01:46:53, Epoch: 9, Batch: 530, Training Loss: 0.026490730792284013, LR: 0.001
Time, 2019-01-07T01:46:54, Epoch: 9, Batch: 540, Training Loss: 0.025227865576744078, LR: 0.001
Time, 2019-01-07T01:46:54, Epoch: 9, Batch: 550, Training Loss: 0.016196265071630477, LR: 0.001
Time, 2019-01-07T01:46:55, Epoch: 9, Batch: 560, Training Loss: 0.024842768907546997, LR: 0.001
Time, 2019-01-07T01:46:56, Epoch: 9, Batch: 570, Training Loss: 0.013676007837057113, LR: 0.001
Time, 2019-01-07T01:46:56, Epoch: 9, Batch: 580, Training Loss: 0.02971286103129387, LR: 0.001
Time, 2019-01-07T01:46:57, Epoch: 9, Batch: 590, Training Loss: 0.009587916731834411, LR: 0.001
Time, 2019-01-07T01:46:58, Epoch: 9, Batch: 600, Training Loss: 0.019328317046165465, LR: 0.001
Time, 2019-01-07T01:46:58, Epoch: 9, Batch: 610, Training Loss: 0.021515509486198424, LR: 0.001
Time, 2019-01-07T01:46:59, Epoch: 9, Batch: 620, Training Loss: 0.018241237103939056, LR: 0.001
Time, 2019-01-07T01:47:00, Epoch: 9, Batch: 630, Training Loss: 0.0206730455160141, LR: 0.001
Time, 2019-01-07T01:47:00, Epoch: 9, Batch: 640, Training Loss: 0.013474906980991363, LR: 0.001
Time, 2019-01-07T01:47:01, Epoch: 9, Batch: 650, Training Loss: 0.03890999481081962, LR: 0.001
Time, 2019-01-07T01:47:02, Epoch: 9, Batch: 660, Training Loss: 0.03248140811920166, LR: 0.001
Time, 2019-01-07T01:47:02, Epoch: 9, Batch: 670, Training Loss: 0.016196347773075104, LR: 0.001
Time, 2019-01-07T01:47:03, Epoch: 9, Batch: 680, Training Loss: 0.012415527552366256, LR: 0.001
Time, 2019-01-07T01:47:04, Epoch: 9, Batch: 690, Training Loss: 0.024586442112922668, LR: 0.001
Time, 2019-01-07T01:47:05, Epoch: 9, Batch: 700, Training Loss: 0.02129998281598091, LR: 0.001
Time, 2019-01-07T01:47:05, Epoch: 9, Batch: 710, Training Loss: 0.026599362120032312, LR: 0.001
Time, 2019-01-07T01:47:06, Epoch: 9, Batch: 720, Training Loss: 0.008128844201564789, LR: 0.001
Time, 2019-01-07T01:47:07, Epoch: 9, Batch: 730, Training Loss: 0.020395638793706893, LR: 0.001
Time, 2019-01-07T01:47:07, Epoch: 9, Batch: 740, Training Loss: 0.01879046857357025, LR: 0.001
Time, 2019-01-07T01:47:08, Epoch: 9, Batch: 750, Training Loss: 0.023636577278375627, LR: 0.001
Time, 2019-01-07T01:47:09, Epoch: 9, Batch: 760, Training Loss: 0.01600489318370819, LR: 0.001
Time, 2019-01-07T01:47:09, Epoch: 9, Batch: 770, Training Loss: 0.02266479581594467, LR: 0.001
Time, 2019-01-07T01:47:10, Epoch: 9, Batch: 780, Training Loss: 0.01545756682753563, LR: 0.001
Time, 2019-01-07T01:47:11, Epoch: 9, Batch: 790, Training Loss: 0.016536976397037505, LR: 0.001
Time, 2019-01-07T01:47:11, Epoch: 9, Batch: 800, Training Loss: 0.024095331132411957, LR: 0.001
Time, 2019-01-07T01:47:12, Epoch: 9, Batch: 810, Training Loss: 0.02956850603222847, LR: 0.001
Time, 2019-01-07T01:47:13, Epoch: 9, Batch: 820, Training Loss: 0.01650380939245224, LR: 0.001
Time, 2019-01-07T01:47:13, Epoch: 9, Batch: 830, Training Loss: 0.01278904229402542, LR: 0.001
Time, 2019-01-07T01:47:14, Epoch: 9, Batch: 840, Training Loss: 0.021840064972639083, LR: 0.001
Time, 2019-01-07T01:47:15, Epoch: 9, Batch: 850, Training Loss: 0.021892911940813064, LR: 0.001
Time, 2019-01-07T01:47:16, Epoch: 9, Batch: 860, Training Loss: 0.026071837916970254, LR: 0.001
Time, 2019-01-07T01:47:16, Epoch: 9, Batch: 870, Training Loss: 0.014774114638566972, LR: 0.001
Time, 2019-01-07T01:47:17, Epoch: 9, Batch: 880, Training Loss: 0.006287458539009094, LR: 0.001
Time, 2019-01-07T01:47:18, Epoch: 9, Batch: 890, Training Loss: 0.02843111753463745, LR: 0.001
Time, 2019-01-07T01:47:18, Epoch: 9, Batch: 900, Training Loss: 0.034488795697689055, LR: 0.001
Time, 2019-01-07T01:47:19, Epoch: 9, Batch: 910, Training Loss: 0.04022885411977768, LR: 0.001
Time, 2019-01-07T01:47:20, Epoch: 9, Batch: 920, Training Loss: 0.03283359780907631, LR: 0.001
Time, 2019-01-07T01:47:20, Epoch: 9, Batch: 930, Training Loss: 0.010389648377895355, LR: 0.001
Epoch: 9, Validation Top 1 acc: 98.73606872558594
Epoch: 9, Validation Top 5 acc: 100.0
Epoch: 9, Validation Set Loss: 0.043802253901958466
Start training epoch 10
Time, 2019-01-07T01:47:24, Epoch: 10, Batch: 10, Training Loss: 0.033047855645418164, LR: 0.001
Time, 2019-01-07T01:47:24, Epoch: 10, Batch: 20, Training Loss: 0.022196071594953536, LR: 0.001
Time, 2019-01-07T01:47:25, Epoch: 10, Batch: 30, Training Loss: 0.03378620892763138, LR: 0.001
Time, 2019-01-07T01:47:26, Epoch: 10, Batch: 40, Training Loss: 0.022411569952964783, LR: 0.001
Time, 2019-01-07T01:47:26, Epoch: 10, Batch: 50, Training Loss: 0.017553943395614623, LR: 0.001
Time, 2019-01-07T01:47:27, Epoch: 10, Batch: 60, Training Loss: 0.01616735979914665, LR: 0.001
Time, 2019-01-07T01:47:28, Epoch: 10, Batch: 70, Training Loss: 0.018436575308442116, LR: 0.001
Time, 2019-01-07T01:47:28, Epoch: 10, Batch: 80, Training Loss: 0.01585654616355896, LR: 0.001
Time, 2019-01-07T01:47:29, Epoch: 10, Batch: 90, Training Loss: 0.01304376870393753, LR: 0.001
Time, 2019-01-07T01:47:30, Epoch: 10, Batch: 100, Training Loss: 0.011790464073419571, LR: 0.001
Time, 2019-01-07T01:47:30, Epoch: 10, Batch: 110, Training Loss: 0.01189674362540245, LR: 0.001
Time, 2019-01-07T01:47:31, Epoch: 10, Batch: 120, Training Loss: 0.014226241409778595, LR: 0.001
Time, 2019-01-07T01:47:32, Epoch: 10, Batch: 130, Training Loss: 0.011975793540477753, LR: 0.001
Time, 2019-01-07T01:47:32, Epoch: 10, Batch: 140, Training Loss: 0.010919582098722458, LR: 0.001
Time, 2019-01-07T01:47:33, Epoch: 10, Batch: 150, Training Loss: 0.01649737134575844, LR: 0.001
Time, 2019-01-07T01:47:34, Epoch: 10, Batch: 160, Training Loss: 0.017944043129682542, LR: 0.001
Time, 2019-01-07T01:47:34, Epoch: 10, Batch: 170, Training Loss: 0.007228674739599228, LR: 0.001
Time, 2019-01-07T01:47:35, Epoch: 10, Batch: 180, Training Loss: 0.018227216601371766, LR: 0.001
Time, 2019-01-07T01:47:36, Epoch: 10, Batch: 190, Training Loss: 0.01183357760310173, LR: 0.001
Time, 2019-01-07T01:47:36, Epoch: 10, Batch: 200, Training Loss: 0.02255602404475212, LR: 0.001
Time, 2019-01-07T01:47:37, Epoch: 10, Batch: 210, Training Loss: 0.025228703022003175, LR: 0.001
Time, 2019-01-07T01:47:38, Epoch: 10, Batch: 220, Training Loss: 0.012422894686460495, LR: 0.001
Time, 2019-01-07T01:47:39, Epoch: 10, Batch: 230, Training Loss: 0.011404121667146683, LR: 0.001
Time, 2019-01-07T01:47:39, Epoch: 10, Batch: 240, Training Loss: 0.01808162108063698, LR: 0.001
Time, 2019-01-07T01:47:40, Epoch: 10, Batch: 250, Training Loss: 0.02431352958083153, LR: 0.001
Time, 2019-01-07T01:47:41, Epoch: 10, Batch: 260, Training Loss: 0.02652830109000206, LR: 0.001
Time, 2019-01-07T01:47:41, Epoch: 10, Batch: 270, Training Loss: 0.011096851527690887, LR: 0.001
Time, 2019-01-07T01:47:42, Epoch: 10, Batch: 280, Training Loss: 0.044809861481189726, LR: 0.001
Time, 2019-01-07T01:47:43, Epoch: 10, Batch: 290, Training Loss: 0.010570564866065979, LR: 0.001
Time, 2019-01-07T01:47:43, Epoch: 10, Batch: 300, Training Loss: 0.03772159852087498, LR: 0.001
Time, 2019-01-07T01:47:44, Epoch: 10, Batch: 310, Training Loss: 0.01773711442947388, LR: 0.001
Time, 2019-01-07T01:47:45, Epoch: 10, Batch: 320, Training Loss: 0.01760856658220291, LR: 0.001
Time, 2019-01-07T01:47:45, Epoch: 10, Batch: 330, Training Loss: 0.01415264755487442, LR: 0.001
Time, 2019-01-07T01:47:46, Epoch: 10, Batch: 340, Training Loss: 0.014640017598867416, LR: 0.001
Time, 2019-01-07T01:47:47, Epoch: 10, Batch: 350, Training Loss: 0.03146672174334526, LR: 0.001
Time, 2019-01-07T01:47:47, Epoch: 10, Batch: 360, Training Loss: 0.020458512008190155, LR: 0.001
Time, 2019-01-07T01:47:48, Epoch: 10, Batch: 370, Training Loss: 0.016507237404584884, LR: 0.001
Time, 2019-01-07T01:47:49, Epoch: 10, Batch: 380, Training Loss: 0.016956320405006407, LR: 0.001
Time, 2019-01-07T01:47:49, Epoch: 10, Batch: 390, Training Loss: 0.018809036910533906, LR: 0.001
Time, 2019-01-07T01:47:50, Epoch: 10, Batch: 400, Training Loss: 0.010684864223003387, LR: 0.001
Time, 2019-01-07T01:47:51, Epoch: 10, Batch: 410, Training Loss: 0.03554681241512299, LR: 0.001
Time, 2019-01-07T01:47:51, Epoch: 10, Batch: 420, Training Loss: 0.014963922649621963, LR: 0.001
Time, 2019-01-07T01:47:52, Epoch: 10, Batch: 430, Training Loss: 0.026658423244953156, LR: 0.001
Time, 2019-01-07T01:47:53, Epoch: 10, Batch: 440, Training Loss: 0.0239401251077652, LR: 0.001
Time, 2019-01-07T01:47:53, Epoch: 10, Batch: 450, Training Loss: 0.039679862931370734, LR: 0.001
Time, 2019-01-07T01:47:54, Epoch: 10, Batch: 460, Training Loss: 0.005198863893747329, LR: 0.001
Time, 2019-01-07T01:47:55, Epoch: 10, Batch: 470, Training Loss: 0.029681956768035887, LR: 0.001
Time, 2019-01-07T01:47:55, Epoch: 10, Batch: 480, Training Loss: 0.012686067819595337, LR: 0.001
Time, 2019-01-07T01:47:56, Epoch: 10, Batch: 490, Training Loss: 0.016329178214073183, LR: 0.001
Time, 2019-01-07T01:47:57, Epoch: 10, Batch: 500, Training Loss: 0.017466506361961363, LR: 0.001
Time, 2019-01-07T01:47:57, Epoch: 10, Batch: 510, Training Loss: 0.014764785021543502, LR: 0.001
Time, 2019-01-07T01:47:58, Epoch: 10, Batch: 520, Training Loss: 0.018394313752651215, LR: 0.001
Time, 2019-01-07T01:47:59, Epoch: 10, Batch: 530, Training Loss: 0.03586350977420807, LR: 0.001
Time, 2019-01-07T01:47:59, Epoch: 10, Batch: 540, Training Loss: 0.03144740089774132, LR: 0.001
Time, 2019-01-07T01:48:00, Epoch: 10, Batch: 550, Training Loss: 0.014467342942953109, LR: 0.001
Time, 2019-01-07T01:48:01, Epoch: 10, Batch: 560, Training Loss: 0.023408490419387817, LR: 0.001
Time, 2019-01-07T01:48:01, Epoch: 10, Batch: 570, Training Loss: 0.02982834503054619, LR: 0.001
Time, 2019-01-07T01:48:02, Epoch: 10, Batch: 580, Training Loss: 0.014411096274852753, LR: 0.001
Time, 2019-01-07T01:48:03, Epoch: 10, Batch: 590, Training Loss: 0.021560222655534745, LR: 0.001
Time, 2019-01-07T01:48:03, Epoch: 10, Batch: 600, Training Loss: 0.028413960337638856, LR: 0.001
Time, 2019-01-07T01:48:04, Epoch: 10, Batch: 610, Training Loss: 0.008493991196155548, LR: 0.001
Time, 2019-01-07T01:48:05, Epoch: 10, Batch: 620, Training Loss: 0.026056647300720215, LR: 0.001
Time, 2019-01-07T01:48:05, Epoch: 10, Batch: 630, Training Loss: 0.01918051615357399, LR: 0.001
Time, 2019-01-07T01:48:06, Epoch: 10, Batch: 640, Training Loss: 0.008772944658994674, LR: 0.001
Time, 2019-01-07T01:48:07, Epoch: 10, Batch: 650, Training Loss: 0.01667538732290268, LR: 0.001
Time, 2019-01-07T01:48:07, Epoch: 10, Batch: 660, Training Loss: 0.01963006928563118, LR: 0.001
Time, 2019-01-07T01:48:08, Epoch: 10, Batch: 670, Training Loss: 0.010952039062976838, LR: 0.001
Time, 2019-01-07T01:48:09, Epoch: 10, Batch: 680, Training Loss: 0.023167774826288224, LR: 0.001
Time, 2019-01-07T01:48:09, Epoch: 10, Batch: 690, Training Loss: 0.014598267525434494, LR: 0.001
Time, 2019-01-07T01:48:10, Epoch: 10, Batch: 700, Training Loss: 0.0358884796500206, LR: 0.001
Time, 2019-01-07T01:48:11, Epoch: 10, Batch: 710, Training Loss: 0.02435195818543434, LR: 0.001
Time, 2019-01-07T01:48:11, Epoch: 10, Batch: 720, Training Loss: 0.02173677012324333, LR: 0.001
Time, 2019-01-07T01:48:12, Epoch: 10, Batch: 730, Training Loss: 0.02384989708662033, LR: 0.001
Time, 2019-01-07T01:48:13, Epoch: 10, Batch: 740, Training Loss: 0.017192164808511733, LR: 0.001
Time, 2019-01-07T01:48:13, Epoch: 10, Batch: 750, Training Loss: 0.020730186253786087, LR: 0.001
Time, 2019-01-07T01:48:14, Epoch: 10, Batch: 760, Training Loss: 0.03873270340263844, LR: 0.001
Time, 2019-01-07T01:48:15, Epoch: 10, Batch: 770, Training Loss: 0.035418134182691574, LR: 0.001
Time, 2019-01-07T01:48:16, Epoch: 10, Batch: 780, Training Loss: 0.02795801907777786, LR: 0.001
Time, 2019-01-07T01:48:17, Epoch: 10, Batch: 790, Training Loss: 0.04185286238789558, LR: 0.001
Time, 2019-01-07T01:48:18, Epoch: 10, Batch: 800, Training Loss: 0.01689644232392311, LR: 0.001
Time, 2019-01-07T01:48:18, Epoch: 10, Batch: 810, Training Loss: 0.014686661213636399, LR: 0.001
Time, 2019-01-07T01:48:19, Epoch: 10, Batch: 820, Training Loss: 0.012084434926509856, LR: 0.001
Time, 2019-01-07T01:48:20, Epoch: 10, Batch: 830, Training Loss: 0.024286220967769622, LR: 0.001
Time, 2019-01-07T01:48:20, Epoch: 10, Batch: 840, Training Loss: 0.019197646528482437, LR: 0.001
Time, 2019-01-07T01:48:21, Epoch: 10, Batch: 850, Training Loss: 0.0134168341755867, LR: 0.001
Time, 2019-01-07T01:48:22, Epoch: 10, Batch: 860, Training Loss: 0.022338875383138657, LR: 0.001
Time, 2019-01-07T01:48:22, Epoch: 10, Batch: 870, Training Loss: 0.02385479062795639, LR: 0.001
Time, 2019-01-07T01:48:23, Epoch: 10, Batch: 880, Training Loss: 0.023137353360652924, LR: 0.001
Time, 2019-01-07T01:48:24, Epoch: 10, Batch: 890, Training Loss: 0.03782100826501846, LR: 0.001
Time, 2019-01-07T01:48:24, Epoch: 10, Batch: 900, Training Loss: 0.03439076133072376, LR: 0.001
Time, 2019-01-07T01:48:25, Epoch: 10, Batch: 910, Training Loss: 0.037303194403648376, LR: 0.001
Time, 2019-01-07T01:48:26, Epoch: 10, Batch: 920, Training Loss: 0.04402979463338852, LR: 0.001
Time, 2019-01-07T01:48:27, Epoch: 10, Batch: 930, Training Loss: 0.03292268142104149, LR: 0.001
Epoch: 10, Validation Top 1 acc: 98.41759490966797
Epoch: 10, Validation Top 5 acc: 100.0
Epoch: 10, Validation Set Loss: 0.050952039659023285
Start training epoch 11
Time, 2019-01-07T01:48:30, Epoch: 11, Batch: 10, Training Loss: 0.022566551715135573, LR: 0.001
Time, 2019-01-07T01:48:31, Epoch: 11, Batch: 20, Training Loss: 0.017619292438030242, LR: 0.001
Time, 2019-01-07T01:48:32, Epoch: 11, Batch: 30, Training Loss: 0.02184625342488289, LR: 0.001
Time, 2019-01-07T01:48:32, Epoch: 11, Batch: 40, Training Loss: 0.027458394318819045, LR: 0.001
Time, 2019-01-07T01:48:33, Epoch: 11, Batch: 50, Training Loss: 0.010386619716882706, LR: 0.001
Time, 2019-01-07T01:48:34, Epoch: 11, Batch: 60, Training Loss: 0.018469273298978805, LR: 0.001
Time, 2019-01-07T01:48:34, Epoch: 11, Batch: 70, Training Loss: 0.017911487072706223, LR: 0.001
Time, 2019-01-07T01:48:35, Epoch: 11, Batch: 80, Training Loss: 0.018858978524804115, LR: 0.001
Time, 2019-01-07T01:48:36, Epoch: 11, Batch: 90, Training Loss: 0.025111666321754454, LR: 0.001
Time, 2019-01-07T01:48:36, Epoch: 11, Batch: 100, Training Loss: 0.02391551062464714, LR: 0.001
Time, 2019-01-07T01:48:37, Epoch: 11, Batch: 110, Training Loss: 0.03825519010424614, LR: 0.001
Time, 2019-01-07T01:48:38, Epoch: 11, Batch: 120, Training Loss: 0.016310161724686623, LR: 0.001
Time, 2019-01-07T01:48:38, Epoch: 11, Batch: 130, Training Loss: 0.02672770172357559, LR: 0.001
Time, 2019-01-07T01:48:39, Epoch: 11, Batch: 140, Training Loss: 0.02865636870265007, LR: 0.001
Time, 2019-01-07T01:48:40, Epoch: 11, Batch: 150, Training Loss: 0.013377604633569717, LR: 0.001
Time, 2019-01-07T01:48:41, Epoch: 11, Batch: 160, Training Loss: 0.01806309148669243, LR: 0.001
Time, 2019-01-07T01:48:41, Epoch: 11, Batch: 170, Training Loss: 0.023739653825759887, LR: 0.001
Time, 2019-01-07T01:48:42, Epoch: 11, Batch: 180, Training Loss: 0.020514991879463196, LR: 0.001
Time, 2019-01-07T01:48:43, Epoch: 11, Batch: 190, Training Loss: 0.019703826308250426, LR: 0.001
Time, 2019-01-07T01:48:43, Epoch: 11, Batch: 200, Training Loss: 0.018923220783472063, LR: 0.001
Time, 2019-01-07T01:48:44, Epoch: 11, Batch: 210, Training Loss: 0.019752305746078492, LR: 0.001
Time, 2019-01-07T01:48:45, Epoch: 11, Batch: 220, Training Loss: 0.015182959288358689, LR: 0.001
Time, 2019-01-07T01:48:45, Epoch: 11, Batch: 230, Training Loss: 0.023599198460578917, LR: 0.001
Time, 2019-01-07T01:48:46, Epoch: 11, Batch: 240, Training Loss: 0.018089182674884796, LR: 0.001
Time, 2019-01-07T01:48:46, Epoch: 11, Batch: 250, Training Loss: 0.020684628933668136, LR: 0.001
Time, 2019-01-07T01:48:47, Epoch: 11, Batch: 260, Training Loss: 0.012606003880500793, LR: 0.001
Time, 2019-01-07T01:48:48, Epoch: 11, Batch: 270, Training Loss: 0.013963454961776733, LR: 0.001
Time, 2019-01-07T01:48:48, Epoch: 11, Batch: 280, Training Loss: 0.010835929214954377, LR: 0.001
Time, 2019-01-07T01:48:49, Epoch: 11, Batch: 290, Training Loss: 0.016447072476148607, LR: 0.001
Time, 2019-01-07T01:48:50, Epoch: 11, Batch: 300, Training Loss: 0.003937194496393204, LR: 0.001
Time, 2019-01-07T01:48:50, Epoch: 11, Batch: 310, Training Loss: 0.005843985080718994, LR: 0.001
Time, 2019-01-07T01:48:51, Epoch: 11, Batch: 320, Training Loss: 0.008720449358224868, LR: 0.001
Time, 2019-01-07T01:48:52, Epoch: 11, Batch: 330, Training Loss: 0.003313203901052475, LR: 0.001
Time, 2019-01-07T01:48:52, Epoch: 11, Batch: 340, Training Loss: 0.02115919515490532, LR: 0.001
Time, 2019-01-07T01:48:53, Epoch: 11, Batch: 350, Training Loss: 0.010687801986932755, LR: 0.001
Time, 2019-01-07T01:48:54, Epoch: 11, Batch: 360, Training Loss: 0.016571073979139327, LR: 0.001
Time, 2019-01-07T01:48:55, Epoch: 11, Batch: 370, Training Loss: 0.013106118887662888, LR: 0.001
Time, 2019-01-07T01:48:55, Epoch: 11, Batch: 380, Training Loss: 0.012160362303256988, LR: 0.001
Time, 2019-01-07T01:48:56, Epoch: 11, Batch: 390, Training Loss: 0.008351057022809982, LR: 0.001
Time, 2019-01-07T01:48:57, Epoch: 11, Batch: 400, Training Loss: 0.01341543197631836, LR: 0.001
Time, 2019-01-07T01:48:57, Epoch: 11, Batch: 410, Training Loss: 0.021522436290979385, LR: 0.001
Time, 2019-01-07T01:48:58, Epoch: 11, Batch: 420, Training Loss: 0.004674052447080612, LR: 0.001
Time, 2019-01-07T01:48:59, Epoch: 11, Batch: 430, Training Loss: 0.013889860361814499, LR: 0.001
Time, 2019-01-07T01:48:59, Epoch: 11, Batch: 440, Training Loss: 0.018906045705080032, LR: 0.001
Time, 2019-01-07T01:49:00, Epoch: 11, Batch: 450, Training Loss: 0.02046143114566803, LR: 0.001
Time, 2019-01-07T01:49:01, Epoch: 11, Batch: 460, Training Loss: 0.022475504875183107, LR: 0.001
Time, 2019-01-07T01:49:01, Epoch: 11, Batch: 470, Training Loss: 0.008285405486822129, LR: 0.001
Time, 2019-01-07T01:49:02, Epoch: 11, Batch: 480, Training Loss: 0.010867615044116975, LR: 0.001
Time, 2019-01-07T01:49:03, Epoch: 11, Batch: 490, Training Loss: 0.007759860157966614, LR: 0.001
Time, 2019-01-07T01:49:03, Epoch: 11, Batch: 500, Training Loss: 0.012142829596996307, LR: 0.001
Time, 2019-01-07T01:49:04, Epoch: 11, Batch: 510, Training Loss: 0.010522910952568054, LR: 0.001
Time, 2019-01-07T01:49:05, Epoch: 11, Batch: 520, Training Loss: 0.01504063531756401, LR: 0.001
Time, 2019-01-07T01:49:05, Epoch: 11, Batch: 530, Training Loss: 0.0158265121281147, LR: 0.001
Time, 2019-01-07T01:49:06, Epoch: 11, Batch: 540, Training Loss: 0.032080402970314024, LR: 0.001
Time, 2019-01-07T01:49:07, Epoch: 11, Batch: 550, Training Loss: 0.013375051692128182, LR: 0.001
Time, 2019-01-07T01:49:07, Epoch: 11, Batch: 560, Training Loss: 0.026114359498023987, LR: 0.001
Time, 2019-01-07T01:49:08, Epoch: 11, Batch: 570, Training Loss: 0.02645660862326622, LR: 0.001
Time, 2019-01-07T01:49:09, Epoch: 11, Batch: 580, Training Loss: 0.03350542262196541, LR: 0.001
Time, 2019-01-07T01:49:09, Epoch: 11, Batch: 590, Training Loss: 0.036089672893285754, LR: 0.001
Time, 2019-01-07T01:49:10, Epoch: 11, Batch: 600, Training Loss: 0.025043948739767074, LR: 0.001
Time, 2019-01-07T01:49:11, Epoch: 11, Batch: 610, Training Loss: 0.019886945188045502, LR: 0.001
Time, 2019-01-07T01:49:12, Epoch: 11, Batch: 620, Training Loss: 0.01624140813946724, LR: 0.001
Time, 2019-01-07T01:49:12, Epoch: 11, Batch: 630, Training Loss: 0.01394582837820053, LR: 0.001
Time, 2019-01-07T01:49:13, Epoch: 11, Batch: 640, Training Loss: 0.028248143941164018, LR: 0.001
Time, 2019-01-07T01:49:14, Epoch: 11, Batch: 650, Training Loss: 0.034608930349349976, LR: 0.001
Time, 2019-01-07T01:49:14, Epoch: 11, Batch: 660, Training Loss: 0.01733924262225628, LR: 0.001
Time, 2019-01-07T01:49:15, Epoch: 11, Batch: 670, Training Loss: 0.019134602695703506, LR: 0.001
Time, 2019-01-07T01:49:16, Epoch: 11, Batch: 680, Training Loss: 0.016332489252090455, LR: 0.001
Time, 2019-01-07T01:49:16, Epoch: 11, Batch: 690, Training Loss: 0.010142602026462555, LR: 0.001
Time, 2019-01-07T01:49:17, Epoch: 11, Batch: 700, Training Loss: 0.00874069482088089, LR: 0.001
Time, 2019-01-07T01:49:18, Epoch: 11, Batch: 710, Training Loss: 0.01701173111796379, LR: 0.001
Time, 2019-01-07T01:49:18, Epoch: 11, Batch: 720, Training Loss: 0.022379546612501144, LR: 0.001
Time, 2019-01-07T01:49:19, Epoch: 11, Batch: 730, Training Loss: 0.029122380912303923, LR: 0.001
Time, 2019-01-07T01:49:20, Epoch: 11, Batch: 740, Training Loss: 0.023493917286396028, LR: 0.001
Time, 2019-01-07T01:49:20, Epoch: 11, Batch: 750, Training Loss: 0.028989720344543456, LR: 0.001
Time, 2019-01-07T01:49:21, Epoch: 11, Batch: 760, Training Loss: 0.02555965855717659, LR: 0.001
Time, 2019-01-07T01:49:21, Epoch: 11, Batch: 770, Training Loss: 0.01851152554154396, LR: 0.001
Time, 2019-01-07T01:49:22, Epoch: 11, Batch: 780, Training Loss: 0.02357555627822876, LR: 0.001
Time, 2019-01-07T01:49:23, Epoch: 11, Batch: 790, Training Loss: 0.01273820623755455, LR: 0.001
Time, 2019-01-07T01:49:23, Epoch: 11, Batch: 800, Training Loss: 0.0203351192176342, LR: 0.001
Time, 2019-01-07T01:49:24, Epoch: 11, Batch: 810, Training Loss: 0.02909896373748779, LR: 0.001
Time, 2019-01-07T01:49:25, Epoch: 11, Batch: 820, Training Loss: 0.008405409753322601, LR: 0.001
Time, 2019-01-07T01:49:25, Epoch: 11, Batch: 830, Training Loss: 0.023647017031908035, LR: 0.001
Time, 2019-01-07T01:49:26, Epoch: 11, Batch: 840, Training Loss: 0.017715178430080414, LR: 0.001
Time, 2019-01-07T01:49:27, Epoch: 11, Batch: 850, Training Loss: 0.006069696694612503, LR: 0.001
Time, 2019-01-07T01:49:27, Epoch: 11, Batch: 860, Training Loss: 0.025093071907758713, LR: 0.001
Time, 2019-01-07T01:49:28, Epoch: 11, Batch: 870, Training Loss: 0.023057582974433898, LR: 0.001
Time, 2019-01-07T01:49:29, Epoch: 11, Batch: 880, Training Loss: 0.017295341938734055, LR: 0.001
Time, 2019-01-07T01:49:29, Epoch: 11, Batch: 890, Training Loss: 0.013245757669210434, LR: 0.001
Time, 2019-01-07T01:49:30, Epoch: 11, Batch: 900, Training Loss: 0.022294024378061293, LR: 0.001
Time, 2019-01-07T01:49:31, Epoch: 11, Batch: 910, Training Loss: 0.0232121042907238, LR: 0.001
Time, 2019-01-07T01:49:32, Epoch: 11, Batch: 920, Training Loss: 0.025394752621650696, LR: 0.001
Time, 2019-01-07T01:49:32, Epoch: 11, Batch: 930, Training Loss: 0.018840944021940233, LR: 0.001
Epoch: 11, Validation Top 1 acc: 98.76592254638672
Epoch: 11, Validation Top 5 acc: 100.0
Epoch: 11, Validation Set Loss: 0.03829903155565262
Start training epoch 12
Time, 2019-01-07T01:49:35, Epoch: 12, Batch: 10, Training Loss: 0.014330637454986573, LR: 0.001
Time, 2019-01-07T01:49:36, Epoch: 12, Batch: 20, Training Loss: 0.008083198964595795, LR: 0.001
Time, 2019-01-07T01:49:37, Epoch: 12, Batch: 30, Training Loss: 0.006878088414669037, LR: 0.001
Time, 2019-01-07T01:49:37, Epoch: 12, Batch: 40, Training Loss: 0.015407878905534744, LR: 0.001
Time, 2019-01-07T01:49:38, Epoch: 12, Batch: 50, Training Loss: 0.02292777895927429, LR: 0.001
Time, 2019-01-07T01:49:39, Epoch: 12, Batch: 60, Training Loss: 0.022371334582567216, LR: 0.001
Time, 2019-01-07T01:49:40, Epoch: 12, Batch: 70, Training Loss: 0.02528376057744026, LR: 0.001
Time, 2019-01-07T01:49:40, Epoch: 12, Batch: 80, Training Loss: 0.012522047758102417, LR: 0.001
Time, 2019-01-07T01:49:41, Epoch: 12, Batch: 90, Training Loss: 0.01453443318605423, LR: 0.001
Time, 2019-01-07T01:49:42, Epoch: 12, Batch: 100, Training Loss: 0.017538630217313767, LR: 0.001
Time, 2019-01-07T01:49:42, Epoch: 12, Batch: 110, Training Loss: 0.011737147718667984, LR: 0.001
Time, 2019-01-07T01:49:43, Epoch: 12, Batch: 120, Training Loss: 0.006896482408046722, LR: 0.001
Time, 2019-01-07T01:49:44, Epoch: 12, Batch: 130, Training Loss: 0.020110587030649184, LR: 0.001
Time, 2019-01-07T01:49:45, Epoch: 12, Batch: 140, Training Loss: 0.010025066137313843, LR: 0.001
Time, 2019-01-07T01:49:45, Epoch: 12, Batch: 150, Training Loss: 0.0072152093052864075, LR: 0.001
Time, 2019-01-07T01:49:46, Epoch: 12, Batch: 160, Training Loss: 0.01773502752184868, LR: 0.001
Time, 2019-01-07T01:49:47, Epoch: 12, Batch: 170, Training Loss: 0.0038387373089790343, LR: 0.001
Time, 2019-01-07T01:49:48, Epoch: 12, Batch: 180, Training Loss: 0.017098788172006607, LR: 0.001
Time, 2019-01-07T01:49:48, Epoch: 12, Batch: 190, Training Loss: 0.020518598705530168, LR: 0.001
Time, 2019-01-07T01:49:49, Epoch: 12, Batch: 200, Training Loss: 0.04351874589920044, LR: 0.001
Time, 2019-01-07T01:49:50, Epoch: 12, Batch: 210, Training Loss: 0.016478384286165236, LR: 0.001
Time, 2019-01-07T01:49:51, Epoch: 12, Batch: 220, Training Loss: 0.020218107104301452, LR: 0.001
Time, 2019-01-07T01:49:51, Epoch: 12, Batch: 230, Training Loss: 0.007735560834407807, LR: 0.001
Time, 2019-01-07T01:49:52, Epoch: 12, Batch: 240, Training Loss: 0.004271864891052246, LR: 0.001
Time, 2019-01-07T01:49:53, Epoch: 12, Batch: 250, Training Loss: 0.023804176598787308, LR: 0.001
Time, 2019-01-07T01:49:53, Epoch: 12, Batch: 260, Training Loss: 0.00452367514371872, LR: 0.001
Time, 2019-01-07T01:49:54, Epoch: 12, Batch: 270, Training Loss: 0.01619950383901596, LR: 0.001
Time, 2019-01-07T01:49:55, Epoch: 12, Batch: 280, Training Loss: 0.032005850970745084, LR: 0.001
Time, 2019-01-07T01:49:55, Epoch: 12, Batch: 290, Training Loss: 0.0159685380756855, LR: 0.001
Time, 2019-01-07T01:49:56, Epoch: 12, Batch: 300, Training Loss: 0.02458818256855011, LR: 0.001
Time, 2019-01-07T01:49:57, Epoch: 12, Batch: 310, Training Loss: 0.012565138190984726, LR: 0.001
Time, 2019-01-07T01:49:57, Epoch: 12, Batch: 320, Training Loss: 0.012597333639860153, LR: 0.001
Time, 2019-01-07T01:49:58, Epoch: 12, Batch: 330, Training Loss: 0.011113103479146957, LR: 0.001
Time, 2019-01-07T01:49:59, Epoch: 12, Batch: 340, Training Loss: 0.004091082513332367, LR: 0.001
Time, 2019-01-07T01:49:59, Epoch: 12, Batch: 350, Training Loss: 0.018895564973354338, LR: 0.001
Time, 2019-01-07T01:50:00, Epoch: 12, Batch: 360, Training Loss: 0.015402845293283462, LR: 0.001
Time, 2019-01-07T01:50:01, Epoch: 12, Batch: 370, Training Loss: 0.013547994196414948, LR: 0.001
Time, 2019-01-07T01:50:01, Epoch: 12, Batch: 380, Training Loss: 0.015272406488656997, LR: 0.001
Time, 2019-01-07T01:50:02, Epoch: 12, Batch: 390, Training Loss: 0.01579780727624893, LR: 0.001
Time, 2019-01-07T01:50:03, Epoch: 12, Batch: 400, Training Loss: 0.011837289482355118, LR: 0.001
Time, 2019-01-07T01:50:04, Epoch: 12, Batch: 410, Training Loss: 0.023838752508163454, LR: 0.001
Time, 2019-01-07T01:50:04, Epoch: 12, Batch: 420, Training Loss: 0.01484612375497818, LR: 0.001
Time, 2019-01-07T01:50:05, Epoch: 12, Batch: 430, Training Loss: 0.018383650481700896, LR: 0.001
Time, 2019-01-07T01:50:06, Epoch: 12, Batch: 440, Training Loss: 0.012575770169496537, LR: 0.001
Time, 2019-01-07T01:50:07, Epoch: 12, Batch: 450, Training Loss: 0.011566900461912156, LR: 0.001
Time, 2019-01-07T01:50:07, Epoch: 12, Batch: 460, Training Loss: 0.010267135500907899, LR: 0.001
Time, 2019-01-07T01:50:08, Epoch: 12, Batch: 470, Training Loss: 0.01454242318868637, LR: 0.001
Time, 2019-01-07T01:50:09, Epoch: 12, Batch: 480, Training Loss: 0.013054610043764115, LR: 0.001
Time, 2019-01-07T01:50:10, Epoch: 12, Batch: 490, Training Loss: 0.012755770236253738, LR: 0.001
Time, 2019-01-07T01:50:10, Epoch: 12, Batch: 500, Training Loss: 0.017832306027412415, LR: 0.001
Time, 2019-01-07T01:50:11, Epoch: 12, Batch: 510, Training Loss: 0.010448980331420898, LR: 0.001
Time, 2019-01-07T01:50:12, Epoch: 12, Batch: 520, Training Loss: 0.008475421369075775, LR: 0.001
Time, 2019-01-07T01:50:13, Epoch: 12, Batch: 530, Training Loss: 0.009461610019207001, LR: 0.001
Time, 2019-01-07T01:50:13, Epoch: 12, Batch: 540, Training Loss: 0.0039049185812473296, LR: 0.001
Time, 2019-01-07T01:50:14, Epoch: 12, Batch: 550, Training Loss: 0.023257584869861604, LR: 0.001
Time, 2019-01-07T01:50:15, Epoch: 12, Batch: 560, Training Loss: 0.007633177936077118, LR: 0.001
Time, 2019-01-07T01:50:15, Epoch: 12, Batch: 570, Training Loss: 0.008788719028234481, LR: 0.001
Time, 2019-01-07T01:50:16, Epoch: 12, Batch: 580, Training Loss: 0.013560215383768082, LR: 0.001
Time, 2019-01-07T01:50:16, Epoch: 12, Batch: 590, Training Loss: 0.02417110130190849, LR: 0.001
Time, 2019-01-07T01:50:17, Epoch: 12, Batch: 600, Training Loss: 0.016523800045251846, LR: 0.001
Time, 2019-01-07T01:50:18, Epoch: 12, Batch: 610, Training Loss: 0.018795726448297502, LR: 0.001
Time, 2019-01-07T01:50:18, Epoch: 12, Batch: 620, Training Loss: 0.013785221427679063, LR: 0.001
Time, 2019-01-07T01:50:19, Epoch: 12, Batch: 630, Training Loss: 0.028049232065677644, LR: 0.001
Time, 2019-01-07T01:50:20, Epoch: 12, Batch: 640, Training Loss: 0.029150088131427766, LR: 0.001
Time, 2019-01-07T01:50:20, Epoch: 12, Batch: 650, Training Loss: 0.015869949758052827, LR: 0.001
Time, 2019-01-07T01:50:21, Epoch: 12, Batch: 660, Training Loss: 0.011791258305311202, LR: 0.001
Time, 2019-01-07T01:50:22, Epoch: 12, Batch: 670, Training Loss: 0.025534140318632124, LR: 0.001
Time, 2019-01-07T01:50:22, Epoch: 12, Batch: 680, Training Loss: 0.006628210842609406, LR: 0.001
Time, 2019-01-07T01:50:23, Epoch: 12, Batch: 690, Training Loss: 0.018208225816488267, LR: 0.001
Time, 2019-01-07T01:50:24, Epoch: 12, Batch: 700, Training Loss: 0.0025828033685684204, LR: 0.001
Time, 2019-01-07T01:50:24, Epoch: 12, Batch: 710, Training Loss: 0.00842154249548912, LR: 0.001
Time, 2019-01-07T01:50:25, Epoch: 12, Batch: 720, Training Loss: 0.014601647108793258, LR: 0.001
Time, 2019-01-07T01:50:26, Epoch: 12, Batch: 730, Training Loss: 0.013522229343652725, LR: 0.001
Time, 2019-01-07T01:50:26, Epoch: 12, Batch: 740, Training Loss: 0.030686213821172714, LR: 0.001
Time, 2019-01-07T01:50:27, Epoch: 12, Batch: 750, Training Loss: 0.02434043660759926, LR: 0.001
Time, 2019-01-07T01:50:28, Epoch: 12, Batch: 760, Training Loss: 0.017390450835227965, LR: 0.001
Time, 2019-01-07T01:50:28, Epoch: 12, Batch: 770, Training Loss: 0.01817302405834198, LR: 0.001
Time, 2019-01-07T01:50:29, Epoch: 12, Batch: 780, Training Loss: 0.012264689803123474, LR: 0.001
Time, 2019-01-07T01:50:30, Epoch: 12, Batch: 790, Training Loss: 0.004373916983604431, LR: 0.001
Time, 2019-01-07T01:50:30, Epoch: 12, Batch: 800, Training Loss: 0.007403662055730819, LR: 0.001
Time, 2019-01-07T01:50:31, Epoch: 12, Batch: 810, Training Loss: 0.01725372448563576, LR: 0.001
Time, 2019-01-07T01:50:32, Epoch: 12, Batch: 820, Training Loss: 0.020097102224826812, LR: 0.001
Time, 2019-01-07T01:50:33, Epoch: 12, Batch: 830, Training Loss: 0.010077320784330369, LR: 0.001
Time, 2019-01-07T01:50:33, Epoch: 12, Batch: 840, Training Loss: 0.008433130383491517, LR: 0.001
Time, 2019-01-07T01:50:34, Epoch: 12, Batch: 850, Training Loss: 0.01814621239900589, LR: 0.001
Time, 2019-01-07T01:50:34, Epoch: 12, Batch: 860, Training Loss: 0.014870242029428483, LR: 0.001
Time, 2019-01-07T01:50:35, Epoch: 12, Batch: 870, Training Loss: 0.011996186524629592, LR: 0.001
Time, 2019-01-07T01:50:36, Epoch: 12, Batch: 880, Training Loss: 0.005497320741415024, LR: 0.001
Time, 2019-01-07T01:50:36, Epoch: 12, Batch: 890, Training Loss: 0.01297500655055046, LR: 0.001
Time, 2019-01-07T01:50:37, Epoch: 12, Batch: 900, Training Loss: 0.029062210768461227, LR: 0.001
Time, 2019-01-07T01:50:38, Epoch: 12, Batch: 910, Training Loss: 0.02382717654109001, LR: 0.001
Time, 2019-01-07T01:50:38, Epoch: 12, Batch: 920, Training Loss: 0.020603576302528383, LR: 0.001
Time, 2019-01-07T01:50:39, Epoch: 12, Batch: 930, Training Loss: 0.03283052965998649, LR: 0.001
Epoch: 12, Validation Top 1 acc: 98.65644836425781
Epoch: 12, Validation Top 5 acc: 100.0
Epoch: 12, Validation Set Loss: 0.04794073849916458
Start training epoch 13
Time, 2019-01-07T01:50:43, Epoch: 13, Batch: 10, Training Loss: 0.01080610677599907, LR: 0.001
Time, 2019-01-07T01:50:43, Epoch: 13, Batch: 20, Training Loss: 0.007556004077196121, LR: 0.001
Time, 2019-01-07T01:50:44, Epoch: 13, Batch: 30, Training Loss: 0.011448878049850463, LR: 0.001
Time, 2019-01-07T01:50:45, Epoch: 13, Batch: 40, Training Loss: 0.017146658152341843, LR: 0.001
Time, 2019-01-07T01:50:45, Epoch: 13, Batch: 50, Training Loss: 0.013044440746307373, LR: 0.001
Time, 2019-01-07T01:50:46, Epoch: 13, Batch: 60, Training Loss: 0.007434828579425812, LR: 0.001
Time, 2019-01-07T01:50:47, Epoch: 13, Batch: 70, Training Loss: 0.007997023314237595, LR: 0.001
Time, 2019-01-07T01:50:47, Epoch: 13, Batch: 80, Training Loss: 0.008692340552806854, LR: 0.001
Time, 2019-01-07T01:50:48, Epoch: 13, Batch: 90, Training Loss: 0.009400928020477295, LR: 0.001
Time, 2019-01-07T01:50:49, Epoch: 13, Batch: 100, Training Loss: 0.0018867582082748413, LR: 0.001
Time, 2019-01-07T01:50:49, Epoch: 13, Batch: 110, Training Loss: 0.01455608382821083, LR: 0.001
Time, 2019-01-07T01:50:50, Epoch: 13, Batch: 120, Training Loss: 0.00498250350356102, LR: 0.001
Time, 2019-01-07T01:50:51, Epoch: 13, Batch: 130, Training Loss: 0.006979193538427353, LR: 0.001
Time, 2019-01-07T01:50:51, Epoch: 13, Batch: 140, Training Loss: 0.005109773576259613, LR: 0.001
Time, 2019-01-07T01:50:52, Epoch: 13, Batch: 150, Training Loss: 0.02232711762189865, LR: 0.001
Time, 2019-01-07T01:50:53, Epoch: 13, Batch: 160, Training Loss: 0.009492842108011245, LR: 0.001
Time, 2019-01-07T01:50:53, Epoch: 13, Batch: 170, Training Loss: 0.010350943356752396, LR: 0.001
Time, 2019-01-07T01:50:54, Epoch: 13, Batch: 180, Training Loss: 0.01792720630764961, LR: 0.001
Time, 2019-01-07T01:50:55, Epoch: 13, Batch: 190, Training Loss: 0.009328679740428924, LR: 0.001
Time, 2019-01-07T01:50:55, Epoch: 13, Batch: 200, Training Loss: 0.0077750861644744875, LR: 0.001
Time, 2019-01-07T01:50:56, Epoch: 13, Batch: 210, Training Loss: 0.0052135176956653595, LR: 0.001
Time, 2019-01-07T01:50:57, Epoch: 13, Batch: 220, Training Loss: 0.025814539194107054, LR: 0.001
Time, 2019-01-07T01:50:57, Epoch: 13, Batch: 230, Training Loss: 0.009103421121835709, LR: 0.001
Time, 2019-01-07T01:50:58, Epoch: 13, Batch: 240, Training Loss: 0.01569722071290016, LR: 0.001
Time, 2019-01-07T01:50:58, Epoch: 13, Batch: 250, Training Loss: 0.004960808902978897, LR: 0.001
Time, 2019-01-07T01:50:59, Epoch: 13, Batch: 260, Training Loss: 0.009413965791463853, LR: 0.001
Time, 2019-01-07T01:51:00, Epoch: 13, Batch: 270, Training Loss: 0.01392572969198227, LR: 0.001
Time, 2019-01-07T01:51:01, Epoch: 13, Batch: 280, Training Loss: 0.012556058168411256, LR: 0.001
Time, 2019-01-07T01:51:01, Epoch: 13, Batch: 290, Training Loss: 0.01030418798327446, LR: 0.001
Time, 2019-01-07T01:51:02, Epoch: 13, Batch: 300, Training Loss: 0.016349109634757043, LR: 0.001
Time, 2019-01-07T01:51:03, Epoch: 13, Batch: 310, Training Loss: 0.006530537456274033, LR: 0.001
Time, 2019-01-07T01:51:03, Epoch: 13, Batch: 320, Training Loss: 0.01247156336903572, LR: 0.001
Time, 2019-01-07T01:51:04, Epoch: 13, Batch: 330, Training Loss: 0.01495438739657402, LR: 0.001
Time, 2019-01-07T01:51:05, Epoch: 13, Batch: 340, Training Loss: 0.007063477486371994, LR: 0.001
Time, 2019-01-07T01:51:05, Epoch: 13, Batch: 350, Training Loss: 0.009866474568843842, LR: 0.001
Time, 2019-01-07T01:51:06, Epoch: 13, Batch: 360, Training Loss: 0.007398087531328201, LR: 0.001
Time, 2019-01-07T01:51:07, Epoch: 13, Batch: 370, Training Loss: 0.013592914119362831, LR: 0.001
Time, 2019-01-07T01:51:08, Epoch: 13, Batch: 380, Training Loss: 0.008175484091043472, LR: 0.001
Time, 2019-01-07T01:51:08, Epoch: 13, Batch: 390, Training Loss: 0.017697948217391967, LR: 0.001
Time, 2019-01-07T01:51:09, Epoch: 13, Batch: 400, Training Loss: 0.012299255281686784, LR: 0.001
Time, 2019-01-07T01:51:10, Epoch: 13, Batch: 410, Training Loss: 0.02278025895357132, LR: 0.001
Time, 2019-01-07T01:51:10, Epoch: 13, Batch: 420, Training Loss: 0.02715780884027481, LR: 0.001
Time, 2019-01-07T01:51:11, Epoch: 13, Batch: 430, Training Loss: 0.0067492187023162845, LR: 0.001
Time, 2019-01-07T01:51:12, Epoch: 13, Batch: 440, Training Loss: 0.01199415624141693, LR: 0.001
Time, 2019-01-07T01:51:13, Epoch: 13, Batch: 450, Training Loss: 0.011215613037347794, LR: 0.001
Time, 2019-01-07T01:51:14, Epoch: 13, Batch: 460, Training Loss: 0.02329101413488388, LR: 0.001
Time, 2019-01-07T01:51:14, Epoch: 13, Batch: 470, Training Loss: 0.011226686090230942, LR: 0.001
Time, 2019-01-07T01:51:15, Epoch: 13, Batch: 480, Training Loss: 0.016780171915888785, LR: 0.001
Time, 2019-01-07T01:51:16, Epoch: 13, Batch: 490, Training Loss: 0.012130308151245116, LR: 0.001
Time, 2019-01-07T01:51:17, Epoch: 13, Batch: 500, Training Loss: 0.025344638526439665, LR: 0.001
Time, 2019-01-07T01:51:18, Epoch: 13, Batch: 510, Training Loss: 0.021792471408843994, LR: 0.001
Time, 2019-01-07T01:51:18, Epoch: 13, Batch: 520, Training Loss: 0.00622105672955513, LR: 0.001
Time, 2019-01-07T01:51:19, Epoch: 13, Batch: 530, Training Loss: 0.012552732229232788, LR: 0.001
Time, 2019-01-07T01:51:20, Epoch: 13, Batch: 540, Training Loss: 0.008831334114074708, LR: 0.001
Time, 2019-01-07T01:51:21, Epoch: 13, Batch: 550, Training Loss: 0.027409330755472184, LR: 0.001
Time, 2019-01-07T01:51:22, Epoch: 13, Batch: 560, Training Loss: 0.003238668292760849, LR: 0.001
Time, 2019-01-07T01:51:22, Epoch: 13, Batch: 570, Training Loss: 0.011218389868736267, LR: 0.001
Time, 2019-01-07T01:51:23, Epoch: 13, Batch: 580, Training Loss: 0.005087023973464966, LR: 0.001
Time, 2019-01-07T01:51:23, Epoch: 13, Batch: 590, Training Loss: 0.016753627359867095, LR: 0.001
Time, 2019-01-07T01:51:24, Epoch: 13, Batch: 600, Training Loss: 0.011752497404813766, LR: 0.001
Time, 2019-01-07T01:51:25, Epoch: 13, Batch: 610, Training Loss: 0.007452582567930221, LR: 0.001
Time, 2019-01-07T01:51:25, Epoch: 13, Batch: 620, Training Loss: 0.014974241703748703, LR: 0.001
Time, 2019-01-07T01:51:26, Epoch: 13, Batch: 630, Training Loss: 0.007110843807458878, LR: 0.001
Time, 2019-01-07T01:51:27, Epoch: 13, Batch: 640, Training Loss: 0.009269486367702483, LR: 0.001
Time, 2019-01-07T01:51:27, Epoch: 13, Batch: 650, Training Loss: 0.006974167376756668, LR: 0.001
Time, 2019-01-07T01:51:28, Epoch: 13, Batch: 660, Training Loss: 0.012377148121595382, LR: 0.001
Time, 2019-01-07T01:51:29, Epoch: 13, Batch: 670, Training Loss: 0.016528083384037016, LR: 0.001
Time, 2019-01-07T01:51:29, Epoch: 13, Batch: 680, Training Loss: 0.009915911406278611, LR: 0.001
Time, 2019-01-07T01:51:30, Epoch: 13, Batch: 690, Training Loss: 0.010785410553216935, LR: 0.001
Time, 2019-01-07T01:51:31, Epoch: 13, Batch: 700, Training Loss: 0.015044917166233063, LR: 0.001
Time, 2019-01-07T01:51:31, Epoch: 13, Batch: 710, Training Loss: 0.013264324516057968, LR: 0.001
Time, 2019-01-07T01:51:32, Epoch: 13, Batch: 720, Training Loss: 0.010715200006961823, LR: 0.001
Time, 2019-01-07T01:51:33, Epoch: 13, Batch: 730, Training Loss: 0.016778161376714708, LR: 0.001
Time, 2019-01-07T01:51:34, Epoch: 13, Batch: 740, Training Loss: 0.00977591574192047, LR: 0.001
Time, 2019-01-07T01:51:34, Epoch: 13, Batch: 750, Training Loss: 0.02087334468960762, LR: 0.001
Time, 2019-01-07T01:51:35, Epoch: 13, Batch: 760, Training Loss: 0.018154271692037583, LR: 0.001
Time, 2019-01-07T01:51:36, Epoch: 13, Batch: 770, Training Loss: 0.021147268265485762, LR: 0.001
Time, 2019-01-07T01:51:36, Epoch: 13, Batch: 780, Training Loss: 0.02191687300801277, LR: 0.001
Time, 2019-01-07T01:51:37, Epoch: 13, Batch: 790, Training Loss: 0.008167033642530441, LR: 0.001
Time, 2019-01-07T01:51:38, Epoch: 13, Batch: 800, Training Loss: 0.015817543119192125, LR: 0.001
Time, 2019-01-07T01:51:38, Epoch: 13, Batch: 810, Training Loss: 0.01864766180515289, LR: 0.001
Time, 2019-01-07T01:51:39, Epoch: 13, Batch: 820, Training Loss: 0.008256994932889939, LR: 0.001
Time, 2019-01-07T01:51:40, Epoch: 13, Batch: 830, Training Loss: 0.01293480172753334, LR: 0.001
Time, 2019-01-07T01:51:40, Epoch: 13, Batch: 840, Training Loss: 0.012191544473171233, LR: 0.001
Time, 2019-01-07T01:51:41, Epoch: 13, Batch: 850, Training Loss: 0.02098582684993744, LR: 0.001
Time, 2019-01-07T01:51:42, Epoch: 13, Batch: 860, Training Loss: 0.01857592761516571, LR: 0.001
Time, 2019-01-07T01:51:42, Epoch: 13, Batch: 870, Training Loss: 0.02157391682267189, LR: 0.001
Time, 2019-01-07T01:51:43, Epoch: 13, Batch: 880, Training Loss: 0.021188710629940034, LR: 0.001
Time, 2019-01-07T01:51:43, Epoch: 13, Batch: 890, Training Loss: 0.024147797375917435, LR: 0.001
Time, 2019-01-07T01:51:44, Epoch: 13, Batch: 900, Training Loss: 0.011779092997312546, LR: 0.001
Time, 2019-01-07T01:51:45, Epoch: 13, Batch: 910, Training Loss: 0.01853424683213234, LR: 0.001
Time, 2019-01-07T01:51:46, Epoch: 13, Batch: 920, Training Loss: 0.008199435472488404, LR: 0.001
Time, 2019-01-07T01:51:46, Epoch: 13, Batch: 930, Training Loss: 0.007632721960544586, LR: 0.001
Epoch: 13, Validation Top 1 acc: 98.86544799804688
Epoch: 13, Validation Top 5 acc: 100.0
Epoch: 13, Validation Set Loss: 0.038832664489746094
Start training epoch 14
Time, 2019-01-07T01:51:49, Epoch: 14, Batch: 10, Training Loss: 0.014996055513620377, LR: 0.001
Time, 2019-01-07T01:51:50, Epoch: 14, Batch: 20, Training Loss: 0.004938466846942902, LR: 0.001
Time, 2019-01-07T01:51:51, Epoch: 14, Batch: 30, Training Loss: 0.005942311137914658, LR: 0.001
Time, 2019-01-07T01:51:51, Epoch: 14, Batch: 40, Training Loss: 0.015673405677080154, LR: 0.001
Time, 2019-01-07T01:51:52, Epoch: 14, Batch: 50, Training Loss: 0.013995712995529175, LR: 0.001
Time, 2019-01-07T01:51:53, Epoch: 14, Batch: 60, Training Loss: 0.015898123383522034, LR: 0.001
Time, 2019-01-07T01:51:53, Epoch: 14, Batch: 70, Training Loss: 0.0050304576754570006, LR: 0.001
Time, 2019-01-07T01:51:54, Epoch: 14, Batch: 80, Training Loss: 0.007045335322618485, LR: 0.001
Time, 2019-01-07T01:51:55, Epoch: 14, Batch: 90, Training Loss: 0.0064836166799068454, LR: 0.001
Time, 2019-01-07T01:51:55, Epoch: 14, Batch: 100, Training Loss: 0.002004481852054596, LR: 0.001
Time, 2019-01-07T01:51:56, Epoch: 14, Batch: 110, Training Loss: 0.006808819621801377, LR: 0.001
Time, 2019-01-07T01:51:57, Epoch: 14, Batch: 120, Training Loss: 0.0067830771207809445, LR: 0.001
Time, 2019-01-07T01:51:57, Epoch: 14, Batch: 130, Training Loss: 0.012479546666145324, LR: 0.001
Time, 2019-01-07T01:51:58, Epoch: 14, Batch: 140, Training Loss: 0.007631780952215195, LR: 0.001
Time, 2019-01-07T01:51:59, Epoch: 14, Batch: 150, Training Loss: 0.007924754172563553, LR: 0.001
Time, 2019-01-07T01:51:59, Epoch: 14, Batch: 160, Training Loss: 0.017110655456781386, LR: 0.001
Time, 2019-01-07T01:52:00, Epoch: 14, Batch: 170, Training Loss: 0.028332357108592988, LR: 0.001
Time, 2019-01-07T01:52:01, Epoch: 14, Batch: 180, Training Loss: 0.012465566396713257, LR: 0.001
Time, 2019-01-07T01:52:01, Epoch: 14, Batch: 190, Training Loss: 0.030867676436901092, LR: 0.001
Time, 2019-01-07T01:52:02, Epoch: 14, Batch: 200, Training Loss: 0.02051846757531166, LR: 0.001
Time, 2019-01-07T01:52:03, Epoch: 14, Batch: 210, Training Loss: 0.014939599484205247, LR: 0.001
Time, 2019-01-07T01:52:03, Epoch: 14, Batch: 220, Training Loss: 0.02044510319828987, LR: 0.001
Time, 2019-01-07T01:52:04, Epoch: 14, Batch: 230, Training Loss: 0.0081094428896904, LR: 0.001
Time, 2019-01-07T01:52:05, Epoch: 14, Batch: 240, Training Loss: 0.013633540272712708, LR: 0.001
Time, 2019-01-07T01:52:05, Epoch: 14, Batch: 250, Training Loss: 0.026427850872278214, LR: 0.001
Time, 2019-01-07T01:52:06, Epoch: 14, Batch: 260, Training Loss: 0.00770936906337738, LR: 0.001
Time, 2019-01-07T01:52:07, Epoch: 14, Batch: 270, Training Loss: 0.025051067769527435, LR: 0.001
Time, 2019-01-07T01:52:07, Epoch: 14, Batch: 280, Training Loss: 0.010861789435148239, LR: 0.001
Time, 2019-01-07T01:52:08, Epoch: 14, Batch: 290, Training Loss: 0.01509004905819893, LR: 0.001
Time, 2019-01-07T01:52:09, Epoch: 14, Batch: 300, Training Loss: 0.011907992511987686, LR: 0.001
Time, 2019-01-07T01:52:10, Epoch: 14, Batch: 310, Training Loss: 0.012447452545166016, LR: 0.001
Time, 2019-01-07T01:52:10, Epoch: 14, Batch: 320, Training Loss: 0.005176682025194168, LR: 0.001
Time, 2019-01-07T01:52:11, Epoch: 14, Batch: 330, Training Loss: 0.0339960977435112, LR: 0.001
Time, 2019-01-07T01:52:12, Epoch: 14, Batch: 340, Training Loss: 0.017696733027696608, LR: 0.001
Time, 2019-01-07T01:52:12, Epoch: 14, Batch: 350, Training Loss: 0.027684235572814943, LR: 0.001
Time, 2019-01-07T01:52:13, Epoch: 14, Batch: 360, Training Loss: 0.01579051911830902, LR: 0.001
Time, 2019-01-07T01:52:14, Epoch: 14, Batch: 370, Training Loss: 0.009246737509965897, LR: 0.001
Time, 2019-01-07T01:52:14, Epoch: 14, Batch: 380, Training Loss: 0.03239202052354813, LR: 0.001
Time, 2019-01-07T01:52:15, Epoch: 14, Batch: 390, Training Loss: 0.017785631120204926, LR: 0.001
Time, 2019-01-07T01:52:16, Epoch: 14, Batch: 400, Training Loss: 0.018755614757537842, LR: 0.001
Time, 2019-01-07T01:52:16, Epoch: 14, Batch: 410, Training Loss: 0.019323884695768356, LR: 0.001
Time, 2019-01-07T01:52:17, Epoch: 14, Batch: 420, Training Loss: 0.01321067363023758, LR: 0.001
Time, 2019-01-07T01:52:18, Epoch: 14, Batch: 430, Training Loss: 0.01719265282154083, LR: 0.001
Time, 2019-01-07T01:52:18, Epoch: 14, Batch: 440, Training Loss: 0.018231921643018723, LR: 0.001
Time, 2019-01-07T01:52:19, Epoch: 14, Batch: 450, Training Loss: 0.017693299055099487, LR: 0.001
Time, 2019-01-07T01:52:20, Epoch: 14, Batch: 460, Training Loss: 0.012115044146776199, LR: 0.001
Time, 2019-01-07T01:52:20, Epoch: 14, Batch: 470, Training Loss: 0.014496157318353653, LR: 0.001
Time, 2019-01-07T01:52:21, Epoch: 14, Batch: 480, Training Loss: 0.01922788918018341, LR: 0.001
Time, 2019-01-07T01:52:22, Epoch: 14, Batch: 490, Training Loss: 0.013529825955629349, LR: 0.001
Time, 2019-01-07T01:52:23, Epoch: 14, Batch: 500, Training Loss: 0.014492401480674743, LR: 0.001
Time, 2019-01-07T01:52:23, Epoch: 14, Batch: 510, Training Loss: 0.014657818526029587, LR: 0.001
Time, 2019-01-07T01:52:24, Epoch: 14, Batch: 520, Training Loss: 0.003806369751691818, LR: 0.001
Time, 2019-01-07T01:52:25, Epoch: 14, Batch: 530, Training Loss: 0.011980770528316498, LR: 0.001
Time, 2019-01-07T01:52:25, Epoch: 14, Batch: 540, Training Loss: 0.005154627561569214, LR: 0.001
Time, 2019-01-07T01:52:26, Epoch: 14, Batch: 550, Training Loss: 0.011203396320343017, LR: 0.001
Time, 2019-01-07T01:52:27, Epoch: 14, Batch: 560, Training Loss: 0.019996972382068635, LR: 0.001
Time, 2019-01-07T01:52:28, Epoch: 14, Batch: 570, Training Loss: 0.018070267885923384, LR: 0.001
Time, 2019-01-07T01:52:28, Epoch: 14, Batch: 580, Training Loss: 0.010522054135799408, LR: 0.001
Time, 2019-01-07T01:52:29, Epoch: 14, Batch: 590, Training Loss: 0.012175207585096359, LR: 0.001
Time, 2019-01-07T01:52:30, Epoch: 14, Batch: 600, Training Loss: 0.009939366579055786, LR: 0.001
Time, 2019-01-07T01:52:31, Epoch: 14, Batch: 610, Training Loss: 0.009965350478887558, LR: 0.001
Time, 2019-01-07T01:52:32, Epoch: 14, Batch: 620, Training Loss: 0.014991012960672378, LR: 0.001
Time, 2019-01-07T01:52:32, Epoch: 14, Batch: 630, Training Loss: 0.0078105881810188295, LR: 0.001
Time, 2019-01-07T01:52:33, Epoch: 14, Batch: 640, Training Loss: 0.020847152918577194, LR: 0.001
Time, 2019-01-07T01:52:34, Epoch: 14, Batch: 650, Training Loss: 0.008144301921129226, LR: 0.001
Time, 2019-01-07T01:52:35, Epoch: 14, Batch: 660, Training Loss: 0.010412172228097916, LR: 0.001
Time, 2019-01-07T01:52:36, Epoch: 14, Batch: 670, Training Loss: 0.009248431771993637, LR: 0.001
Time, 2019-01-07T01:52:36, Epoch: 14, Batch: 680, Training Loss: 0.009911281615495681, LR: 0.001
Time, 2019-01-07T01:52:37, Epoch: 14, Batch: 690, Training Loss: 0.017446264624595642, LR: 0.001
Time, 2019-01-07T01:52:38, Epoch: 14, Batch: 700, Training Loss: 0.03583438917994499, LR: 0.001
Time, 2019-01-07T01:52:38, Epoch: 14, Batch: 710, Training Loss: 0.01184052899479866, LR: 0.001
Time, 2019-01-07T01:52:39, Epoch: 14, Batch: 720, Training Loss: 0.02450516223907471, LR: 0.001
Time, 2019-01-07T01:52:40, Epoch: 14, Batch: 730, Training Loss: 0.018043845146894454, LR: 0.001
Time, 2019-01-07T01:52:40, Epoch: 14, Batch: 740, Training Loss: 0.02515217140316963, LR: 0.001
Time, 2019-01-07T01:52:41, Epoch: 14, Batch: 750, Training Loss: 0.024767449498176573, LR: 0.001
Time, 2019-01-07T01:52:42, Epoch: 14, Batch: 760, Training Loss: 0.02717069610953331, LR: 0.001
Time, 2019-01-07T01:52:43, Epoch: 14, Batch: 770, Training Loss: 0.013584895431995392, LR: 0.001
Time, 2019-01-07T01:52:43, Epoch: 14, Batch: 780, Training Loss: 0.02428414039313793, LR: 0.001
Time, 2019-01-07T01:52:44, Epoch: 14, Batch: 790, Training Loss: 0.03867746740579605, LR: 0.001
Time, 2019-01-07T01:52:45, Epoch: 14, Batch: 800, Training Loss: 0.022070129960775377, LR: 0.001
Time, 2019-01-07T01:52:46, Epoch: 14, Batch: 810, Training Loss: 0.010154909640550613, LR: 0.001
Time, 2019-01-07T01:52:46, Epoch: 14, Batch: 820, Training Loss: 0.01636221781373024, LR: 0.001
Time, 2019-01-07T01:52:47, Epoch: 14, Batch: 830, Training Loss: 0.012713408470153809, LR: 0.001
Time, 2019-01-07T01:52:48, Epoch: 14, Batch: 840, Training Loss: 0.013115953654050827, LR: 0.001
Time, 2019-01-07T01:52:48, Epoch: 14, Batch: 850, Training Loss: 0.02949807420372963, LR: 0.001
Time, 2019-01-07T01:52:49, Epoch: 14, Batch: 860, Training Loss: 0.007845058292150497, LR: 0.001
Time, 2019-01-07T01:52:50, Epoch: 14, Batch: 870, Training Loss: 0.01890101283788681, LR: 0.001
Time, 2019-01-07T01:52:51, Epoch: 14, Batch: 880, Training Loss: 0.012263043969869613, LR: 0.001
Time, 2019-01-07T01:52:52, Epoch: 14, Batch: 890, Training Loss: 0.008542610704898835, LR: 0.001
Time, 2019-01-07T01:52:52, Epoch: 14, Batch: 900, Training Loss: 0.017393700033426284, LR: 0.001
Time, 2019-01-07T01:52:53, Epoch: 14, Batch: 910, Training Loss: 0.020780488848686218, LR: 0.001
Time, 2019-01-07T01:52:54, Epoch: 14, Batch: 920, Training Loss: 0.019115447998046875, LR: 0.001
Time, 2019-01-07T01:52:54, Epoch: 14, Batch: 930, Training Loss: 0.01154327169060707, LR: 0.001
Epoch: 14, Validation Top 1 acc: 98.52706909179688
Epoch: 14, Validation Top 5 acc: 100.0
Epoch: 14, Validation Set Loss: 0.05014963820576668
Start training epoch 15
Time, 2019-01-07T01:52:58, Epoch: 15, Batch: 10, Training Loss: 0.023350057005882264, LR: 0.001
Time, 2019-01-07T01:52:58, Epoch: 15, Batch: 20, Training Loss: 0.026914361864328384, LR: 0.001
Time, 2019-01-07T01:52:59, Epoch: 15, Batch: 30, Training Loss: 0.015219463780522347, LR: 0.001
Time, 2019-01-07T01:53:00, Epoch: 15, Batch: 40, Training Loss: 0.007629049569368362, LR: 0.001
Time, 2019-01-07T01:53:00, Epoch: 15, Batch: 50, Training Loss: 0.006834011524915695, LR: 0.001
Time, 2019-01-07T01:53:01, Epoch: 15, Batch: 60, Training Loss: 0.012477576732635498, LR: 0.001
Time, 2019-01-07T01:53:02, Epoch: 15, Batch: 70, Training Loss: 0.01763790026307106, LR: 0.001
Time, 2019-01-07T01:53:02, Epoch: 15, Batch: 80, Training Loss: 0.006537312269210815, LR: 0.001
Time, 2019-01-07T01:53:03, Epoch: 15, Batch: 90, Training Loss: 0.011289051175117493, LR: 0.001
Time, 2019-01-07T01:53:04, Epoch: 15, Batch: 100, Training Loss: 0.010924717783927918, LR: 0.001
Time, 2019-01-07T01:53:05, Epoch: 15, Batch: 110, Training Loss: 0.0029430583119392393, LR: 0.001
Time, 2019-01-07T01:53:05, Epoch: 15, Batch: 120, Training Loss: 0.013150565326213837, LR: 0.001
Time, 2019-01-07T01:53:06, Epoch: 15, Batch: 130, Training Loss: 0.01059945896267891, LR: 0.001
Time, 2019-01-07T01:53:07, Epoch: 15, Batch: 140, Training Loss: 0.008461307734251022, LR: 0.001
Time, 2019-01-07T01:53:07, Epoch: 15, Batch: 150, Training Loss: 0.01285591647028923, LR: 0.001
Time, 2019-01-07T01:53:08, Epoch: 15, Batch: 160, Training Loss: 0.017918645590543746, LR: 0.001
Time, 2019-01-07T01:53:09, Epoch: 15, Batch: 170, Training Loss: 0.011524716764688492, LR: 0.001
Time, 2019-01-07T01:53:09, Epoch: 15, Batch: 180, Training Loss: 0.013356049358844758, LR: 0.001
Time, 2019-01-07T01:53:10, Epoch: 15, Batch: 190, Training Loss: 0.011223939061164857, LR: 0.001
Time, 2019-01-07T01:53:11, Epoch: 15, Batch: 200, Training Loss: 0.014410310983657837, LR: 0.001
Time, 2019-01-07T01:53:11, Epoch: 15, Batch: 210, Training Loss: 0.007859233766794205, LR: 0.001
Time, 2019-01-07T01:53:12, Epoch: 15, Batch: 220, Training Loss: 0.011268768459558487, LR: 0.001
Time, 2019-01-07T01:53:13, Epoch: 15, Batch: 230, Training Loss: 0.005593517422676086, LR: 0.001
Time, 2019-01-07T01:53:13, Epoch: 15, Batch: 240, Training Loss: 0.01647149845957756, LR: 0.001
Time, 2019-01-07T01:53:14, Epoch: 15, Batch: 250, Training Loss: 0.01853364408016205, LR: 0.001
Time, 2019-01-07T01:53:15, Epoch: 15, Batch: 260, Training Loss: 0.02183642387390137, LR: 0.001
Time, 2019-01-07T01:53:16, Epoch: 15, Batch: 270, Training Loss: 0.012888987362384797, LR: 0.001
Time, 2019-01-07T01:53:16, Epoch: 15, Batch: 280, Training Loss: 0.031765058636665344, LR: 0.001
Time, 2019-01-07T01:53:17, Epoch: 15, Batch: 290, Training Loss: 0.03506132736802101, LR: 0.001
Time, 2019-01-07T01:53:18, Epoch: 15, Batch: 300, Training Loss: 0.02732769176363945, LR: 0.001
Time, 2019-01-07T01:53:19, Epoch: 15, Batch: 310, Training Loss: 0.022992141544818878, LR: 0.001
Time, 2019-01-07T01:53:19, Epoch: 15, Batch: 320, Training Loss: 0.025115738064050673, LR: 0.001
Time, 2019-01-07T01:53:20, Epoch: 15, Batch: 330, Training Loss: 0.024682530015707017, LR: 0.001
Time, 2019-01-07T01:53:21, Epoch: 15, Batch: 340, Training Loss: 0.00653003454208374, LR: 0.001
Time, 2019-01-07T01:53:21, Epoch: 15, Batch: 350, Training Loss: 0.016719311475753784, LR: 0.001
Time, 2019-01-07T01:53:22, Epoch: 15, Batch: 360, Training Loss: 0.01691751629114151, LR: 0.001
Time, 2019-01-07T01:53:23, Epoch: 15, Batch: 370, Training Loss: 0.023975148051977157, LR: 0.001
Time, 2019-01-07T01:53:24, Epoch: 15, Batch: 380, Training Loss: 0.00731666311621666, LR: 0.001
Time, 2019-01-07T01:53:24, Epoch: 15, Batch: 390, Training Loss: 0.011964099854230881, LR: 0.001
Time, 2019-01-07T01:53:25, Epoch: 15, Batch: 400, Training Loss: 0.018341255187988282, LR: 0.001
Time, 2019-01-07T01:53:26, Epoch: 15, Batch: 410, Training Loss: 0.018003125488758088, LR: 0.001
Time, 2019-01-07T01:53:26, Epoch: 15, Batch: 420, Training Loss: 0.01756169945001602, LR: 0.001
Time, 2019-01-07T01:53:27, Epoch: 15, Batch: 430, Training Loss: 0.007854737341403961, LR: 0.001
Time, 2019-01-07T01:53:28, Epoch: 15, Batch: 440, Training Loss: 0.022563252598047256, LR: 0.001
Time, 2019-01-07T01:53:29, Epoch: 15, Batch: 450, Training Loss: 0.02099272161722183, LR: 0.001
Time, 2019-01-07T01:53:29, Epoch: 15, Batch: 460, Training Loss: 0.012720566242933273, LR: 0.001
Time, 2019-01-07T01:53:30, Epoch: 15, Batch: 470, Training Loss: 0.0181014284491539, LR: 0.001
Time, 2019-01-07T01:53:31, Epoch: 15, Batch: 480, Training Loss: 0.012804423272609711, LR: 0.001
Time, 2019-01-07T01:53:32, Epoch: 15, Batch: 490, Training Loss: 0.018984073400497438, LR: 0.001
Time, 2019-01-07T01:53:33, Epoch: 15, Batch: 500, Training Loss: 0.0021559596061706543, LR: 0.001
Time, 2019-01-07T01:53:33, Epoch: 15, Batch: 510, Training Loss: 0.00952034667134285, LR: 0.001
Time, 2019-01-07T01:53:34, Epoch: 15, Batch: 520, Training Loss: 0.009551422297954559, LR: 0.001
Time, 2019-01-07T01:53:35, Epoch: 15, Batch: 530, Training Loss: 0.006532863527536392, LR: 0.001
Time, 2019-01-07T01:53:36, Epoch: 15, Batch: 540, Training Loss: 0.008515474945306778, LR: 0.001
Time, 2019-01-07T01:53:36, Epoch: 15, Batch: 550, Training Loss: 0.012066563963890076, LR: 0.001
Time, 2019-01-07T01:53:37, Epoch: 15, Batch: 560, Training Loss: 0.015612799674272537, LR: 0.001
Time, 2019-01-07T01:53:38, Epoch: 15, Batch: 570, Training Loss: 0.011151637136936187, LR: 0.001
Time, 2019-01-07T01:53:38, Epoch: 15, Batch: 580, Training Loss: 0.027050995826721193, LR: 0.001
Time, 2019-01-07T01:53:39, Epoch: 15, Batch: 590, Training Loss: 0.0111076258122921, LR: 0.001
Time, 2019-01-07T01:53:40, Epoch: 15, Batch: 600, Training Loss: 0.029819536209106445, LR: 0.001
Time, 2019-01-07T01:53:40, Epoch: 15, Batch: 610, Training Loss: 0.009386899322271347, LR: 0.001
Time, 2019-01-07T01:53:41, Epoch: 15, Batch: 620, Training Loss: 0.018550580367445946, LR: 0.001
Time, 2019-01-07T01:53:42, Epoch: 15, Batch: 630, Training Loss: 0.03025987520813942, LR: 0.001
Time, 2019-01-07T01:53:43, Epoch: 15, Batch: 640, Training Loss: 0.025840235501527788, LR: 0.001
Time, 2019-01-07T01:53:43, Epoch: 15, Batch: 650, Training Loss: 0.01126944124698639, LR: 0.001
Time, 2019-01-07T01:53:44, Epoch: 15, Batch: 660, Training Loss: 0.013833419978618621, LR: 0.001
Time, 2019-01-07T01:53:45, Epoch: 15, Batch: 670, Training Loss: 0.012918081879615784, LR: 0.001
Time, 2019-01-07T01:53:45, Epoch: 15, Batch: 680, Training Loss: 0.01824408397078514, LR: 0.001
Time, 2019-01-07T01:53:46, Epoch: 15, Batch: 690, Training Loss: 0.007815871387720108, LR: 0.001
Time, 2019-01-07T01:53:47, Epoch: 15, Batch: 700, Training Loss: 0.011091527342796326, LR: 0.001
Time, 2019-01-07T01:53:47, Epoch: 15, Batch: 710, Training Loss: 0.006124603003263474, LR: 0.001
Time, 2019-01-07T01:53:48, Epoch: 15, Batch: 720, Training Loss: 0.008453515917062759, LR: 0.001
Time, 2019-01-07T01:53:49, Epoch: 15, Batch: 730, Training Loss: 0.029426415264606477, LR: 0.001
Time, 2019-01-07T01:53:50, Epoch: 15, Batch: 740, Training Loss: 0.021633952856063843, LR: 0.001
Time, 2019-01-07T01:53:50, Epoch: 15, Batch: 750, Training Loss: 0.01018986850976944, LR: 0.001
Time, 2019-01-07T01:53:51, Epoch: 15, Batch: 760, Training Loss: 0.004835497587919235, LR: 0.001
Time, 2019-01-07T01:53:52, Epoch: 15, Batch: 770, Training Loss: 0.006556452810764312, LR: 0.001
Time, 2019-01-07T01:53:53, Epoch: 15, Batch: 780, Training Loss: 0.014686830341815948, LR: 0.001
Time, 2019-01-07T01:53:53, Epoch: 15, Batch: 790, Training Loss: 0.01105731800198555, LR: 0.001
Time, 2019-01-07T01:53:54, Epoch: 15, Batch: 800, Training Loss: 0.019897535815834998, LR: 0.001
Time, 2019-01-07T01:53:55, Epoch: 15, Batch: 810, Training Loss: 0.013950803130865098, LR: 0.001
Time, 2019-01-07T01:53:55, Epoch: 15, Batch: 820, Training Loss: 0.008472707122564316, LR: 0.001
Time, 2019-01-07T01:53:56, Epoch: 15, Batch: 830, Training Loss: 0.013675009459257126, LR: 0.001
Time, 2019-01-07T01:53:57, Epoch: 15, Batch: 840, Training Loss: 0.013153343647718429, LR: 0.001
Time, 2019-01-07T01:53:58, Epoch: 15, Batch: 850, Training Loss: 0.01668599173426628, LR: 0.001
Time, 2019-01-07T01:53:58, Epoch: 15, Batch: 860, Training Loss: 0.010718757659196854, LR: 0.001
Time, 2019-01-07T01:53:59, Epoch: 15, Batch: 870, Training Loss: 0.011584694683551788, LR: 0.001
Time, 2019-01-07T01:54:00, Epoch: 15, Batch: 880, Training Loss: 0.0062646567821502686, LR: 0.001
Time, 2019-01-07T01:54:01, Epoch: 15, Batch: 890, Training Loss: 0.021068674325942994, LR: 0.001
Time, 2019-01-07T01:54:01, Epoch: 15, Batch: 900, Training Loss: 0.015521200746297837, LR: 0.001
Time, 2019-01-07T01:54:02, Epoch: 15, Batch: 910, Training Loss: 0.010765011608600616, LR: 0.001
Time, 2019-01-07T01:54:03, Epoch: 15, Batch: 920, Training Loss: 0.007711677998304367, LR: 0.001
Time, 2019-01-07T01:54:03, Epoch: 15, Batch: 930, Training Loss: 0.005218802392482758, LR: 0.001
Epoch: 15, Validation Top 1 acc: 98.8853530883789
Epoch: 15, Validation Top 5 acc: 100.0
Epoch: 15, Validation Set Loss: 0.043572910130023956
Start training epoch 16
Time, 2019-01-07T01:54:07, Epoch: 16, Batch: 10, Training Loss: 0.007269519567489624, LR: 0.001
Time, 2019-01-07T01:54:08, Epoch: 16, Batch: 20, Training Loss: 0.009115919470787048, LR: 0.001
Time, 2019-01-07T01:54:08, Epoch: 16, Batch: 30, Training Loss: 0.010173401236534119, LR: 0.001
Time, 2019-01-07T01:54:09, Epoch: 16, Batch: 40, Training Loss: 0.004166574776172638, LR: 0.001
Time, 2019-01-07T01:54:10, Epoch: 16, Batch: 50, Training Loss: 0.007543385773897171, LR: 0.001
Time, 2019-01-07T01:54:10, Epoch: 16, Batch: 60, Training Loss: 0.014841897785663605, LR: 0.001
Time, 2019-01-07T01:54:11, Epoch: 16, Batch: 70, Training Loss: 0.00882508084177971, LR: 0.001
Time, 2019-01-07T01:54:12, Epoch: 16, Batch: 80, Training Loss: 0.009888874739408493, LR: 0.001
Time, 2019-01-07T01:54:13, Epoch: 16, Batch: 90, Training Loss: 0.006902257353067398, LR: 0.001
Time, 2019-01-07T01:54:13, Epoch: 16, Batch: 100, Training Loss: 0.006303221732378006, LR: 0.001
Time, 2019-01-07T01:54:14, Epoch: 16, Batch: 110, Training Loss: 0.002179606258869171, LR: 0.001
Time, 2019-01-07T01:54:15, Epoch: 16, Batch: 120, Training Loss: 0.006068166345357895, LR: 0.001
Time, 2019-01-07T01:54:15, Epoch: 16, Batch: 130, Training Loss: 0.011548612266778946, LR: 0.001
Time, 2019-01-07T01:54:16, Epoch: 16, Batch: 140, Training Loss: 0.003682976961135864, LR: 0.001
Time, 2019-01-07T01:54:17, Epoch: 16, Batch: 150, Training Loss: 0.0059146851301193236, LR: 0.001
Time, 2019-01-07T01:54:17, Epoch: 16, Batch: 160, Training Loss: 0.00922715589404106, LR: 0.001
Time, 2019-01-07T01:54:18, Epoch: 16, Batch: 170, Training Loss: 0.00443420484662056, LR: 0.001
Time, 2019-01-07T01:54:19, Epoch: 16, Batch: 180, Training Loss: 0.005946524441242218, LR: 0.001
Time, 2019-01-07T01:54:19, Epoch: 16, Batch: 190, Training Loss: 0.011045562475919724, LR: 0.001
Time, 2019-01-07T01:54:20, Epoch: 16, Batch: 200, Training Loss: 0.014189675450325012, LR: 0.001
Time, 2019-01-07T01:54:21, Epoch: 16, Batch: 210, Training Loss: 0.005038167536258698, LR: 0.001
Time, 2019-01-07T01:54:21, Epoch: 16, Batch: 220, Training Loss: 0.008791149407625199, LR: 0.001
Time, 2019-01-07T01:54:22, Epoch: 16, Batch: 230, Training Loss: 0.011448556929826737, LR: 0.001
Time, 2019-01-07T01:54:23, Epoch: 16, Batch: 240, Training Loss: 0.006673669815063477, LR: 0.001
Time, 2019-01-07T01:54:23, Epoch: 16, Batch: 250, Training Loss: 0.016130990535020828, LR: 0.001
Time, 2019-01-07T01:54:24, Epoch: 16, Batch: 260, Training Loss: 0.009259290993213654, LR: 0.001
Time, 2019-01-07T01:54:25, Epoch: 16, Batch: 270, Training Loss: 0.028039196878671645, LR: 0.001
Time, 2019-01-07T01:54:25, Epoch: 16, Batch: 280, Training Loss: 0.01792566105723381, LR: 0.001
Time, 2019-01-07T01:54:26, Epoch: 16, Batch: 290, Training Loss: 0.012662418931722642, LR: 0.001
Time, 2019-01-07T01:54:27, Epoch: 16, Batch: 300, Training Loss: 0.033392585813999176, LR: 0.001
Time, 2019-01-07T01:54:27, Epoch: 16, Batch: 310, Training Loss: 0.00392131507396698, LR: 0.001
Time, 2019-01-07T01:54:28, Epoch: 16, Batch: 320, Training Loss: 0.006407629698514938, LR: 0.001
Time, 2019-01-07T01:54:28, Epoch: 16, Batch: 330, Training Loss: 0.011505359783768654, LR: 0.001
Time, 2019-01-07T01:54:29, Epoch: 16, Batch: 340, Training Loss: 0.028158445656299592, LR: 0.001
Time, 2019-01-07T01:54:30, Epoch: 16, Batch: 350, Training Loss: 0.02381499409675598, LR: 0.001
Time, 2019-01-07T01:54:30, Epoch: 16, Batch: 360, Training Loss: 0.00924278050661087, LR: 0.001
Time, 2019-01-07T01:54:31, Epoch: 16, Batch: 370, Training Loss: 0.012229929119348526, LR: 0.001
Time, 2019-01-07T01:54:32, Epoch: 16, Batch: 380, Training Loss: 0.01066637858748436, LR: 0.001
Time, 2019-01-07T01:54:32, Epoch: 16, Batch: 390, Training Loss: 0.006230420619249344, LR: 0.001
Time, 2019-01-07T01:54:33, Epoch: 16, Batch: 400, Training Loss: 0.009288324415683747, LR: 0.001
Time, 2019-01-07T01:54:34, Epoch: 16, Batch: 410, Training Loss: 0.009596079587936401, LR: 0.001
Time, 2019-01-07T01:54:34, Epoch: 16, Batch: 420, Training Loss: 0.02338404208421707, LR: 0.001
Time, 2019-01-07T01:54:35, Epoch: 16, Batch: 430, Training Loss: 0.016241086274385454, LR: 0.001
Time, 2019-01-07T01:54:36, Epoch: 16, Batch: 440, Training Loss: 0.021332236379384993, LR: 0.001
Time, 2019-01-07T01:54:37, Epoch: 16, Batch: 450, Training Loss: 0.00901443175971508, LR: 0.001
Time, 2019-01-07T01:54:37, Epoch: 16, Batch: 460, Training Loss: 0.014229733496904373, LR: 0.001
Time, 2019-01-07T01:54:38, Epoch: 16, Batch: 470, Training Loss: 0.010182733088731766, LR: 0.001
Time, 2019-01-07T01:54:39, Epoch: 16, Batch: 480, Training Loss: 0.0077844254672527315, LR: 0.001
Time, 2019-01-07T01:54:39, Epoch: 16, Batch: 490, Training Loss: 0.004090318083763122, LR: 0.001
Time, 2019-01-07T01:54:40, Epoch: 16, Batch: 500, Training Loss: 0.012669528275728226, LR: 0.001
Time, 2019-01-07T01:54:41, Epoch: 16, Batch: 510, Training Loss: 0.01381019651889801, LR: 0.001
Time, 2019-01-07T01:54:42, Epoch: 16, Batch: 520, Training Loss: 0.004517196118831635, LR: 0.001
Time, 2019-01-07T01:54:42, Epoch: 16, Batch: 530, Training Loss: 0.013905679434537887, LR: 0.001
Time, 2019-01-07T01:54:43, Epoch: 16, Batch: 540, Training Loss: 0.010727570205926896, LR: 0.001
Time, 2019-01-07T01:54:44, Epoch: 16, Batch: 550, Training Loss: 0.014008340239524842, LR: 0.001
Time, 2019-01-07T01:54:45, Epoch: 16, Batch: 560, Training Loss: 0.0037256911396980285, LR: 0.001
Time, 2019-01-07T01:54:46, Epoch: 16, Batch: 570, Training Loss: 0.015259134769439697, LR: 0.001
Time, 2019-01-07T01:54:46, Epoch: 16, Batch: 580, Training Loss: 0.009325712174177169, LR: 0.001
Time, 2019-01-07T01:54:47, Epoch: 16, Batch: 590, Training Loss: 0.009824670851230621, LR: 0.001
Time, 2019-01-07T01:54:48, Epoch: 16, Batch: 600, Training Loss: 0.015048171579837798, LR: 0.001
Time, 2019-01-07T01:54:49, Epoch: 16, Batch: 610, Training Loss: 0.005096612870693207, LR: 0.001
Time, 2019-01-07T01:54:50, Epoch: 16, Batch: 620, Training Loss: 0.004817811399698257, LR: 0.001
Time, 2019-01-07T01:54:51, Epoch: 16, Batch: 630, Training Loss: 0.004904108494520188, LR: 0.001
Time, 2019-01-07T01:54:52, Epoch: 16, Batch: 640, Training Loss: 0.0156942218542099, LR: 0.001
Time, 2019-01-07T01:54:52, Epoch: 16, Batch: 650, Training Loss: 0.009207392483949662, LR: 0.001
Time, 2019-01-07T01:54:53, Epoch: 16, Batch: 660, Training Loss: 0.008749204128980637, LR: 0.001
Time, 2019-01-07T01:54:54, Epoch: 16, Batch: 670, Training Loss: 0.007898972928524017, LR: 0.001
Time, 2019-01-07T01:54:54, Epoch: 16, Batch: 680, Training Loss: 0.01797463595867157, LR: 0.001
Time, 2019-01-07T01:54:55, Epoch: 16, Batch: 690, Training Loss: 0.01491885855793953, LR: 0.001
Time, 2019-01-07T01:54:56, Epoch: 16, Batch: 700, Training Loss: 0.009386475384235381, LR: 0.001
Time, 2019-01-07T01:54:56, Epoch: 16, Batch: 710, Training Loss: 0.006932613253593445, LR: 0.001
Time, 2019-01-07T01:54:57, Epoch: 16, Batch: 720, Training Loss: 0.008363984525203705, LR: 0.001
Time, 2019-01-07T01:54:58, Epoch: 16, Batch: 730, Training Loss: 0.014058350026607514, LR: 0.001
Time, 2019-01-07T01:54:58, Epoch: 16, Batch: 740, Training Loss: 0.015521643310785293, LR: 0.001
Time, 2019-01-07T01:54:59, Epoch: 16, Batch: 750, Training Loss: 0.017227628827095033, LR: 0.001
Time, 2019-01-07T01:55:00, Epoch: 16, Batch: 760, Training Loss: 0.01719249039888382, LR: 0.001
Time, 2019-01-07T01:55:00, Epoch: 16, Batch: 770, Training Loss: 0.02220430299639702, LR: 0.001
Time, 2019-01-07T01:55:01, Epoch: 16, Batch: 780, Training Loss: 0.014981186389923096, LR: 0.001
Time, 2019-01-07T01:55:02, Epoch: 16, Batch: 790, Training Loss: 0.02256501168012619, LR: 0.001
Time, 2019-01-07T01:55:02, Epoch: 16, Batch: 800, Training Loss: 0.014109938591718673, LR: 0.001
Time, 2019-01-07T01:55:03, Epoch: 16, Batch: 810, Training Loss: 0.023154015839099883, LR: 0.001
Time, 2019-01-07T01:55:04, Epoch: 16, Batch: 820, Training Loss: 0.020588810741901397, LR: 0.001
Time, 2019-01-07T01:55:04, Epoch: 16, Batch: 830, Training Loss: 0.024579180032014848, LR: 0.001
Time, 2019-01-07T01:55:05, Epoch: 16, Batch: 840, Training Loss: 0.020451658964157106, LR: 0.001
Time, 2019-01-07T01:55:06, Epoch: 16, Batch: 850, Training Loss: 0.02876003533601761, LR: 0.001
Time, 2019-01-07T01:55:06, Epoch: 16, Batch: 860, Training Loss: 0.03915285617113114, LR: 0.001
Time, 2019-01-07T01:55:07, Epoch: 16, Batch: 870, Training Loss: 0.04375737458467484, LR: 0.001
Time, 2019-01-07T01:55:08, Epoch: 16, Batch: 880, Training Loss: 0.028611012548208237, LR: 0.001
Time, 2019-01-07T01:55:08, Epoch: 16, Batch: 890, Training Loss: 0.02337683513760567, LR: 0.001
Time, 2019-01-07T01:55:09, Epoch: 16, Batch: 900, Training Loss: 0.01140228882431984, LR: 0.001
Time, 2019-01-07T01:55:10, Epoch: 16, Batch: 910, Training Loss: 0.01021866947412491, LR: 0.001
Time, 2019-01-07T01:55:10, Epoch: 16, Batch: 920, Training Loss: 0.016517345607280732, LR: 0.001
Time, 2019-01-07T01:55:11, Epoch: 16, Batch: 930, Training Loss: 0.010881490260362624, LR: 0.001
Epoch: 16, Validation Top 1 acc: 98.61663818359375
Epoch: 16, Validation Top 5 acc: 100.0
Epoch: 16, Validation Set Loss: 0.05280136317014694
Start training epoch 17
Time, 2019-01-07T01:55:14, Epoch: 17, Batch: 10, Training Loss: 0.01677109971642494, LR: 0.001
Time, 2019-01-07T01:55:15, Epoch: 17, Batch: 20, Training Loss: 0.01808551847934723, LR: 0.001
Time, 2019-01-07T01:55:16, Epoch: 17, Batch: 30, Training Loss: 0.008469907939434052, LR: 0.001
Time, 2019-01-07T01:55:16, Epoch: 17, Batch: 40, Training Loss: 0.010861769318580627, LR: 0.001
Time, 2019-01-07T01:55:17, Epoch: 17, Batch: 50, Training Loss: 0.014669554680585862, LR: 0.001
Time, 2019-01-07T01:55:18, Epoch: 17, Batch: 60, Training Loss: 0.02098257318139076, LR: 0.001
Time, 2019-01-07T01:55:18, Epoch: 17, Batch: 70, Training Loss: 0.011301598697900771, LR: 0.001
Time, 2019-01-07T01:55:19, Epoch: 17, Batch: 80, Training Loss: 0.012092217057943343, LR: 0.001
Time, 2019-01-07T01:55:20, Epoch: 17, Batch: 90, Training Loss: 0.009804182499647141, LR: 0.001
Time, 2019-01-07T01:55:20, Epoch: 17, Batch: 100, Training Loss: 0.01413889229297638, LR: 0.001
Time, 2019-01-07T01:55:21, Epoch: 17, Batch: 110, Training Loss: 0.024118627607822418, LR: 0.001
Time, 2019-01-07T01:55:22, Epoch: 17, Batch: 120, Training Loss: 0.013827594369649887, LR: 0.001
Time, 2019-01-07T01:55:22, Epoch: 17, Batch: 130, Training Loss: 0.011406554281711579, LR: 0.001
Time, 2019-01-07T01:55:23, Epoch: 17, Batch: 140, Training Loss: 0.017368648201227188, LR: 0.001
Time, 2019-01-07T01:55:24, Epoch: 17, Batch: 150, Training Loss: 0.021994277089834213, LR: 0.001
Time, 2019-01-07T01:55:24, Epoch: 17, Batch: 160, Training Loss: 0.01384507343173027, LR: 0.001
Time, 2019-01-07T01:55:25, Epoch: 17, Batch: 170, Training Loss: 0.011205249279737473, LR: 0.001
Time, 2019-01-07T01:55:26, Epoch: 17, Batch: 180, Training Loss: 0.019875727593898773, LR: 0.001
Time, 2019-01-07T01:55:26, Epoch: 17, Batch: 190, Training Loss: 0.016433385014533997, LR: 0.001
Time, 2019-01-07T01:55:27, Epoch: 17, Batch: 200, Training Loss: 0.010911239683628083, LR: 0.001
Time, 2019-01-07T01:55:28, Epoch: 17, Batch: 210, Training Loss: 0.0032488368451595306, LR: 0.001
Time, 2019-01-07T01:55:28, Epoch: 17, Batch: 220, Training Loss: 0.012380661815404892, LR: 0.001
Time, 2019-01-07T01:55:29, Epoch: 17, Batch: 230, Training Loss: 0.0039121657609939575, LR: 0.001
Time, 2019-01-07T01:55:29, Epoch: 17, Batch: 240, Training Loss: 0.02113029658794403, LR: 0.001
Time, 2019-01-07T01:55:30, Epoch: 17, Batch: 250, Training Loss: 0.01486589089035988, LR: 0.001
Time, 2019-01-07T01:55:31, Epoch: 17, Batch: 260, Training Loss: 0.014079812169075012, LR: 0.001
Time, 2019-01-07T01:55:31, Epoch: 17, Batch: 270, Training Loss: 0.0043370254337787625, LR: 0.001
Time, 2019-01-07T01:55:32, Epoch: 17, Batch: 280, Training Loss: 0.01680946871638298, LR: 0.001
Time, 2019-01-07T01:55:33, Epoch: 17, Batch: 290, Training Loss: 0.013112077862024308, LR: 0.001
Time, 2019-01-07T01:55:33, Epoch: 17, Batch: 300, Training Loss: 0.013169760257005692, LR: 0.001
Time, 2019-01-07T01:55:34, Epoch: 17, Batch: 310, Training Loss: 0.009492121636867523, LR: 0.001
Time, 2019-01-07T01:55:35, Epoch: 17, Batch: 320, Training Loss: 0.0032170571386814116, LR: 0.001
Time, 2019-01-07T01:55:35, Epoch: 17, Batch: 330, Training Loss: 0.014603128284215927, LR: 0.001
Time, 2019-01-07T01:55:36, Epoch: 17, Batch: 340, Training Loss: 0.006325479596853256, LR: 0.001
Time, 2019-01-07T01:55:37, Epoch: 17, Batch: 350, Training Loss: 0.012650459259748458, LR: 0.001
Time, 2019-01-07T01:55:38, Epoch: 17, Batch: 360, Training Loss: 0.014471927285194397, LR: 0.001
Time, 2019-01-07T01:55:38, Epoch: 17, Batch: 370, Training Loss: 0.013173891603946686, LR: 0.001
Time, 2019-01-07T01:55:39, Epoch: 17, Batch: 380, Training Loss: 0.007255999743938446, LR: 0.001
Time, 2019-01-07T01:55:40, Epoch: 17, Batch: 390, Training Loss: 0.017935863882303237, LR: 0.001
Time, 2019-01-07T01:55:40, Epoch: 17, Batch: 400, Training Loss: 0.005412786453962326, LR: 0.001
Time, 2019-01-07T01:55:41, Epoch: 17, Batch: 410, Training Loss: 0.007794642448425293, LR: 0.001
Time, 2019-01-07T01:55:42, Epoch: 17, Batch: 420, Training Loss: 0.010824880003929139, LR: 0.001
Time, 2019-01-07T01:55:42, Epoch: 17, Batch: 430, Training Loss: 0.00695817694067955, LR: 0.001
Time, 2019-01-07T01:55:43, Epoch: 17, Batch: 440, Training Loss: 0.009435869008302688, LR: 0.001
Time, 2019-01-07T01:55:44, Epoch: 17, Batch: 450, Training Loss: 0.003987175226211548, LR: 0.001
Time, 2019-01-07T01:55:44, Epoch: 17, Batch: 460, Training Loss: 0.008021252602338791, LR: 0.001
Time, 2019-01-07T01:55:45, Epoch: 17, Batch: 470, Training Loss: 0.0072108939290046695, LR: 0.001
Time, 2019-01-07T01:55:45, Epoch: 17, Batch: 480, Training Loss: 0.02235127054154873, LR: 0.001
Time, 2019-01-07T01:55:46, Epoch: 17, Batch: 490, Training Loss: 0.011879146099090576, LR: 0.001
Time, 2019-01-07T01:55:47, Epoch: 17, Batch: 500, Training Loss: 0.005776656419038772, LR: 0.001
Time, 2019-01-07T01:55:48, Epoch: 17, Batch: 510, Training Loss: 0.0292163722217083, LR: 0.001
Time, 2019-01-07T01:55:48, Epoch: 17, Batch: 520, Training Loss: 0.0019641518592834473, LR: 0.001
Time, 2019-01-07T01:55:49, Epoch: 17, Batch: 530, Training Loss: 0.018900707364082336, LR: 0.001
Time, 2019-01-07T01:55:50, Epoch: 17, Batch: 540, Training Loss: 0.020525177568197252, LR: 0.001
Time, 2019-01-07T01:55:50, Epoch: 17, Batch: 550, Training Loss: 0.017294858396053315, LR: 0.001
Time, 2019-01-07T01:55:51, Epoch: 17, Batch: 560, Training Loss: 0.027261809259653092, LR: 0.001
Time, 2019-01-07T01:55:52, Epoch: 17, Batch: 570, Training Loss: 0.012486541271209716, LR: 0.001
Time, 2019-01-07T01:55:52, Epoch: 17, Batch: 580, Training Loss: 0.007248925417661667, LR: 0.001
Time, 2019-01-07T01:55:53, Epoch: 17, Batch: 590, Training Loss: 0.016893769055604933, LR: 0.001
Time, 2019-01-07T01:55:54, Epoch: 17, Batch: 600, Training Loss: 0.002083428204059601, LR: 0.001
Time, 2019-01-07T01:55:54, Epoch: 17, Batch: 610, Training Loss: 0.01633869335055351, LR: 0.001
Time, 2019-01-07T01:55:55, Epoch: 17, Batch: 620, Training Loss: 0.00955066755414009, LR: 0.001
Time, 2019-01-07T01:55:56, Epoch: 17, Batch: 630, Training Loss: 0.014397429674863816, LR: 0.001
Time, 2019-01-07T01:55:56, Epoch: 17, Batch: 640, Training Loss: 0.017984062433242798, LR: 0.001
Time, 2019-01-07T01:55:57, Epoch: 17, Batch: 650, Training Loss: 0.011196647584438325, LR: 0.001
Time, 2019-01-07T01:55:58, Epoch: 17, Batch: 660, Training Loss: 0.022076471149921416, LR: 0.001
Time, 2019-01-07T01:55:58, Epoch: 17, Batch: 670, Training Loss: 0.025891832262277602, LR: 0.001
Time, 2019-01-07T01:55:59, Epoch: 17, Batch: 680, Training Loss: 0.013544538617134094, LR: 0.001
Time, 2019-01-07T01:56:00, Epoch: 17, Batch: 690, Training Loss: 0.012071720510721206, LR: 0.001
Time, 2019-01-07T01:56:00, Epoch: 17, Batch: 700, Training Loss: 0.009336541593074798, LR: 0.001
Time, 2019-01-07T01:56:01, Epoch: 17, Batch: 710, Training Loss: 0.004680205881595611, LR: 0.001
Time, 2019-01-07T01:56:02, Epoch: 17, Batch: 720, Training Loss: 0.020069434493780135, LR: 0.001
Time, 2019-01-07T01:56:02, Epoch: 17, Batch: 730, Training Loss: 0.008949913084506989, LR: 0.001
Time, 2019-01-07T01:56:03, Epoch: 17, Batch: 740, Training Loss: 0.009904918819665908, LR: 0.001
Time, 2019-01-07T01:56:04, Epoch: 17, Batch: 750, Training Loss: 0.01559109166264534, LR: 0.001
Time, 2019-01-07T01:56:04, Epoch: 17, Batch: 760, Training Loss: 0.008384228497743607, LR: 0.001
Time, 2019-01-07T01:56:05, Epoch: 17, Batch: 770, Training Loss: 0.0037697553634643555, LR: 0.001
Time, 2019-01-07T01:56:06, Epoch: 17, Batch: 780, Training Loss: 0.009111760556697846, LR: 0.001
Time, 2019-01-07T01:56:06, Epoch: 17, Batch: 790, Training Loss: 0.009495965391397476, LR: 0.001
Time, 2019-01-07T01:56:07, Epoch: 17, Batch: 800, Training Loss: 0.01187194287776947, LR: 0.001
Time, 2019-01-07T01:56:08, Epoch: 17, Batch: 810, Training Loss: 0.014738595485687256, LR: 0.001
Time, 2019-01-07T01:56:08, Epoch: 17, Batch: 820, Training Loss: 0.007963721454143525, LR: 0.001
Time, 2019-01-07T01:56:09, Epoch: 17, Batch: 830, Training Loss: 0.0030357323586940764, LR: 0.001
Time, 2019-01-07T01:56:10, Epoch: 17, Batch: 840, Training Loss: 0.010246442258358001, LR: 0.001
Time, 2019-01-07T01:56:10, Epoch: 17, Batch: 850, Training Loss: 0.009953036159276962, LR: 0.001
Time, 2019-01-07T01:56:11, Epoch: 17, Batch: 860, Training Loss: 0.005274894088506699, LR: 0.001
Time, 2019-01-07T01:56:12, Epoch: 17, Batch: 870, Training Loss: 0.027379575371742248, LR: 0.001
Time, 2019-01-07T01:56:12, Epoch: 17, Batch: 880, Training Loss: 0.017842745035886766, LR: 0.001
Time, 2019-01-07T01:56:13, Epoch: 17, Batch: 890, Training Loss: 0.012544004619121552, LR: 0.001
Time, 2019-01-07T01:56:14, Epoch: 17, Batch: 900, Training Loss: 0.01681368574500084, LR: 0.001
Time, 2019-01-07T01:56:15, Epoch: 17, Batch: 910, Training Loss: 0.006261406838893891, LR: 0.001
Time, 2019-01-07T01:56:15, Epoch: 17, Batch: 920, Training Loss: 0.01528354436159134, LR: 0.001
Time, 2019-01-07T01:56:16, Epoch: 17, Batch: 930, Training Loss: 0.01066824272274971, LR: 0.001
Epoch: 17, Validation Top 1 acc: 98.84554290771484
Epoch: 17, Validation Top 5 acc: 100.0
Epoch: 17, Validation Set Loss: 0.04352647066116333
Start training epoch 18
Time, 2019-01-07T01:56:19, Epoch: 18, Batch: 10, Training Loss: 0.011919692158699036, LR: 0.001
Time, 2019-01-07T01:56:20, Epoch: 18, Batch: 20, Training Loss: 0.006229410320520401, LR: 0.001
Time, 2019-01-07T01:56:21, Epoch: 18, Batch: 30, Training Loss: 0.007610110938549042, LR: 0.001
Time, 2019-01-07T01:56:21, Epoch: 18, Batch: 40, Training Loss: 0.006964593380689621, LR: 0.001
Time, 2019-01-07T01:56:22, Epoch: 18, Batch: 50, Training Loss: 0.008509541302919388, LR: 0.001
Time, 2019-01-07T01:56:23, Epoch: 18, Batch: 60, Training Loss: 0.001636373996734619, LR: 0.001
Time, 2019-01-07T01:56:24, Epoch: 18, Batch: 70, Training Loss: 0.0023190513253211974, LR: 0.001
Time, 2019-01-07T01:56:24, Epoch: 18, Batch: 80, Training Loss: 0.0034341007471084596, LR: 0.001
Time, 2019-01-07T01:56:25, Epoch: 18, Batch: 90, Training Loss: 0.00543694719672203, LR: 0.001
Time, 2019-01-07T01:56:26, Epoch: 18, Batch: 100, Training Loss: 0.005374253541231155, LR: 0.001
Time, 2019-01-07T01:56:26, Epoch: 18, Batch: 110, Training Loss: 0.004813326150178909, LR: 0.001
Time, 2019-01-07T01:56:27, Epoch: 18, Batch: 120, Training Loss: 0.004068700224161148, LR: 0.001
Time, 2019-01-07T01:56:28, Epoch: 18, Batch: 130, Training Loss: 0.003120861202478409, LR: 0.001
Time, 2019-01-07T01:56:28, Epoch: 18, Batch: 140, Training Loss: 0.002712172269821167, LR: 0.001
Time, 2019-01-07T01:56:29, Epoch: 18, Batch: 150, Training Loss: 0.0037132076919078826, LR: 0.001
Time, 2019-01-07T01:56:30, Epoch: 18, Batch: 160, Training Loss: 0.0036029770970344545, LR: 0.001
Time, 2019-01-07T01:56:30, Epoch: 18, Batch: 170, Training Loss: 0.004964816570281983, LR: 0.001
Time, 2019-01-07T01:56:31, Epoch: 18, Batch: 180, Training Loss: 0.0028264880180358886, LR: 0.001
Time, 2019-01-07T01:56:32, Epoch: 18, Batch: 190, Training Loss: 0.0018702521920204163, LR: 0.001
Time, 2019-01-07T01:56:33, Epoch: 18, Batch: 200, Training Loss: 0.013380900025367737, LR: 0.001
Time, 2019-01-07T01:56:33, Epoch: 18, Batch: 210, Training Loss: 0.0015289314091205598, LR: 0.001
Time, 2019-01-07T01:56:34, Epoch: 18, Batch: 220, Training Loss: 0.0013989269733428954, LR: 0.001
Time, 2019-01-07T01:56:35, Epoch: 18, Batch: 230, Training Loss: 0.0024461165070533753, LR: 0.001
Time, 2019-01-07T01:56:35, Epoch: 18, Batch: 240, Training Loss: 0.005472256243228913, LR: 0.001
Time, 2019-01-07T01:56:36, Epoch: 18, Batch: 250, Training Loss: 0.0030338861048221587, LR: 0.001
Time, 2019-01-07T01:56:37, Epoch: 18, Batch: 260, Training Loss: 0.011556543409824371, LR: 0.001
Time, 2019-01-07T01:56:37, Epoch: 18, Batch: 270, Training Loss: 0.0029303058981895447, LR: 0.001
Time, 2019-01-07T01:56:38, Epoch: 18, Batch: 280, Training Loss: 0.007965875416994095, LR: 0.001
Time, 2019-01-07T01:56:39, Epoch: 18, Batch: 290, Training Loss: 0.009073990583419799, LR: 0.001
Time, 2019-01-07T01:56:39, Epoch: 18, Batch: 300, Training Loss: 0.0065586425364017485, LR: 0.001
Time, 2019-01-07T01:56:40, Epoch: 18, Batch: 310, Training Loss: 0.0016982212662696838, LR: 0.001
Time, 2019-01-07T01:56:41, Epoch: 18, Batch: 320, Training Loss: 0.003565913438796997, LR: 0.001
Time, 2019-01-07T01:56:41, Epoch: 18, Batch: 330, Training Loss: 0.0030787765979766847, LR: 0.001
Time, 2019-01-07T01:56:42, Epoch: 18, Batch: 340, Training Loss: 0.006130390614271164, LR: 0.001
Time, 2019-01-07T01:56:43, Epoch: 18, Batch: 350, Training Loss: 0.005208859592676163, LR: 0.001
Time, 2019-01-07T01:56:43, Epoch: 18, Batch: 360, Training Loss: 0.0041136018931865696, LR: 0.001
Time, 2019-01-07T01:56:44, Epoch: 18, Batch: 370, Training Loss: 0.004256065934896469, LR: 0.001
Time, 2019-01-07T01:56:45, Epoch: 18, Batch: 380, Training Loss: 0.00437227264046669, LR: 0.001
Time, 2019-01-07T01:56:45, Epoch: 18, Batch: 390, Training Loss: 0.0022705048322677612, LR: 0.001
Time, 2019-01-07T01:56:46, Epoch: 18, Batch: 400, Training Loss: 0.006571708619594574, LR: 0.001
Time, 2019-01-07T01:56:47, Epoch: 18, Batch: 410, Training Loss: 0.0025960080325603484, LR: 0.001
Time, 2019-01-07T01:56:47, Epoch: 18, Batch: 420, Training Loss: 0.01370694413781166, LR: 0.001
Time, 2019-01-07T01:56:48, Epoch: 18, Batch: 430, Training Loss: 0.009148319065570832, LR: 0.001
Time, 2019-01-07T01:56:49, Epoch: 18, Batch: 440, Training Loss: 0.004292207956314087, LR: 0.001
Time, 2019-01-07T01:56:50, Epoch: 18, Batch: 450, Training Loss: 0.005797535181045532, LR: 0.001
Time, 2019-01-07T01:56:50, Epoch: 18, Batch: 460, Training Loss: 0.002146976441144943, LR: 0.001
Time, 2019-01-07T01:56:51, Epoch: 18, Batch: 470, Training Loss: 0.0028425559401512148, LR: 0.001
Time, 2019-01-07T01:56:51, Epoch: 18, Batch: 480, Training Loss: 0.013128639757633209, LR: 0.001
Time, 2019-01-07T01:56:52, Epoch: 18, Batch: 490, Training Loss: 0.006308295577764511, LR: 0.001
Time, 2019-01-07T01:56:53, Epoch: 18, Batch: 500, Training Loss: 0.006971748918294907, LR: 0.001
Time, 2019-01-07T01:56:54, Epoch: 18, Batch: 510, Training Loss: 0.008840357512235641, LR: 0.001
Time, 2019-01-07T01:56:54, Epoch: 18, Batch: 520, Training Loss: 0.0036297708749771116, LR: 0.001
Time, 2019-01-07T01:56:55, Epoch: 18, Batch: 530, Training Loss: 0.0023504331707954406, LR: 0.001
Time, 2019-01-07T01:56:56, Epoch: 18, Batch: 540, Training Loss: 0.010160052031278611, LR: 0.001
Time, 2019-01-07T01:56:56, Epoch: 18, Batch: 550, Training Loss: 0.004068620502948761, LR: 0.001
Time, 2019-01-07T01:56:57, Epoch: 18, Batch: 560, Training Loss: 0.004348096251487732, LR: 0.001
Time, 2019-01-07T01:56:58, Epoch: 18, Batch: 570, Training Loss: 0.009550781548023224, LR: 0.001
Time, 2019-01-07T01:56:58, Epoch: 18, Batch: 580, Training Loss: 0.01807241216301918, LR: 0.001
Time, 2019-01-07T01:56:59, Epoch: 18, Batch: 590, Training Loss: 0.004513567686080933, LR: 0.001
Time, 2019-01-07T01:57:00, Epoch: 18, Batch: 600, Training Loss: 0.015986111760139466, LR: 0.001
Time, 2019-01-07T01:57:00, Epoch: 18, Batch: 610, Training Loss: 0.015112296491861344, LR: 0.001
Time, 2019-01-07T01:57:01, Epoch: 18, Batch: 620, Training Loss: 0.006961177289485932, LR: 0.001
Time, 2019-01-07T01:57:02, Epoch: 18, Batch: 630, Training Loss: 0.0045363157987594604, LR: 0.001
Time, 2019-01-07T01:57:02, Epoch: 18, Batch: 640, Training Loss: 0.014569516479969024, LR: 0.001
Time, 2019-01-07T01:57:03, Epoch: 18, Batch: 650, Training Loss: 0.005743450671434403, LR: 0.001
Time, 2019-01-07T01:57:04, Epoch: 18, Batch: 660, Training Loss: 0.005079493671655655, LR: 0.001
Time, 2019-01-07T01:57:04, Epoch: 18, Batch: 670, Training Loss: 0.004591058194637299, LR: 0.001
Time, 2019-01-07T01:57:05, Epoch: 18, Batch: 680, Training Loss: 0.005650487542152405, LR: 0.001
Time, 2019-01-07T01:57:06, Epoch: 18, Batch: 690, Training Loss: 0.008578860759735107, LR: 0.001
Time, 2019-01-07T01:57:06, Epoch: 18, Batch: 700, Training Loss: 0.004490634799003601, LR: 0.001
Time, 2019-01-07T01:57:07, Epoch: 18, Batch: 710, Training Loss: 0.011464261263608933, LR: 0.001
Time, 2019-01-07T01:57:08, Epoch: 18, Batch: 720, Training Loss: 0.005574321001768112, LR: 0.001
Time, 2019-01-07T01:57:08, Epoch: 18, Batch: 730, Training Loss: 0.011723502725362777, LR: 0.001
Time, 2019-01-07T01:57:09, Epoch: 18, Batch: 740, Training Loss: 0.00906398892402649, LR: 0.001
Time, 2019-01-07T01:57:10, Epoch: 18, Batch: 750, Training Loss: 0.0040911674499511715, LR: 0.001
Time, 2019-01-07T01:57:10, Epoch: 18, Batch: 760, Training Loss: 0.008486395329236984, LR: 0.001
Time, 2019-01-07T01:57:11, Epoch: 18, Batch: 770, Training Loss: 0.004836011677980423, LR: 0.001
Time, 2019-01-07T01:57:12, Epoch: 18, Batch: 780, Training Loss: 0.00566689595580101, LR: 0.001
Time, 2019-01-07T01:57:12, Epoch: 18, Batch: 790, Training Loss: 0.011046978831291198, LR: 0.001
Time, 2019-01-07T01:57:13, Epoch: 18, Batch: 800, Training Loss: 0.013692820817232132, LR: 0.001
Time, 2019-01-07T01:57:14, Epoch: 18, Batch: 810, Training Loss: 0.015571381151676177, LR: 0.001
Time, 2019-01-07T01:57:14, Epoch: 18, Batch: 820, Training Loss: 0.011639081686735154, LR: 0.001
Time, 2019-01-07T01:57:15, Epoch: 18, Batch: 830, Training Loss: 0.01548221856355667, LR: 0.001
Time, 2019-01-07T01:57:15, Epoch: 18, Batch: 840, Training Loss: 0.012591483443975449, LR: 0.001
Time, 2019-01-07T01:57:16, Epoch: 18, Batch: 850, Training Loss: 0.005423659086227417, LR: 0.001
Time, 2019-01-07T01:57:17, Epoch: 18, Batch: 860, Training Loss: 0.008189187198877335, LR: 0.001
Time, 2019-01-07T01:57:17, Epoch: 18, Batch: 870, Training Loss: 0.015264970064163209, LR: 0.001
Time, 2019-01-07T01:57:18, Epoch: 18, Batch: 880, Training Loss: 0.0076004423201084135, LR: 0.001
Time, 2019-01-07T01:57:19, Epoch: 18, Batch: 890, Training Loss: 0.007242539525032043, LR: 0.001
Time, 2019-01-07T01:57:19, Epoch: 18, Batch: 900, Training Loss: 0.014968056231737137, LR: 0.001
Time, 2019-01-07T01:57:20, Epoch: 18, Batch: 910, Training Loss: 0.005785586684942246, LR: 0.001
Time, 2019-01-07T01:57:21, Epoch: 18, Batch: 920, Training Loss: 0.010522153228521347, LR: 0.001
Time, 2019-01-07T01:57:21, Epoch: 18, Batch: 930, Training Loss: 0.0074775852262973785, LR: 0.001
Epoch: 18, Validation Top 1 acc: 98.87539672851562
Epoch: 18, Validation Top 5 acc: 99.99005126953125
Epoch: 18, Validation Set Loss: 0.041862957179546356
Start training epoch 19
Time, 2019-01-07T01:57:25, Epoch: 19, Batch: 10, Training Loss: 0.012299346178770066, LR: 0.001
Time, 2019-01-07T01:57:25, Epoch: 19, Batch: 20, Training Loss: 0.0038054361939430237, LR: 0.001
Time, 2019-01-07T01:57:26, Epoch: 19, Batch: 30, Training Loss: 0.011664652079343796, LR: 0.001
Time, 2019-01-07T01:57:26, Epoch: 19, Batch: 40, Training Loss: 0.01026918664574623, LR: 0.001
Time, 2019-01-07T01:57:27, Epoch: 19, Batch: 50, Training Loss: 0.004875968396663666, LR: 0.001
Time, 2019-01-07T01:57:28, Epoch: 19, Batch: 60, Training Loss: 0.005647652596235275, LR: 0.001
Time, 2019-01-07T01:57:28, Epoch: 19, Batch: 70, Training Loss: 0.006173136085271836, LR: 0.001
Time, 2019-01-07T01:57:29, Epoch: 19, Batch: 80, Training Loss: 0.005301811546087265, LR: 0.001
Time, 2019-01-07T01:57:30, Epoch: 19, Batch: 90, Training Loss: 0.0026022329926490783, LR: 0.001
Time, 2019-01-07T01:57:31, Epoch: 19, Batch: 100, Training Loss: 0.003802405297756195, LR: 0.001
Time, 2019-01-07T01:57:31, Epoch: 19, Batch: 110, Training Loss: 0.005104078352451325, LR: 0.001
Time, 2019-01-07T01:57:32, Epoch: 19, Batch: 120, Training Loss: 0.0013943031430244447, LR: 0.001
Time, 2019-01-07T01:57:33, Epoch: 19, Batch: 130, Training Loss: 0.005708590149879456, LR: 0.001
Time, 2019-01-07T01:57:33, Epoch: 19, Batch: 140, Training Loss: 0.008717700093984603, LR: 0.001
Time, 2019-01-07T01:57:34, Epoch: 19, Batch: 150, Training Loss: 0.013237515836954117, LR: 0.001
Time, 2019-01-07T01:57:35, Epoch: 19, Batch: 160, Training Loss: 0.013906298577785492, LR: 0.001
Time, 2019-01-07T01:57:35, Epoch: 19, Batch: 170, Training Loss: 0.006751854717731476, LR: 0.001
Time, 2019-01-07T01:57:36, Epoch: 19, Batch: 180, Training Loss: 0.0031870603561401367, LR: 0.001
Time, 2019-01-07T01:57:37, Epoch: 19, Batch: 190, Training Loss: 0.028012144565582275, LR: 0.001
Time, 2019-01-07T01:57:37, Epoch: 19, Batch: 200, Training Loss: 0.013156304508447647, LR: 0.001
Time, 2019-01-07T01:57:38, Epoch: 19, Batch: 210, Training Loss: 0.011605360358953477, LR: 0.001
Time, 2019-01-07T01:57:38, Epoch: 19, Batch: 220, Training Loss: 0.0073276437819004055, LR: 0.001
Time, 2019-01-07T01:57:39, Epoch: 19, Batch: 230, Training Loss: 0.009079322218894958, LR: 0.001
Time, 2019-01-07T01:57:40, Epoch: 19, Batch: 240, Training Loss: 0.0044077299535274506, LR: 0.001
Time, 2019-01-07T01:57:40, Epoch: 19, Batch: 250, Training Loss: 0.006556139886379242, LR: 0.001
Time, 2019-01-07T01:57:41, Epoch: 19, Batch: 260, Training Loss: 0.010895034670829773, LR: 0.001
Time, 2019-01-07T01:57:42, Epoch: 19, Batch: 270, Training Loss: 0.020449433475732803, LR: 0.001
Time, 2019-01-07T01:57:42, Epoch: 19, Batch: 280, Training Loss: 0.012800148874521255, LR: 0.001
Time, 2019-01-07T01:57:43, Epoch: 19, Batch: 290, Training Loss: 0.01381758600473404, LR: 0.001
Time, 2019-01-07T01:57:44, Epoch: 19, Batch: 300, Training Loss: 0.01483403444290161, LR: 0.001
Time, 2019-01-07T01:57:45, Epoch: 19, Batch: 310, Training Loss: 0.00842835232615471, LR: 0.001
Time, 2019-01-07T01:57:45, Epoch: 19, Batch: 320, Training Loss: 0.012675277143716811, LR: 0.001
Time, 2019-01-07T01:57:46, Epoch: 19, Batch: 330, Training Loss: 0.013213077187538147, LR: 0.001
Time, 2019-01-07T01:57:47, Epoch: 19, Batch: 340, Training Loss: 0.01805407479405403, LR: 0.001
Time, 2019-01-07T01:57:47, Epoch: 19, Batch: 350, Training Loss: 0.01161184161901474, LR: 0.001
Time, 2019-01-07T01:57:48, Epoch: 19, Batch: 360, Training Loss: 0.009307879209518432, LR: 0.001
Time, 2019-01-07T01:57:49, Epoch: 19, Batch: 370, Training Loss: 0.012506543099880219, LR: 0.001
Time, 2019-01-07T01:57:49, Epoch: 19, Batch: 380, Training Loss: 0.009637350589036942, LR: 0.001
Time, 2019-01-07T01:57:50, Epoch: 19, Batch: 390, Training Loss: 0.010791606456041335, LR: 0.001
Time, 2019-01-07T01:57:50, Epoch: 19, Batch: 400, Training Loss: 0.002424856275320053, LR: 0.001
Time, 2019-01-07T01:57:51, Epoch: 19, Batch: 410, Training Loss: 0.006433150172233582, LR: 0.001
Time, 2019-01-07T01:57:52, Epoch: 19, Batch: 420, Training Loss: 0.014420630037784576, LR: 0.001
Time, 2019-01-07T01:57:52, Epoch: 19, Batch: 430, Training Loss: 0.010676031559705734, LR: 0.001
Time, 2019-01-07T01:57:53, Epoch: 19, Batch: 440, Training Loss: 0.0030984655022621155, LR: 0.001
Time, 2019-01-07T01:57:54, Epoch: 19, Batch: 450, Training Loss: 0.007285735756158829, LR: 0.001
Time, 2019-01-07T01:57:55, Epoch: 19, Batch: 460, Training Loss: 0.007008101046085358, LR: 0.001
Time, 2019-01-07T01:57:55, Epoch: 19, Batch: 470, Training Loss: 0.0028588689863681795, LR: 0.001
Time, 2019-01-07T01:57:56, Epoch: 19, Batch: 480, Training Loss: 0.012021009624004365, LR: 0.001
Time, 2019-01-07T01:57:57, Epoch: 19, Batch: 490, Training Loss: 0.008011829107999802, LR: 0.001
Time, 2019-01-07T01:57:57, Epoch: 19, Batch: 500, Training Loss: 0.012524450942873955, LR: 0.001
Time, 2019-01-07T01:57:58, Epoch: 19, Batch: 510, Training Loss: 0.013736199587583542, LR: 0.001
Time, 2019-01-07T01:57:59, Epoch: 19, Batch: 520, Training Loss: 0.02098166197538376, LR: 0.001
Time, 2019-01-07T01:58:00, Epoch: 19, Batch: 530, Training Loss: 0.012212269753217698, LR: 0.001
Time, 2019-01-07T01:58:00, Epoch: 19, Batch: 540, Training Loss: 0.011130949109792709, LR: 0.001
Time, 2019-01-07T01:58:01, Epoch: 19, Batch: 550, Training Loss: 0.015724222362041473, LR: 0.001
Time, 2019-01-07T01:58:02, Epoch: 19, Batch: 560, Training Loss: 0.004527562856674194, LR: 0.001
Time, 2019-01-07T01:58:02, Epoch: 19, Batch: 570, Training Loss: 0.007560617476701737, LR: 0.001
Time, 2019-01-07T01:58:03, Epoch: 19, Batch: 580, Training Loss: 0.008425693958997726, LR: 0.001
Time, 2019-01-07T01:58:04, Epoch: 19, Batch: 590, Training Loss: 0.004865758121013641, LR: 0.001
Time, 2019-01-07T01:58:04, Epoch: 19, Batch: 600, Training Loss: 0.00871090143918991, LR: 0.001
Time, 2019-01-07T01:58:05, Epoch: 19, Batch: 610, Training Loss: 0.01566263511776924, LR: 0.001
Time, 2019-01-07T01:58:06, Epoch: 19, Batch: 620, Training Loss: 0.007892018556594849, LR: 0.001
Time, 2019-01-07T01:58:07, Epoch: 19, Batch: 630, Training Loss: 0.004042413830757141, LR: 0.001
Time, 2019-01-07T01:58:07, Epoch: 19, Batch: 640, Training Loss: 0.01810799613595009, LR: 0.001
Time, 2019-01-07T01:58:08, Epoch: 19, Batch: 650, Training Loss: 0.0037292182445526124, LR: 0.001
Time, 2019-01-07T01:58:09, Epoch: 19, Batch: 660, Training Loss: 0.0159675732254982, LR: 0.001
Time, 2019-01-07T01:58:10, Epoch: 19, Batch: 670, Training Loss: 0.012058864533901214, LR: 0.001
Time, 2019-01-07T01:58:11, Epoch: 19, Batch: 680, Training Loss: 0.006513267010450363, LR: 0.001
Time, 2019-01-07T01:58:11, Epoch: 19, Batch: 690, Training Loss: 0.00880371853709221, LR: 0.001
Time, 2019-01-07T01:58:12, Epoch: 19, Batch: 700, Training Loss: 0.026687034964561464, LR: 0.001
Time, 2019-01-07T01:58:13, Epoch: 19, Batch: 710, Training Loss: 0.007929260283708573, LR: 0.001
Time, 2019-01-07T01:58:14, Epoch: 19, Batch: 720, Training Loss: 0.01297362670302391, LR: 0.001
Time, 2019-01-07T01:58:14, Epoch: 19, Batch: 730, Training Loss: 0.012159273028373718, LR: 0.001
Time, 2019-01-07T01:58:15, Epoch: 19, Batch: 740, Training Loss: 0.006924398243427277, LR: 0.001
Time, 2019-01-07T01:58:16, Epoch: 19, Batch: 750, Training Loss: 0.015237069875001907, LR: 0.001
Time, 2019-01-07T01:58:17, Epoch: 19, Batch: 760, Training Loss: 0.010067316889762878, LR: 0.001
Time, 2019-01-07T01:58:17, Epoch: 19, Batch: 770, Training Loss: 0.005193110555410385, LR: 0.001
Time, 2019-01-07T01:58:18, Epoch: 19, Batch: 780, Training Loss: 0.0113503098487854, LR: 0.001
Time, 2019-01-07T01:58:19, Epoch: 19, Batch: 790, Training Loss: 0.021847279369831087, LR: 0.001
Time, 2019-01-07T01:58:19, Epoch: 19, Batch: 800, Training Loss: 0.016850249469280244, LR: 0.001
Time, 2019-01-07T01:58:20, Epoch: 19, Batch: 810, Training Loss: 0.00999564528465271, LR: 0.001
Time, 2019-01-07T01:58:21, Epoch: 19, Batch: 820, Training Loss: 0.008791834861040116, LR: 0.001
Time, 2019-01-07T01:58:21, Epoch: 19, Batch: 830, Training Loss: 0.005750838667154312, LR: 0.001
Time, 2019-01-07T01:58:22, Epoch: 19, Batch: 840, Training Loss: 0.013643204420804977, LR: 0.001
Time, 2019-01-07T01:58:23, Epoch: 19, Batch: 850, Training Loss: 0.011318455636501312, LR: 0.001
Time, 2019-01-07T01:58:23, Epoch: 19, Batch: 860, Training Loss: 0.018021547794342042, LR: 0.001
Time, 2019-01-07T01:58:24, Epoch: 19, Batch: 870, Training Loss: 0.006966008245944977, LR: 0.001
Time, 2019-01-07T01:58:25, Epoch: 19, Batch: 880, Training Loss: 0.014316052943468095, LR: 0.001
Time, 2019-01-07T01:58:26, Epoch: 19, Batch: 890, Training Loss: 0.007336109131574631, LR: 0.001
Time, 2019-01-07T01:58:27, Epoch: 19, Batch: 900, Training Loss: 0.016219568997621538, LR: 0.001
Time, 2019-01-07T01:58:27, Epoch: 19, Batch: 910, Training Loss: 0.014799819141626359, LR: 0.001
Time, 2019-01-07T01:58:28, Epoch: 19, Batch: 920, Training Loss: 0.012914987653493882, LR: 0.001
Time, 2019-01-07T01:58:29, Epoch: 19, Batch: 930, Training Loss: 0.015245098620653152, LR: 0.001
Epoch: 19, Validation Top 1 acc: 98.65644836425781
Epoch: 19, Validation Top 5 acc: 100.0
Epoch: 19, Validation Set Loss: 0.04622883349657059
Start training epoch 20
Time, 2019-01-07T01:58:32, Epoch: 20, Batch: 10, Training Loss: 0.007424885779619217, LR: 0.001
Time, 2019-01-07T01:58:33, Epoch: 20, Batch: 20, Training Loss: 0.003802144527435303, LR: 0.001
Time, 2019-01-07T01:58:33, Epoch: 20, Batch: 30, Training Loss: 0.006087192147970199, LR: 0.001
Time, 2019-01-07T01:58:34, Epoch: 20, Batch: 40, Training Loss: 0.004755623638629913, LR: 0.001
Time, 2019-01-07T01:58:35, Epoch: 20, Batch: 50, Training Loss: 0.0031132772564888, LR: 0.001
Time, 2019-01-07T01:58:35, Epoch: 20, Batch: 60, Training Loss: 0.00689961314201355, LR: 0.001
Time, 2019-01-07T01:58:36, Epoch: 20, Batch: 70, Training Loss: 0.016374529153108597, LR: 0.001
Time, 2019-01-07T01:58:36, Epoch: 20, Batch: 80, Training Loss: 0.013153775036334992, LR: 0.001
Time, 2019-01-07T01:58:37, Epoch: 20, Batch: 90, Training Loss: 0.007139643281698227, LR: 0.001
Time, 2019-01-07T01:58:38, Epoch: 20, Batch: 100, Training Loss: 0.007526670396327972, LR: 0.001
Time, 2019-01-07T01:58:39, Epoch: 20, Batch: 110, Training Loss: 0.006575395911931991, LR: 0.001
Time, 2019-01-07T01:58:39, Epoch: 20, Batch: 120, Training Loss: 0.006185448169708252, LR: 0.001
Time, 2019-01-07T01:58:40, Epoch: 20, Batch: 130, Training Loss: 0.018840865045785905, LR: 0.001
Time, 2019-01-07T01:58:41, Epoch: 20, Batch: 140, Training Loss: 0.008012790232896805, LR: 0.001
Time, 2019-01-07T01:58:41, Epoch: 20, Batch: 150, Training Loss: 0.002614489197731018, LR: 0.001
Time, 2019-01-07T01:58:42, Epoch: 20, Batch: 160, Training Loss: 0.006847967207431793, LR: 0.001
Time, 2019-01-07T01:58:43, Epoch: 20, Batch: 170, Training Loss: 0.01154964342713356, LR: 0.001
Time, 2019-01-07T01:58:43, Epoch: 20, Batch: 180, Training Loss: 0.0049249626696109775, LR: 0.001
Time, 2019-01-07T01:58:44, Epoch: 20, Batch: 190, Training Loss: 0.010739780217409133, LR: 0.001
Time, 2019-01-07T01:58:45, Epoch: 20, Batch: 200, Training Loss: 0.004966378957033157, LR: 0.001
Time, 2019-01-07T01:58:45, Epoch: 20, Batch: 210, Training Loss: 0.005210354924201965, LR: 0.001
Time, 2019-01-07T01:58:46, Epoch: 20, Batch: 220, Training Loss: 0.00198507159948349, LR: 0.001
Time, 2019-01-07T01:58:47, Epoch: 20, Batch: 230, Training Loss: 0.010608506202697755, LR: 0.001
Time, 2019-01-07T01:58:47, Epoch: 20, Batch: 240, Training Loss: 0.015473916381597518, LR: 0.001
Time, 2019-01-07T01:58:48, Epoch: 20, Batch: 250, Training Loss: 0.0012820437550544739, LR: 0.001
Time, 2019-01-07T01:58:49, Epoch: 20, Batch: 260, Training Loss: 0.005446287989616394, LR: 0.001
Time, 2019-01-07T01:58:50, Epoch: 20, Batch: 270, Training Loss: 0.007658784836530685, LR: 0.001
Time, 2019-01-07T01:58:50, Epoch: 20, Batch: 280, Training Loss: 0.0029889784753322603, LR: 0.001
Time, 2019-01-07T01:58:51, Epoch: 20, Batch: 290, Training Loss: 0.008776436001062394, LR: 0.001
Time, 2019-01-07T01:58:52, Epoch: 20, Batch: 300, Training Loss: 0.00161239355802536, LR: 0.001
Time, 2019-01-07T01:58:53, Epoch: 20, Batch: 310, Training Loss: 0.005438832938671112, LR: 0.001
Time, 2019-01-07T01:58:53, Epoch: 20, Batch: 320, Training Loss: 0.0073916956782341, LR: 0.001
Time, 2019-01-07T01:58:54, Epoch: 20, Batch: 330, Training Loss: 0.02236369624733925, LR: 0.001
Time, 2019-01-07T01:58:55, Epoch: 20, Batch: 340, Training Loss: 0.005794303119182586, LR: 0.001
Time, 2019-01-07T01:58:56, Epoch: 20, Batch: 350, Training Loss: 0.017827725410461424, LR: 0.001
Time, 2019-01-07T01:58:56, Epoch: 20, Batch: 360, Training Loss: 0.0039411671459674835, LR: 0.001
Time, 2019-01-07T01:58:57, Epoch: 20, Batch: 370, Training Loss: 0.01964712142944336, LR: 0.001
Time, 2019-01-07T01:58:58, Epoch: 20, Batch: 380, Training Loss: 0.002931758761405945, LR: 0.001
Time, 2019-01-07T01:58:58, Epoch: 20, Batch: 390, Training Loss: 0.00795072242617607, LR: 0.001
Time, 2019-01-07T01:58:59, Epoch: 20, Batch: 400, Training Loss: 0.012521431595087052, LR: 0.001
Time, 2019-01-07T01:59:00, Epoch: 20, Batch: 410, Training Loss: 0.018834544718265532, LR: 0.001
Time, 2019-01-07T01:59:01, Epoch: 20, Batch: 420, Training Loss: 0.010370580106973648, LR: 0.001
Time, 2019-01-07T01:59:01, Epoch: 20, Batch: 430, Training Loss: 0.015405577421188355, LR: 0.001
Time, 2019-01-07T01:59:02, Epoch: 20, Batch: 440, Training Loss: 0.0154690682888031, LR: 0.001
Time, 2019-01-07T01:59:03, Epoch: 20, Batch: 450, Training Loss: 0.02201015651226044, LR: 0.001
Time, 2019-01-07T01:59:04, Epoch: 20, Batch: 460, Training Loss: 0.007275793701410294, LR: 0.001
Time, 2019-01-07T01:59:05, Epoch: 20, Batch: 470, Training Loss: 0.010339707881212235, LR: 0.001
Time, 2019-01-07T01:59:05, Epoch: 20, Batch: 480, Training Loss: 0.005759372189640999, LR: 0.001
Time, 2019-01-07T01:59:06, Epoch: 20, Batch: 490, Training Loss: 0.009858836978673935, LR: 0.001
Time, 2019-01-07T01:59:07, Epoch: 20, Batch: 500, Training Loss: 0.004652834683656693, LR: 0.001
Time, 2019-01-07T01:59:07, Epoch: 20, Batch: 510, Training Loss: 0.007630802690982819, LR: 0.001
Time, 2019-01-07T01:59:08, Epoch: 20, Batch: 520, Training Loss: 0.008850733935832977, LR: 0.001
Time, 2019-01-07T01:59:09, Epoch: 20, Batch: 530, Training Loss: 0.008545873314142227, LR: 0.001
Time, 2019-01-07T01:59:09, Epoch: 20, Batch: 540, Training Loss: 0.007249770313501358, LR: 0.001
Time, 2019-01-07T01:59:10, Epoch: 20, Batch: 550, Training Loss: 0.008048254251480102, LR: 0.001
Time, 2019-01-07T01:59:11, Epoch: 20, Batch: 560, Training Loss: 0.003087889403104782, LR: 0.001
Time, 2019-01-07T01:59:11, Epoch: 20, Batch: 570, Training Loss: 0.011690370738506317, LR: 0.001
Time, 2019-01-07T01:59:12, Epoch: 20, Batch: 580, Training Loss: 0.012830357998609543, LR: 0.001
Time, 2019-01-07T01:59:13, Epoch: 20, Batch: 590, Training Loss: 0.0060756891965866085, LR: 0.001
Time, 2019-01-07T01:59:13, Epoch: 20, Batch: 600, Training Loss: 0.0026606321334838867, LR: 0.001
Time, 2019-01-07T01:59:14, Epoch: 20, Batch: 610, Training Loss: 0.018042466789484023, LR: 0.001
Time, 2019-01-07T01:59:15, Epoch: 20, Batch: 620, Training Loss: 0.002122323215007782, LR: 0.001
Time, 2019-01-07T01:59:15, Epoch: 20, Batch: 630, Training Loss: 0.010765300691127777, LR: 0.001
Time, 2019-01-07T01:59:16, Epoch: 20, Batch: 640, Training Loss: 0.014335133135318756, LR: 0.001
Time, 2019-01-07T01:59:17, Epoch: 20, Batch: 650, Training Loss: 0.0102882981300354, LR: 0.001
Time, 2019-01-07T01:59:17, Epoch: 20, Batch: 660, Training Loss: 0.01777043268084526, LR: 0.001
Time, 2019-01-07T01:59:18, Epoch: 20, Batch: 670, Training Loss: 0.023758213222026824, LR: 0.001
Time, 2019-01-07T01:59:19, Epoch: 20, Batch: 680, Training Loss: 0.023095397651195525, LR: 0.001
Time, 2019-01-07T01:59:19, Epoch: 20, Batch: 690, Training Loss: 0.029739001393318178, LR: 0.001
Time, 2019-01-07T01:59:20, Epoch: 20, Batch: 700, Training Loss: 0.008498786389827729, LR: 0.001
Time, 2019-01-07T01:59:21, Epoch: 20, Batch: 710, Training Loss: 0.024270106852054597, LR: 0.001
Time, 2019-01-07T01:59:21, Epoch: 20, Batch: 720, Training Loss: 0.014623890072107315, LR: 0.001
Time, 2019-01-07T01:59:22, Epoch: 20, Batch: 730, Training Loss: 0.024989596754312515, LR: 0.001
Time, 2019-01-07T01:59:23, Epoch: 20, Batch: 740, Training Loss: 0.01607785075902939, LR: 0.001
Time, 2019-01-07T01:59:23, Epoch: 20, Batch: 750, Training Loss: 0.005389407277107239, LR: 0.001
Time, 2019-01-07T01:59:24, Epoch: 20, Batch: 760, Training Loss: 0.023711083829402922, LR: 0.001
Time, 2019-01-07T01:59:25, Epoch: 20, Batch: 770, Training Loss: 0.004949437081813812, LR: 0.001
Time, 2019-01-07T01:59:25, Epoch: 20, Batch: 780, Training Loss: 0.016909436509013175, LR: 0.001
Time, 2019-01-07T01:59:26, Epoch: 20, Batch: 790, Training Loss: 0.005713862180709839, LR: 0.001
Time, 2019-01-07T01:59:27, Epoch: 20, Batch: 800, Training Loss: 0.013067010790109634, LR: 0.001
Time, 2019-01-07T01:59:28, Epoch: 20, Batch: 810, Training Loss: 0.01492200568318367, LR: 0.001
Time, 2019-01-07T01:59:29, Epoch: 20, Batch: 820, Training Loss: 0.021350057423114778, LR: 0.001
Time, 2019-01-07T01:59:29, Epoch: 20, Batch: 830, Training Loss: 0.006880670040845871, LR: 0.001
Time, 2019-01-07T01:59:30, Epoch: 20, Batch: 840, Training Loss: 0.010935632884502411, LR: 0.001
Time, 2019-01-07T01:59:31, Epoch: 20, Batch: 850, Training Loss: 0.008045709133148194, LR: 0.001
Time, 2019-01-07T01:59:32, Epoch: 20, Batch: 860, Training Loss: 0.007614445686340332, LR: 0.001
Time, 2019-01-07T01:59:32, Epoch: 20, Batch: 870, Training Loss: 0.025448456406593323, LR: 0.001
Time, 2019-01-07T01:59:33, Epoch: 20, Batch: 880, Training Loss: 0.027564671635627747, LR: 0.001
Time, 2019-01-07T01:59:34, Epoch: 20, Batch: 890, Training Loss: 0.005559171736240387, LR: 0.001
Time, 2019-01-07T01:59:35, Epoch: 20, Batch: 900, Training Loss: 0.002987608313560486, LR: 0.001
Time, 2019-01-07T01:59:35, Epoch: 20, Batch: 910, Training Loss: 0.014235495030879975, LR: 0.001
Time, 2019-01-07T01:59:36, Epoch: 20, Batch: 920, Training Loss: 0.008828002214431762, LR: 0.001
Time, 2019-01-07T01:59:37, Epoch: 20, Batch: 930, Training Loss: 0.022393184900283813, LR: 0.001
Epoch: 20, Validation Top 1 acc: 98.52706909179688
Epoch: 20, Validation Top 5 acc: 99.99005126953125
Epoch: 20, Validation Set Loss: 0.047864627093076706
Start training epoch 21
Time, 2019-01-07T01:59:40, Epoch: 21, Batch: 10, Training Loss: 0.02524598315358162, LR: 0.001
Time, 2019-01-07T01:59:41, Epoch: 21, Batch: 20, Training Loss: 0.002451612800359726, LR: 0.001
Time, 2019-01-07T01:59:41, Epoch: 21, Batch: 30, Training Loss: 0.011776052415370941, LR: 0.001
Time, 2019-01-07T01:59:42, Epoch: 21, Batch: 40, Training Loss: 0.013700180500745774, LR: 0.001
Time, 2019-01-07T01:59:43, Epoch: 21, Batch: 50, Training Loss: 0.006619748473167419, LR: 0.001
Time, 2019-01-07T01:59:43, Epoch: 21, Batch: 60, Training Loss: 0.006201149523258209, LR: 0.001
Time, 2019-01-07T01:59:44, Epoch: 21, Batch: 70, Training Loss: 0.004082828760147095, LR: 0.001
Time, 2019-01-07T01:59:45, Epoch: 21, Batch: 80, Training Loss: 0.01428648754954338, LR: 0.001
Time, 2019-01-07T01:59:45, Epoch: 21, Batch: 90, Training Loss: 0.008671974390745163, LR: 0.001
Time, 2019-01-07T01:59:46, Epoch: 21, Batch: 100, Training Loss: 0.0025306694209575654, LR: 0.001
Time, 2019-01-07T01:59:47, Epoch: 21, Batch: 110, Training Loss: 0.008442455530166626, LR: 0.001
Time, 2019-01-07T01:59:47, Epoch: 21, Batch: 120, Training Loss: 0.011223978549242019, LR: 0.001
Time, 2019-01-07T01:59:48, Epoch: 21, Batch: 130, Training Loss: 0.008737041801214217, LR: 0.001
Time, 2019-01-07T01:59:49, Epoch: 21, Batch: 140, Training Loss: 0.00759725421667099, LR: 0.001
Time, 2019-01-07T01:59:49, Epoch: 21, Batch: 150, Training Loss: 0.005084782838821411, LR: 0.001
Time, 2019-01-07T01:59:50, Epoch: 21, Batch: 160, Training Loss: 0.0023319654166698454, LR: 0.001
Time, 2019-01-07T01:59:51, Epoch: 21, Batch: 170, Training Loss: 0.007014756649732589, LR: 0.001
Time, 2019-01-07T01:59:51, Epoch: 21, Batch: 180, Training Loss: 0.010317555069923401, LR: 0.001
Time, 2019-01-07T01:59:52, Epoch: 21, Batch: 190, Training Loss: 0.0032227009534835814, LR: 0.001
Time, 2019-01-07T01:59:53, Epoch: 21, Batch: 200, Training Loss: 0.014095982909202576, LR: 0.001
Time, 2019-01-07T01:59:54, Epoch: 21, Batch: 210, Training Loss: 0.017156700789928436, LR: 0.001
Time, 2019-01-07T01:59:54, Epoch: 21, Batch: 220, Training Loss: 0.008676941692829131, LR: 0.001
Time, 2019-01-07T01:59:55, Epoch: 21, Batch: 230, Training Loss: 0.007776162773370743, LR: 0.001
Time, 2019-01-07T01:59:56, Epoch: 21, Batch: 240, Training Loss: 0.007657539844512939, LR: 0.001
Time, 2019-01-07T01:59:56, Epoch: 21, Batch: 250, Training Loss: 0.0008625030517578125, LR: 0.001
Time, 2019-01-07T01:59:57, Epoch: 21, Batch: 260, Training Loss: 0.02287127897143364, LR: 0.001
Time, 2019-01-07T01:59:58, Epoch: 21, Batch: 270, Training Loss: 0.002595481276512146, LR: 0.001
Time, 2019-01-07T01:59:59, Epoch: 21, Batch: 280, Training Loss: 0.007902664691209793, LR: 0.001
Time, 2019-01-07T01:59:59, Epoch: 21, Batch: 290, Training Loss: 0.002282281219959259, LR: 0.001
Time, 2019-01-07T02:00:00, Epoch: 21, Batch: 300, Training Loss: 0.004364325851202011, LR: 0.001
Time, 2019-01-07T02:00:01, Epoch: 21, Batch: 310, Training Loss: 0.011525524407625198, LR: 0.001
Time, 2019-01-07T02:00:01, Epoch: 21, Batch: 320, Training Loss: 0.007599148899316788, LR: 0.001
Time, 2019-01-07T02:00:02, Epoch: 21, Batch: 330, Training Loss: 0.0070564568042755125, LR: 0.001
Time, 2019-01-07T02:00:03, Epoch: 21, Batch: 340, Training Loss: 0.006825896352529526, LR: 0.001
Time, 2019-01-07T02:00:03, Epoch: 21, Batch: 350, Training Loss: 0.015699336677789687, LR: 0.001
Time, 2019-01-07T02:00:04, Epoch: 21, Batch: 360, Training Loss: 0.02004583477973938, LR: 0.001
Time, 2019-01-07T02:00:05, Epoch: 21, Batch: 370, Training Loss: 0.008616027981042862, LR: 0.001
Time, 2019-01-07T02:00:06, Epoch: 21, Batch: 380, Training Loss: 0.0180256187915802, LR: 0.001
Time, 2019-01-07T02:00:06, Epoch: 21, Batch: 390, Training Loss: 0.012199715524911881, LR: 0.001
Time, 2019-01-07T02:00:07, Epoch: 21, Batch: 400, Training Loss: 0.0077559158205986025, LR: 0.001
Time, 2019-01-07T02:00:08, Epoch: 21, Batch: 410, Training Loss: 0.009400484710931778, LR: 0.001
Time, 2019-01-07T02:00:08, Epoch: 21, Batch: 420, Training Loss: 0.0033996179699897766, LR: 0.001
Time, 2019-01-07T02:00:09, Epoch: 21, Batch: 430, Training Loss: 0.0073890820145607, LR: 0.001
Time, 2019-01-07T02:00:10, Epoch: 21, Batch: 440, Training Loss: 0.023099078238010405, LR: 0.001
Time, 2019-01-07T02:00:10, Epoch: 21, Batch: 450, Training Loss: 0.0025284811854362488, LR: 0.001
Time, 2019-01-07T02:00:11, Epoch: 21, Batch: 460, Training Loss: 0.013094097375869751, LR: 0.001
Time, 2019-01-07T02:00:12, Epoch: 21, Batch: 470, Training Loss: 0.003992615640163422, LR: 0.001
Time, 2019-01-07T02:00:12, Epoch: 21, Batch: 480, Training Loss: 0.003205074369907379, LR: 0.001
Time, 2019-01-07T02:00:13, Epoch: 21, Batch: 490, Training Loss: 0.00783332735300064, LR: 0.001
Time, 2019-01-07T02:00:14, Epoch: 21, Batch: 500, Training Loss: 0.007419837266206741, LR: 0.001
Time, 2019-01-07T02:00:14, Epoch: 21, Batch: 510, Training Loss: 0.004278585314750671, LR: 0.001
Time, 2019-01-07T02:00:15, Epoch: 21, Batch: 520, Training Loss: 0.019417739659547805, LR: 0.001
Time, 2019-01-07T02:00:16, Epoch: 21, Batch: 530, Training Loss: 0.01490628644824028, LR: 0.001
Time, 2019-01-07T02:00:17, Epoch: 21, Batch: 540, Training Loss: 0.012011998891830444, LR: 0.001
Time, 2019-01-07T02:00:17, Epoch: 21, Batch: 550, Training Loss: 0.0021181240677833556, LR: 0.001
Time, 2019-01-07T02:00:18, Epoch: 21, Batch: 560, Training Loss: 0.009617970883846283, LR: 0.001
Time, 2019-01-07T02:00:19, Epoch: 21, Batch: 570, Training Loss: 0.011341390013694764, LR: 0.001
Time, 2019-01-07T02:00:19, Epoch: 21, Batch: 580, Training Loss: 0.005204842984676361, LR: 0.001
Time, 2019-01-07T02:00:20, Epoch: 21, Batch: 590, Training Loss: 0.012012466788291931, LR: 0.001
Time, 2019-01-07T02:00:21, Epoch: 21, Batch: 600, Training Loss: 0.007765992730855942, LR: 0.001
Time, 2019-01-07T02:00:21, Epoch: 21, Batch: 610, Training Loss: 0.008991048485040665, LR: 0.001
Time, 2019-01-07T02:00:22, Epoch: 21, Batch: 620, Training Loss: 0.01459110677242279, LR: 0.001
Time, 2019-01-07T02:00:23, Epoch: 21, Batch: 630, Training Loss: 0.0020627498626708983, LR: 0.001
Time, 2019-01-07T02:00:24, Epoch: 21, Batch: 640, Training Loss: 0.00567053109407425, LR: 0.001
Time, 2019-01-07T02:00:24, Epoch: 21, Batch: 650, Training Loss: 0.005670999735593795, LR: 0.001
Time, 2019-01-07T02:00:25, Epoch: 21, Batch: 660, Training Loss: 0.011229737102985382, LR: 0.001
Time, 2019-01-07T02:00:26, Epoch: 21, Batch: 670, Training Loss: 0.02522193044424057, LR: 0.001
Time, 2019-01-07T02:00:26, Epoch: 21, Batch: 680, Training Loss: 0.013735327124595641, LR: 0.001
Time, 2019-01-07T02:00:27, Epoch: 21, Batch: 690, Training Loss: 0.0068536423146724704, LR: 0.001
Time, 2019-01-07T02:00:28, Epoch: 21, Batch: 700, Training Loss: 0.0018350988626480102, LR: 0.001
Time, 2019-01-07T02:00:28, Epoch: 21, Batch: 710, Training Loss: 0.007526176422834397, LR: 0.001
Time, 2019-01-07T02:00:29, Epoch: 21, Batch: 720, Training Loss: 0.003815678507089615, LR: 0.001
Time, 2019-01-07T02:00:30, Epoch: 21, Batch: 730, Training Loss: 0.008526676893234253, LR: 0.001
Time, 2019-01-07T02:00:31, Epoch: 21, Batch: 740, Training Loss: 0.012176690995693207, LR: 0.001
Time, 2019-01-07T02:00:32, Epoch: 21, Batch: 750, Training Loss: 0.002423956245183945, LR: 0.001
Time, 2019-01-07T02:00:32, Epoch: 21, Batch: 760, Training Loss: 0.007680200785398483, LR: 0.001
Time, 2019-01-07T02:00:33, Epoch: 21, Batch: 770, Training Loss: 0.004715128242969513, LR: 0.001
Time, 2019-01-07T02:00:34, Epoch: 21, Batch: 780, Training Loss: 0.0034462101757526398, LR: 0.001
Time, 2019-01-07T02:00:34, Epoch: 21, Batch: 790, Training Loss: 0.006467808037996292, LR: 0.001
Time, 2019-01-07T02:00:35, Epoch: 21, Batch: 800, Training Loss: 0.00226021409034729, LR: 0.001
Time, 2019-01-07T02:00:36, Epoch: 21, Batch: 810, Training Loss: 0.011550151556730271, LR: 0.001
Time, 2019-01-07T02:00:36, Epoch: 21, Batch: 820, Training Loss: 0.0207424595952034, LR: 0.001
Time, 2019-01-07T02:00:37, Epoch: 21, Batch: 830, Training Loss: 0.003153276443481445, LR: 0.001
Time, 2019-01-07T02:00:38, Epoch: 21, Batch: 840, Training Loss: 0.00508689284324646, LR: 0.001
Time, 2019-01-07T02:00:38, Epoch: 21, Batch: 850, Training Loss: 0.009220165759325027, LR: 0.001
Time, 2019-01-07T02:00:39, Epoch: 21, Batch: 860, Training Loss: 0.01681520640850067, LR: 0.001
Time, 2019-01-07T02:00:40, Epoch: 21, Batch: 870, Training Loss: 0.005094674229621887, LR: 0.001
Time, 2019-01-07T02:00:41, Epoch: 21, Batch: 880, Training Loss: 0.013193326443433762, LR: 0.001
Time, 2019-01-07T02:00:41, Epoch: 21, Batch: 890, Training Loss: 0.015745732188224792, LR: 0.001
Time, 2019-01-07T02:00:42, Epoch: 21, Batch: 900, Training Loss: 0.006510515511035919, LR: 0.001
Time, 2019-01-07T02:00:43, Epoch: 21, Batch: 910, Training Loss: 0.0053906597197055815, LR: 0.001
Time, 2019-01-07T02:00:44, Epoch: 21, Batch: 920, Training Loss: 0.004384949803352356, LR: 0.001
Time, 2019-01-07T02:00:45, Epoch: 21, Batch: 930, Training Loss: 0.006792248785495758, LR: 0.001
Epoch: 21, Validation Top 1 acc: 98.90525817871094
Epoch: 21, Validation Top 5 acc: 100.0
Epoch: 21, Validation Set Loss: 0.039604175835847855
Start training epoch 22
Time, 2019-01-07T02:00:48, Epoch: 22, Batch: 10, Training Loss: 0.002939525991678238, LR: 0.001
Time, 2019-01-07T02:00:49, Epoch: 22, Batch: 20, Training Loss: 0.003597124665975571, LR: 0.001
Time, 2019-01-07T02:00:49, Epoch: 22, Batch: 30, Training Loss: 0.008933301270008086, LR: 0.001
Time, 2019-01-07T02:00:50, Epoch: 22, Batch: 40, Training Loss: 0.001912762224674225, LR: 0.001
Time, 2019-01-07T02:00:51, Epoch: 22, Batch: 50, Training Loss: 0.001919325441122055, LR: 0.001
Time, 2019-01-07T02:00:51, Epoch: 22, Batch: 60, Training Loss: 0.008474918454885483, LR: 0.001
Time, 2019-01-07T02:00:52, Epoch: 22, Batch: 70, Training Loss: 0.0007807016372680664, LR: 0.001
Time, 2019-01-07T02:00:53, Epoch: 22, Batch: 80, Training Loss: 0.008770927786827087, LR: 0.001
Time, 2019-01-07T02:00:54, Epoch: 22, Batch: 90, Training Loss: 0.007188734412193298, LR: 0.001
Time, 2019-01-07T02:00:54, Epoch: 22, Batch: 100, Training Loss: 0.008335713297128677, LR: 0.001
Time, 2019-01-07T02:00:55, Epoch: 22, Batch: 110, Training Loss: 0.0021845713257789613, LR: 0.001
Time, 2019-01-07T02:00:56, Epoch: 22, Batch: 120, Training Loss: 0.0017149887979030609, LR: 0.001
Time, 2019-01-07T02:00:57, Epoch: 22, Batch: 130, Training Loss: 0.0028144404292106628, LR: 0.001
Time, 2019-01-07T02:00:57, Epoch: 22, Batch: 140, Training Loss: 0.003464853763580322, LR: 0.001
Time, 2019-01-07T02:00:58, Epoch: 22, Batch: 150, Training Loss: 0.00551268458366394, LR: 0.001
Time, 2019-01-07T02:00:59, Epoch: 22, Batch: 160, Training Loss: 0.0011362582445144653, LR: 0.001
Time, 2019-01-07T02:01:00, Epoch: 22, Batch: 170, Training Loss: 0.00251198410987854, LR: 0.001
Time, 2019-01-07T02:01:00, Epoch: 22, Batch: 180, Training Loss: 0.0007153108716011047, LR: 0.001
Time, 2019-01-07T02:01:01, Epoch: 22, Batch: 190, Training Loss: 0.006710892915725708, LR: 0.001
Time, 2019-01-07T02:01:02, Epoch: 22, Batch: 200, Training Loss: 0.008558403700590134, LR: 0.001
Time, 2019-01-07T02:01:03, Epoch: 22, Batch: 210, Training Loss: 0.014561115950345992, LR: 0.001
Time, 2019-01-07T02:01:04, Epoch: 22, Batch: 220, Training Loss: 0.018244864791631697, LR: 0.001
Time, 2019-01-07T02:01:05, Epoch: 22, Batch: 230, Training Loss: 0.014845601469278335, LR: 0.001
Time, 2019-01-07T02:01:05, Epoch: 22, Batch: 240, Training Loss: 0.00797719731926918, LR: 0.001
Time, 2019-01-07T02:01:06, Epoch: 22, Batch: 250, Training Loss: 0.007905402779579162, LR: 0.001
Time, 2019-01-07T02:01:07, Epoch: 22, Batch: 260, Training Loss: 0.008451925218105316, LR: 0.001
Time, 2019-01-07T02:01:08, Epoch: 22, Batch: 270, Training Loss: 0.008632048964500427, LR: 0.001
Time, 2019-01-07T02:01:09, Epoch: 22, Batch: 280, Training Loss: 0.002392999827861786, LR: 0.001
Time, 2019-01-07T02:01:09, Epoch: 22, Batch: 290, Training Loss: 0.0062842130661010746, LR: 0.001
Time, 2019-01-07T02:01:10, Epoch: 22, Batch: 300, Training Loss: 0.0029895246028900147, LR: 0.001
Time, 2019-01-07T02:01:11, Epoch: 22, Batch: 310, Training Loss: 0.006025237590074539, LR: 0.001
Time, 2019-01-07T02:01:12, Epoch: 22, Batch: 320, Training Loss: 0.0016272082924842835, LR: 0.001
Time, 2019-01-07T02:01:12, Epoch: 22, Batch: 330, Training Loss: 0.0037726566195487978, LR: 0.001
Time, 2019-01-07T02:01:13, Epoch: 22, Batch: 340, Training Loss: 0.004622969776391983, LR: 0.001
Time, 2019-01-07T02:01:14, Epoch: 22, Batch: 350, Training Loss: 0.0023758217692375185, LR: 0.001
Time, 2019-01-07T02:01:15, Epoch: 22, Batch: 360, Training Loss: 0.0010811120271682739, LR: 0.001
Time, 2019-01-07T02:01:15, Epoch: 22, Batch: 370, Training Loss: 0.003407059609889984, LR: 0.001
Time, 2019-01-07T02:01:16, Epoch: 22, Batch: 380, Training Loss: 0.009605461359024048, LR: 0.001
Time, 2019-01-07T02:01:17, Epoch: 22, Batch: 390, Training Loss: 0.002314344048500061, LR: 0.001
Time, 2019-01-07T02:01:17, Epoch: 22, Batch: 400, Training Loss: 0.002921581268310547, LR: 0.001
Time, 2019-01-07T02:01:18, Epoch: 22, Batch: 410, Training Loss: 0.011392668634653092, LR: 0.001
Time, 2019-01-07T02:01:19, Epoch: 22, Batch: 420, Training Loss: 0.003877246379852295, LR: 0.001
Time, 2019-01-07T02:01:19, Epoch: 22, Batch: 430, Training Loss: 0.009103772044181824, LR: 0.001
Time, 2019-01-07T02:01:20, Epoch: 22, Batch: 440, Training Loss: 0.007630899548530579, LR: 0.001
Time, 2019-01-07T02:01:21, Epoch: 22, Batch: 450, Training Loss: 0.0010775379836559297, LR: 0.001
Time, 2019-01-07T02:01:21, Epoch: 22, Batch: 460, Training Loss: 0.003218214213848114, LR: 0.001
Time, 2019-01-07T02:01:22, Epoch: 22, Batch: 470, Training Loss: 0.0029643185436725615, LR: 0.001
Time, 2019-01-07T02:01:23, Epoch: 22, Batch: 480, Training Loss: 0.0025424271821975707, LR: 0.001
Time, 2019-01-07T02:01:24, Epoch: 22, Batch: 490, Training Loss: 0.008017140626907348, LR: 0.001
Time, 2019-01-07T02:01:24, Epoch: 22, Batch: 500, Training Loss: 0.0010848060250282288, LR: 0.001
Time, 2019-01-07T02:01:25, Epoch: 22, Batch: 510, Training Loss: 0.005084002017974853, LR: 0.001
Time, 2019-01-07T02:01:26, Epoch: 22, Batch: 520, Training Loss: 0.008512340486049652, LR: 0.001
Time, 2019-01-07T02:01:27, Epoch: 22, Batch: 530, Training Loss: 0.015803969651460647, LR: 0.001
Time, 2019-01-07T02:01:28, Epoch: 22, Batch: 540, Training Loss: 0.019155919551849365, LR: 0.001
Time, 2019-01-07T02:01:29, Epoch: 22, Batch: 550, Training Loss: 0.0021032750606536864, LR: 0.001
Time, 2019-01-07T02:01:29, Epoch: 22, Batch: 560, Training Loss: 0.011711905151605606, LR: 0.001
Time, 2019-01-07T02:01:30, Epoch: 22, Batch: 570, Training Loss: 0.010987594723701477, LR: 0.001
Time, 2019-01-07T02:01:31, Epoch: 22, Batch: 580, Training Loss: 0.013780834525823593, LR: 0.001
Time, 2019-01-07T02:01:31, Epoch: 22, Batch: 590, Training Loss: 0.006255892664194107, LR: 0.001
Time, 2019-01-07T02:01:32, Epoch: 22, Batch: 600, Training Loss: 0.006934073567390442, LR: 0.001
Time, 2019-01-07T02:01:33, Epoch: 22, Batch: 610, Training Loss: 0.002076166868209839, LR: 0.001
Time, 2019-01-07T02:01:33, Epoch: 22, Batch: 620, Training Loss: 0.01057283878326416, LR: 0.001
Time, 2019-01-07T02:01:34, Epoch: 22, Batch: 630, Training Loss: 0.008565845340490342, LR: 0.001
Time, 2019-01-07T02:01:35, Epoch: 22, Batch: 640, Training Loss: 0.004841601848602295, LR: 0.001
Time, 2019-01-07T02:01:36, Epoch: 22, Batch: 650, Training Loss: 0.005301129817962646, LR: 0.001
Time, 2019-01-07T02:01:36, Epoch: 22, Batch: 660, Training Loss: 0.00608687549829483, LR: 0.001
Time, 2019-01-07T02:01:37, Epoch: 22, Batch: 670, Training Loss: 0.012926806509494782, LR: 0.001
Time, 2019-01-07T02:01:38, Epoch: 22, Batch: 680, Training Loss: 0.007232746481895447, LR: 0.001
Time, 2019-01-07T02:01:38, Epoch: 22, Batch: 690, Training Loss: 0.006151343882083893, LR: 0.001
Time, 2019-01-07T02:01:39, Epoch: 22, Batch: 700, Training Loss: 0.0058323323726654054, LR: 0.001
Time, 2019-01-07T02:01:40, Epoch: 22, Batch: 710, Training Loss: 0.018292547762393953, LR: 0.001
Time, 2019-01-07T02:01:40, Epoch: 22, Batch: 720, Training Loss: 0.008136790245771408, LR: 0.001
Time, 2019-01-07T02:01:41, Epoch: 22, Batch: 730, Training Loss: 0.0035322867333889008, LR: 0.001
Time, 2019-01-07T02:01:42, Epoch: 22, Batch: 740, Training Loss: 0.004424894601106644, LR: 0.001
Time, 2019-01-07T02:01:43, Epoch: 22, Batch: 750, Training Loss: 0.005305582284927368, LR: 0.001
Time, 2019-01-07T02:01:44, Epoch: 22, Batch: 760, Training Loss: 0.007235459238290787, LR: 0.001
Time, 2019-01-07T02:01:44, Epoch: 22, Batch: 770, Training Loss: 0.0040512815117836, LR: 0.001
Time, 2019-01-07T02:01:45, Epoch: 22, Batch: 780, Training Loss: 0.0035776518285274505, LR: 0.001
Time, 2019-01-07T02:01:46, Epoch: 22, Batch: 790, Training Loss: 0.011045168340206146, LR: 0.001
Time, 2019-01-07T02:01:47, Epoch: 22, Batch: 800, Training Loss: 0.011483309417963028, LR: 0.001
Time, 2019-01-07T02:01:48, Epoch: 22, Batch: 810, Training Loss: 0.010365881770849229, LR: 0.001
Time, 2019-01-07T02:01:48, Epoch: 22, Batch: 820, Training Loss: 0.0077381618320941925, LR: 0.001
Time, 2019-01-07T02:01:49, Epoch: 22, Batch: 830, Training Loss: 0.006984053552150727, LR: 0.001
Time, 2019-01-07T02:01:50, Epoch: 22, Batch: 840, Training Loss: 0.005656500905752182, LR: 0.001
Time, 2019-01-07T02:01:51, Epoch: 22, Batch: 850, Training Loss: 0.016462332010269164, LR: 0.001
Time, 2019-01-07T02:01:51, Epoch: 22, Batch: 860, Training Loss: 0.010383140295743942, LR: 0.001
Time, 2019-01-07T02:01:52, Epoch: 22, Batch: 870, Training Loss: 0.01379089504480362, LR: 0.001
Time, 2019-01-07T02:01:53, Epoch: 22, Batch: 880, Training Loss: 0.00930919498205185, LR: 0.001
Time, 2019-01-07T02:01:53, Epoch: 22, Batch: 890, Training Loss: 0.03917434141039848, LR: 0.001
Time, 2019-01-07T02:01:54, Epoch: 22, Batch: 900, Training Loss: 0.003431488573551178, LR: 0.001
Time, 2019-01-07T02:01:55, Epoch: 22, Batch: 910, Training Loss: 0.023201313614845277, LR: 0.001
Time, 2019-01-07T02:01:56, Epoch: 22, Batch: 920, Training Loss: 0.02487606406211853, LR: 0.001
Time, 2019-01-07T02:01:56, Epoch: 22, Batch: 930, Training Loss: 0.02133961394429207, LR: 0.001
Epoch: 22, Validation Top 1 acc: 98.45740509033203
Epoch: 22, Validation Top 5 acc: 99.98009490966797
Epoch: 22, Validation Set Loss: 0.06181567534804344
Start training epoch 23
Time, 2019-01-07T02:02:00, Epoch: 23, Batch: 10, Training Loss: 0.003671242296695709, LR: 0.001
Time, 2019-01-07T02:02:00, Epoch: 23, Batch: 20, Training Loss: 0.006515295803546905, LR: 0.001
Time, 2019-01-07T02:02:01, Epoch: 23, Batch: 30, Training Loss: 0.0064882591366767885, LR: 0.001
Time, 2019-01-07T02:02:02, Epoch: 23, Batch: 40, Training Loss: 0.0069545343518257145, LR: 0.001
Time, 2019-01-07T02:02:02, Epoch: 23, Batch: 50, Training Loss: 0.012527605891227723, LR: 0.001
Time, 2019-01-07T02:02:03, Epoch: 23, Batch: 60, Training Loss: 0.013542433083057404, LR: 0.001
Time, 2019-01-07T02:02:04, Epoch: 23, Batch: 70, Training Loss: 0.001992267370223999, LR: 0.001
Time, 2019-01-07T02:02:04, Epoch: 23, Batch: 80, Training Loss: 0.025707822293043137, LR: 0.001
Time, 2019-01-07T02:02:05, Epoch: 23, Batch: 90, Training Loss: 0.0029028892517089845, LR: 0.001
Time, 2019-01-07T02:02:06, Epoch: 23, Batch: 100, Training Loss: 0.008582532405853271, LR: 0.001
Time, 2019-01-07T02:02:06, Epoch: 23, Batch: 110, Training Loss: 0.007310526072978973, LR: 0.001
Time, 2019-01-07T02:02:07, Epoch: 23, Batch: 120, Training Loss: 0.006425771117210388, LR: 0.001
Time, 2019-01-07T02:02:08, Epoch: 23, Batch: 130, Training Loss: 0.015164940059185028, LR: 0.001
Time, 2019-01-07T02:02:08, Epoch: 23, Batch: 140, Training Loss: 0.013568415492773055, LR: 0.001
Time, 2019-01-07T02:02:09, Epoch: 23, Batch: 150, Training Loss: 0.00948045328259468, LR: 0.001
Time, 2019-01-07T02:02:10, Epoch: 23, Batch: 160, Training Loss: 0.013999608904123306, LR: 0.001
Time, 2019-01-07T02:02:10, Epoch: 23, Batch: 170, Training Loss: 0.006064271926879883, LR: 0.001
Time, 2019-01-07T02:02:11, Epoch: 23, Batch: 180, Training Loss: 0.008759481459856033, LR: 0.001
Time, 2019-01-07T02:02:12, Epoch: 23, Batch: 190, Training Loss: 0.003337763249874115, LR: 0.001
Time, 2019-01-07T02:02:13, Epoch: 23, Batch: 200, Training Loss: 0.00714666023850441, LR: 0.001
Time, 2019-01-07T02:02:13, Epoch: 23, Batch: 210, Training Loss: 0.005770335346460343, LR: 0.001
Time, 2019-01-07T02:02:14, Epoch: 23, Batch: 220, Training Loss: 0.01808062717318535, LR: 0.001
Time, 2019-01-07T02:02:15, Epoch: 23, Batch: 230, Training Loss: 0.005317263305187225, LR: 0.001
Time, 2019-01-07T02:02:15, Epoch: 23, Batch: 240, Training Loss: 0.005490602552890777, LR: 0.001
Time, 2019-01-07T02:02:16, Epoch: 23, Batch: 250, Training Loss: 0.014865030348300935, LR: 0.001
Time, 2019-01-07T02:02:16, Epoch: 23, Batch: 260, Training Loss: 0.0019343264400959014, LR: 0.001
Time, 2019-01-07T02:02:17, Epoch: 23, Batch: 270, Training Loss: 0.007357703894376755, LR: 0.001
Time, 2019-01-07T02:02:18, Epoch: 23, Batch: 280, Training Loss: 0.007949768006801606, LR: 0.001
Time, 2019-01-07T02:02:19, Epoch: 23, Batch: 290, Training Loss: 0.0029986537992954254, LR: 0.001
Time, 2019-01-07T02:02:19, Epoch: 23, Batch: 300, Training Loss: 0.014581286907196045, LR: 0.001
Time, 2019-01-07T02:02:20, Epoch: 23, Batch: 310, Training Loss: 0.005206523090600967, LR: 0.001
Time, 2019-01-07T02:02:21, Epoch: 23, Batch: 320, Training Loss: 0.003000252693891525, LR: 0.001
Time, 2019-01-07T02:02:22, Epoch: 23, Batch: 330, Training Loss: 0.0068011261522769925, LR: 0.001
Time, 2019-01-07T02:02:23, Epoch: 23, Batch: 340, Training Loss: 0.005155900120735168, LR: 0.001
Time, 2019-01-07T02:02:24, Epoch: 23, Batch: 350, Training Loss: 0.007990826666355134, LR: 0.001
Time, 2019-01-07T02:02:24, Epoch: 23, Batch: 360, Training Loss: 0.003839368373155594, LR: 0.001
Time, 2019-01-07T02:02:25, Epoch: 23, Batch: 370, Training Loss: 0.008073525130748748, LR: 0.001
Time, 2019-01-07T02:02:26, Epoch: 23, Batch: 380, Training Loss: 0.012925562262535096, LR: 0.001
Time, 2019-01-07T02:02:27, Epoch: 23, Batch: 390, Training Loss: 0.010948492586612702, LR: 0.001
Time, 2019-01-07T02:02:27, Epoch: 23, Batch: 400, Training Loss: 0.023848237097263338, LR: 0.001
Time, 2019-01-07T02:02:28, Epoch: 23, Batch: 410, Training Loss: 0.016712192445993423, LR: 0.001
Time, 2019-01-07T02:02:29, Epoch: 23, Batch: 420, Training Loss: 0.016579437255859374, LR: 0.001
Time, 2019-01-07T02:02:29, Epoch: 23, Batch: 430, Training Loss: 0.03351290002465248, LR: 0.001
Time, 2019-01-07T02:02:30, Epoch: 23, Batch: 440, Training Loss: 0.009626803547143936, LR: 0.001
Time, 2019-01-07T02:02:31, Epoch: 23, Batch: 450, Training Loss: 0.03854227289557457, LR: 0.001
Time, 2019-01-07T02:02:32, Epoch: 23, Batch: 460, Training Loss: 0.011653287708759308, LR: 0.001
Time, 2019-01-07T02:02:32, Epoch: 23, Batch: 470, Training Loss: 0.007651472836732865, LR: 0.001
Time, 2019-01-07T02:02:33, Epoch: 23, Batch: 480, Training Loss: 0.01791331022977829, LR: 0.001
Time, 2019-01-07T02:02:34, Epoch: 23, Batch: 490, Training Loss: 0.0113426074385643, LR: 0.001
Time, 2019-01-07T02:02:35, Epoch: 23, Batch: 500, Training Loss: 0.01442376971244812, LR: 0.001
Time, 2019-01-07T02:02:35, Epoch: 23, Batch: 510, Training Loss: 0.0171371266245842, LR: 0.001
Time, 2019-01-07T02:02:36, Epoch: 23, Batch: 520, Training Loss: 0.020795100927352907, LR: 0.001
Time, 2019-01-07T02:02:37, Epoch: 23, Batch: 530, Training Loss: 0.018121126294136047, LR: 0.001
Time, 2019-01-07T02:02:37, Epoch: 23, Batch: 540, Training Loss: 0.02529068514704704, LR: 0.001
Time, 2019-01-07T02:02:38, Epoch: 23, Batch: 550, Training Loss: 0.016560996323823927, LR: 0.001
Time, 2019-01-07T02:02:39, Epoch: 23, Batch: 560, Training Loss: 0.028974102437496187, LR: 0.001
Time, 2019-01-07T02:02:39, Epoch: 23, Batch: 570, Training Loss: 0.01752641350030899, LR: 0.001
Time, 2019-01-07T02:02:40, Epoch: 23, Batch: 580, Training Loss: 0.008569689095020294, LR: 0.001
Time, 2019-01-07T02:02:41, Epoch: 23, Batch: 590, Training Loss: 0.004992828518152237, LR: 0.001
Time, 2019-01-07T02:02:42, Epoch: 23, Batch: 600, Training Loss: 0.01270100474357605, LR: 0.001
Time, 2019-01-07T02:02:42, Epoch: 23, Batch: 610, Training Loss: 0.015152491629123688, LR: 0.001
Time, 2019-01-07T02:02:43, Epoch: 23, Batch: 620, Training Loss: 0.022938576340675355, LR: 0.001
Time, 2019-01-07T02:02:44, Epoch: 23, Batch: 630, Training Loss: 0.0059196330606937405, LR: 0.001
Time, 2019-01-07T02:02:45, Epoch: 23, Batch: 640, Training Loss: 0.03164297193288803, LR: 0.001
Time, 2019-01-07T02:02:45, Epoch: 23, Batch: 650, Training Loss: 0.030999285727739335, LR: 0.001
Time, 2019-01-07T02:02:46, Epoch: 23, Batch: 660, Training Loss: 0.024213116616010666, LR: 0.001
Time, 2019-01-07T02:02:47, Epoch: 23, Batch: 670, Training Loss: 0.01778150796890259, LR: 0.001
Time, 2019-01-07T02:02:48, Epoch: 23, Batch: 680, Training Loss: 0.01207868680357933, LR: 0.001
Time, 2019-01-07T02:02:49, Epoch: 23, Batch: 690, Training Loss: 0.022967208921909333, LR: 0.001
Time, 2019-01-07T02:02:49, Epoch: 23, Batch: 700, Training Loss: 0.012932825833559036, LR: 0.001
Time, 2019-01-07T02:02:50, Epoch: 23, Batch: 710, Training Loss: 0.02319919541478157, LR: 0.001
Time, 2019-01-07T02:02:51, Epoch: 23, Batch: 720, Training Loss: 0.0043768733739852905, LR: 0.001
Time, 2019-01-07T02:02:52, Epoch: 23, Batch: 730, Training Loss: 0.01232439950108528, LR: 0.001
Time, 2019-01-07T02:02:53, Epoch: 23, Batch: 740, Training Loss: 0.00905730202794075, LR: 0.001
Time, 2019-01-07T02:02:53, Epoch: 23, Batch: 750, Training Loss: 0.010063312202692031, LR: 0.001
Time, 2019-01-07T02:02:54, Epoch: 23, Batch: 760, Training Loss: 0.004232394695281983, LR: 0.001
Time, 2019-01-07T02:02:55, Epoch: 23, Batch: 770, Training Loss: 0.02415561452507973, LR: 0.001
Time, 2019-01-07T02:02:56, Epoch: 23, Batch: 780, Training Loss: 0.010042767971754074, LR: 0.001
Time, 2019-01-07T02:02:57, Epoch: 23, Batch: 790, Training Loss: 0.01931825280189514, LR: 0.001
Time, 2019-01-07T02:02:57, Epoch: 23, Batch: 800, Training Loss: 0.026457472890615463, LR: 0.001
Time, 2019-01-07T02:02:58, Epoch: 23, Batch: 810, Training Loss: 0.014682266861200333, LR: 0.001
Time, 2019-01-07T02:02:59, Epoch: 23, Batch: 820, Training Loss: 0.016498104482889176, LR: 0.001
Time, 2019-01-07T02:03:00, Epoch: 23, Batch: 830, Training Loss: 0.01869419068098068, LR: 0.001
Time, 2019-01-07T02:03:00, Epoch: 23, Batch: 840, Training Loss: 0.031269052624702455, LR: 0.001
Time, 2019-01-07T02:03:01, Epoch: 23, Batch: 850, Training Loss: 0.003392259776592255, LR: 0.001
Time, 2019-01-07T02:03:02, Epoch: 23, Batch: 860, Training Loss: 0.014618240296840668, LR: 0.001
Time, 2019-01-07T02:03:03, Epoch: 23, Batch: 870, Training Loss: 0.00822598934173584, LR: 0.001
Time, 2019-01-07T02:03:03, Epoch: 23, Batch: 880, Training Loss: 0.021994375437498093, LR: 0.001
Time, 2019-01-07T02:03:04, Epoch: 23, Batch: 890, Training Loss: 0.007362750172615051, LR: 0.001
Time, 2019-01-07T02:03:05, Epoch: 23, Batch: 900, Training Loss: 0.0031142115592956545, LR: 0.001
Time, 2019-01-07T02:03:06, Epoch: 23, Batch: 910, Training Loss: 0.01857614889740944, LR: 0.001
Time, 2019-01-07T02:03:06, Epoch: 23, Batch: 920, Training Loss: 0.006475019454956055, LR: 0.001
Time, 2019-01-07T02:03:07, Epoch: 23, Batch: 930, Training Loss: 0.02353570982813835, LR: 0.001
Epoch: 23, Validation Top 1 acc: 98.59673309326172
Epoch: 23, Validation Top 5 acc: 99.98009490966797
Epoch: 23, Validation Set Loss: 0.04934537410736084
Start training epoch 24
Time, 2019-01-07T02:03:11, Epoch: 24, Batch: 10, Training Loss: 0.020345477759838103, LR: 0.001
Time, 2019-01-07T02:03:11, Epoch: 24, Batch: 20, Training Loss: 0.007815808057785034, LR: 0.001
Time, 2019-01-07T02:03:12, Epoch: 24, Batch: 30, Training Loss: 0.0032817572355270386, LR: 0.001
Time, 2019-01-07T02:03:13, Epoch: 24, Batch: 40, Training Loss: 0.0031697504222393037, LR: 0.001
Time, 2019-01-07T02:03:14, Epoch: 24, Batch: 50, Training Loss: 0.013474341481924057, LR: 0.001
Time, 2019-01-07T02:03:14, Epoch: 24, Batch: 60, Training Loss: 0.006797648221254349, LR: 0.001
Time, 2019-01-07T02:03:15, Epoch: 24, Batch: 70, Training Loss: 0.033361437171697615, LR: 0.001
Time, 2019-01-07T02:03:16, Epoch: 24, Batch: 80, Training Loss: 0.013255929201841354, LR: 0.001
Time, 2019-01-07T02:03:17, Epoch: 24, Batch: 90, Training Loss: 0.014913062751293182, LR: 0.001
Time, 2019-01-07T02:03:17, Epoch: 24, Batch: 100, Training Loss: 0.006215134263038635, LR: 0.001
Time, 2019-01-07T02:03:18, Epoch: 24, Batch: 110, Training Loss: 0.0027987971901893617, LR: 0.001
Time, 2019-01-07T02:03:19, Epoch: 24, Batch: 120, Training Loss: 0.019169164448976518, LR: 0.001
Time, 2019-01-07T02:03:20, Epoch: 24, Batch: 130, Training Loss: 0.00305139422416687, LR: 0.001
Time, 2019-01-07T02:03:20, Epoch: 24, Batch: 140, Training Loss: 0.007722987234592438, LR: 0.001
Time, 2019-01-07T02:03:21, Epoch: 24, Batch: 150, Training Loss: 0.0034328818321228026, LR: 0.001
Time, 2019-01-07T02:03:22, Epoch: 24, Batch: 160, Training Loss: 0.0025119394063949584, LR: 0.001
Time, 2019-01-07T02:03:22, Epoch: 24, Batch: 170, Training Loss: 0.0070877112448215485, LR: 0.001
Time, 2019-01-07T02:03:23, Epoch: 24, Batch: 180, Training Loss: 0.008680561184883117, LR: 0.001
Time, 2019-01-07T02:03:24, Epoch: 24, Batch: 190, Training Loss: 0.004373322427272797, LR: 0.001
Time, 2019-01-07T02:03:24, Epoch: 24, Batch: 200, Training Loss: 0.0018614515662193297, LR: 0.001
Time, 2019-01-07T02:03:25, Epoch: 24, Batch: 210, Training Loss: 0.005783195793628693, LR: 0.001
Time, 2019-01-07T02:03:26, Epoch: 24, Batch: 220, Training Loss: 0.010180595517158508, LR: 0.001
Time, 2019-01-07T02:03:27, Epoch: 24, Batch: 230, Training Loss: 0.00707969069480896, LR: 0.001
Time, 2019-01-07T02:03:27, Epoch: 24, Batch: 240, Training Loss: 0.010843800008296966, LR: 0.001
Time, 2019-01-07T02:03:28, Epoch: 24, Batch: 250, Training Loss: 0.004352082312107086, LR: 0.001
Time, 2019-01-07T02:03:29, Epoch: 24, Batch: 260, Training Loss: 0.006102093309164047, LR: 0.001
Time, 2019-01-07T02:03:29, Epoch: 24, Batch: 270, Training Loss: 0.004610732942819595, LR: 0.001
Time, 2019-01-07T02:03:30, Epoch: 24, Batch: 280, Training Loss: 0.001564931869506836, LR: 0.001
Time, 2019-01-07T02:03:31, Epoch: 24, Batch: 290, Training Loss: 0.0036397218704223634, LR: 0.001
Time, 2019-01-07T02:03:31, Epoch: 24, Batch: 300, Training Loss: 0.011225763708353043, LR: 0.001
Time, 2019-01-07T02:03:32, Epoch: 24, Batch: 310, Training Loss: 0.007535842806100845, LR: 0.001
Time, 2019-01-07T02:03:33, Epoch: 24, Batch: 320, Training Loss: 0.008985520899295807, LR: 0.001
Time, 2019-01-07T02:03:33, Epoch: 24, Batch: 330, Training Loss: 0.008705282956361771, LR: 0.001
Time, 2019-01-07T02:03:34, Epoch: 24, Batch: 340, Training Loss: 0.007637923955917359, LR: 0.001
Time, 2019-01-07T02:03:35, Epoch: 24, Batch: 350, Training Loss: 0.015624159574508667, LR: 0.001
Time, 2019-01-07T02:03:35, Epoch: 24, Batch: 360, Training Loss: 0.011827255040407181, LR: 0.001
Time, 2019-01-07T02:03:36, Epoch: 24, Batch: 370, Training Loss: 0.00297863632440567, LR: 0.001
Time, 2019-01-07T02:03:37, Epoch: 24, Batch: 380, Training Loss: 0.015039238333702087, LR: 0.001
Time, 2019-01-07T02:03:37, Epoch: 24, Batch: 390, Training Loss: 0.005692155659198761, LR: 0.001
Time, 2019-01-07T02:03:38, Epoch: 24, Batch: 400, Training Loss: 0.007665398716926575, LR: 0.001
Time, 2019-01-07T02:03:39, Epoch: 24, Batch: 410, Training Loss: 0.009890463203191757, LR: 0.001
Time, 2019-01-07T02:03:39, Epoch: 24, Batch: 420, Training Loss: 0.011473436653614045, LR: 0.001
Time, 2019-01-07T02:03:40, Epoch: 24, Batch: 430, Training Loss: 0.0037211790680885316, LR: 0.001
Time, 2019-01-07T02:03:41, Epoch: 24, Batch: 440, Training Loss: 0.012980087101459504, LR: 0.001
Time, 2019-01-07T02:03:41, Epoch: 24, Batch: 450, Training Loss: 0.0025369882583618163, LR: 0.001
Time, 2019-01-07T02:03:42, Epoch: 24, Batch: 460, Training Loss: 0.0026930525898933412, LR: 0.001
Time, 2019-01-07T02:03:42, Epoch: 24, Batch: 470, Training Loss: 0.007988419383764267, LR: 0.001
Time, 2019-01-07T02:03:43, Epoch: 24, Batch: 480, Training Loss: 0.024763210862874984, LR: 0.001
Time, 2019-01-07T02:03:44, Epoch: 24, Batch: 490, Training Loss: 0.010593485087156296, LR: 0.001
Time, 2019-01-07T02:03:44, Epoch: 24, Batch: 500, Training Loss: 0.026013197749853133, LR: 0.001
Time, 2019-01-07T02:03:45, Epoch: 24, Batch: 510, Training Loss: 0.011670350283384322, LR: 0.001
Time, 2019-01-07T02:03:46, Epoch: 24, Batch: 520, Training Loss: 0.0266736276447773, LR: 0.001
Time, 2019-01-07T02:03:46, Epoch: 24, Batch: 530, Training Loss: 0.013563718646764755, LR: 0.001
Time, 2019-01-07T02:03:47, Epoch: 24, Batch: 540, Training Loss: 0.0033266469836235046, LR: 0.001
Time, 2019-01-07T02:03:48, Epoch: 24, Batch: 550, Training Loss: 0.019751835614442825, LR: 0.001
Time, 2019-01-07T02:03:49, Epoch: 24, Batch: 560, Training Loss: 0.002575180679559708, LR: 0.001
Time, 2019-01-07T02:03:50, Epoch: 24, Batch: 570, Training Loss: 0.017118356376886367, LR: 0.001
Time, 2019-01-07T02:03:50, Epoch: 24, Batch: 580, Training Loss: 0.00866299495100975, LR: 0.001
Time, 2019-01-07T02:03:51, Epoch: 24, Batch: 590, Training Loss: 0.0009100317955017089, LR: 0.001
Time, 2019-01-07T02:03:52, Epoch: 24, Batch: 600, Training Loss: 0.0051971301436424255, LR: 0.001
Time, 2019-01-07T02:03:52, Epoch: 24, Batch: 610, Training Loss: 0.005490361899137497, LR: 0.001
Time, 2019-01-07T02:03:53, Epoch: 24, Batch: 620, Training Loss: 0.008915611356496812, LR: 0.001
Time, 2019-01-07T02:03:54, Epoch: 24, Batch: 630, Training Loss: 0.010091467201709748, LR: 0.001
Time, 2019-01-07T02:03:55, Epoch: 24, Batch: 640, Training Loss: 0.00808197185397148, LR: 0.001
Time, 2019-01-07T02:03:55, Epoch: 24, Batch: 650, Training Loss: 0.006278667598962784, LR: 0.001
Time, 2019-01-07T02:03:56, Epoch: 24, Batch: 660, Training Loss: 0.008605073392391204, LR: 0.001
Time, 2019-01-07T02:03:57, Epoch: 24, Batch: 670, Training Loss: 0.007274794578552246, LR: 0.001
Time, 2019-01-07T02:03:57, Epoch: 24, Batch: 680, Training Loss: 0.00522601380944252, LR: 0.001
Time, 2019-01-07T02:03:58, Epoch: 24, Batch: 690, Training Loss: 0.01204349622130394, LR: 0.001
Time, 2019-01-07T02:03:59, Epoch: 24, Batch: 700, Training Loss: 0.009760144352912902, LR: 0.001
Time, 2019-01-07T02:03:59, Epoch: 24, Batch: 710, Training Loss: 0.020420968532562256, LR: 0.001
Time, 2019-01-07T02:04:00, Epoch: 24, Batch: 720, Training Loss: 0.006933795660734177, LR: 0.001
Time, 2019-01-07T02:04:01, Epoch: 24, Batch: 730, Training Loss: 0.017412466555833818, LR: 0.001
Time, 2019-01-07T02:04:01, Epoch: 24, Batch: 740, Training Loss: 0.00920008048415184, LR: 0.001
Time, 2019-01-07T02:04:02, Epoch: 24, Batch: 750, Training Loss: 0.006779641658067703, LR: 0.001
Time, 2019-01-07T02:04:03, Epoch: 24, Batch: 760, Training Loss: 0.010147425532341003, LR: 0.001
Time, 2019-01-07T02:04:03, Epoch: 24, Batch: 770, Training Loss: 0.002688705176115036, LR: 0.001
Time, 2019-01-07T02:04:04, Epoch: 24, Batch: 780, Training Loss: 0.009749735891819, LR: 0.001
Time, 2019-01-07T02:04:05, Epoch: 24, Batch: 790, Training Loss: 0.0051628068089485165, LR: 0.001
Time, 2019-01-07T02:04:05, Epoch: 24, Batch: 800, Training Loss: 0.007540890574455261, LR: 0.001
Time, 2019-01-07T02:04:06, Epoch: 24, Batch: 810, Training Loss: 0.018614865094423293, LR: 0.001
Time, 2019-01-07T02:04:07, Epoch: 24, Batch: 820, Training Loss: 0.007719093561172485, LR: 0.001
Time, 2019-01-07T02:04:07, Epoch: 24, Batch: 830, Training Loss: 0.004359277337789536, LR: 0.001
Time, 2019-01-07T02:04:08, Epoch: 24, Batch: 840, Training Loss: 0.0069546237587928775, LR: 0.001
Time, 2019-01-07T02:04:09, Epoch: 24, Batch: 850, Training Loss: 0.00747525691986084, LR: 0.001
Time, 2019-01-07T02:04:09, Epoch: 24, Batch: 860, Training Loss: 0.016822344809770583, LR: 0.001
Time, 2019-01-07T02:04:10, Epoch: 24, Batch: 870, Training Loss: 0.019658338278532028, LR: 0.001
Time, 2019-01-07T02:04:11, Epoch: 24, Batch: 880, Training Loss: 0.010070538520812989, LR: 0.001
Time, 2019-01-07T02:04:11, Epoch: 24, Batch: 890, Training Loss: 0.02040063664317131, LR: 0.001
Time, 2019-01-07T02:04:12, Epoch: 24, Batch: 900, Training Loss: 0.015875330567359923, LR: 0.001
Time, 2019-01-07T02:04:13, Epoch: 24, Batch: 910, Training Loss: 0.011531996726989745, LR: 0.001
Time, 2019-01-07T02:04:13, Epoch: 24, Batch: 920, Training Loss: 0.006337673962116241, LR: 0.001
Time, 2019-01-07T02:04:14, Epoch: 24, Batch: 930, Training Loss: 0.003314663469791412, LR: 0.001
Epoch: 24, Validation Top 1 acc: 98.61663818359375
Epoch: 24, Validation Top 5 acc: 100.0
Epoch: 24, Validation Set Loss: 0.04965677857398987
Start training epoch 25
Time, 2019-01-07T02:04:17, Epoch: 25, Batch: 10, Training Loss: 0.002326194941997528, LR: 0.001
Time, 2019-01-07T02:04:18, Epoch: 25, Batch: 20, Training Loss: 0.013834447413682938, LR: 0.001
Time, 2019-01-07T02:04:19, Epoch: 25, Batch: 30, Training Loss: 0.004786062240600586, LR: 0.001
Time, 2019-01-07T02:04:19, Epoch: 25, Batch: 40, Training Loss: 0.014604375511407853, LR: 0.001
Time, 2019-01-07T02:04:20, Epoch: 25, Batch: 50, Training Loss: 0.007757310569286346, LR: 0.001
Time, 2019-01-07T02:04:21, Epoch: 25, Batch: 60, Training Loss: 0.005576598644256592, LR: 0.001
Time, 2019-01-07T02:04:21, Epoch: 25, Batch: 70, Training Loss: 0.0032875314354896545, LR: 0.001
Time, 2019-01-07T02:04:22, Epoch: 25, Batch: 80, Training Loss: 0.013310322165489196, LR: 0.001
Time, 2019-01-07T02:04:23, Epoch: 25, Batch: 90, Training Loss: 0.009068546444177627, LR: 0.001
Time, 2019-01-07T02:04:23, Epoch: 25, Batch: 100, Training Loss: 0.0030775345861911775, LR: 0.001
Time, 2019-01-07T02:04:24, Epoch: 25, Batch: 110, Training Loss: 0.006799680739641189, LR: 0.001
Time, 2019-01-07T02:04:25, Epoch: 25, Batch: 120, Training Loss: 0.00806659534573555, LR: 0.001
Time, 2019-01-07T02:04:25, Epoch: 25, Batch: 130, Training Loss: 0.014949635416269303, LR: 0.001
Time, 2019-01-07T02:04:26, Epoch: 25, Batch: 140, Training Loss: 0.0017506256699562073, LR: 0.001
Time, 2019-01-07T02:04:27, Epoch: 25, Batch: 150, Training Loss: 0.004069376736879349, LR: 0.001
Time, 2019-01-07T02:04:27, Epoch: 25, Batch: 160, Training Loss: 0.004704147577285767, LR: 0.001
Time, 2019-01-07T02:04:28, Epoch: 25, Batch: 170, Training Loss: 0.00165298730134964, LR: 0.001
Time, 2019-01-07T02:04:29, Epoch: 25, Batch: 180, Training Loss: 0.0013719648122787476, LR: 0.001
Time, 2019-01-07T02:04:29, Epoch: 25, Batch: 190, Training Loss: 0.003473765403032303, LR: 0.001
Time, 2019-01-07T02:04:30, Epoch: 25, Batch: 200, Training Loss: 0.00820937603712082, LR: 0.001
Time, 2019-01-07T02:04:31, Epoch: 25, Batch: 210, Training Loss: 0.004357555508613586, LR: 0.001
Time, 2019-01-07T02:04:31, Epoch: 25, Batch: 220, Training Loss: 0.003287525475025177, LR: 0.001
Time, 2019-01-07T02:04:32, Epoch: 25, Batch: 230, Training Loss: 0.00861196219921112, LR: 0.001
Time, 2019-01-07T02:04:33, Epoch: 25, Batch: 240, Training Loss: 0.0029682934284210203, LR: 0.001
Time, 2019-01-07T02:04:33, Epoch: 25, Batch: 250, Training Loss: 0.005016745626926422, LR: 0.001
Time, 2019-01-07T02:04:34, Epoch: 25, Batch: 260, Training Loss: 0.00614730566740036, LR: 0.001
Time, 2019-01-07T02:04:35, Epoch: 25, Batch: 270, Training Loss: 0.009587668627500535, LR: 0.001
Time, 2019-01-07T02:04:35, Epoch: 25, Batch: 280, Training Loss: 0.00420578420162201, LR: 0.001
Time, 2019-01-07T02:04:36, Epoch: 25, Batch: 290, Training Loss: 0.0011601626873016357, LR: 0.001
Time, 2019-01-07T02:04:37, Epoch: 25, Batch: 300, Training Loss: 0.007358887046575546, LR: 0.001
Time, 2019-01-07T02:04:38, Epoch: 25, Batch: 310, Training Loss: 0.004584555327892303, LR: 0.001
Time, 2019-01-07T02:04:38, Epoch: 25, Batch: 320, Training Loss: 0.0018490895628929139, LR: 0.001
Time, 2019-01-07T02:04:39, Epoch: 25, Batch: 330, Training Loss: 0.0025663800537586213, LR: 0.001
Time, 2019-01-07T02:04:39, Epoch: 25, Batch: 340, Training Loss: 0.0040484145283699036, LR: 0.001
Time, 2019-01-07T02:04:40, Epoch: 25, Batch: 350, Training Loss: 0.002434523403644562, LR: 0.001
Time, 2019-01-07T02:04:41, Epoch: 25, Batch: 360, Training Loss: 0.007098256051540375, LR: 0.001
Time, 2019-01-07T02:04:41, Epoch: 25, Batch: 370, Training Loss: 0.006232982873916626, LR: 0.001
Time, 2019-01-07T02:04:42, Epoch: 25, Batch: 380, Training Loss: 0.010985152423381805, LR: 0.001
Time, 2019-01-07T02:04:43, Epoch: 25, Batch: 390, Training Loss: 0.016529781371355058, LR: 0.001
Time, 2019-01-07T02:04:44, Epoch: 25, Batch: 400, Training Loss: 0.0033309906721115112, LR: 0.001
Time, 2019-01-07T02:04:44, Epoch: 25, Batch: 410, Training Loss: 0.006476535648107529, LR: 0.001
Time, 2019-01-07T02:04:45, Epoch: 25, Batch: 420, Training Loss: 0.004693625867366791, LR: 0.001
Time, 2019-01-07T02:04:46, Epoch: 25, Batch: 430, Training Loss: 0.014392086118459702, LR: 0.001
Time, 2019-01-07T02:04:46, Epoch: 25, Batch: 440, Training Loss: 0.011124670505523682, LR: 0.001
Time, 2019-01-07T02:04:47, Epoch: 25, Batch: 450, Training Loss: 0.004612471908330917, LR: 0.001
Time, 2019-01-07T02:04:48, Epoch: 25, Batch: 460, Training Loss: 0.0025979220867156983, LR: 0.001
Time, 2019-01-07T02:04:48, Epoch: 25, Batch: 470, Training Loss: 0.013802466541528701, LR: 0.001
Time, 2019-01-07T02:04:49, Epoch: 25, Batch: 480, Training Loss: 0.0028353095054626464, LR: 0.001
Time, 2019-01-07T02:04:50, Epoch: 25, Batch: 490, Training Loss: 0.007606719434261322, LR: 0.001
Time, 2019-01-07T02:04:50, Epoch: 25, Batch: 500, Training Loss: 0.006142053008079529, LR: 0.001
Time, 2019-01-07T02:04:51, Epoch: 25, Batch: 510, Training Loss: 0.002829144895076752, LR: 0.001
Time, 2019-01-07T02:04:52, Epoch: 25, Batch: 520, Training Loss: 0.004087232053279877, LR: 0.001
Time, 2019-01-07T02:04:52, Epoch: 25, Batch: 530, Training Loss: 0.0035609304904937744, LR: 0.001
Time, 2019-01-07T02:04:53, Epoch: 25, Batch: 540, Training Loss: 0.0048604153096675875, LR: 0.001
Time, 2019-01-07T02:04:54, Epoch: 25, Batch: 550, Training Loss: 0.01256198137998581, LR: 0.001
Time, 2019-01-07T02:04:54, Epoch: 25, Batch: 560, Training Loss: 0.019164907932281493, LR: 0.001
Time, 2019-01-07T02:04:55, Epoch: 25, Batch: 570, Training Loss: 0.011443983018398284, LR: 0.001
Time, 2019-01-07T02:04:56, Epoch: 25, Batch: 580, Training Loss: 0.007362829148769378, LR: 0.001
Time, 2019-01-07T02:04:56, Epoch: 25, Batch: 590, Training Loss: 0.008617504686117172, LR: 0.001
Time, 2019-01-07T02:04:57, Epoch: 25, Batch: 600, Training Loss: 0.0157629057765007, LR: 0.001
Time, 2019-01-07T02:04:57, Epoch: 25, Batch: 610, Training Loss: 0.007569241523742676, LR: 0.001
Time, 2019-01-07T02:04:58, Epoch: 25, Batch: 620, Training Loss: 0.012040340155363084, LR: 0.001
Time, 2019-01-07T02:04:59, Epoch: 25, Batch: 630, Training Loss: 0.012919163703918457, LR: 0.001
Time, 2019-01-07T02:05:00, Epoch: 25, Batch: 640, Training Loss: 0.017154254019260406, LR: 0.001
Time, 2019-01-07T02:05:00, Epoch: 25, Batch: 650, Training Loss: 0.006871835887432098, LR: 0.001
Time, 2019-01-07T02:05:01, Epoch: 25, Batch: 660, Training Loss: 0.0035718947649002073, LR: 0.001
Time, 2019-01-07T02:05:02, Epoch: 25, Batch: 670, Training Loss: 0.004074151813983917, LR: 0.001
Time, 2019-01-07T02:05:02, Epoch: 25, Batch: 680, Training Loss: 0.007441531866788864, LR: 0.001
Time, 2019-01-07T02:05:03, Epoch: 25, Batch: 690, Training Loss: 0.024133534729480745, LR: 0.001
Time, 2019-01-07T02:05:03, Epoch: 25, Batch: 700, Training Loss: 0.009886661171913147, LR: 0.001
Time, 2019-01-07T02:05:04, Epoch: 25, Batch: 710, Training Loss: 0.0053632184863090515, LR: 0.001
Time, 2019-01-07T02:05:05, Epoch: 25, Batch: 720, Training Loss: 0.0075444296002388, LR: 0.001
Time, 2019-01-07T02:05:05, Epoch: 25, Batch: 730, Training Loss: 0.02441335320472717, LR: 0.001
Time, 2019-01-07T02:05:06, Epoch: 25, Batch: 740, Training Loss: 0.0030200868844985963, LR: 0.001
Time, 2019-01-07T02:05:07, Epoch: 25, Batch: 750, Training Loss: 0.010930526256561279, LR: 0.001
Time, 2019-01-07T02:05:07, Epoch: 25, Batch: 760, Training Loss: 0.0025097861886024474, LR: 0.001
Time, 2019-01-07T02:05:08, Epoch: 25, Batch: 770, Training Loss: 0.002860890328884125, LR: 0.001
Time, 2019-01-07T02:05:09, Epoch: 25, Batch: 780, Training Loss: 0.006416314840316772, LR: 0.001
Time, 2019-01-07T02:05:10, Epoch: 25, Batch: 790, Training Loss: 0.006934648007154464, LR: 0.001
Time, 2019-01-07T02:05:10, Epoch: 25, Batch: 800, Training Loss: 0.005830164253711701, LR: 0.001
Time, 2019-01-07T02:05:11, Epoch: 25, Batch: 810, Training Loss: 0.008808275312185287, LR: 0.001
Time, 2019-01-07T02:05:12, Epoch: 25, Batch: 820, Training Loss: 0.010525870323181152, LR: 0.001
Time, 2019-01-07T02:05:12, Epoch: 25, Batch: 830, Training Loss: 0.00609678104519844, LR: 0.001
Time, 2019-01-07T02:05:13, Epoch: 25, Batch: 840, Training Loss: 0.009069015085697175, LR: 0.001
Time, 2019-01-07T02:05:14, Epoch: 25, Batch: 850, Training Loss: 0.0092718668282032, LR: 0.001
Time, 2019-01-07T02:05:14, Epoch: 25, Batch: 860, Training Loss: 0.008818867802619933, LR: 0.001
Time, 2019-01-07T02:05:15, Epoch: 25, Batch: 870, Training Loss: 0.006549497693777084, LR: 0.001
Time, 2019-01-07T02:05:16, Epoch: 25, Batch: 880, Training Loss: 0.0034023568034172056, LR: 0.001
Time, 2019-01-07T02:05:16, Epoch: 25, Batch: 890, Training Loss: 0.003750746697187424, LR: 0.001
Time, 2019-01-07T02:05:17, Epoch: 25, Batch: 900, Training Loss: 0.005731946229934693, LR: 0.001
Time, 2019-01-07T02:05:17, Epoch: 25, Batch: 910, Training Loss: 0.015409540385007858, LR: 0.001
Time, 2019-01-07T02:05:18, Epoch: 25, Batch: 920, Training Loss: 0.016310714185237885, LR: 0.001
Time, 2019-01-07T02:05:19, Epoch: 25, Batch: 930, Training Loss: 0.016813939064741136, LR: 0.001
Epoch: 25, Validation Top 1 acc: 98.57682800292969
Epoch: 25, Validation Top 5 acc: 100.0
Epoch: 25, Validation Set Loss: 0.05684009939432144
Start training epoch 26
Time, 2019-01-07T02:05:22, Epoch: 26, Batch: 10, Training Loss: 0.0024820216000080108, LR: 0.001
Time, 2019-01-07T02:05:23, Epoch: 26, Batch: 20, Training Loss: 0.00812242180109024, LR: 0.001
Time, 2019-01-07T02:05:24, Epoch: 26, Batch: 30, Training Loss: 0.021397946774959563, LR: 0.001
Time, 2019-01-07T02:05:24, Epoch: 26, Batch: 40, Training Loss: 0.001454925537109375, LR: 0.001
Time, 2019-01-07T02:05:25, Epoch: 26, Batch: 50, Training Loss: 0.005154909193515777, LR: 0.001
Time, 2019-01-07T02:05:26, Epoch: 26, Batch: 60, Training Loss: 0.004347756505012512, LR: 0.001
Time, 2019-01-07T02:05:27, Epoch: 26, Batch: 70, Training Loss: 0.004193469882011414, LR: 0.001
Time, 2019-01-07T02:05:27, Epoch: 26, Batch: 80, Training Loss: 0.005155619233846664, LR: 0.001
Time, 2019-01-07T02:05:28, Epoch: 26, Batch: 90, Training Loss: 0.003991283476352692, LR: 0.001
Time, 2019-01-07T02:05:29, Epoch: 26, Batch: 100, Training Loss: 0.0034722350537776947, LR: 0.001
Time, 2019-01-07T02:05:29, Epoch: 26, Batch: 110, Training Loss: 0.0012510664761066436, LR: 0.001
Time, 2019-01-07T02:05:30, Epoch: 26, Batch: 120, Training Loss: 0.004309648275375366, LR: 0.001
Time, 2019-01-07T02:05:31, Epoch: 26, Batch: 130, Training Loss: 0.0034121543169021605, LR: 0.001
Time, 2019-01-07T02:05:31, Epoch: 26, Batch: 140, Training Loss: 0.0037298865616321565, LR: 0.001
Time, 2019-01-07T02:05:32, Epoch: 26, Batch: 150, Training Loss: 0.00891140103340149, LR: 0.001
Time, 2019-01-07T02:05:33, Epoch: 26, Batch: 160, Training Loss: 0.0015796780586242675, LR: 0.001
Time, 2019-01-07T02:05:33, Epoch: 26, Batch: 170, Training Loss: 0.014703261107206345, LR: 0.001
Time, 2019-01-07T02:05:34, Epoch: 26, Batch: 180, Training Loss: 0.012658998370170593, LR: 0.001
Time, 2019-01-07T02:05:35, Epoch: 26, Batch: 190, Training Loss: 0.009217460453510285, LR: 0.001
Time, 2019-01-07T02:05:35, Epoch: 26, Batch: 200, Training Loss: 0.002668145298957825, LR: 0.001
Time, 2019-01-07T02:05:36, Epoch: 26, Batch: 210, Training Loss: 0.0006863325834274292, LR: 0.001
Time, 2019-01-07T02:05:37, Epoch: 26, Batch: 220, Training Loss: 0.003113625943660736, LR: 0.001
Time, 2019-01-07T02:05:38, Epoch: 26, Batch: 230, Training Loss: 0.004632596671581268, LR: 0.001
Time, 2019-01-07T02:05:40, Epoch: 26, Batch: 240, Training Loss: 0.004239803552627564, LR: 0.001
Time, 2019-01-07T02:05:40, Epoch: 26, Batch: 250, Training Loss: 0.0050707146525383, LR: 0.001
Time, 2019-01-07T02:05:41, Epoch: 26, Batch: 260, Training Loss: 0.006686520576477051, LR: 0.001
Time, 2019-01-07T02:05:42, Epoch: 26, Batch: 270, Training Loss: 0.017052764445543288, LR: 0.001
Time, 2019-01-07T02:05:43, Epoch: 26, Batch: 280, Training Loss: 0.003884250670671463, LR: 0.001
Time, 2019-01-07T02:05:44, Epoch: 26, Batch: 290, Training Loss: 0.007541494071483612, LR: 0.001
Time, 2019-01-07T02:05:44, Epoch: 26, Batch: 300, Training Loss: 0.007936808466911315, LR: 0.001
Time, 2019-01-07T02:05:45, Epoch: 26, Batch: 310, Training Loss: 0.008486634492874146, LR: 0.001
Time, 2019-01-07T02:05:46, Epoch: 26, Batch: 320, Training Loss: 0.0014973819255828858, LR: 0.001
Time, 2019-01-07T02:05:46, Epoch: 26, Batch: 330, Training Loss: 0.003762282431125641, LR: 0.001
Time, 2019-01-07T02:05:47, Epoch: 26, Batch: 340, Training Loss: 0.004271061718463897, LR: 0.001
Time, 2019-01-07T02:05:48, Epoch: 26, Batch: 350, Training Loss: 0.0060261920094490055, LR: 0.001
Time, 2019-01-07T02:05:48, Epoch: 26, Batch: 360, Training Loss: 0.01262759044766426, LR: 0.001
Time, 2019-01-07T02:05:49, Epoch: 26, Batch: 370, Training Loss: 0.0025445982813835145, LR: 0.001
Time, 2019-01-07T02:05:50, Epoch: 26, Batch: 380, Training Loss: 0.010268127173185348, LR: 0.001
Time, 2019-01-07T02:05:50, Epoch: 26, Batch: 390, Training Loss: 0.015252460539340974, LR: 0.001
Time, 2019-01-07T02:05:51, Epoch: 26, Batch: 400, Training Loss: 0.005501911789178848, LR: 0.001
Time, 2019-01-07T02:05:52, Epoch: 26, Batch: 410, Training Loss: 0.0045113623142242435, LR: 0.001
Time, 2019-01-07T02:05:52, Epoch: 26, Batch: 420, Training Loss: 0.017910080403089522, LR: 0.001
Time, 2019-01-07T02:05:53, Epoch: 26, Batch: 430, Training Loss: 0.007162699103355407, LR: 0.001
Time, 2019-01-07T02:05:54, Epoch: 26, Batch: 440, Training Loss: 0.009707991033792496, LR: 0.001
Time, 2019-01-07T02:05:54, Epoch: 26, Batch: 450, Training Loss: 0.01938069611787796, LR: 0.001
Time, 2019-01-07T02:05:55, Epoch: 26, Batch: 460, Training Loss: 0.005057079344987869, LR: 0.001
Time, 2019-01-07T02:05:56, Epoch: 26, Batch: 470, Training Loss: 0.013000643253326416, LR: 0.001
Time, 2019-01-07T02:05:56, Epoch: 26, Batch: 480, Training Loss: 0.000921332836151123, LR: 0.001
Time, 2019-01-07T02:05:57, Epoch: 26, Batch: 490, Training Loss: 0.006523212045431137, LR: 0.001
Time, 2019-01-07T02:05:58, Epoch: 26, Batch: 500, Training Loss: 0.005961336195468903, LR: 0.001
Time, 2019-01-07T02:05:58, Epoch: 26, Batch: 510, Training Loss: 0.011971095204353332, LR: 0.001
Time, 2019-01-07T02:05:59, Epoch: 26, Batch: 520, Training Loss: 0.016767064481973647, LR: 0.001
Time, 2019-01-07T02:05:59, Epoch: 26, Batch: 530, Training Loss: 0.011612015217542649, LR: 0.001
Time, 2019-01-07T02:06:00, Epoch: 26, Batch: 540, Training Loss: 0.012800753116607666, LR: 0.001
Time, 2019-01-07T02:06:01, Epoch: 26, Batch: 550, Training Loss: 0.004360964894294739, LR: 0.001
Time, 2019-01-07T02:06:01, Epoch: 26, Batch: 560, Training Loss: 0.004635673016309738, LR: 0.001
Time, 2019-01-07T02:06:02, Epoch: 26, Batch: 570, Training Loss: 0.008709856122732163, LR: 0.001
Time, 2019-01-07T02:06:03, Epoch: 26, Batch: 580, Training Loss: 0.007240621745586396, LR: 0.001
Time, 2019-01-07T02:06:03, Epoch: 26, Batch: 590, Training Loss: 0.004628375172615051, LR: 0.001
Time, 2019-01-07T02:06:04, Epoch: 26, Batch: 600, Training Loss: 0.005991960316896439, LR: 0.001
Time, 2019-01-07T02:06:05, Epoch: 26, Batch: 610, Training Loss: 0.0042729452252388, LR: 0.001
Time, 2019-01-07T02:06:05, Epoch: 26, Batch: 620, Training Loss: 0.013826904445886612, LR: 0.001
Time, 2019-01-07T02:06:06, Epoch: 26, Batch: 630, Training Loss: 0.014298979938030244, LR: 0.001
Time, 2019-01-07T02:06:07, Epoch: 26, Batch: 640, Training Loss: 0.004074196517467499, LR: 0.001
Time, 2019-01-07T02:06:07, Epoch: 26, Batch: 650, Training Loss: 0.022478818148374557, LR: 0.001
Time, 2019-01-07T02:06:08, Epoch: 26, Batch: 660, Training Loss: 0.009942860901355743, LR: 0.001
Time, 2019-01-07T02:06:09, Epoch: 26, Batch: 670, Training Loss: 0.00797012895345688, LR: 0.001
Time, 2019-01-07T02:06:09, Epoch: 26, Batch: 680, Training Loss: 0.023751626163721083, LR: 0.001
Time, 2019-01-07T02:06:10, Epoch: 26, Batch: 690, Training Loss: 0.008584532886743546, LR: 0.001
Time, 2019-01-07T02:06:11, Epoch: 26, Batch: 700, Training Loss: 0.01722876578569412, LR: 0.001
Time, 2019-01-07T02:06:11, Epoch: 26, Batch: 710, Training Loss: 0.008289074152708053, LR: 0.001
Time, 2019-01-07T02:06:12, Epoch: 26, Batch: 720, Training Loss: 0.006360741704702378, LR: 0.001
Time, 2019-01-07T02:06:12, Epoch: 26, Batch: 730, Training Loss: 0.003088255226612091, LR: 0.001
Time, 2019-01-07T02:06:13, Epoch: 26, Batch: 740, Training Loss: 0.004914144426584244, LR: 0.001
Time, 2019-01-07T02:06:14, Epoch: 26, Batch: 750, Training Loss: 0.002557946741580963, LR: 0.001
Time, 2019-01-07T02:06:14, Epoch: 26, Batch: 760, Training Loss: 0.0019067361950874328, LR: 0.001
Time, 2019-01-07T02:06:15, Epoch: 26, Batch: 770, Training Loss: 0.0033906221389770506, LR: 0.001
Time, 2019-01-07T02:06:16, Epoch: 26, Batch: 780, Training Loss: 0.008730553835630418, LR: 0.001
Time, 2019-01-07T02:06:16, Epoch: 26, Batch: 790, Training Loss: 0.0069777585566043855, LR: 0.001
Time, 2019-01-07T02:06:17, Epoch: 26, Batch: 800, Training Loss: 0.009891585260629655, LR: 0.001
Time, 2019-01-07T02:06:18, Epoch: 26, Batch: 810, Training Loss: 0.0050928622484207155, LR: 0.001
Time, 2019-01-07T02:06:18, Epoch: 26, Batch: 820, Training Loss: 0.01169218197464943, LR: 0.001
Time, 2019-01-07T02:06:19, Epoch: 26, Batch: 830, Training Loss: 0.0041181623935699465, LR: 0.001
Time, 2019-01-07T02:06:20, Epoch: 26, Batch: 840, Training Loss: 0.0038752198219299317, LR: 0.001
Time, 2019-01-07T02:06:20, Epoch: 26, Batch: 850, Training Loss: 0.009117954224348069, LR: 0.001
Time, 2019-01-07T02:06:21, Epoch: 26, Batch: 860, Training Loss: 0.0019541382789611816, LR: 0.001
Time, 2019-01-07T02:06:22, Epoch: 26, Batch: 870, Training Loss: 0.014799896627664566, LR: 0.001
Time, 2019-01-07T02:06:22, Epoch: 26, Batch: 880, Training Loss: 0.002371722459793091, LR: 0.001
Time, 2019-01-07T02:06:23, Epoch: 26, Batch: 890, Training Loss: 0.004127368330955505, LR: 0.001
Time, 2019-01-07T02:06:24, Epoch: 26, Batch: 900, Training Loss: 0.0031591325998306273, LR: 0.001
Time, 2019-01-07T02:06:24, Epoch: 26, Batch: 910, Training Loss: 0.012154014408588409, LR: 0.001
Time, 2019-01-07T02:06:25, Epoch: 26, Batch: 920, Training Loss: 0.015600945055484771, LR: 0.001
Time, 2019-01-07T02:06:25, Epoch: 26, Batch: 930, Training Loss: 0.006069349497556687, LR: 0.001
Epoch: 26, Validation Top 1 acc: 98.63654327392578
Epoch: 26, Validation Top 5 acc: 99.98009490966797
Epoch: 26, Validation Set Loss: 0.050267018377780914
Start training epoch 27
Time, 2019-01-07T02:06:29, Epoch: 27, Batch: 10, Training Loss: 0.01610758975148201, LR: 0.001
Time, 2019-01-07T02:06:29, Epoch: 27, Batch: 20, Training Loss: 0.006744222342967987, LR: 0.001
Time, 2019-01-07T02:06:30, Epoch: 27, Batch: 30, Training Loss: 0.0009483262896537781, LR: 0.001
Time, 2019-01-07T02:06:31, Epoch: 27, Batch: 40, Training Loss: 0.002173776924610138, LR: 0.001
Time, 2019-01-07T02:06:31, Epoch: 27, Batch: 50, Training Loss: 0.016378799825906752, LR: 0.001
Time, 2019-01-07T02:06:32, Epoch: 27, Batch: 60, Training Loss: 0.004973393678665161, LR: 0.001
Time, 2019-01-07T02:06:32, Epoch: 27, Batch: 70, Training Loss: 0.008793441206216812, LR: 0.001
Time, 2019-01-07T02:06:33, Epoch: 27, Batch: 80, Training Loss: 0.007060247659683228, LR: 0.001
Time, 2019-01-07T02:06:34, Epoch: 27, Batch: 90, Training Loss: 0.0022464066743850706, LR: 0.001
Time, 2019-01-07T02:06:34, Epoch: 27, Batch: 100, Training Loss: 0.00269768163561821, LR: 0.001
Time, 2019-01-07T02:06:35, Epoch: 27, Batch: 110, Training Loss: 0.0009241148829460144, LR: 0.001
Time, 2019-01-07T02:06:36, Epoch: 27, Batch: 120, Training Loss: 0.002896527200937271, LR: 0.001
Time, 2019-01-07T02:06:37, Epoch: 27, Batch: 130, Training Loss: 0.006836838275194168, LR: 0.001
Time, 2019-01-07T02:06:37, Epoch: 27, Batch: 140, Training Loss: 0.005833190679550171, LR: 0.001
Time, 2019-01-07T02:06:38, Epoch: 27, Batch: 150, Training Loss: 0.01984141319990158, LR: 0.001
Time, 2019-01-07T02:06:39, Epoch: 27, Batch: 160, Training Loss: 0.004712097346782684, LR: 0.001
Time, 2019-01-07T02:06:39, Epoch: 27, Batch: 170, Training Loss: 0.007360780239105224, LR: 0.001
Time, 2019-01-07T02:06:40, Epoch: 27, Batch: 180, Training Loss: 0.004040509462356567, LR: 0.001
Time, 2019-01-07T02:06:41, Epoch: 27, Batch: 190, Training Loss: 0.0022780537605285645, LR: 0.001
Time, 2019-01-07T02:06:41, Epoch: 27, Batch: 200, Training Loss: 0.003793409466743469, LR: 0.001
Time, 2019-01-07T02:06:42, Epoch: 27, Batch: 210, Training Loss: 0.0046809673309326175, LR: 0.001
Time, 2019-01-07T02:06:43, Epoch: 27, Batch: 220, Training Loss: 0.0022054314613342285, LR: 0.001
Time, 2019-01-07T02:06:43, Epoch: 27, Batch: 230, Training Loss: 0.004938063770532608, LR: 0.001
Time, 2019-01-07T02:06:44, Epoch: 27, Batch: 240, Training Loss: 0.009668156504631042, LR: 0.001
Time, 2019-01-07T02:06:45, Epoch: 27, Batch: 250, Training Loss: 0.012743490189313889, LR: 0.001
Time, 2019-01-07T02:06:45, Epoch: 27, Batch: 260, Training Loss: 0.002616671472787857, LR: 0.001
Time, 2019-01-07T02:06:46, Epoch: 27, Batch: 270, Training Loss: 0.0027977660298347472, LR: 0.001
Time, 2019-01-07T02:06:47, Epoch: 27, Batch: 280, Training Loss: 0.0025201842188835146, LR: 0.001
Time, 2019-01-07T02:06:47, Epoch: 27, Batch: 290, Training Loss: 0.008670445531606674, LR: 0.001
Time, 2019-01-07T02:06:48, Epoch: 27, Batch: 300, Training Loss: 0.009452016651630401, LR: 0.001
Time, 2019-01-07T02:06:49, Epoch: 27, Batch: 310, Training Loss: 0.001956067979335785, LR: 0.001
Time, 2019-01-07T02:06:49, Epoch: 27, Batch: 320, Training Loss: 0.001772947609424591, LR: 0.001
Time, 2019-01-07T02:06:50, Epoch: 27, Batch: 330, Training Loss: 0.0014940500259399414, LR: 0.001
Time, 2019-01-07T02:06:51, Epoch: 27, Batch: 340, Training Loss: 0.001894885301589966, LR: 0.001
Time, 2019-01-07T02:06:51, Epoch: 27, Batch: 350, Training Loss: 0.005348233133554458, LR: 0.001
Time, 2019-01-07T02:06:52, Epoch: 27, Batch: 360, Training Loss: 0.001890842616558075, LR: 0.001
Time, 2019-01-07T02:06:53, Epoch: 27, Batch: 370, Training Loss: 0.004232138395309448, LR: 0.001
Time, 2019-01-07T02:06:53, Epoch: 27, Batch: 380, Training Loss: 0.0023983165621757506, LR: 0.001
Time, 2019-01-07T02:06:54, Epoch: 27, Batch: 390, Training Loss: 0.0017375811934471131, LR: 0.001
Time, 2019-01-07T02:06:55, Epoch: 27, Batch: 400, Training Loss: 0.005520147085189819, LR: 0.001
Time, 2019-01-07T02:06:55, Epoch: 27, Batch: 410, Training Loss: 0.004529838263988495, LR: 0.001
Time, 2019-01-07T02:06:56, Epoch: 27, Batch: 420, Training Loss: 0.006298869848251343, LR: 0.001
Time, 2019-01-07T02:06:56, Epoch: 27, Batch: 430, Training Loss: 0.01474611610174179, LR: 0.001
Time, 2019-01-07T02:06:57, Epoch: 27, Batch: 440, Training Loss: 0.00819200575351715, LR: 0.001
Time, 2019-01-07T02:06:58, Epoch: 27, Batch: 450, Training Loss: 0.011390931904315948, LR: 0.001
Time, 2019-01-07T02:06:58, Epoch: 27, Batch: 460, Training Loss: 0.0014744751155376435, LR: 0.001
Time, 2019-01-07T02:06:59, Epoch: 27, Batch: 470, Training Loss: 0.002178780734539032, LR: 0.001
Time, 2019-01-07T02:07:00, Epoch: 27, Batch: 480, Training Loss: 0.0060898266732692715, LR: 0.001
Time, 2019-01-07T02:07:00, Epoch: 27, Batch: 490, Training Loss: 0.003330962359905243, LR: 0.001
Time, 2019-01-07T02:07:01, Epoch: 27, Batch: 500, Training Loss: 0.003542008250951767, LR: 0.001
Time, 2019-01-07T02:07:02, Epoch: 27, Batch: 510, Training Loss: 0.004455448687076568, LR: 0.001
Time, 2019-01-07T02:07:02, Epoch: 27, Batch: 520, Training Loss: 0.009492655098438264, LR: 0.001
Time, 2019-01-07T02:07:03, Epoch: 27, Batch: 530, Training Loss: 0.0066775262355804445, LR: 0.001
Time, 2019-01-07T02:07:04, Epoch: 27, Batch: 540, Training Loss: 0.0030676349997520447, LR: 0.001
Time, 2019-01-07T02:07:04, Epoch: 27, Batch: 550, Training Loss: 0.014912891387939452, LR: 0.001
Time, 2019-01-07T02:07:05, Epoch: 27, Batch: 560, Training Loss: 0.0027172908186912535, LR: 0.001
Time, 2019-01-07T02:07:06, Epoch: 27, Batch: 570, Training Loss: 0.004554024338722229, LR: 0.001
Time, 2019-01-07T02:07:06, Epoch: 27, Batch: 580, Training Loss: 0.01104520782828331, LR: 0.001
Time, 2019-01-07T02:07:07, Epoch: 27, Batch: 590, Training Loss: 0.0069449953734874725, LR: 0.001
Time, 2019-01-07T02:07:08, Epoch: 27, Batch: 600, Training Loss: 0.006260629743337631, LR: 0.001
Time, 2019-01-07T02:07:08, Epoch: 27, Batch: 610, Training Loss: 0.007908153533935546, LR: 0.001
Time, 2019-01-07T02:07:09, Epoch: 27, Batch: 620, Training Loss: 0.0006955578923225403, LR: 0.001
Time, 2019-01-07T02:07:10, Epoch: 27, Batch: 630, Training Loss: 0.0026277899742126465, LR: 0.001
Time, 2019-01-07T02:07:10, Epoch: 27, Batch: 640, Training Loss: 0.006399519741535187, LR: 0.001
Time, 2019-01-07T02:07:11, Epoch: 27, Batch: 650, Training Loss: 0.004266221821308136, LR: 0.001
Time, 2019-01-07T02:07:12, Epoch: 27, Batch: 660, Training Loss: 0.008376219868659973, LR: 0.001
Time, 2019-01-07T02:07:12, Epoch: 27, Batch: 670, Training Loss: 0.01002553030848503, LR: 0.001
Time, 2019-01-07T02:07:13, Epoch: 27, Batch: 680, Training Loss: 0.0017926901578903197, LR: 0.001
Time, 2019-01-07T02:07:14, Epoch: 27, Batch: 690, Training Loss: 0.0038609564304351806, LR: 0.001
Time, 2019-01-07T02:07:14, Epoch: 27, Batch: 700, Training Loss: 0.004079019278287887, LR: 0.001
Time, 2019-01-07T02:07:15, Epoch: 27, Batch: 710, Training Loss: 0.006333069503307342, LR: 0.001
Time, 2019-01-07T02:07:16, Epoch: 27, Batch: 720, Training Loss: 0.008788082748651505, LR: 0.001
Time, 2019-01-07T02:07:16, Epoch: 27, Batch: 730, Training Loss: 0.002271352708339691, LR: 0.001
Time, 2019-01-07T02:07:17, Epoch: 27, Batch: 740, Training Loss: 0.0024956434965133665, LR: 0.001
Time, 2019-01-07T02:07:18, Epoch: 27, Batch: 750, Training Loss: 0.00858226716518402, LR: 0.001
Time, 2019-01-07T02:07:18, Epoch: 27, Batch: 760, Training Loss: 0.003920222818851471, LR: 0.001
Time, 2019-01-07T02:07:19, Epoch: 27, Batch: 770, Training Loss: 0.0067257359623909, LR: 0.001
Time, 2019-01-07T02:07:19, Epoch: 27, Batch: 780, Training Loss: 0.0018007159233093262, LR: 0.001
Time, 2019-01-07T02:07:20, Epoch: 27, Batch: 790, Training Loss: 0.0024162188172340395, LR: 0.001
Time, 2019-01-07T02:07:21, Epoch: 27, Batch: 800, Training Loss: 0.002305060625076294, LR: 0.001
Time, 2019-01-07T02:07:21, Epoch: 27, Batch: 810, Training Loss: 0.003389115631580353, LR: 0.001
Time, 2019-01-07T02:07:22, Epoch: 27, Batch: 820, Training Loss: 0.007118438184261322, LR: 0.001
Time, 2019-01-07T02:07:23, Epoch: 27, Batch: 830, Training Loss: 0.004339541494846344, LR: 0.001
Time, 2019-01-07T02:07:23, Epoch: 27, Batch: 840, Training Loss: 0.009063097834587096, LR: 0.001
Time, 2019-01-07T02:07:24, Epoch: 27, Batch: 850, Training Loss: 0.004509104043245315, LR: 0.001
Time, 2019-01-07T02:07:25, Epoch: 27, Batch: 860, Training Loss: 0.004007643461227417, LR: 0.001
Time, 2019-01-07T02:07:25, Epoch: 27, Batch: 870, Training Loss: 0.00909825712442398, LR: 0.001
Time, 2019-01-07T02:07:26, Epoch: 27, Batch: 880, Training Loss: 0.020122847706079482, LR: 0.001
Time, 2019-01-07T02:07:27, Epoch: 27, Batch: 890, Training Loss: 0.004360842704772949, LR: 0.001
Time, 2019-01-07T02:07:27, Epoch: 27, Batch: 900, Training Loss: 0.007585674524307251, LR: 0.001
Time, 2019-01-07T02:07:28, Epoch: 27, Batch: 910, Training Loss: 0.003111087530851364, LR: 0.001
Time, 2019-01-07T02:07:29, Epoch: 27, Batch: 920, Training Loss: 0.014430688321590423, LR: 0.001
Time, 2019-01-07T02:07:29, Epoch: 27, Batch: 930, Training Loss: 0.004092387855052948, LR: 0.001
Epoch: 27, Validation Top 1 acc: 98.86544799804688
Epoch: 27, Validation Top 5 acc: 100.0
Epoch: 27, Validation Set Loss: 0.04167122021317482
Start training epoch 28
Time, 2019-01-07T02:07:32, Epoch: 28, Batch: 10, Training Loss: 0.002369581162929535, LR: 0.001
Time, 2019-01-07T02:07:33, Epoch: 28, Batch: 20, Training Loss: 0.0034562289714813233, LR: 0.001
Time, 2019-01-07T02:07:34, Epoch: 28, Batch: 30, Training Loss: 0.0030417144298553467, LR: 0.001
Time, 2019-01-07T02:07:34, Epoch: 28, Batch: 40, Training Loss: 0.0024684235453605653, LR: 0.001
Time, 2019-01-07T02:07:35, Epoch: 28, Batch: 50, Training Loss: 0.0032147467136383057, LR: 0.001
Time, 2019-01-07T02:07:36, Epoch: 28, Batch: 60, Training Loss: 0.012334076315164566, LR: 0.001
Time, 2019-01-07T02:07:36, Epoch: 28, Batch: 70, Training Loss: 0.003918172419071197, LR: 0.001
Time, 2019-01-07T02:07:37, Epoch: 28, Batch: 80, Training Loss: 0.0011449873447418213, LR: 0.001
Time, 2019-01-07T02:07:38, Epoch: 28, Batch: 90, Training Loss: 0.003868202865123749, LR: 0.001
Time, 2019-01-07T02:07:38, Epoch: 28, Batch: 100, Training Loss: 0.003266502171754837, LR: 0.001
Time, 2019-01-07T02:07:39, Epoch: 28, Batch: 110, Training Loss: 0.004447777569293976, LR: 0.001
Time, 2019-01-07T02:07:40, Epoch: 28, Batch: 120, Training Loss: 0.0036984309554100037, LR: 0.001
Time, 2019-01-07T02:07:40, Epoch: 28, Batch: 130, Training Loss: 0.008434809744358063, LR: 0.001
Time, 2019-01-07T02:07:41, Epoch: 28, Batch: 140, Training Loss: 0.006039271503686905, LR: 0.001
Time, 2019-01-07T02:07:42, Epoch: 28, Batch: 150, Training Loss: 0.003197336196899414, LR: 0.001
Time, 2019-01-07T02:07:42, Epoch: 28, Batch: 160, Training Loss: 0.0006711944937705993, LR: 0.001
Time, 2019-01-07T02:07:43, Epoch: 28, Batch: 170, Training Loss: 0.0037013843655586244, LR: 0.001
Time, 2019-01-07T02:07:43, Epoch: 28, Batch: 180, Training Loss: 0.006277915835380554, LR: 0.001
Time, 2019-01-07T02:07:44, Epoch: 28, Batch: 190, Training Loss: 0.0011012732982635498, LR: 0.001
Time, 2019-01-07T02:07:45, Epoch: 28, Batch: 200, Training Loss: 0.0018232345581054687, LR: 0.001
Time, 2019-01-07T02:07:45, Epoch: 28, Batch: 210, Training Loss: 0.008000850677490234, LR: 0.001
Time, 2019-01-07T02:07:46, Epoch: 28, Batch: 220, Training Loss: 0.002389998733997345, LR: 0.001
Time, 2019-01-07T02:07:47, Epoch: 28, Batch: 230, Training Loss: 0.005698364973068237, LR: 0.001
Time, 2019-01-07T02:07:47, Epoch: 28, Batch: 240, Training Loss: 0.007087188214063645, LR: 0.001
Time, 2019-01-07T02:07:48, Epoch: 28, Batch: 250, Training Loss: 0.002166503667831421, LR: 0.001
Time, 2019-01-07T02:07:49, Epoch: 28, Batch: 260, Training Loss: 0.003908035904169082, LR: 0.001
Time, 2019-01-07T02:07:49, Epoch: 28, Batch: 270, Training Loss: 0.001712585985660553, LR: 0.001
Time, 2019-01-07T02:07:50, Epoch: 28, Batch: 280, Training Loss: 0.002352379262447357, LR: 0.001
Time, 2019-01-07T02:07:51, Epoch: 28, Batch: 290, Training Loss: 0.0013679787516593933, LR: 0.001
Time, 2019-01-07T02:07:51, Epoch: 28, Batch: 300, Training Loss: 0.0054414719343185425, LR: 0.001
Time, 2019-01-07T02:07:52, Epoch: 28, Batch: 310, Training Loss: 0.006574122607707978, LR: 0.001
Time, 2019-01-07T02:07:53, Epoch: 28, Batch: 320, Training Loss: 0.003658430278301239, LR: 0.001
Time, 2019-01-07T02:07:54, Epoch: 28, Batch: 330, Training Loss: 0.0026631712913513183, LR: 0.001
Time, 2019-01-07T02:07:54, Epoch: 28, Batch: 340, Training Loss: 0.002125018835067749, LR: 0.001
Time, 2019-01-07T02:07:55, Epoch: 28, Batch: 350, Training Loss: 0.0006782323122024536, LR: 0.001
Time, 2019-01-07T02:07:56, Epoch: 28, Batch: 360, Training Loss: 0.003570029139518738, LR: 0.001
Time, 2019-01-07T02:07:56, Epoch: 28, Batch: 370, Training Loss: 0.004114784300327301, LR: 0.001
Time, 2019-01-07T02:07:57, Epoch: 28, Batch: 380, Training Loss: 0.003903263807296753, LR: 0.001
Time, 2019-01-07T02:07:58, Epoch: 28, Batch: 390, Training Loss: 0.0023371592164039613, LR: 0.001
Time, 2019-01-07T02:07:58, Epoch: 28, Batch: 400, Training Loss: 0.011291610449552536, LR: 0.001
Time, 2019-01-07T02:07:59, Epoch: 28, Batch: 410, Training Loss: 0.0006845995783805847, LR: 0.001
Time, 2019-01-07T02:08:00, Epoch: 28, Batch: 420, Training Loss: 0.008516985177993774, LR: 0.001
Time, 2019-01-07T02:08:00, Epoch: 28, Batch: 430, Training Loss: 0.009588080644607543, LR: 0.001
Time, 2019-01-07T02:08:01, Epoch: 28, Batch: 440, Training Loss: 0.01631823554635048, LR: 0.001
Time, 2019-01-07T02:08:02, Epoch: 28, Batch: 450, Training Loss: 0.002987494319677353, LR: 0.001
Time, 2019-01-07T02:08:03, Epoch: 28, Batch: 460, Training Loss: 0.009525879472494125, LR: 0.001
Time, 2019-01-07T02:08:03, Epoch: 28, Batch: 470, Training Loss: 0.00411074012517929, LR: 0.001
Time, 2019-01-07T02:08:04, Epoch: 28, Batch: 480, Training Loss: 0.007727522403001785, LR: 0.001
Time, 2019-01-07T02:08:04, Epoch: 28, Batch: 490, Training Loss: 0.005762346088886261, LR: 0.001
Time, 2019-01-07T02:08:05, Epoch: 28, Batch: 500, Training Loss: 0.002713441848754883, LR: 0.001
Time, 2019-01-07T02:08:06, Epoch: 28, Batch: 510, Training Loss: 0.00133572518825531, LR: 0.001
Time, 2019-01-07T02:08:06, Epoch: 28, Batch: 520, Training Loss: 0.006838928163051605, LR: 0.001
Time, 2019-01-07T02:08:07, Epoch: 28, Batch: 530, Training Loss: 0.0027116961777210236, LR: 0.001
Time, 2019-01-07T02:08:08, Epoch: 28, Batch: 540, Training Loss: 0.007115705311298371, LR: 0.001
Time, 2019-01-07T02:08:08, Epoch: 28, Batch: 550, Training Loss: 0.007635073363780975, LR: 0.001
Time, 2019-01-07T02:08:09, Epoch: 28, Batch: 560, Training Loss: 0.0015078812837600708, LR: 0.001
Time, 2019-01-07T02:08:10, Epoch: 28, Batch: 570, Training Loss: 0.00971430391073227, LR: 0.001
Time, 2019-01-07T02:08:10, Epoch: 28, Batch: 580, Training Loss: 0.008075448125600815, LR: 0.001
Time, 2019-01-07T02:08:11, Epoch: 28, Batch: 590, Training Loss: 0.004794025421142578, LR: 0.001
Time, 2019-01-07T02:08:11, Epoch: 28, Batch: 600, Training Loss: 0.005196458101272583, LR: 0.001
Time, 2019-01-07T02:08:12, Epoch: 28, Batch: 610, Training Loss: 0.001188434660434723, LR: 0.001
Time, 2019-01-07T02:08:13, Epoch: 28, Batch: 620, Training Loss: 0.0018255531787872315, LR: 0.001
Time, 2019-01-07T02:08:13, Epoch: 28, Batch: 630, Training Loss: 0.005148057639598846, LR: 0.001
Time, 2019-01-07T02:08:14, Epoch: 28, Batch: 640, Training Loss: 0.0010798647999763489, LR: 0.001
Time, 2019-01-07T02:08:15, Epoch: 28, Batch: 650, Training Loss: 0.005289323627948761, LR: 0.001
Time, 2019-01-07T02:08:15, Epoch: 28, Batch: 660, Training Loss: 0.00355108380317688, LR: 0.001
Time, 2019-01-07T02:08:16, Epoch: 28, Batch: 670, Training Loss: 0.0058498367667198185, LR: 0.001
Time, 2019-01-07T02:08:17, Epoch: 28, Batch: 680, Training Loss: 0.0028759583830833433, LR: 0.001
Time, 2019-01-07T02:08:17, Epoch: 28, Batch: 690, Training Loss: 0.005240748822689057, LR: 0.001
Time, 2019-01-07T02:08:18, Epoch: 28, Batch: 700, Training Loss: 0.008806314319372177, LR: 0.001
Time, 2019-01-07T02:08:18, Epoch: 28, Batch: 710, Training Loss: 0.0034137532114982606, LR: 0.001
Time, 2019-01-07T02:08:19, Epoch: 28, Batch: 720, Training Loss: 0.004775024950504303, LR: 0.001
Time, 2019-01-07T02:08:20, Epoch: 28, Batch: 730, Training Loss: 0.00126965194940567, LR: 0.001
Time, 2019-01-07T02:08:20, Epoch: 28, Batch: 740, Training Loss: 0.007675106823444367, LR: 0.001
Time, 2019-01-07T02:08:21, Epoch: 28, Batch: 750, Training Loss: 0.0065660253167152405, LR: 0.001
Time, 2019-01-07T02:08:22, Epoch: 28, Batch: 760, Training Loss: 0.0016486316919326783, LR: 0.001
Time, 2019-01-07T02:08:22, Epoch: 28, Batch: 770, Training Loss: 0.0036229029297828673, LR: 0.001
Time, 2019-01-07T02:08:23, Epoch: 28, Batch: 780, Training Loss: 0.0028027743101119995, LR: 0.001
Time, 2019-01-07T02:08:24, Epoch: 28, Batch: 790, Training Loss: 0.005056400597095489, LR: 0.001
Time, 2019-01-07T02:08:24, Epoch: 28, Batch: 800, Training Loss: 0.006385428458452224, LR: 0.001
Time, 2019-01-07T02:08:25, Epoch: 28, Batch: 810, Training Loss: 0.0004961058497428894, LR: 0.001
Time, 2019-01-07T02:08:26, Epoch: 28, Batch: 820, Training Loss: 0.004759094119071961, LR: 0.001
Time, 2019-01-07T02:08:26, Epoch: 28, Batch: 830, Training Loss: 0.003474167734384537, LR: 0.001
Time, 2019-01-07T02:08:27, Epoch: 28, Batch: 840, Training Loss: 0.002375476062297821, LR: 0.001
Time, 2019-01-07T02:08:28, Epoch: 28, Batch: 850, Training Loss: 0.0031536683440208436, LR: 0.001
Time, 2019-01-07T02:08:28, Epoch: 28, Batch: 860, Training Loss: 0.004706583917140961, LR: 0.001
Time, 2019-01-07T02:08:29, Epoch: 28, Batch: 870, Training Loss: 0.006403721868991852, LR: 0.001
Time, 2019-01-07T02:08:30, Epoch: 28, Batch: 880, Training Loss: 0.001496177911758423, LR: 0.001
Time, 2019-01-07T02:08:30, Epoch: 28, Batch: 890, Training Loss: 0.0043736487627029415, LR: 0.001
Time, 2019-01-07T02:08:31, Epoch: 28, Batch: 900, Training Loss: 0.0034705184400081633, LR: 0.001
Time, 2019-01-07T02:08:32, Epoch: 28, Batch: 910, Training Loss: 0.010556600242853164, LR: 0.001
Time, 2019-01-07T02:08:32, Epoch: 28, Batch: 920, Training Loss: 0.007108871638774872, LR: 0.001
Time, 2019-01-07T02:08:33, Epoch: 28, Batch: 930, Training Loss: 0.005753011256456375, LR: 0.001
Epoch: 28, Validation Top 1 acc: 98.74601745605469
Epoch: 28, Validation Top 5 acc: 100.0
Epoch: 28, Validation Set Loss: 0.053668081760406494
Start training epoch 29
Time, 2019-01-07T02:08:37, Epoch: 29, Batch: 10, Training Loss: 0.0030856937170028686, LR: 0.001
Time, 2019-01-07T02:08:37, Epoch: 29, Batch: 20, Training Loss: 0.004349755495786667, LR: 0.001
Time, 2019-01-07T02:08:38, Epoch: 29, Batch: 30, Training Loss: 0.00704292356967926, LR: 0.001
Time, 2019-01-07T02:08:39, Epoch: 29, Batch: 40, Training Loss: 0.0034813515841960905, LR: 0.001
Time, 2019-01-07T02:08:39, Epoch: 29, Batch: 50, Training Loss: 0.008357623219490051, LR: 0.001
Time, 2019-01-07T02:08:40, Epoch: 29, Batch: 60, Training Loss: 0.011167895048856735, LR: 0.001
Time, 2019-01-07T02:08:40, Epoch: 29, Batch: 70, Training Loss: 0.0012970492243766785, LR: 0.001
Time, 2019-01-07T02:08:41, Epoch: 29, Batch: 80, Training Loss: 0.004053743183612823, LR: 0.001
Time, 2019-01-07T02:08:42, Epoch: 29, Batch: 90, Training Loss: 0.008414170145988465, LR: 0.001
Time, 2019-01-07T02:08:43, Epoch: 29, Batch: 100, Training Loss: 0.005942846089601517, LR: 0.001
Time, 2019-01-07T02:08:43, Epoch: 29, Batch: 110, Training Loss: 0.0014375269412994385, LR: 0.001
Time, 2019-01-07T02:08:44, Epoch: 29, Batch: 120, Training Loss: 0.008994659781455994, LR: 0.001
Time, 2019-01-07T02:08:45, Epoch: 29, Batch: 130, Training Loss: 0.01924648880958557, LR: 0.001
Time, 2019-01-07T02:08:46, Epoch: 29, Batch: 140, Training Loss: 0.010721362382173538, LR: 0.001
Time, 2019-01-07T02:08:46, Epoch: 29, Batch: 150, Training Loss: 0.004685238003730774, LR: 0.001
Time, 2019-01-07T02:08:47, Epoch: 29, Batch: 160, Training Loss: 0.0064277246594429014, LR: 0.001
Time, 2019-01-07T02:08:48, Epoch: 29, Batch: 170, Training Loss: 0.0013424500823020935, LR: 0.001
Time, 2019-01-07T02:08:48, Epoch: 29, Batch: 180, Training Loss: 0.009526316821575165, LR: 0.001
Time, 2019-01-07T02:08:49, Epoch: 29, Batch: 190, Training Loss: 0.007091207057237625, LR: 0.001
Time, 2019-01-07T02:08:50, Epoch: 29, Batch: 200, Training Loss: 0.009425039589405059, LR: 0.001
Time, 2019-01-07T02:08:50, Epoch: 29, Batch: 210, Training Loss: 0.004564021527767181, LR: 0.001
Time, 2019-01-07T02:08:51, Epoch: 29, Batch: 220, Training Loss: 0.007050458341836929, LR: 0.001
Time, 2019-01-07T02:08:52, Epoch: 29, Batch: 230, Training Loss: 0.012082288414239884, LR: 0.001
Time, 2019-01-07T02:08:52, Epoch: 29, Batch: 240, Training Loss: 0.005545259267091751, LR: 0.001
Time, 2019-01-07T02:08:53, Epoch: 29, Batch: 250, Training Loss: 0.004123334586620331, LR: 0.001
Time, 2019-01-07T02:08:54, Epoch: 29, Batch: 260, Training Loss: 0.0031421810388565065, LR: 0.001
Time, 2019-01-07T02:08:54, Epoch: 29, Batch: 270, Training Loss: 0.0008854284882545471, LR: 0.001
Time, 2019-01-07T02:08:55, Epoch: 29, Batch: 280, Training Loss: 0.00040672197937965395, LR: 0.001
Time, 2019-01-07T02:08:56, Epoch: 29, Batch: 290, Training Loss: 0.005308188498020172, LR: 0.001
Time, 2019-01-07T02:08:56, Epoch: 29, Batch: 300, Training Loss: 0.0018896356225013733, LR: 0.001
Time, 2019-01-07T02:08:57, Epoch: 29, Batch: 310, Training Loss: 0.007284271717071533, LR: 0.001
Time, 2019-01-07T02:08:58, Epoch: 29, Batch: 320, Training Loss: 0.012192031741142273, LR: 0.001
Time, 2019-01-07T02:08:59, Epoch: 29, Batch: 330, Training Loss: 0.004809761047363281, LR: 0.001
Time, 2019-01-07T02:08:59, Epoch: 29, Batch: 340, Training Loss: 0.00089789479970932, LR: 0.001
Time, 2019-01-07T02:09:00, Epoch: 29, Batch: 350, Training Loss: 0.005787531286478043, LR: 0.001
Time, 2019-01-07T02:09:01, Epoch: 29, Batch: 360, Training Loss: 0.011549800634384155, LR: 0.001
Time, 2019-01-07T02:09:02, Epoch: 29, Batch: 370, Training Loss: 0.009165038168430329, LR: 0.001
Time, 2019-01-07T02:09:02, Epoch: 29, Batch: 380, Training Loss: 0.008350217342376709, LR: 0.001
Time, 2019-01-07T02:09:03, Epoch: 29, Batch: 390, Training Loss: 0.004534265398979187, LR: 0.001
Time, 2019-01-07T02:09:04, Epoch: 29, Batch: 400, Training Loss: 0.0015016064047813416, LR: 0.001
Time, 2019-01-07T02:09:05, Epoch: 29, Batch: 410, Training Loss: 0.01887907609343529, LR: 0.001
Time, 2019-01-07T02:09:06, Epoch: 29, Batch: 420, Training Loss: 0.01103387475013733, LR: 0.001
Time, 2019-01-07T02:09:07, Epoch: 29, Batch: 430, Training Loss: 0.019400546699762343, LR: 0.001
Time, 2019-01-07T02:09:07, Epoch: 29, Batch: 440, Training Loss: 0.007754004001617432, LR: 0.001
Time, 2019-01-07T02:09:08, Epoch: 29, Batch: 450, Training Loss: 0.007532519847154617, LR: 0.001
Time, 2019-01-07T02:09:09, Epoch: 29, Batch: 460, Training Loss: 0.0066878579556941984, LR: 0.001
Time, 2019-01-07T02:09:09, Epoch: 29, Batch: 470, Training Loss: 0.005804187059402466, LR: 0.001
Time, 2019-01-07T02:09:10, Epoch: 29, Batch: 480, Training Loss: 0.008010207116603852, LR: 0.001
Time, 2019-01-07T02:09:11, Epoch: 29, Batch: 490, Training Loss: 0.014567264169454575, LR: 0.001
Time, 2019-01-07T02:09:11, Epoch: 29, Batch: 500, Training Loss: 0.0020486265420913696, LR: 0.001
Time, 2019-01-07T02:09:12, Epoch: 29, Batch: 510, Training Loss: 0.0020550146698951723, LR: 0.001
Time, 2019-01-07T02:09:13, Epoch: 29, Batch: 520, Training Loss: 0.007882557064294814, LR: 0.001
Time, 2019-01-07T02:09:13, Epoch: 29, Batch: 530, Training Loss: 0.003337669372558594, LR: 0.001
Time, 2019-01-07T02:09:14, Epoch: 29, Batch: 540, Training Loss: 0.005817872285842895, LR: 0.001
Time, 2019-01-07T02:09:15, Epoch: 29, Batch: 550, Training Loss: 0.005953363329172135, LR: 0.001
Time, 2019-01-07T02:09:15, Epoch: 29, Batch: 560, Training Loss: 0.008683940023183822, LR: 0.001
Time, 2019-01-07T02:09:16, Epoch: 29, Batch: 570, Training Loss: 0.007833552360534669, LR: 0.001
Time, 2019-01-07T02:09:17, Epoch: 29, Batch: 580, Training Loss: 0.0020647406578063964, LR: 0.001
Time, 2019-01-07T02:09:17, Epoch: 29, Batch: 590, Training Loss: 0.002270924299955368, LR: 0.001
Time, 2019-01-07T02:09:18, Epoch: 29, Batch: 600, Training Loss: 0.004175589978694915, LR: 0.001
Time, 2019-01-07T02:09:18, Epoch: 29, Batch: 610, Training Loss: 0.004612618684768676, LR: 0.001
Time, 2019-01-07T02:09:19, Epoch: 29, Batch: 620, Training Loss: 0.00734560489654541, LR: 0.001
Time, 2019-01-07T02:09:20, Epoch: 29, Batch: 630, Training Loss: 0.006103610992431641, LR: 0.001
Time, 2019-01-07T02:09:20, Epoch: 29, Batch: 640, Training Loss: 0.00410391166806221, LR: 0.001
Time, 2019-01-07T02:09:21, Epoch: 29, Batch: 650, Training Loss: 0.0035437896847724916, LR: 0.001
Time, 2019-01-07T02:09:22, Epoch: 29, Batch: 660, Training Loss: 0.002166161686182022, LR: 0.001
Time, 2019-01-07T02:09:22, Epoch: 29, Batch: 670, Training Loss: 0.0016860798001289367, LR: 0.001
Time, 2019-01-07T02:09:23, Epoch: 29, Batch: 680, Training Loss: 0.0011876404285430908, LR: 0.001
Time, 2019-01-07T02:09:24, Epoch: 29, Batch: 690, Training Loss: 0.004965305328369141, LR: 0.001
Time, 2019-01-07T02:09:24, Epoch: 29, Batch: 700, Training Loss: 0.0033256679773330687, LR: 0.001
Time, 2019-01-07T02:09:25, Epoch: 29, Batch: 710, Training Loss: 0.005377137660980224, LR: 0.001
Time, 2019-01-07T02:09:26, Epoch: 29, Batch: 720, Training Loss: 0.009410879760980605, LR: 0.001
Time, 2019-01-07T02:09:26, Epoch: 29, Batch: 730, Training Loss: 0.0041335724294185635, LR: 0.001
Time, 2019-01-07T02:09:27, Epoch: 29, Batch: 740, Training Loss: 0.005528315901756287, LR: 0.001
Time, 2019-01-07T02:09:28, Epoch: 29, Batch: 750, Training Loss: 0.013528936356306077, LR: 0.001
Time, 2019-01-07T02:09:29, Epoch: 29, Batch: 760, Training Loss: 0.009660448133945464, LR: 0.001
Time, 2019-01-07T02:09:29, Epoch: 29, Batch: 770, Training Loss: 0.006580373644828797, LR: 0.001
Time, 2019-01-07T02:09:30, Epoch: 29, Batch: 780, Training Loss: 0.006144599616527557, LR: 0.001
Time, 2019-01-07T02:09:31, Epoch: 29, Batch: 790, Training Loss: 0.012388856709003448, LR: 0.001
Time, 2019-01-07T02:09:31, Epoch: 29, Batch: 800, Training Loss: 0.008844508975744247, LR: 0.001
Time, 2019-01-07T02:09:32, Epoch: 29, Batch: 810, Training Loss: 0.005183985829353333, LR: 0.001
Time, 2019-01-07T02:09:33, Epoch: 29, Batch: 820, Training Loss: 0.008072996884584427, LR: 0.001
Time, 2019-01-07T02:09:33, Epoch: 29, Batch: 830, Training Loss: 0.008938422799110413, LR: 0.001
Time, 2019-01-07T02:09:34, Epoch: 29, Batch: 840, Training Loss: 0.011602138727903366, LR: 0.001
Time, 2019-01-07T02:09:35, Epoch: 29, Batch: 850, Training Loss: 0.008794184029102325, LR: 0.001
Time, 2019-01-07T02:09:35, Epoch: 29, Batch: 860, Training Loss: 0.014101575314998626, LR: 0.001
Time, 2019-01-07T02:09:36, Epoch: 29, Batch: 870, Training Loss: 0.004925681650638581, LR: 0.001
Time, 2019-01-07T02:09:36, Epoch: 29, Batch: 880, Training Loss: 0.011431605368852616, LR: 0.001
Time, 2019-01-07T02:09:37, Epoch: 29, Batch: 890, Training Loss: 0.014306113868951798, LR: 0.001
Time, 2019-01-07T02:09:38, Epoch: 29, Batch: 900, Training Loss: 0.004438129812479019, LR: 0.001
Time, 2019-01-07T02:09:38, Epoch: 29, Batch: 910, Training Loss: 0.009428957849740982, LR: 0.001
Time, 2019-01-07T02:09:39, Epoch: 29, Batch: 920, Training Loss: 0.015240460634231567, LR: 0.001
Time, 2019-01-07T02:09:40, Epoch: 29, Batch: 930, Training Loss: 0.011317126452922821, LR: 0.001
Epoch: 29, Validation Top 1 acc: 98.53702545166016
Epoch: 29, Validation Top 5 acc: 100.0
Epoch: 29, Validation Set Loss: 0.058598440140485764
Start training epoch 30
Time, 2019-01-07T02:09:43, Epoch: 30, Batch: 10, Training Loss: 0.006144596636295319, LR: 0.001
Time, 2019-01-07T02:09:43, Epoch: 30, Batch: 20, Training Loss: 0.01610065996646881, LR: 0.001
Time, 2019-01-07T02:09:44, Epoch: 30, Batch: 30, Training Loss: 0.003936654329299927, LR: 0.001
Time, 2019-01-07T02:09:45, Epoch: 30, Batch: 40, Training Loss: 0.01067766398191452, LR: 0.001
Time, 2019-01-07T02:09:46, Epoch: 30, Batch: 50, Training Loss: 0.0038337230682373045, LR: 0.001
Time, 2019-01-07T02:09:46, Epoch: 30, Batch: 60, Training Loss: 0.0022847771644592283, LR: 0.001
Time, 2019-01-07T02:09:47, Epoch: 30, Batch: 70, Training Loss: 0.003789796680212021, LR: 0.001
Time, 2019-01-07T02:09:48, Epoch: 30, Batch: 80, Training Loss: 0.00578375905752182, LR: 0.001
Time, 2019-01-07T02:09:48, Epoch: 30, Batch: 90, Training Loss: 0.0020184040069580076, LR: 0.001
Time, 2019-01-07T02:09:49, Epoch: 30, Batch: 100, Training Loss: 0.01151130348443985, LR: 0.001
Time, 2019-01-07T02:09:50, Epoch: 30, Batch: 110, Training Loss: 0.006924276053905487, LR: 0.001
Time, 2019-01-07T02:09:50, Epoch: 30, Batch: 120, Training Loss: 0.008997581154108047, LR: 0.001
Time, 2019-01-07T02:09:51, Epoch: 30, Batch: 130, Training Loss: 0.011289162188768386, LR: 0.001
Time, 2019-01-07T02:09:52, Epoch: 30, Batch: 140, Training Loss: 0.01515829786658287, LR: 0.001
Time, 2019-01-07T02:09:52, Epoch: 30, Batch: 150, Training Loss: 0.019852088391780855, LR: 0.001
Time, 2019-01-07T02:09:53, Epoch: 30, Batch: 160, Training Loss: 0.031143488734960555, LR: 0.001
Time, 2019-01-07T02:09:54, Epoch: 30, Batch: 170, Training Loss: 0.004446379840373993, LR: 0.001
Time, 2019-01-07T02:09:54, Epoch: 30, Batch: 180, Training Loss: 0.005689441412687302, LR: 0.001
Time, 2019-01-07T02:09:55, Epoch: 30, Batch: 190, Training Loss: 0.010007695853710174, LR: 0.001
Time, 2019-01-07T02:09:56, Epoch: 30, Batch: 200, Training Loss: 0.017728304862976073, LR: 0.001
Time, 2019-01-07T02:09:56, Epoch: 30, Batch: 210, Training Loss: 0.00828605592250824, LR: 0.001
Time, 2019-01-07T02:09:57, Epoch: 30, Batch: 220, Training Loss: 0.0023986078798770905, LR: 0.001
Time, 2019-01-07T02:09:58, Epoch: 30, Batch: 230, Training Loss: 0.014881842583417893, LR: 0.001
Time, 2019-01-07T02:09:58, Epoch: 30, Batch: 240, Training Loss: 0.006739138066768647, LR: 0.001
Time, 2019-01-07T02:09:59, Epoch: 30, Batch: 250, Training Loss: 0.01276121884584427, LR: 0.001
Time, 2019-01-07T02:09:59, Epoch: 30, Batch: 260, Training Loss: 0.013729438185691833, LR: 0.001
Time, 2019-01-07T02:10:00, Epoch: 30, Batch: 270, Training Loss: 0.0023569434881210325, LR: 0.001
Time, 2019-01-07T02:10:01, Epoch: 30, Batch: 280, Training Loss: 0.007778654992580414, LR: 0.001
Time, 2019-01-07T02:10:01, Epoch: 30, Batch: 290, Training Loss: 0.024966637790203094, LR: 0.001
Time, 2019-01-07T02:10:02, Epoch: 30, Batch: 300, Training Loss: 0.004859471321105957, LR: 0.001
Time, 2019-01-07T02:10:03, Epoch: 30, Batch: 310, Training Loss: 0.007465219497680664, LR: 0.001
Time, 2019-01-07T02:10:03, Epoch: 30, Batch: 320, Training Loss: 0.015900905430316924, LR: 0.001
Time, 2019-01-07T02:10:04, Epoch: 30, Batch: 330, Training Loss: 0.007734756171703339, LR: 0.001
Time, 2019-01-07T02:10:05, Epoch: 30, Batch: 340, Training Loss: 0.003627303242683411, LR: 0.001
Time, 2019-01-07T02:10:05, Epoch: 30, Batch: 350, Training Loss: 0.0043560460209846495, LR: 0.001
Time, 2019-01-07T02:10:06, Epoch: 30, Batch: 360, Training Loss: 0.003876657783985138, LR: 0.001
Time, 2019-01-07T02:10:07, Epoch: 30, Batch: 370, Training Loss: 0.014567138254642486, LR: 0.001
Time, 2019-01-07T02:10:07, Epoch: 30, Batch: 380, Training Loss: 0.008027180284261703, LR: 0.001
Time, 2019-01-07T02:10:08, Epoch: 30, Batch: 390, Training Loss: 0.01850420981645584, LR: 0.001
Time, 2019-01-07T02:10:09, Epoch: 30, Batch: 400, Training Loss: 0.004418704658746719, LR: 0.001
Time, 2019-01-07T02:10:09, Epoch: 30, Batch: 410, Training Loss: 0.008201444149017334, LR: 0.001
Time, 2019-01-07T02:10:10, Epoch: 30, Batch: 420, Training Loss: 0.011388176679611206, LR: 0.001
Time, 2019-01-07T02:10:11, Epoch: 30, Batch: 430, Training Loss: 0.007445311546325684, LR: 0.001
Time, 2019-01-07T02:10:11, Epoch: 30, Batch: 440, Training Loss: 0.008664967864751816, LR: 0.001
Time, 2019-01-07T02:10:12, Epoch: 30, Batch: 450, Training Loss: 0.022856421768665314, LR: 0.001
Time, 2019-01-07T02:10:13, Epoch: 30, Batch: 460, Training Loss: 0.0071960888803005215, LR: 0.001
Time, 2019-01-07T02:10:13, Epoch: 30, Batch: 470, Training Loss: 0.017457839101552963, LR: 0.001
Time, 2019-01-07T02:10:14, Epoch: 30, Batch: 480, Training Loss: 0.012303724139928817, LR: 0.001
Time, 2019-01-07T02:10:15, Epoch: 30, Batch: 490, Training Loss: 0.02193751484155655, LR: 0.001
Time, 2019-01-07T02:10:15, Epoch: 30, Batch: 500, Training Loss: 0.03385780900716782, LR: 0.001
Time, 2019-01-07T02:10:16, Epoch: 30, Batch: 510, Training Loss: 0.014151029288768768, LR: 0.001
Time, 2019-01-07T02:10:17, Epoch: 30, Batch: 520, Training Loss: 0.013496184349060058, LR: 0.001
Time, 2019-01-07T02:10:17, Epoch: 30, Batch: 530, Training Loss: 0.029132932424545288, LR: 0.001
Time, 2019-01-07T02:10:18, Epoch: 30, Batch: 540, Training Loss: 0.009373030811548232, LR: 0.001
Time, 2019-01-07T02:10:19, Epoch: 30, Batch: 550, Training Loss: 0.006072995066642761, LR: 0.001
Time, 2019-01-07T02:10:19, Epoch: 30, Batch: 560, Training Loss: 0.0075474753975868225, LR: 0.001
Time, 2019-01-07T02:10:20, Epoch: 30, Batch: 570, Training Loss: 0.019077473133802415, LR: 0.001
Time, 2019-01-07T02:10:21, Epoch: 30, Batch: 580, Training Loss: 0.004731637239456177, LR: 0.001
Time, 2019-01-07T02:10:21, Epoch: 30, Batch: 590, Training Loss: 0.017209260910749435, LR: 0.001
Time, 2019-01-07T02:10:22, Epoch: 30, Batch: 600, Training Loss: 0.001030893623828888, LR: 0.001
Time, 2019-01-07T02:10:23, Epoch: 30, Batch: 610, Training Loss: 0.01573629304766655, LR: 0.001
Time, 2019-01-07T02:10:23, Epoch: 30, Batch: 620, Training Loss: 0.011573128402233124, LR: 0.001
Time, 2019-01-07T02:10:24, Epoch: 30, Batch: 630, Training Loss: 0.0031512722373008726, LR: 0.001
Time, 2019-01-07T02:10:25, Epoch: 30, Batch: 640, Training Loss: 0.005426979064941407, LR: 0.001
Time, 2019-01-07T02:10:25, Epoch: 30, Batch: 650, Training Loss: 0.007051076740026474, LR: 0.001
Time, 2019-01-07T02:10:26, Epoch: 30, Batch: 660, Training Loss: 0.0054567232728004456, LR: 0.001
Time, 2019-01-07T02:10:27, Epoch: 30, Batch: 670, Training Loss: 0.006273540854454041, LR: 0.001
Time, 2019-01-07T02:10:27, Epoch: 30, Batch: 680, Training Loss: 0.004000722616910935, LR: 0.001
Time, 2019-01-07T02:10:28, Epoch: 30, Batch: 690, Training Loss: 0.009789765626192094, LR: 0.001
Time, 2019-01-07T02:10:29, Epoch: 30, Batch: 700, Training Loss: 0.007732304185628891, LR: 0.001
Time, 2019-01-07T02:10:29, Epoch: 30, Batch: 710, Training Loss: 0.009454759955406188, LR: 0.001
Time, 2019-01-07T02:10:30, Epoch: 30, Batch: 720, Training Loss: 0.018663522601127625, LR: 0.001
Time, 2019-01-07T02:10:30, Epoch: 30, Batch: 730, Training Loss: 0.0058850497007369995, LR: 0.001
Time, 2019-01-07T02:10:31, Epoch: 30, Batch: 740, Training Loss: 0.024042393267154693, LR: 0.001
Time, 2019-01-07T02:10:32, Epoch: 30, Batch: 750, Training Loss: 0.012279397249221802, LR: 0.001
Time, 2019-01-07T02:10:32, Epoch: 30, Batch: 760, Training Loss: 0.013756178319454193, LR: 0.001
Time, 2019-01-07T02:10:33, Epoch: 30, Batch: 770, Training Loss: 0.007857053726911544, LR: 0.001
Time, 2019-01-07T02:10:34, Epoch: 30, Batch: 780, Training Loss: 0.01722927838563919, LR: 0.001
Time, 2019-01-07T02:10:34, Epoch: 30, Batch: 790, Training Loss: 0.009601398557424545, LR: 0.001
Time, 2019-01-07T02:10:35, Epoch: 30, Batch: 800, Training Loss: 0.006014052033424378, LR: 0.001
Time, 2019-01-07T02:10:36, Epoch: 30, Batch: 810, Training Loss: 0.014937026798725129, LR: 0.001
Time, 2019-01-07T02:10:36, Epoch: 30, Batch: 820, Training Loss: 0.0027190186083316803, LR: 0.001
Time, 2019-01-07T02:10:37, Epoch: 30, Batch: 830, Training Loss: 0.012402459979057312, LR: 0.001
Time, 2019-01-07T02:10:38, Epoch: 30, Batch: 840, Training Loss: 0.0035445645451545717, LR: 0.001
Time, 2019-01-07T02:10:38, Epoch: 30, Batch: 850, Training Loss: 0.006883005052804947, LR: 0.001
Time, 2019-01-07T02:10:39, Epoch: 30, Batch: 860, Training Loss: 0.0036069467663764954, LR: 0.001
Time, 2019-01-07T02:10:40, Epoch: 30, Batch: 870, Training Loss: 0.012268694490194321, LR: 0.001
Time, 2019-01-07T02:10:41, Epoch: 30, Batch: 880, Training Loss: 0.018896566331386568, LR: 0.001
Time, 2019-01-07T02:10:41, Epoch: 30, Batch: 890, Training Loss: 0.00873883068561554, LR: 0.001
Time, 2019-01-07T02:10:42, Epoch: 30, Batch: 900, Training Loss: 0.009406135976314544, LR: 0.001
Time, 2019-01-07T02:10:43, Epoch: 30, Batch: 910, Training Loss: 0.009257378429174424, LR: 0.001
Time, 2019-01-07T02:10:43, Epoch: 30, Batch: 920, Training Loss: 0.013411030918359757, LR: 0.001
Time, 2019-01-07T02:10:44, Epoch: 30, Batch: 930, Training Loss: 0.008821482211351395, LR: 0.001
Epoch: 30, Validation Top 1 acc: 98.6863021850586
Epoch: 30, Validation Top 5 acc: 100.0
Epoch: 30, Validation Set Loss: 0.047832634299993515
Start training epoch 31
Time, 2019-01-07T02:10:47, Epoch: 31, Batch: 10, Training Loss: 0.0027099698781967163, LR: 0.001
Time, 2019-01-07T02:10:48, Epoch: 31, Batch: 20, Training Loss: 0.003670865297317505, LR: 0.001
Time, 2019-01-07T02:10:49, Epoch: 31, Batch: 30, Training Loss: 0.013478539139032363, LR: 0.001
Time, 2019-01-07T02:10:49, Epoch: 31, Batch: 40, Training Loss: 0.010875898599624633, LR: 0.001
Time, 2019-01-07T02:10:50, Epoch: 31, Batch: 50, Training Loss: 0.0047259747982025145, LR: 0.001
Time, 2019-01-07T02:10:51, Epoch: 31, Batch: 60, Training Loss: 0.002838516980409622, LR: 0.001
Time, 2019-01-07T02:10:51, Epoch: 31, Batch: 70, Training Loss: 0.0038527682423591613, LR: 0.001
Time, 2019-01-07T02:10:52, Epoch: 31, Batch: 80, Training Loss: 0.007742345333099365, LR: 0.001
Time, 2019-01-07T02:10:53, Epoch: 31, Batch: 90, Training Loss: 0.009573477506637573, LR: 0.001
Time, 2019-01-07T02:10:53, Epoch: 31, Batch: 100, Training Loss: 0.001509489119052887, LR: 0.001
Time, 2019-01-07T02:10:54, Epoch: 31, Batch: 110, Training Loss: 0.0036919653415679933, LR: 0.001
Time, 2019-01-07T02:10:55, Epoch: 31, Batch: 120, Training Loss: 0.007653556764125824, LR: 0.001
Time, 2019-01-07T02:10:55, Epoch: 31, Batch: 130, Training Loss: 0.0032394349575042723, LR: 0.001
Time, 2019-01-07T02:10:56, Epoch: 31, Batch: 140, Training Loss: 0.0006168082356452942, LR: 0.001
Time, 2019-01-07T02:10:57, Epoch: 31, Batch: 150, Training Loss: 0.0034012943506240845, LR: 0.001
Time, 2019-01-07T02:10:58, Epoch: 31, Batch: 160, Training Loss: 0.002089656889438629, LR: 0.001
Time, 2019-01-07T02:10:58, Epoch: 31, Batch: 170, Training Loss: 0.0011371992528438567, LR: 0.001
Time, 2019-01-07T02:10:59, Epoch: 31, Batch: 180, Training Loss: 0.0015298008918762207, LR: 0.001
Time, 2019-01-07T02:10:59, Epoch: 31, Batch: 190, Training Loss: 0.003232648968696594, LR: 0.001
Time, 2019-01-07T02:11:00, Epoch: 31, Batch: 200, Training Loss: 0.00168551504611969, LR: 0.001
Time, 2019-01-07T02:11:01, Epoch: 31, Batch: 210, Training Loss: 0.003349360078573227, LR: 0.001
Time, 2019-01-07T02:11:01, Epoch: 31, Batch: 220, Training Loss: 0.0042081035673618315, LR: 0.001
Time, 2019-01-07T02:11:02, Epoch: 31, Batch: 230, Training Loss: 0.0017413169145584107, LR: 0.001
Time, 2019-01-07T02:11:03, Epoch: 31, Batch: 240, Training Loss: 0.001959027349948883, LR: 0.001
Time, 2019-01-07T02:11:03, Epoch: 31, Batch: 250, Training Loss: 0.005796269327402115, LR: 0.001
Time, 2019-01-07T02:11:04, Epoch: 31, Batch: 260, Training Loss: 0.004584388434886932, LR: 0.001
Time, 2019-01-07T02:11:05, Epoch: 31, Batch: 270, Training Loss: 0.006253701448440552, LR: 0.001
Time, 2019-01-07T02:11:05, Epoch: 31, Batch: 280, Training Loss: 0.001857943832874298, LR: 0.001
Time, 2019-01-07T02:11:06, Epoch: 31, Batch: 290, Training Loss: 0.006973766535520553, LR: 0.001
Time, 2019-01-07T02:11:07, Epoch: 31, Batch: 300, Training Loss: 0.0016154766082763672, LR: 0.001
Time, 2019-01-07T02:11:07, Epoch: 31, Batch: 310, Training Loss: 0.011268367618322372, LR: 0.001
Time, 2019-01-07T02:11:08, Epoch: 31, Batch: 320, Training Loss: 0.002786877751350403, LR: 0.001
Time, 2019-01-07T02:11:09, Epoch: 31, Batch: 330, Training Loss: 0.006648138165473938, LR: 0.001
Time, 2019-01-07T02:11:09, Epoch: 31, Batch: 340, Training Loss: 0.0013530731201171875, LR: 0.001
Time, 2019-01-07T02:11:10, Epoch: 31, Batch: 350, Training Loss: 0.002892671525478363, LR: 0.001
Time, 2019-01-07T02:11:11, Epoch: 31, Batch: 360, Training Loss: 0.0013459481298923493, LR: 0.001
Time, 2019-01-07T02:11:11, Epoch: 31, Batch: 370, Training Loss: 0.003312443196773529, LR: 0.001
Time, 2019-01-07T02:11:12, Epoch: 31, Batch: 380, Training Loss: 0.0008085794746875763, LR: 0.001
Time, 2019-01-07T02:11:13, Epoch: 31, Batch: 390, Training Loss: 0.004566131532192231, LR: 0.001
Time, 2019-01-07T02:11:13, Epoch: 31, Batch: 400, Training Loss: 0.00159931480884552, LR: 0.001
Time, 2019-01-07T02:11:14, Epoch: 31, Batch: 410, Training Loss: 0.003045812249183655, LR: 0.001
Time, 2019-01-07T02:11:15, Epoch: 31, Batch: 420, Training Loss: 0.0027208179235458374, LR: 0.001
Time, 2019-01-07T02:11:15, Epoch: 31, Batch: 430, Training Loss: 0.0020928099751472474, LR: 0.001
Time, 2019-01-07T02:11:16, Epoch: 31, Batch: 440, Training Loss: 0.005188795924186707, LR: 0.001
Time, 2019-01-07T02:11:17, Epoch: 31, Batch: 450, Training Loss: 0.00456419363617897, LR: 0.001
Time, 2019-01-07T02:11:17, Epoch: 31, Batch: 460, Training Loss: 0.005731453001499176, LR: 0.001
Time, 2019-01-07T02:11:18, Epoch: 31, Batch: 470, Training Loss: 0.0014053292572498322, LR: 0.001
Time, 2019-01-07T02:11:19, Epoch: 31, Batch: 480, Training Loss: 0.0007435262203216552, LR: 0.001
Time, 2019-01-07T02:11:20, Epoch: 31, Batch: 490, Training Loss: 0.0025225326418876646, LR: 0.001
Time, 2019-01-07T02:11:20, Epoch: 31, Batch: 500, Training Loss: 0.021713386476039886, LR: 0.001
Time, 2019-01-07T02:11:21, Epoch: 31, Batch: 510, Training Loss: 0.0020886659622192383, LR: 0.001
Time, 2019-01-07T02:11:22, Epoch: 31, Batch: 520, Training Loss: 0.0011264339089393615, LR: 0.001
Time, 2019-01-07T02:11:23, Epoch: 31, Batch: 530, Training Loss: 0.0035849571228027343, LR: 0.001
Time, 2019-01-07T02:11:23, Epoch: 31, Batch: 540, Training Loss: 0.006980665773153305, LR: 0.001
Time, 2019-01-07T02:11:24, Epoch: 31, Batch: 550, Training Loss: 0.0036750853061676024, LR: 0.001
Time, 2019-01-07T02:11:25, Epoch: 31, Batch: 560, Training Loss: 0.008050501346588135, LR: 0.001
Time, 2019-01-07T02:11:25, Epoch: 31, Batch: 570, Training Loss: 0.004085944592952728, LR: 0.001
Time, 2019-01-07T02:11:26, Epoch: 31, Batch: 580, Training Loss: 0.002071794867515564, LR: 0.001
Time, 2019-01-07T02:11:27, Epoch: 31, Batch: 590, Training Loss: 0.004219731688499451, LR: 0.001
Time, 2019-01-07T02:11:27, Epoch: 31, Batch: 600, Training Loss: 0.0023973248898983, LR: 0.001
Time, 2019-01-07T02:11:28, Epoch: 31, Batch: 610, Training Loss: 0.0013944759964942932, LR: 0.001
Time, 2019-01-07T02:11:29, Epoch: 31, Batch: 620, Training Loss: 0.004342184960842132, LR: 0.001
Time, 2019-01-07T02:11:29, Epoch: 31, Batch: 630, Training Loss: 0.000764244794845581, LR: 0.001
Time, 2019-01-07T02:11:30, Epoch: 31, Batch: 640, Training Loss: 0.00602874755859375, LR: 0.001
Time, 2019-01-07T02:11:31, Epoch: 31, Batch: 650, Training Loss: 0.014086703956127166, LR: 0.001
Time, 2019-01-07T02:11:31, Epoch: 31, Batch: 660, Training Loss: 0.006643564999103546, LR: 0.001
Time, 2019-01-07T02:11:32, Epoch: 31, Batch: 670, Training Loss: 0.007884718477725983, LR: 0.001
Time, 2019-01-07T02:11:33, Epoch: 31, Batch: 680, Training Loss: 0.012175585329532623, LR: 0.001
Time, 2019-01-07T02:11:33, Epoch: 31, Batch: 690, Training Loss: 0.0027393132448196413, LR: 0.001
Time, 2019-01-07T02:11:34, Epoch: 31, Batch: 700, Training Loss: 0.01033414825797081, LR: 0.001
Time, 2019-01-07T02:11:35, Epoch: 31, Batch: 710, Training Loss: 0.003141556680202484, LR: 0.001
Time, 2019-01-07T02:11:36, Epoch: 31, Batch: 720, Training Loss: 0.0013211145997047424, LR: 0.001
Time, 2019-01-07T02:11:36, Epoch: 31, Batch: 730, Training Loss: 0.01965675726532936, LR: 0.001
Time, 2019-01-07T02:11:37, Epoch: 31, Batch: 740, Training Loss: 0.006321704387664795, LR: 0.001
Time, 2019-01-07T02:11:38, Epoch: 31, Batch: 750, Training Loss: 0.0023046836256980898, LR: 0.001
Time, 2019-01-07T02:11:39, Epoch: 31, Batch: 760, Training Loss: 0.008097659051418304, LR: 0.001
Time, 2019-01-07T02:11:39, Epoch: 31, Batch: 770, Training Loss: 0.009185437858104707, LR: 0.001
Time, 2019-01-07T02:11:40, Epoch: 31, Batch: 780, Training Loss: 0.0109232097864151, LR: 0.001
Time, 2019-01-07T02:11:41, Epoch: 31, Batch: 790, Training Loss: 0.002196146547794342, LR: 0.001
Time, 2019-01-07T02:11:42, Epoch: 31, Batch: 800, Training Loss: 0.0010913103818893433, LR: 0.001
Time, 2019-01-07T02:11:42, Epoch: 31, Batch: 810, Training Loss: 0.011043432354927062, LR: 0.001
Time, 2019-01-07T02:11:43, Epoch: 31, Batch: 820, Training Loss: 0.0027552083134651183, LR: 0.001
Time, 2019-01-07T02:11:43, Epoch: 31, Batch: 830, Training Loss: 0.0031701087951660155, LR: 0.001
Time, 2019-01-07T02:11:44, Epoch: 31, Batch: 840, Training Loss: 0.014807532727718353, LR: 0.001
Time, 2019-01-07T02:11:45, Epoch: 31, Batch: 850, Training Loss: 0.0025697752833366396, LR: 0.001
Time, 2019-01-07T02:11:45, Epoch: 31, Batch: 860, Training Loss: 0.004665844142436981, LR: 0.001
Time, 2019-01-07T02:11:46, Epoch: 31, Batch: 870, Training Loss: 0.0018391266465187072, LR: 0.001
Time, 2019-01-07T02:11:47, Epoch: 31, Batch: 880, Training Loss: 0.0060419395565986635, LR: 0.001
Time, 2019-01-07T02:11:47, Epoch: 31, Batch: 890, Training Loss: 0.0017116308212280274, LR: 0.001
Time, 2019-01-07T02:11:48, Epoch: 31, Batch: 900, Training Loss: 0.006121017038822174, LR: 0.001
Time, 2019-01-07T02:11:48, Epoch: 31, Batch: 910, Training Loss: 0.007964888960123062, LR: 0.001
Time, 2019-01-07T02:11:49, Epoch: 31, Batch: 920, Training Loss: 0.0055674068629741665, LR: 0.001
Time, 2019-01-07T02:11:50, Epoch: 31, Batch: 930, Training Loss: 0.007408562302589417, LR: 0.001
Epoch: 31, Validation Top 1 acc: 98.75597381591797
Epoch: 31, Validation Top 5 acc: 100.0
Epoch: 31, Validation Set Loss: 0.04899057745933533
Start training epoch 32
Time, 2019-01-07T02:11:53, Epoch: 32, Batch: 10, Training Loss: 0.012816140055656433, LR: 0.001
Time, 2019-01-07T02:11:54, Epoch: 32, Batch: 20, Training Loss: 0.0033803507685661316, LR: 0.001
Time, 2019-01-07T02:11:55, Epoch: 32, Batch: 30, Training Loss: 0.0013408198952674865, LR: 0.001
Time, 2019-01-07T02:11:55, Epoch: 32, Batch: 40, Training Loss: 0.0020707935094833375, LR: 0.001
Time, 2019-01-07T02:11:56, Epoch: 32, Batch: 50, Training Loss: 0.005483896285295486, LR: 0.001
Time, 2019-01-07T02:11:57, Epoch: 32, Batch: 60, Training Loss: 0.002418959140777588, LR: 0.001
Time, 2019-01-07T02:11:58, Epoch: 32, Batch: 70, Training Loss: 0.005691716074943542, LR: 0.001
Time, 2019-01-07T02:11:58, Epoch: 32, Batch: 80, Training Loss: 0.003403301537036896, LR: 0.001
Time, 2019-01-07T02:11:59, Epoch: 32, Batch: 90, Training Loss: 0.0009019449353218079, LR: 0.001
Time, 2019-01-07T02:12:00, Epoch: 32, Batch: 100, Training Loss: 0.0011810913681983947, LR: 0.001
Time, 2019-01-07T02:12:00, Epoch: 32, Batch: 110, Training Loss: 0.002025020122528076, LR: 0.001
Time, 2019-01-07T02:12:01, Epoch: 32, Batch: 120, Training Loss: 0.002401691675186157, LR: 0.001
Time, 2019-01-07T02:12:02, Epoch: 32, Batch: 130, Training Loss: 0.0016111671924591065, LR: 0.001
Time, 2019-01-07T02:12:02, Epoch: 32, Batch: 140, Training Loss: 0.0011868879199028015, LR: 0.001
Time, 2019-01-07T02:12:03, Epoch: 32, Batch: 150, Training Loss: 0.0035724133253097535, LR: 0.001
Time, 2019-01-07T02:12:04, Epoch: 32, Batch: 160, Training Loss: 0.005556288361549378, LR: 0.001
Time, 2019-01-07T02:12:04, Epoch: 32, Batch: 170, Training Loss: 0.0025024011731147764, LR: 0.001
Time, 2019-01-07T02:12:05, Epoch: 32, Batch: 180, Training Loss: 0.010435323417186736, LR: 0.001
Time, 2019-01-07T02:12:06, Epoch: 32, Batch: 190, Training Loss: 0.0018119201064109801, LR: 0.001
Time, 2019-01-07T02:12:06, Epoch: 32, Batch: 200, Training Loss: 0.005715751647949218, LR: 0.001
Time, 2019-01-07T02:12:07, Epoch: 32, Batch: 210, Training Loss: 0.0013707280158996582, LR: 0.001
Time, 2019-01-07T02:12:08, Epoch: 32, Batch: 220, Training Loss: 0.0015930168330669404, LR: 0.001
Time, 2019-01-07T02:12:09, Epoch: 32, Batch: 230, Training Loss: 0.00034750401973724363, LR: 0.001
Time, 2019-01-07T02:12:09, Epoch: 32, Batch: 240, Training Loss: 0.0038645520806312563, LR: 0.001
Time, 2019-01-07T02:12:10, Epoch: 32, Batch: 250, Training Loss: 0.001034654676914215, LR: 0.001
Time, 2019-01-07T02:12:11, Epoch: 32, Batch: 260, Training Loss: 0.0018930181860923768, LR: 0.001
Time, 2019-01-07T02:12:12, Epoch: 32, Batch: 270, Training Loss: 0.0008442699909210205, LR: 0.001
Time, 2019-01-07T02:12:12, Epoch: 32, Batch: 280, Training Loss: 0.0035373233258724213, LR: 0.001
Time, 2019-01-07T02:12:13, Epoch: 32, Batch: 290, Training Loss: 0.002561713755130768, LR: 0.001
Time, 2019-01-07T02:12:14, Epoch: 32, Batch: 300, Training Loss: 0.00997706800699234, LR: 0.001
Time, 2019-01-07T02:12:15, Epoch: 32, Batch: 310, Training Loss: 0.0007508739829063416, LR: 0.001
Time, 2019-01-07T02:12:15, Epoch: 32, Batch: 320, Training Loss: 0.023675118386745454, LR: 0.001
Time, 2019-01-07T02:12:16, Epoch: 32, Batch: 330, Training Loss: 0.00839906483888626, LR: 0.001
Time, 2019-01-07T02:12:17, Epoch: 32, Batch: 340, Training Loss: 0.003560368716716766, LR: 0.001
Time, 2019-01-07T02:12:18, Epoch: 32, Batch: 350, Training Loss: 0.007683857530355454, LR: 0.001
Time, 2019-01-07T02:12:19, Epoch: 32, Batch: 360, Training Loss: 0.003155750036239624, LR: 0.001
Time, 2019-01-07T02:12:19, Epoch: 32, Batch: 370, Training Loss: 0.000940924882888794, LR: 0.001
Time, 2019-01-07T02:12:20, Epoch: 32, Batch: 380, Training Loss: 0.0012437641620635987, LR: 0.001
Time, 2019-01-07T02:12:21, Epoch: 32, Batch: 390, Training Loss: 0.00175495445728302, LR: 0.001
Time, 2019-01-07T02:12:21, Epoch: 32, Batch: 400, Training Loss: 0.009103883802890778, LR: 0.001
Time, 2019-01-07T02:12:22, Epoch: 32, Batch: 410, Training Loss: 0.0014216110110282898, LR: 0.001
Time, 2019-01-07T02:12:23, Epoch: 32, Batch: 420, Training Loss: 0.00514262318611145, LR: 0.001
Time, 2019-01-07T02:12:24, Epoch: 32, Batch: 430, Training Loss: 0.0006845131516456604, LR: 0.001
Time, 2019-01-07T02:12:25, Epoch: 32, Batch: 440, Training Loss: 0.0009274661540985107, LR: 0.001
Time, 2019-01-07T02:12:25, Epoch: 32, Batch: 450, Training Loss: 0.001958659291267395, LR: 0.001
Time, 2019-01-07T02:12:26, Epoch: 32, Batch: 460, Training Loss: 0.0052346006035804745, LR: 0.001
Time, 2019-01-07T02:12:27, Epoch: 32, Batch: 470, Training Loss: 0.008816717565059662, LR: 0.001
Time, 2019-01-07T02:12:27, Epoch: 32, Batch: 480, Training Loss: 0.004283173382282257, LR: 0.001
Time, 2019-01-07T02:12:28, Epoch: 32, Batch: 490, Training Loss: 0.007969267666339874, LR: 0.001
Time, 2019-01-07T02:12:29, Epoch: 32, Batch: 500, Training Loss: 0.003205016255378723, LR: 0.001
Time, 2019-01-07T02:12:30, Epoch: 32, Batch: 510, Training Loss: 0.002585524320602417, LR: 0.001
Time, 2019-01-07T02:12:30, Epoch: 32, Batch: 520, Training Loss: 0.004615781456232071, LR: 0.001
Time, 2019-01-07T02:12:31, Epoch: 32, Batch: 530, Training Loss: 0.0034437909722328185, LR: 0.001
Time, 2019-01-07T02:12:32, Epoch: 32, Batch: 540, Training Loss: 0.005500200390815735, LR: 0.001
Time, 2019-01-07T02:12:33, Epoch: 32, Batch: 550, Training Loss: 0.00553382933139801, LR: 0.001
Time, 2019-01-07T02:12:33, Epoch: 32, Batch: 560, Training Loss: 0.0012652039527893066, LR: 0.001
Time, 2019-01-07T02:12:34, Epoch: 32, Batch: 570, Training Loss: 0.012891484797000885, LR: 0.001
Time, 2019-01-07T02:12:35, Epoch: 32, Batch: 580, Training Loss: 0.003282558172941208, LR: 0.001
Time, 2019-01-07T02:12:36, Epoch: 32, Batch: 590, Training Loss: 0.002546118199825287, LR: 0.001
Time, 2019-01-07T02:12:37, Epoch: 32, Batch: 600, Training Loss: 0.0030482605099678038, LR: 0.001
Time, 2019-01-07T02:12:38, Epoch: 32, Batch: 610, Training Loss: 0.005317677557468414, LR: 0.001
Time, 2019-01-07T02:12:38, Epoch: 32, Batch: 620, Training Loss: 0.002679622173309326, LR: 0.001
Time, 2019-01-07T02:12:39, Epoch: 32, Batch: 630, Training Loss: 0.00658348947763443, LR: 0.001
Time, 2019-01-07T02:12:40, Epoch: 32, Batch: 640, Training Loss: 0.004901023209095001, LR: 0.001
Time, 2019-01-07T02:12:40, Epoch: 32, Batch: 650, Training Loss: 0.012729434669017792, LR: 0.001
Time, 2019-01-07T02:12:41, Epoch: 32, Batch: 660, Training Loss: 0.002552909404039383, LR: 0.001
Time, 2019-01-07T02:12:42, Epoch: 32, Batch: 670, Training Loss: 0.005048094689846039, LR: 0.001
Time, 2019-01-07T02:12:42, Epoch: 32, Batch: 680, Training Loss: 0.004601563513278961, LR: 0.001
Time, 2019-01-07T02:12:43, Epoch: 32, Batch: 690, Training Loss: 0.012933529913425446, LR: 0.001
Time, 2019-01-07T02:12:44, Epoch: 32, Batch: 700, Training Loss: 0.006000760942697525, LR: 0.001
Time, 2019-01-07T02:12:44, Epoch: 32, Batch: 710, Training Loss: 0.0018702447414398193, LR: 0.001
Time, 2019-01-07T02:12:45, Epoch: 32, Batch: 720, Training Loss: 0.015080680698156356, LR: 0.001
Time, 2019-01-07T02:12:46, Epoch: 32, Batch: 730, Training Loss: 0.00810648500919342, LR: 0.001
Time, 2019-01-07T02:12:46, Epoch: 32, Batch: 740, Training Loss: 0.004377055168151856, LR: 0.001
Time, 2019-01-07T02:12:47, Epoch: 32, Batch: 750, Training Loss: 0.005828368663787842, LR: 0.001
Time, 2019-01-07T02:12:48, Epoch: 32, Batch: 760, Training Loss: 0.004467071592807769, LR: 0.001
Time, 2019-01-07T02:12:48, Epoch: 32, Batch: 770, Training Loss: 0.0017499655485153198, LR: 0.001
Time, 2019-01-07T02:12:49, Epoch: 32, Batch: 780, Training Loss: 0.0020905159413814544, LR: 0.001
Time, 2019-01-07T02:12:49, Epoch: 32, Batch: 790, Training Loss: 0.002061490714550018, LR: 0.001
Time, 2019-01-07T02:12:50, Epoch: 32, Batch: 800, Training Loss: 0.003454301506280899, LR: 0.001
Time, 2019-01-07T02:12:51, Epoch: 32, Batch: 810, Training Loss: 0.009009797871112824, LR: 0.001
Time, 2019-01-07T02:12:51, Epoch: 32, Batch: 820, Training Loss: 0.002964816987514496, LR: 0.001
Time, 2019-01-07T02:12:52, Epoch: 32, Batch: 830, Training Loss: 0.004082374274730682, LR: 0.001
Time, 2019-01-07T02:12:53, Epoch: 32, Batch: 840, Training Loss: 0.004003992676734925, LR: 0.001
Time, 2019-01-07T02:12:54, Epoch: 32, Batch: 850, Training Loss: 0.008178818225860595, LR: 0.001
Time, 2019-01-07T02:12:54, Epoch: 32, Batch: 860, Training Loss: 0.005912676453590393, LR: 0.001
Time, 2019-01-07T02:12:55, Epoch: 32, Batch: 870, Training Loss: 0.004146064817905426, LR: 0.001
Time, 2019-01-07T02:12:55, Epoch: 32, Batch: 880, Training Loss: 0.015285538882017136, LR: 0.001
Time, 2019-01-07T02:12:56, Epoch: 32, Batch: 890, Training Loss: 0.006249482929706574, LR: 0.001
Time, 2019-01-07T02:12:57, Epoch: 32, Batch: 900, Training Loss: 0.024370310455560686, LR: 0.001
Time, 2019-01-07T02:12:57, Epoch: 32, Batch: 910, Training Loss: 0.004220016300678253, LR: 0.001
Time, 2019-01-07T02:12:58, Epoch: 32, Batch: 920, Training Loss: 0.00790717378258705, LR: 0.001
Time, 2019-01-07T02:12:59, Epoch: 32, Batch: 930, Training Loss: 0.02023985981941223, LR: 0.001
Epoch: 32, Validation Top 1 acc: 98.65644836425781
Epoch: 32, Validation Top 5 acc: 99.98009490966797
Epoch: 32, Validation Set Loss: 0.05389490723609924
Start training epoch 33
Time, 2019-01-07T02:13:02, Epoch: 33, Batch: 10, Training Loss: 0.006301844865083695, LR: 0.0001
Time, 2019-01-07T02:13:02, Epoch: 33, Batch: 20, Training Loss: 0.008304189145565032, LR: 0.0001
Time, 2019-01-07T02:13:03, Epoch: 33, Batch: 30, Training Loss: 0.004317352175712585, LR: 0.0001
Time, 2019-01-07T02:13:04, Epoch: 33, Batch: 40, Training Loss: 0.007213936001062393, LR: 0.0001
Time, 2019-01-07T02:13:04, Epoch: 33, Batch: 50, Training Loss: 0.008086954802274704, LR: 0.0001
Time, 2019-01-07T02:13:05, Epoch: 33, Batch: 60, Training Loss: 0.00878988802433014, LR: 0.0001
Time, 2019-01-07T02:13:06, Epoch: 33, Batch: 70, Training Loss: 0.004213480651378632, LR: 0.0001
Time, 2019-01-07T02:13:06, Epoch: 33, Batch: 80, Training Loss: 0.0038864582777023314, LR: 0.0001
Time, 2019-01-07T02:13:07, Epoch: 33, Batch: 90, Training Loss: 0.0036232024431228638, LR: 0.0001
Time, 2019-01-07T02:13:07, Epoch: 33, Batch: 100, Training Loss: 0.015218795090913773, LR: 0.0001
Time, 2019-01-07T02:13:08, Epoch: 33, Batch: 110, Training Loss: 0.004500836133956909, LR: 0.0001
Time, 2019-01-07T02:13:09, Epoch: 33, Batch: 120, Training Loss: 0.001207686960697174, LR: 0.0001
Time, 2019-01-07T02:13:09, Epoch: 33, Batch: 130, Training Loss: 0.006098709255456925, LR: 0.0001
Time, 2019-01-07T02:13:10, Epoch: 33, Batch: 140, Training Loss: 0.0011025309562683106, LR: 0.0001
Time, 2019-01-07T02:13:11, Epoch: 33, Batch: 150, Training Loss: 0.0017924875020980835, LR: 0.0001
Time, 2019-01-07T02:13:11, Epoch: 33, Batch: 160, Training Loss: 0.0014422595500946044, LR: 0.0001
Time, 2019-01-07T02:13:12, Epoch: 33, Batch: 170, Training Loss: 0.004783480614423752, LR: 0.0001
Time, 2019-01-07T02:13:13, Epoch: 33, Batch: 180, Training Loss: 0.0031898781657218934, LR: 0.0001
Time, 2019-01-07T02:13:13, Epoch: 33, Batch: 190, Training Loss: 0.0005774781107902527, LR: 0.0001
Time, 2019-01-07T02:13:14, Epoch: 33, Batch: 200, Training Loss: 0.0013037353754043579, LR: 0.0001
Time, 2019-01-07T02:13:15, Epoch: 33, Batch: 210, Training Loss: 0.0032161757349967956, LR: 0.0001
Time, 2019-01-07T02:13:16, Epoch: 33, Batch: 220, Training Loss: 0.0023308686912059786, LR: 0.0001
Time, 2019-01-07T02:13:16, Epoch: 33, Batch: 230, Training Loss: 0.0037979312241077424, LR: 0.0001
Time, 2019-01-07T02:13:17, Epoch: 33, Batch: 240, Training Loss: 0.0022338926792144775, LR: 0.0001
Time, 2019-01-07T02:13:17, Epoch: 33, Batch: 250, Training Loss: 0.0012047171592712402, LR: 0.0001
Time, 2019-01-07T02:13:18, Epoch: 33, Batch: 260, Training Loss: 0.0018705114722251891, LR: 0.0001
Time, 2019-01-07T02:13:19, Epoch: 33, Batch: 270, Training Loss: 0.0030880659818649293, LR: 0.0001
Time, 2019-01-07T02:13:19, Epoch: 33, Batch: 280, Training Loss: 0.0074198290705680845, LR: 0.0001
Time, 2019-01-07T02:13:20, Epoch: 33, Batch: 290, Training Loss: 0.003805822879076004, LR: 0.0001
Time, 2019-01-07T02:13:21, Epoch: 33, Batch: 300, Training Loss: 0.005587555468082428, LR: 0.0001
Time, 2019-01-07T02:13:21, Epoch: 33, Batch: 310, Training Loss: 0.003088594973087311, LR: 0.0001
Time, 2019-01-07T02:13:22, Epoch: 33, Batch: 320, Training Loss: 0.0007132180035114288, LR: 0.0001
Time, 2019-01-07T02:13:23, Epoch: 33, Batch: 330, Training Loss: 0.0021920345723629, LR: 0.0001
Time, 2019-01-07T02:13:24, Epoch: 33, Batch: 340, Training Loss: 0.01232553943991661, LR: 0.0001
Time, 2019-01-07T02:13:24, Epoch: 33, Batch: 350, Training Loss: 0.0064030222594738005, LR: 0.0001
Time, 2019-01-07T02:13:25, Epoch: 33, Batch: 360, Training Loss: 0.0026778548955917357, LR: 0.0001
Time, 2019-01-07T02:13:26, Epoch: 33, Batch: 370, Training Loss: 0.003485695272684097, LR: 0.0001
Time, 2019-01-07T02:13:26, Epoch: 33, Batch: 380, Training Loss: 0.005581901222467422, LR: 0.0001
Time, 2019-01-07T02:13:27, Epoch: 33, Batch: 390, Training Loss: 0.0032839886844158173, LR: 0.0001
Time, 2019-01-07T02:13:27, Epoch: 33, Batch: 400, Training Loss: 0.0012122303247451781, LR: 0.0001
Time, 2019-01-07T02:13:28, Epoch: 33, Batch: 410, Training Loss: 0.002290661633014679, LR: 0.0001
Time, 2019-01-07T02:13:29, Epoch: 33, Batch: 420, Training Loss: 0.004965906590223312, LR: 0.0001
Time, 2019-01-07T02:13:30, Epoch: 33, Batch: 430, Training Loss: 0.0035659149289131165, LR: 0.0001
Time, 2019-01-07T02:13:31, Epoch: 33, Batch: 440, Training Loss: 0.0015345476567745209, LR: 0.0001
Time, 2019-01-07T02:13:31, Epoch: 33, Batch: 450, Training Loss: 0.0013028167188167572, LR: 0.0001
Time, 2019-01-07T02:13:32, Epoch: 33, Batch: 460, Training Loss: 0.009367992728948593, LR: 0.0001
Time, 2019-01-07T02:13:33, Epoch: 33, Batch: 470, Training Loss: 0.0070428669452667235, LR: 0.0001
Time, 2019-01-07T02:13:33, Epoch: 33, Batch: 480, Training Loss: 0.0013815850019454957, LR: 0.0001
Time, 2019-01-07T02:13:34, Epoch: 33, Batch: 490, Training Loss: 0.001483367383480072, LR: 0.0001
Time, 2019-01-07T02:13:35, Epoch: 33, Batch: 500, Training Loss: 0.000799781084060669, LR: 0.0001
Time, 2019-01-07T02:13:36, Epoch: 33, Batch: 510, Training Loss: 0.0077096611261367794, LR: 0.0001
Time, 2019-01-07T02:13:37, Epoch: 33, Batch: 520, Training Loss: 0.0009220257401466369, LR: 0.0001
Time, 2019-01-07T02:13:38, Epoch: 33, Batch: 530, Training Loss: 0.010246454179286957, LR: 0.0001
Time, 2019-01-07T02:13:38, Epoch: 33, Batch: 540, Training Loss: 0.002955155074596405, LR: 0.0001
Time, 2019-01-07T02:13:39, Epoch: 33, Batch: 550, Training Loss: 0.0010164886713027953, LR: 0.0001
Time, 2019-01-07T02:13:40, Epoch: 33, Batch: 560, Training Loss: 0.0008404850959777832, LR: 0.0001
Time, 2019-01-07T02:13:40, Epoch: 33, Batch: 570, Training Loss: 0.0005405217409133912, LR: 0.0001
Time, 2019-01-07T02:13:41, Epoch: 33, Batch: 580, Training Loss: 0.00745849609375, LR: 0.0001
Time, 2019-01-07T02:13:42, Epoch: 33, Batch: 590, Training Loss: 0.0009761884808540344, LR: 0.0001
Time, 2019-01-07T02:13:42, Epoch: 33, Batch: 600, Training Loss: 0.0035883471369743346, LR: 0.0001
Time, 2019-01-07T02:13:43, Epoch: 33, Batch: 610, Training Loss: 0.0010653004050254823, LR: 0.0001
Time, 2019-01-07T02:13:43, Epoch: 33, Batch: 620, Training Loss: 0.00580633133649826, LR: 0.0001
Time, 2019-01-07T02:13:44, Epoch: 33, Batch: 630, Training Loss: 0.0013504937291145326, LR: 0.0001
Time, 2019-01-07T02:13:45, Epoch: 33, Batch: 640, Training Loss: 0.0019721433520317076, LR: 0.0001
Time, 2019-01-07T02:13:45, Epoch: 33, Batch: 650, Training Loss: 0.003319171071052551, LR: 0.0001
Time, 2019-01-07T02:13:46, Epoch: 33, Batch: 660, Training Loss: 0.001465088129043579, LR: 0.0001
Time, 2019-01-07T02:13:47, Epoch: 33, Batch: 670, Training Loss: 0.0076760433614254, LR: 0.0001
Time, 2019-01-07T02:13:47, Epoch: 33, Batch: 680, Training Loss: 0.0030483290553092956, LR: 0.0001
Time, 2019-01-07T02:13:48, Epoch: 33, Batch: 690, Training Loss: 0.00034051313996315004, LR: 0.0001
Time, 2019-01-07T02:13:49, Epoch: 33, Batch: 700, Training Loss: 0.0021824315190315247, LR: 0.0001
Time, 2019-01-07T02:13:49, Epoch: 33, Batch: 710, Training Loss: 0.0021650493144989014, LR: 0.0001
Time, 2019-01-07T02:13:50, Epoch: 33, Batch: 720, Training Loss: 0.0015434011816978454, LR: 0.0001
Time, 2019-01-07T02:13:51, Epoch: 33, Batch: 730, Training Loss: 0.0024678103625774385, LR: 0.0001
Time, 2019-01-07T02:13:51, Epoch: 33, Batch: 740, Training Loss: 0.002736055850982666, LR: 0.0001
Time, 2019-01-07T02:13:52, Epoch: 33, Batch: 750, Training Loss: 0.0009730547666549682, LR: 0.0001
Time, 2019-01-07T02:13:53, Epoch: 33, Batch: 760, Training Loss: 0.0014247074723243713, LR: 0.0001
Time, 2019-01-07T02:13:53, Epoch: 33, Batch: 770, Training Loss: 0.002218494564294815, LR: 0.0001
Time, 2019-01-07T02:13:54, Epoch: 33, Batch: 780, Training Loss: 0.009867957979440688, LR: 0.0001
Time, 2019-01-07T02:13:55, Epoch: 33, Batch: 790, Training Loss: 0.00035570114850997926, LR: 0.0001
Time, 2019-01-07T02:13:55, Epoch: 33, Batch: 800, Training Loss: 0.0017472609877586364, LR: 0.0001
Time, 2019-01-07T02:13:56, Epoch: 33, Batch: 810, Training Loss: 0.006911510974168778, LR: 0.0001
Time, 2019-01-07T02:13:57, Epoch: 33, Batch: 820, Training Loss: 0.0006201133131980896, LR: 0.0001
Time, 2019-01-07T02:13:57, Epoch: 33, Batch: 830, Training Loss: 0.005705296993255615, LR: 0.0001
Time, 2019-01-07T02:13:58, Epoch: 33, Batch: 840, Training Loss: 0.0024339944124221803, LR: 0.0001
Time, 2019-01-07T02:13:58, Epoch: 33, Batch: 850, Training Loss: 0.0007125735282897949, LR: 0.0001
Time, 2019-01-07T02:13:59, Epoch: 33, Batch: 860, Training Loss: 0.0015128687024116515, LR: 0.0001
Time, 2019-01-07T02:14:00, Epoch: 33, Batch: 870, Training Loss: 0.0019899338483810427, LR: 0.0001
Time, 2019-01-07T02:14:01, Epoch: 33, Batch: 880, Training Loss: 0.004337283968925476, LR: 0.0001
Time, 2019-01-07T02:14:01, Epoch: 33, Batch: 890, Training Loss: 0.0014727786183357239, LR: 0.0001
Time, 2019-01-07T02:14:02, Epoch: 33, Batch: 900, Training Loss: 0.0021687716245651243, LR: 0.0001
Time, 2019-01-07T02:14:02, Epoch: 33, Batch: 910, Training Loss: 0.001054491102695465, LR: 0.0001
Time, 2019-01-07T02:14:03, Epoch: 33, Batch: 920, Training Loss: 0.0009833574295043945, LR: 0.0001
Time, 2019-01-07T02:14:04, Epoch: 33, Batch: 930, Training Loss: 0.0025368332862854004, LR: 0.0001
Epoch: 33, Validation Top 1 acc: 98.90525817871094
Epoch: 33, Validation Top 5 acc: 100.0
Epoch: 33, Validation Set Loss: 0.04142894223332405
Start training epoch 34
Time, 2019-01-07T02:14:07, Epoch: 34, Batch: 10, Training Loss: 0.0004939883947372437, LR: 0.0001
Time, 2019-01-07T02:14:08, Epoch: 34, Batch: 20, Training Loss: 0.0008375048637390137, LR: 0.0001
Time, 2019-01-07T02:14:08, Epoch: 34, Batch: 30, Training Loss: 0.0006983071565628052, LR: 0.0001
Time, 2019-01-07T02:14:09, Epoch: 34, Batch: 40, Training Loss: 0.004933099448680878, LR: 0.0001
Time, 2019-01-07T02:14:09, Epoch: 34, Batch: 50, Training Loss: 0.0005377024412155151, LR: 0.0001
Time, 2019-01-07T02:14:10, Epoch: 34, Batch: 60, Training Loss: 0.0008507236838340759, LR: 0.0001
Time, 2019-01-07T02:14:11, Epoch: 34, Batch: 70, Training Loss: 0.000624401867389679, LR: 0.0001
Time, 2019-01-07T02:14:11, Epoch: 34, Batch: 80, Training Loss: 0.0010897114872932435, LR: 0.0001
Time, 2019-01-07T02:14:12, Epoch: 34, Batch: 90, Training Loss: 0.001474897563457489, LR: 0.0001
Time, 2019-01-07T02:14:13, Epoch: 34, Batch: 100, Training Loss: 0.001558460295200348, LR: 0.0001
Time, 2019-01-07T02:14:13, Epoch: 34, Batch: 110, Training Loss: 0.0005124375224113464, LR: 0.0001
Time, 2019-01-07T02:14:14, Epoch: 34, Batch: 120, Training Loss: 0.0006632566452026367, LR: 0.0001
Time, 2019-01-07T02:14:15, Epoch: 34, Batch: 130, Training Loss: 0.0008663713932037353, LR: 0.0001
Time, 2019-01-07T02:14:15, Epoch: 34, Batch: 140, Training Loss: 0.0008700922131538391, LR: 0.0001
Time, 2019-01-07T02:14:16, Epoch: 34, Batch: 150, Training Loss: 0.0009478762745857239, LR: 0.0001
Time, 2019-01-07T02:14:17, Epoch: 34, Batch: 160, Training Loss: 0.0011557236313819886, LR: 0.0001
Time, 2019-01-07T02:14:17, Epoch: 34, Batch: 170, Training Loss: 0.001143646240234375, LR: 0.0001
Time, 2019-01-07T02:14:18, Epoch: 34, Batch: 180, Training Loss: 0.0005799070000648499, LR: 0.0001
Time, 2019-01-07T02:14:19, Epoch: 34, Batch: 190, Training Loss: 0.0018491238355636598, LR: 0.0001
Time, 2019-01-07T02:14:20, Epoch: 34, Batch: 200, Training Loss: 0.0015882499516010284, LR: 0.0001
Time, 2019-01-07T02:14:20, Epoch: 34, Batch: 210, Training Loss: 0.0007162466645240784, LR: 0.0001
Time, 2019-01-07T02:14:21, Epoch: 34, Batch: 220, Training Loss: 0.0008135512471199036, LR: 0.0001
Time, 2019-01-07T02:14:22, Epoch: 34, Batch: 230, Training Loss: 0.0010242164134979248, LR: 0.0001
Time, 2019-01-07T02:14:22, Epoch: 34, Batch: 240, Training Loss: 0.00034826844930648806, LR: 0.0001
Time, 2019-01-07T02:14:23, Epoch: 34, Batch: 250, Training Loss: 0.00040120333433151244, LR: 0.0001
Time, 2019-01-07T02:14:24, Epoch: 34, Batch: 260, Training Loss: 0.0007790446281433106, LR: 0.0001
Time, 2019-01-07T02:14:25, Epoch: 34, Batch: 270, Training Loss: 0.0003069952130317688, LR: 0.0001
Time, 2019-01-07T02:14:25, Epoch: 34, Batch: 280, Training Loss: 0.0006002426147460937, LR: 0.0001
Time, 2019-01-07T02:14:26, Epoch: 34, Batch: 290, Training Loss: 0.00029681622982025146, LR: 0.0001
Time, 2019-01-07T02:14:27, Epoch: 34, Batch: 300, Training Loss: 0.00026940107345581056, LR: 0.0001
Time, 2019-01-07T02:14:27, Epoch: 34, Batch: 310, Training Loss: 0.0006829306483268737, LR: 0.0001
Time, 2019-01-07T02:14:28, Epoch: 34, Batch: 320, Training Loss: 0.0009635686874389648, LR: 0.0001
Time, 2019-01-07T02:14:29, Epoch: 34, Batch: 330, Training Loss: 0.0006569162011146546, LR: 0.0001
Time, 2019-01-07T02:14:29, Epoch: 34, Batch: 340, Training Loss: 0.0009991511702537536, LR: 0.0001
Time, 2019-01-07T02:14:30, Epoch: 34, Batch: 350, Training Loss: 0.0018590718507766723, LR: 0.0001
Time, 2019-01-07T02:14:31, Epoch: 34, Batch: 360, Training Loss: 0.0016839295625686646, LR: 0.0001
Time, 2019-01-07T02:14:31, Epoch: 34, Batch: 370, Training Loss: 0.0018009528517723083, LR: 0.0001
Time, 2019-01-07T02:14:32, Epoch: 34, Batch: 380, Training Loss: 0.0011433705687522889, LR: 0.0001
Time, 2019-01-07T02:14:33, Epoch: 34, Batch: 390, Training Loss: 0.0005167782306671143, LR: 0.0001
Time, 2019-01-07T02:14:34, Epoch: 34, Batch: 400, Training Loss: 0.0012349747121334076, LR: 0.0001
Time, 2019-01-07T02:14:34, Epoch: 34, Batch: 410, Training Loss: 0.00021183788776397706, LR: 0.0001
Time, 2019-01-07T02:14:35, Epoch: 34, Batch: 420, Training Loss: 0.0006960347294807434, LR: 0.0001
Time, 2019-01-07T02:14:36, Epoch: 34, Batch: 430, Training Loss: 0.0006288528442382812, LR: 0.0001
Time, 2019-01-07T02:14:36, Epoch: 34, Batch: 440, Training Loss: 0.0007553651928901672, LR: 0.0001
Time, 2019-01-07T02:14:37, Epoch: 34, Batch: 450, Training Loss: 0.0011205583810806274, LR: 0.0001
Time, 2019-01-07T02:14:37, Epoch: 34, Batch: 460, Training Loss: 0.001395547389984131, LR: 0.0001
Time, 2019-01-07T02:14:38, Epoch: 34, Batch: 470, Training Loss: 0.0011636480689048768, LR: 0.0001
Time, 2019-01-07T02:14:39, Epoch: 34, Batch: 480, Training Loss: 0.0006035804748535156, LR: 0.0001
Time, 2019-01-07T02:14:40, Epoch: 34, Batch: 490, Training Loss: 0.002797745168209076, LR: 0.0001
Time, 2019-01-07T02:14:41, Epoch: 34, Batch: 500, Training Loss: 0.0004226356744766235, LR: 0.0001
Time, 2019-01-07T02:14:41, Epoch: 34, Batch: 510, Training Loss: 0.0005841732025146484, LR: 0.0001
Time, 2019-01-07T02:14:42, Epoch: 34, Batch: 520, Training Loss: 0.00046427398920059206, LR: 0.0001
Time, 2019-01-07T02:14:43, Epoch: 34, Batch: 530, Training Loss: 0.0007803410291671753, LR: 0.0001
Time, 2019-01-07T02:14:43, Epoch: 34, Batch: 540, Training Loss: 0.001479928195476532, LR: 0.0001
Time, 2019-01-07T02:14:44, Epoch: 34, Batch: 550, Training Loss: 0.0009007692337036133, LR: 0.0001
Time, 2019-01-07T02:14:44, Epoch: 34, Batch: 560, Training Loss: 0.0010478481650352477, LR: 0.0001
Time, 2019-01-07T02:14:45, Epoch: 34, Batch: 570, Training Loss: 0.0004894688725471496, LR: 0.0001
Time, 2019-01-07T02:14:46, Epoch: 34, Batch: 580, Training Loss: 0.0006920874118804931, LR: 0.0001
Time, 2019-01-07T02:14:46, Epoch: 34, Batch: 590, Training Loss: 0.003871433436870575, LR: 0.0001
Time, 2019-01-07T02:14:47, Epoch: 34, Batch: 600, Training Loss: 0.0011833876371383667, LR: 0.0001
Time, 2019-01-07T02:14:47, Epoch: 34, Batch: 610, Training Loss: 0.0009617939591407776, LR: 0.0001
Time, 2019-01-07T02:14:48, Epoch: 34, Batch: 620, Training Loss: 0.0017558097839355468, LR: 0.0001
Time, 2019-01-07T02:14:49, Epoch: 34, Batch: 630, Training Loss: 0.0005153819918632507, LR: 0.0001
Time, 2019-01-07T02:14:49, Epoch: 34, Batch: 640, Training Loss: 0.0009122900664806366, LR: 0.0001
Time, 2019-01-07T02:14:50, Epoch: 34, Batch: 650, Training Loss: 0.00035714805126190184, LR: 0.0001
Time, 2019-01-07T02:14:51, Epoch: 34, Batch: 660, Training Loss: 0.0006824389100074768, LR: 0.0001
Time, 2019-01-07T02:14:51, Epoch: 34, Batch: 670, Training Loss: 0.002673894166946411, LR: 0.0001
Time, 2019-01-07T02:14:52, Epoch: 34, Batch: 680, Training Loss: 0.0008868813514709472, LR: 0.0001
Time, 2019-01-07T02:14:52, Epoch: 34, Batch: 690, Training Loss: 0.0006504133343696594, LR: 0.0001
Time, 2019-01-07T02:14:53, Epoch: 34, Batch: 700, Training Loss: 0.0004680156707763672, LR: 0.0001
Time, 2019-01-07T02:14:54, Epoch: 34, Batch: 710, Training Loss: 0.00040788799524307253, LR: 0.0001
Time, 2019-01-07T02:14:54, Epoch: 34, Batch: 720, Training Loss: 0.0012151941657066346, LR: 0.0001
Time, 2019-01-07T02:14:55, Epoch: 34, Batch: 730, Training Loss: 0.0004204615950584412, LR: 0.0001
Time, 2019-01-07T02:14:56, Epoch: 34, Batch: 740, Training Loss: 0.001395389437675476, LR: 0.0001
Time, 2019-01-07T02:14:56, Epoch: 34, Batch: 750, Training Loss: 0.000678013265132904, LR: 0.0001
Time, 2019-01-07T02:14:57, Epoch: 34, Batch: 760, Training Loss: 0.0005164563655853272, LR: 0.0001
Time, 2019-01-07T02:14:57, Epoch: 34, Batch: 770, Training Loss: 0.0005765527486801148, LR: 0.0001
Time, 2019-01-07T02:14:58, Epoch: 34, Batch: 780, Training Loss: 0.0005241110920906067, LR: 0.0001
Time, 2019-01-07T02:14:59, Epoch: 34, Batch: 790, Training Loss: 0.0020875677466392515, LR: 0.0001
Time, 2019-01-07T02:14:59, Epoch: 34, Batch: 800, Training Loss: 0.0006016969680786132, LR: 0.0001
Time, 2019-01-07T02:15:00, Epoch: 34, Batch: 810, Training Loss: 0.0005527585744857788, LR: 0.0001
Time, 2019-01-07T02:15:01, Epoch: 34, Batch: 820, Training Loss: 0.0009206041693687439, LR: 0.0001
Time, 2019-01-07T02:15:02, Epoch: 34, Batch: 830, Training Loss: 0.0002283141016960144, LR: 0.0001
Time, 2019-01-07T02:15:02, Epoch: 34, Batch: 840, Training Loss: 0.0031933657824993133, LR: 0.0001
Time, 2019-01-07T02:15:03, Epoch: 34, Batch: 850, Training Loss: 0.0006200656294822693, LR: 0.0001
Time, 2019-01-07T02:15:04, Epoch: 34, Batch: 860, Training Loss: 0.0008340880274772644, LR: 0.0001
Time, 2019-01-07T02:15:04, Epoch: 34, Batch: 870, Training Loss: 0.0006479725241661071, LR: 0.0001
Time, 2019-01-07T02:15:05, Epoch: 34, Batch: 880, Training Loss: 0.001510186493396759, LR: 0.0001
Time, 2019-01-07T02:15:06, Epoch: 34, Batch: 890, Training Loss: 0.001304522156715393, LR: 0.0001
Time, 2019-01-07T02:15:06, Epoch: 34, Batch: 900, Training Loss: 0.001869341731071472, LR: 0.0001
Time, 2019-01-07T02:15:07, Epoch: 34, Batch: 910, Training Loss: 0.0007010847330093384, LR: 0.0001
Time, 2019-01-07T02:15:07, Epoch: 34, Batch: 920, Training Loss: 0.0002767354249954224, LR: 0.0001
Time, 2019-01-07T02:15:08, Epoch: 34, Batch: 930, Training Loss: 0.0014378815889358521, LR: 0.0001
Epoch: 34, Validation Top 1 acc: 98.89530181884766
Epoch: 34, Validation Top 5 acc: 100.0
Epoch: 34, Validation Set Loss: 0.04188399389386177
Start training epoch 35
Time, 2019-01-07T02:15:11, Epoch: 35, Batch: 10, Training Loss: 0.000654110312461853, LR: 0.0001
Time, 2019-01-07T02:15:12, Epoch: 35, Batch: 20, Training Loss: 0.00046045184135437014, LR: 0.0001
Time, 2019-01-07T02:15:13, Epoch: 35, Batch: 30, Training Loss: 0.0006603509187698364, LR: 0.0001
Time, 2019-01-07T02:15:13, Epoch: 35, Batch: 40, Training Loss: 0.0007333680987358093, LR: 0.0001
Time, 2019-01-07T02:15:14, Epoch: 35, Batch: 50, Training Loss: 0.0007309690117835999, LR: 0.0001
Time, 2019-01-07T02:15:15, Epoch: 35, Batch: 60, Training Loss: 0.0004954800009727478, LR: 0.0001
Time, 2019-01-07T02:15:15, Epoch: 35, Batch: 70, Training Loss: 0.0005129560828208924, LR: 0.0001
Time, 2019-01-07T02:15:16, Epoch: 35, Batch: 80, Training Loss: 0.00047874301671981814, LR: 0.0001
Time, 2019-01-07T02:15:17, Epoch: 35, Batch: 90, Training Loss: 0.0005133032798767089, LR: 0.0001
Time, 2019-01-07T02:15:17, Epoch: 35, Batch: 100, Training Loss: 0.00037301331758499146, LR: 0.0001
Time, 2019-01-07T02:15:18, Epoch: 35, Batch: 110, Training Loss: 0.0006886556744575501, LR: 0.0001
Time, 2019-01-07T02:15:19, Epoch: 35, Batch: 120, Training Loss: 0.0003964737057685852, LR: 0.0001
Time, 2019-01-07T02:15:19, Epoch: 35, Batch: 130, Training Loss: 0.0032632529735565187, LR: 0.0001
Time, 2019-01-07T02:15:20, Epoch: 35, Batch: 140, Training Loss: 0.0005728378891944885, LR: 0.0001
Time, 2019-01-07T02:15:21, Epoch: 35, Batch: 150, Training Loss: 0.0007206425070762634, LR: 0.0001
Time, 2019-01-07T02:15:21, Epoch: 35, Batch: 160, Training Loss: 0.0005051851272583008, LR: 0.0001
Time, 2019-01-07T02:15:22, Epoch: 35, Batch: 170, Training Loss: 0.0005636334419250489, LR: 0.0001
Time, 2019-01-07T02:15:23, Epoch: 35, Batch: 180, Training Loss: 0.0004958391189575195, LR: 0.0001
Time, 2019-01-07T02:15:23, Epoch: 35, Batch: 190, Training Loss: 0.0007107138633728027, LR: 0.0001
Time, 2019-01-07T02:15:24, Epoch: 35, Batch: 200, Training Loss: 0.0005027994513511658, LR: 0.0001
Time, 2019-01-07T02:15:25, Epoch: 35, Batch: 210, Training Loss: 0.00029672831296920774, LR: 0.0001
Time, 2019-01-07T02:15:25, Epoch: 35, Batch: 220, Training Loss: 0.0011256635189056396, LR: 0.0001
Time, 2019-01-07T02:15:26, Epoch: 35, Batch: 230, Training Loss: 0.00022336989641189576, LR: 0.0001
Time, 2019-01-07T02:15:27, Epoch: 35, Batch: 240, Training Loss: 0.00028454959392547606, LR: 0.0001
Time, 2019-01-07T02:15:27, Epoch: 35, Batch: 250, Training Loss: 0.0003561481833457947, LR: 0.0001
Time, 2019-01-07T02:15:28, Epoch: 35, Batch: 260, Training Loss: 0.0005824163556098938, LR: 0.0001
Time, 2019-01-07T02:15:29, Epoch: 35, Batch: 270, Training Loss: 0.0006391212344169616, LR: 0.0001
Time, 2019-01-07T02:15:29, Epoch: 35, Batch: 280, Training Loss: 0.000829024612903595, LR: 0.0001
Time, 2019-01-07T02:15:30, Epoch: 35, Batch: 290, Training Loss: 0.000581371784210205, LR: 0.0001
Time, 2019-01-07T02:15:31, Epoch: 35, Batch: 300, Training Loss: 0.00044322460889816283, LR: 0.0001
Time, 2019-01-07T02:15:31, Epoch: 35, Batch: 310, Training Loss: 0.0007044389843940735, LR: 0.0001
Time, 2019-01-07T02:15:32, Epoch: 35, Batch: 320, Training Loss: 0.0007341235876083374, LR: 0.0001
Time, 2019-01-07T02:15:33, Epoch: 35, Batch: 330, Training Loss: 0.0005047380924224854, LR: 0.0001
Time, 2019-01-07T02:15:34, Epoch: 35, Batch: 340, Training Loss: 0.0005434334278106689, LR: 0.0001
Time, 2019-01-07T02:15:34, Epoch: 35, Batch: 350, Training Loss: 0.0011837810277938842, LR: 0.0001
Time, 2019-01-07T02:15:35, Epoch: 35, Batch: 360, Training Loss: 0.0005541712045669556, LR: 0.0001
Time, 2019-01-07T02:15:36, Epoch: 35, Batch: 370, Training Loss: 0.0009307786822319031, LR: 0.0001
Time, 2019-01-07T02:15:36, Epoch: 35, Batch: 380, Training Loss: 0.0025859951972961428, LR: 0.0001
Time, 2019-01-07T02:15:37, Epoch: 35, Batch: 390, Training Loss: 0.0012322202324867248, LR: 0.0001
Time, 2019-01-07T02:15:38, Epoch: 35, Batch: 400, Training Loss: 0.00022599101066589355, LR: 0.0001
Time, 2019-01-07T02:15:38, Epoch: 35, Batch: 410, Training Loss: 0.0004176527261734009, LR: 0.0001
Time, 2019-01-07T02:15:39, Epoch: 35, Batch: 420, Training Loss: 0.0012470975518226624, LR: 0.0001
Time, 2019-01-07T02:15:40, Epoch: 35, Batch: 430, Training Loss: 0.0011415496468544007, LR: 0.0001
Time, 2019-01-07T02:15:40, Epoch: 35, Batch: 440, Training Loss: 0.00033469349145889284, LR: 0.0001
Time, 2019-01-07T02:15:41, Epoch: 35, Batch: 450, Training Loss: 0.00037339329719543457, LR: 0.0001
Time, 2019-01-07T02:15:42, Epoch: 35, Batch: 460, Training Loss: 0.0008673876523971558, LR: 0.0001
Time, 2019-01-07T02:15:42, Epoch: 35, Batch: 470, Training Loss: 0.0010753124952316284, LR: 0.0001
Time, 2019-01-07T02:15:43, Epoch: 35, Batch: 480, Training Loss: 0.00022271126508712768, LR: 0.0001
Time, 2019-01-07T02:15:43, Epoch: 35, Batch: 490, Training Loss: 0.0007126465439796448, LR: 0.0001
Time, 2019-01-07T02:15:44, Epoch: 35, Batch: 500, Training Loss: 0.0001980319619178772, LR: 0.0001
Time, 2019-01-07T02:15:45, Epoch: 35, Batch: 510, Training Loss: 0.0003758549690246582, LR: 0.0001
Time, 2019-01-07T02:15:45, Epoch: 35, Batch: 520, Training Loss: 0.0003283843398094177, LR: 0.0001
Time, 2019-01-07T02:15:46, Epoch: 35, Batch: 530, Training Loss: 0.0005709394812583923, LR: 0.0001
Time, 2019-01-07T02:15:47, Epoch: 35, Batch: 540, Training Loss: 0.0006828293204307556, LR: 0.0001
Time, 2019-01-07T02:15:47, Epoch: 35, Batch: 550, Training Loss: 0.00034948587417602537, LR: 0.0001
Time, 2019-01-07T02:15:48, Epoch: 35, Batch: 560, Training Loss: 0.0004900708794593811, LR: 0.0001
Time, 2019-01-07T02:15:49, Epoch: 35, Batch: 570, Training Loss: 0.00024172663688659668, LR: 0.0001
Time, 2019-01-07T02:15:49, Epoch: 35, Batch: 580, Training Loss: 0.0011411219835281371, LR: 0.0001
Time, 2019-01-07T02:15:50, Epoch: 35, Batch: 590, Training Loss: 0.0011941716074943542, LR: 0.0001
Time, 2019-01-07T02:15:51, Epoch: 35, Batch: 600, Training Loss: 0.0004570081830024719, LR: 0.0001
Time, 2019-01-07T02:15:51, Epoch: 35, Batch: 610, Training Loss: 0.0006036698818206787, LR: 0.0001
Time, 2019-01-07T02:15:52, Epoch: 35, Batch: 620, Training Loss: 0.0005577430129051208, LR: 0.0001
Time, 2019-01-07T02:15:53, Epoch: 35, Batch: 630, Training Loss: 0.0010813489556312562, LR: 0.0001
Time, 2019-01-07T02:15:53, Epoch: 35, Batch: 640, Training Loss: 0.00048277080059051515, LR: 0.0001
Time, 2019-01-07T02:15:54, Epoch: 35, Batch: 650, Training Loss: 0.0024304673075675963, LR: 0.0001
Time, 2019-01-07T02:15:55, Epoch: 35, Batch: 660, Training Loss: 0.000651530921459198, LR: 0.0001
Time, 2019-01-07T02:15:55, Epoch: 35, Batch: 670, Training Loss: 0.00039129555225372317, LR: 0.0001
Time, 2019-01-07T02:15:56, Epoch: 35, Batch: 680, Training Loss: 0.0005584627389907837, LR: 0.0001
Time, 2019-01-07T02:15:57, Epoch: 35, Batch: 690, Training Loss: 0.0009679272770881653, LR: 0.0001
Time, 2019-01-07T02:15:57, Epoch: 35, Batch: 700, Training Loss: 0.00031767040491104126, LR: 0.0001
Time, 2019-01-07T02:15:58, Epoch: 35, Batch: 710, Training Loss: 0.000772973895072937, LR: 0.0001
Time, 2019-01-07T02:15:58, Epoch: 35, Batch: 720, Training Loss: 0.0005558133125305176, LR: 0.0001
Time, 2019-01-07T02:15:59, Epoch: 35, Batch: 730, Training Loss: 0.00039585307240486145, LR: 0.0001
Time, 2019-01-07T02:16:00, Epoch: 35, Batch: 740, Training Loss: 0.0005359023809432984, LR: 0.0001
Time, 2019-01-07T02:16:00, Epoch: 35, Batch: 750, Training Loss: 0.0004281878471374512, LR: 0.0001
Time, 2019-01-07T02:16:01, Epoch: 35, Batch: 760, Training Loss: 0.0008904039859771729, LR: 0.0001
Time, 2019-01-07T02:16:02, Epoch: 35, Batch: 770, Training Loss: 0.00029983818531036376, LR: 0.0001
Time, 2019-01-07T02:16:02, Epoch: 35, Batch: 780, Training Loss: 0.0006760671734809875, LR: 0.0001
Time, 2019-01-07T02:16:03, Epoch: 35, Batch: 790, Training Loss: 0.0003522440791130066, LR: 0.0001
Time, 2019-01-07T02:16:04, Epoch: 35, Batch: 800, Training Loss: 0.000519445538520813, LR: 0.0001
Time, 2019-01-07T02:16:04, Epoch: 35, Batch: 810, Training Loss: 0.0001582115888595581, LR: 0.0001
Time, 2019-01-07T02:16:05, Epoch: 35, Batch: 820, Training Loss: 0.003752875328063965, LR: 0.0001
Time, 2019-01-07T02:16:06, Epoch: 35, Batch: 830, Training Loss: 0.0009192481637001037, LR: 0.0001
Time, 2019-01-07T02:16:06, Epoch: 35, Batch: 840, Training Loss: 0.0006030775606632232, LR: 0.0001
Time, 2019-01-07T02:16:07, Epoch: 35, Batch: 850, Training Loss: 0.0005753427743911743, LR: 0.0001
Time, 2019-01-07T02:16:07, Epoch: 35, Batch: 860, Training Loss: 0.0004967108368873596, LR: 0.0001
Time, 2019-01-07T02:16:08, Epoch: 35, Batch: 870, Training Loss: 0.0005129680037498474, LR: 0.0001
Time, 2019-01-07T02:16:09, Epoch: 35, Batch: 880, Training Loss: 0.0009319737553596496, LR: 0.0001
Time, 2019-01-07T02:16:09, Epoch: 35, Batch: 890, Training Loss: 0.00019256770610809326, LR: 0.0001
Time, 2019-01-07T02:16:10, Epoch: 35, Batch: 900, Training Loss: 0.0005155399441719056, LR: 0.0001
Time, 2019-01-07T02:16:11, Epoch: 35, Batch: 910, Training Loss: 0.00036104321479797364, LR: 0.0001
Time, 2019-01-07T02:16:11, Epoch: 35, Batch: 920, Training Loss: 0.00027780532836914064, LR: 0.0001
Time, 2019-01-07T02:16:12, Epoch: 35, Batch: 930, Training Loss: 0.0011000901460647583, LR: 0.0001
Epoch: 35, Validation Top 1 acc: 98.92515563964844
Epoch: 35, Validation Top 5 acc: 100.0
Epoch: 35, Validation Set Loss: 0.04074528440833092
Start training epoch 36
Time, 2019-01-07T02:16:15, Epoch: 36, Batch: 10, Training Loss: 0.0003753975033760071, LR: 0.0001
Time, 2019-01-07T02:16:16, Epoch: 36, Batch: 20, Training Loss: 0.00031827688217163087, LR: 0.0001
Time, 2019-01-07T02:16:17, Epoch: 36, Batch: 30, Training Loss: 0.0014744654297828674, LR: 0.0001
Time, 2019-01-07T02:16:17, Epoch: 36, Batch: 40, Training Loss: 0.000590553879737854, LR: 0.0001
Time, 2019-01-07T02:16:18, Epoch: 36, Batch: 50, Training Loss: 0.0002754136919975281, LR: 0.0001
Time, 2019-01-07T02:16:18, Epoch: 36, Batch: 60, Training Loss: 0.0003445267677307129, LR: 0.0001
Time, 2019-01-07T02:16:19, Epoch: 36, Batch: 70, Training Loss: 0.0005633145570755005, LR: 0.0001
Time, 2019-01-07T02:16:20, Epoch: 36, Batch: 80, Training Loss: 0.0004067748785018921, LR: 0.0001
Time, 2019-01-07T02:16:20, Epoch: 36, Batch: 90, Training Loss: 0.0001119270920753479, LR: 0.0001
Time, 2019-01-07T02:16:21, Epoch: 36, Batch: 100, Training Loss: 0.00029477328062057497, LR: 0.0001
Time, 2019-01-07T02:16:22, Epoch: 36, Batch: 110, Training Loss: 0.00028415322303771974, LR: 0.0001
Time, 2019-01-07T02:16:22, Epoch: 36, Batch: 120, Training Loss: 0.0010677114129066466, LR: 0.0001
Time, 2019-01-07T02:16:23, Epoch: 36, Batch: 130, Training Loss: 0.00030736178159713746, LR: 0.0001
Time, 2019-01-07T02:16:23, Epoch: 36, Batch: 140, Training Loss: 0.000839407742023468, LR: 0.0001
Time, 2019-01-07T02:16:24, Epoch: 36, Batch: 150, Training Loss: 0.00047125369310379027, LR: 0.0001
Time, 2019-01-07T02:16:25, Epoch: 36, Batch: 160, Training Loss: 0.00040463954210281374, LR: 0.0001
Time, 2019-01-07T02:16:25, Epoch: 36, Batch: 170, Training Loss: 0.0005476176738739014, LR: 0.0001
Time, 2019-01-07T02:16:26, Epoch: 36, Batch: 180, Training Loss: 0.00034189969301223755, LR: 0.0001
Time, 2019-01-07T02:16:27, Epoch: 36, Batch: 190, Training Loss: 0.0004979804158210755, LR: 0.0001
Time, 2019-01-07T02:16:27, Epoch: 36, Batch: 200, Training Loss: 0.0003361955285072327, LR: 0.0001
Time, 2019-01-07T02:16:28, Epoch: 36, Batch: 210, Training Loss: 0.0002807393670082092, LR: 0.0001
Time, 2019-01-07T02:16:29, Epoch: 36, Batch: 220, Training Loss: 0.00025272071361541747, LR: 0.0001
Time, 2019-01-07T02:16:29, Epoch: 36, Batch: 230, Training Loss: 0.0001681789755821228, LR: 0.0001
Time, 2019-01-07T02:16:30, Epoch: 36, Batch: 240, Training Loss: 0.000443151593208313, LR: 0.0001
Time, 2019-01-07T02:16:31, Epoch: 36, Batch: 250, Training Loss: 0.0003910057246685028, LR: 0.0001
Time, 2019-01-07T02:16:31, Epoch: 36, Batch: 260, Training Loss: 0.0003469511866569519, LR: 0.0001
Time, 2019-01-07T02:16:32, Epoch: 36, Batch: 270, Training Loss: 0.0004666045308113098, LR: 0.0001
Time, 2019-01-07T02:16:33, Epoch: 36, Batch: 280, Training Loss: 0.00033843666315078733, LR: 0.0001
Time, 2019-01-07T02:16:33, Epoch: 36, Batch: 290, Training Loss: 0.0007109791040420532, LR: 0.0001
Time, 2019-01-07T02:16:34, Epoch: 36, Batch: 300, Training Loss: 0.0003743588924407959, LR: 0.0001
Time, 2019-01-07T02:16:35, Epoch: 36, Batch: 310, Training Loss: 0.00024375021457672118, LR: 0.0001
Time, 2019-01-07T02:16:35, Epoch: 36, Batch: 320, Training Loss: 0.0006334975361824036, LR: 0.0001
Time, 2019-01-07T02:16:36, Epoch: 36, Batch: 330, Training Loss: 0.00039632320404052733, LR: 0.0001
Time, 2019-01-07T02:16:37, Epoch: 36, Batch: 340, Training Loss: 0.0006146684288978576, LR: 0.0001
Time, 2019-01-07T02:16:37, Epoch: 36, Batch: 350, Training Loss: 0.0002430438995361328, LR: 0.0001
Time, 2019-01-07T02:16:38, Epoch: 36, Batch: 360, Training Loss: 0.0003743484616279602, LR: 0.0001
Time, 2019-01-07T02:16:39, Epoch: 36, Batch: 370, Training Loss: 0.0025291480123996735, LR: 0.0001
Time, 2019-01-07T02:16:39, Epoch: 36, Batch: 380, Training Loss: 0.00048221796751022337, LR: 0.0001
Time, 2019-01-07T02:16:40, Epoch: 36, Batch: 390, Training Loss: 0.0005275458097457886, LR: 0.0001
Time, 2019-01-07T02:16:41, Epoch: 36, Batch: 400, Training Loss: 0.000458429753780365, LR: 0.0001
Time, 2019-01-07T02:16:41, Epoch: 36, Batch: 410, Training Loss: 0.00023889243602752684, LR: 0.0001
Time, 2019-01-07T02:16:42, Epoch: 36, Batch: 420, Training Loss: 0.0004220426082611084, LR: 0.0001
Time, 2019-01-07T02:16:43, Epoch: 36, Batch: 430, Training Loss: 0.0004608094692230225, LR: 0.0001
Time, 2019-01-07T02:16:43, Epoch: 36, Batch: 440, Training Loss: 0.0005828440189361572, LR: 0.0001
Time, 2019-01-07T02:16:44, Epoch: 36, Batch: 450, Training Loss: 0.00048602968454360964, LR: 0.0001
Time, 2019-01-07T02:16:45, Epoch: 36, Batch: 460, Training Loss: 0.00036341995000839234, LR: 0.0001
Time, 2019-01-07T02:16:45, Epoch: 36, Batch: 470, Training Loss: 0.00033921301364898683, LR: 0.0001
Time, 2019-01-07T02:16:46, Epoch: 36, Batch: 480, Training Loss: 0.0002446830272674561, LR: 0.0001
Time, 2019-01-07T02:16:47, Epoch: 36, Batch: 490, Training Loss: 0.00021376609802246094, LR: 0.0001
Time, 2019-01-07T02:16:47, Epoch: 36, Batch: 500, Training Loss: 0.0014744490385055543, LR: 0.0001
Time, 2019-01-07T02:16:48, Epoch: 36, Batch: 510, Training Loss: 0.0013269051909446717, LR: 0.0001
Time, 2019-01-07T02:16:49, Epoch: 36, Batch: 520, Training Loss: 0.0007235974073410035, LR: 0.0001
Time, 2019-01-07T02:16:49, Epoch: 36, Batch: 530, Training Loss: 0.0003361046314239502, LR: 0.0001
Time, 2019-01-07T02:16:50, Epoch: 36, Batch: 540, Training Loss: 0.000356099009513855, LR: 0.0001
Time, 2019-01-07T02:16:51, Epoch: 36, Batch: 550, Training Loss: 0.0013171717524528503, LR: 0.0001
Time, 2019-01-07T02:16:51, Epoch: 36, Batch: 560, Training Loss: 0.00042724162340164186, LR: 0.0001
Time, 2019-01-07T02:16:52, Epoch: 36, Batch: 570, Training Loss: 0.0007063522934913635, LR: 0.0001
Time, 2019-01-07T02:16:53, Epoch: 36, Batch: 580, Training Loss: 0.0007443413138389588, LR: 0.0001
Time, 2019-01-07T02:16:54, Epoch: 36, Batch: 590, Training Loss: 0.000520363450050354, LR: 0.0001
Time, 2019-01-07T02:16:55, Epoch: 36, Batch: 600, Training Loss: 0.00048453062772750853, LR: 0.0001
Time, 2019-01-07T02:16:55, Epoch: 36, Batch: 610, Training Loss: 0.0007649078965187073, LR: 0.0001
Time, 2019-01-07T02:16:56, Epoch: 36, Batch: 620, Training Loss: 0.0008509621024131775, LR: 0.0001
Time, 2019-01-07T02:16:57, Epoch: 36, Batch: 630, Training Loss: 0.0006413757801055908, LR: 0.0001
Time, 2019-01-07T02:16:57, Epoch: 36, Batch: 640, Training Loss: 0.0005205869674682617, LR: 0.0001
Time, 2019-01-07T02:16:58, Epoch: 36, Batch: 650, Training Loss: 0.000569501519203186, LR: 0.0001
Time, 2019-01-07T02:16:59, Epoch: 36, Batch: 660, Training Loss: 0.0003481388092041016, LR: 0.0001
Time, 2019-01-07T02:16:59, Epoch: 36, Batch: 670, Training Loss: 0.0003725498914718628, LR: 0.0001
Time, 2019-01-07T02:17:00, Epoch: 36, Batch: 680, Training Loss: 0.00033331215381622316, LR: 0.0001
Time, 2019-01-07T02:17:01, Epoch: 36, Batch: 690, Training Loss: 0.0007836684584617615, LR: 0.0001
Time, 2019-01-07T02:17:01, Epoch: 36, Batch: 700, Training Loss: 0.0003231510519981384, LR: 0.0001
Time, 2019-01-07T02:17:02, Epoch: 36, Batch: 710, Training Loss: 0.0004622936248779297, LR: 0.0001
Time, 2019-01-07T02:17:03, Epoch: 36, Batch: 720, Training Loss: 0.0003094598650932312, LR: 0.0001
Time, 2019-01-07T02:17:03, Epoch: 36, Batch: 730, Training Loss: 0.000980694591999054, LR: 0.0001
Time, 2019-01-07T02:17:04, Epoch: 36, Batch: 740, Training Loss: 0.0004019975662231445, LR: 0.0001
Time, 2019-01-07T02:17:05, Epoch: 36, Batch: 750, Training Loss: 0.0003667078912258148, LR: 0.0001
Time, 2019-01-07T02:17:05, Epoch: 36, Batch: 760, Training Loss: 0.0008552193641662598, LR: 0.0001
Time, 2019-01-07T02:17:06, Epoch: 36, Batch: 770, Training Loss: 0.0006347909569740295, LR: 0.0001
Time, 2019-01-07T02:17:07, Epoch: 36, Batch: 780, Training Loss: 0.0004345700144767761, LR: 0.0001
Time, 2019-01-07T02:17:07, Epoch: 36, Batch: 790, Training Loss: 0.0002301633358001709, LR: 0.0001
Time, 2019-01-07T02:17:08, Epoch: 36, Batch: 800, Training Loss: 0.00041590631008148193, LR: 0.0001
Time, 2019-01-07T02:17:09, Epoch: 36, Batch: 810, Training Loss: 0.0002342313528060913, LR: 0.0001
Time, 2019-01-07T02:17:09, Epoch: 36, Batch: 820, Training Loss: 0.00034867078065872195, LR: 0.0001
Time, 2019-01-07T02:17:10, Epoch: 36, Batch: 830, Training Loss: 0.0004930928349494934, LR: 0.0001
Time, 2019-01-07T02:17:11, Epoch: 36, Batch: 840, Training Loss: 0.0001765131950378418, LR: 0.0001
Time, 2019-01-07T02:17:11, Epoch: 36, Batch: 850, Training Loss: 0.00022164136171340943, LR: 0.0001
Time, 2019-01-07T02:17:12, Epoch: 36, Batch: 860, Training Loss: 0.0004381328821182251, LR: 0.0001
Time, 2019-01-07T02:17:13, Epoch: 36, Batch: 870, Training Loss: 0.003383307158946991, LR: 0.0001
Time, 2019-01-07T02:17:13, Epoch: 36, Batch: 880, Training Loss: 0.0004866108298301697, LR: 0.0001
Time, 2019-01-07T02:17:14, Epoch: 36, Batch: 890, Training Loss: 0.00024064183235168458, LR: 0.0001
Time, 2019-01-07T02:17:15, Epoch: 36, Batch: 900, Training Loss: 0.0004266694188117981, LR: 0.0001
Time, 2019-01-07T02:17:15, Epoch: 36, Batch: 910, Training Loss: 0.0004084095358848572, LR: 0.0001
Time, 2019-01-07T02:17:16, Epoch: 36, Batch: 920, Training Loss: 0.0005222871899604797, LR: 0.0001
Time, 2019-01-07T02:17:17, Epoch: 36, Batch: 930, Training Loss: 0.0006126970052719116, LR: 0.0001
Epoch: 36, Validation Top 1 acc: 98.9649658203125
Epoch: 36, Validation Top 5 acc: 100.0
Epoch: 36, Validation Set Loss: 0.04186856746673584
Start training epoch 37
Time, 2019-01-07T02:17:20, Epoch: 37, Batch: 10, Training Loss: 0.00026421099901199343, LR: 0.0001
Time, 2019-01-07T02:17:20, Epoch: 37, Batch: 20, Training Loss: 0.00046936869621276854, LR: 0.0001
Time, 2019-01-07T02:17:21, Epoch: 37, Batch: 30, Training Loss: 0.0005825355648994446, LR: 0.0001
Time, 2019-01-07T02:17:22, Epoch: 37, Batch: 40, Training Loss: 0.0002357184886932373, LR: 0.0001
Time, 2019-01-07T02:17:22, Epoch: 37, Batch: 50, Training Loss: 0.0003730013966560364, LR: 0.0001
Time, 2019-01-07T02:17:23, Epoch: 37, Batch: 60, Training Loss: 0.00028956830501556394, LR: 0.0001
Time, 2019-01-07T02:17:24, Epoch: 37, Batch: 70, Training Loss: 0.00027234554290771483, LR: 0.0001
Time, 2019-01-07T02:17:25, Epoch: 37, Batch: 80, Training Loss: 0.0003188237547874451, LR: 0.0001
Time, 2019-01-07T02:17:25, Epoch: 37, Batch: 90, Training Loss: 0.00017858147621154785, LR: 0.0001
Time, 2019-01-07T02:17:26, Epoch: 37, Batch: 100, Training Loss: 0.0004174545407295227, LR: 0.0001
Time, 2019-01-07T02:17:27, Epoch: 37, Batch: 110, Training Loss: 0.0007790863513946533, LR: 0.0001
Time, 2019-01-07T02:17:27, Epoch: 37, Batch: 120, Training Loss: 0.00012483298778533935, LR: 0.0001
Time, 2019-01-07T02:17:28, Epoch: 37, Batch: 130, Training Loss: 0.00020985603332519532, LR: 0.0001
Time, 2019-01-07T02:17:29, Epoch: 37, Batch: 140, Training Loss: 0.000196819007396698, LR: 0.0001
Time, 2019-01-07T02:17:29, Epoch: 37, Batch: 150, Training Loss: 0.000841878354549408, LR: 0.0001
Time, 2019-01-07T02:17:30, Epoch: 37, Batch: 160, Training Loss: 0.000369030237197876, LR: 0.0001
Time, 2019-01-07T02:17:31, Epoch: 37, Batch: 170, Training Loss: 0.00033350437879562376, LR: 0.0001
Time, 2019-01-07T02:17:32, Epoch: 37, Batch: 180, Training Loss: 0.0004030093550682068, LR: 0.0001
Time, 2019-01-07T02:17:32, Epoch: 37, Batch: 190, Training Loss: 0.0004667073488235474, LR: 0.0001
Time, 2019-01-07T02:17:33, Epoch: 37, Batch: 200, Training Loss: 0.0005194902420043946, LR: 0.0001
Time, 2019-01-07T02:17:34, Epoch: 37, Batch: 210, Training Loss: 0.00014086663722991943, LR: 0.0001
Time, 2019-01-07T02:17:34, Epoch: 37, Batch: 220, Training Loss: 0.0005179554224014282, LR: 0.0001
Time, 2019-01-07T02:17:35, Epoch: 37, Batch: 230, Training Loss: 0.0005390100181102752, LR: 0.0001
Time, 2019-01-07T02:17:36, Epoch: 37, Batch: 240, Training Loss: 0.000546574592590332, LR: 0.0001
Time, 2019-01-07T02:17:36, Epoch: 37, Batch: 250, Training Loss: 0.0005345299839973449, LR: 0.0001
Time, 2019-01-07T02:17:37, Epoch: 37, Batch: 260, Training Loss: 0.00036539286375045774, LR: 0.0001
Time, 2019-01-07T02:17:38, Epoch: 37, Batch: 270, Training Loss: 0.00030670166015625, LR: 0.0001
Time, 2019-01-07T02:17:38, Epoch: 37, Batch: 280, Training Loss: 0.0006156817078590393, LR: 0.0001
Time, 2019-01-07T02:17:39, Epoch: 37, Batch: 290, Training Loss: 0.0003583982586860657, LR: 0.0001
Time, 2019-01-07T02:17:40, Epoch: 37, Batch: 300, Training Loss: 0.000189228355884552, LR: 0.0001
Time, 2019-01-07T02:17:41, Epoch: 37, Batch: 310, Training Loss: 0.0004567787051200867, LR: 0.0001
Time, 2019-01-07T02:17:41, Epoch: 37, Batch: 320, Training Loss: 0.00040731877088546754, LR: 0.0001
Time, 2019-01-07T02:17:42, Epoch: 37, Batch: 330, Training Loss: 0.0002376258373260498, LR: 0.0001
Time, 2019-01-07T02:17:43, Epoch: 37, Batch: 340, Training Loss: 0.0004327714443206787, LR: 0.0001
Time, 2019-01-07T02:17:44, Epoch: 37, Batch: 350, Training Loss: 0.00016520023345947265, LR: 0.0001
Time, 2019-01-07T02:17:44, Epoch: 37, Batch: 360, Training Loss: 0.0002853885293006897, LR: 0.0001
Time, 2019-01-07T02:17:45, Epoch: 37, Batch: 370, Training Loss: 0.00035381019115447997, LR: 0.0001
Time, 2019-01-07T02:17:46, Epoch: 37, Batch: 380, Training Loss: 0.00015116482973098755, LR: 0.0001
Time, 2019-01-07T02:17:46, Epoch: 37, Batch: 390, Training Loss: 0.00024018585681915284, LR: 0.0001
Time, 2019-01-07T02:17:47, Epoch: 37, Batch: 400, Training Loss: 0.0006237521767616272, LR: 0.0001
Time, 2019-01-07T02:17:48, Epoch: 37, Batch: 410, Training Loss: 0.00018314272165298462, LR: 0.0001
Time, 2019-01-07T02:17:48, Epoch: 37, Batch: 420, Training Loss: 0.0002761855721473694, LR: 0.0001
Time, 2019-01-07T02:17:49, Epoch: 37, Batch: 430, Training Loss: 0.00039669424295425416, LR: 0.0001
Time, 2019-01-07T02:17:50, Epoch: 37, Batch: 440, Training Loss: 0.0002879038453102112, LR: 0.0001
Time, 2019-01-07T02:17:50, Epoch: 37, Batch: 450, Training Loss: 0.0002604067325592041, LR: 0.0001
Time, 2019-01-07T02:17:51, Epoch: 37, Batch: 460, Training Loss: 0.000667862594127655, LR: 0.0001
Time, 2019-01-07T02:17:52, Epoch: 37, Batch: 470, Training Loss: 0.00016636401414871216, LR: 0.0001
Time, 2019-01-07T02:17:52, Epoch: 37, Batch: 480, Training Loss: 0.00034303367137908933, LR: 0.0001
Time, 2019-01-07T02:17:53, Epoch: 37, Batch: 490, Training Loss: 0.0003362193703651428, LR: 0.0001
Time, 2019-01-07T02:17:54, Epoch: 37, Batch: 500, Training Loss: 0.0004335567355155945, LR: 0.0001
Time, 2019-01-07T02:17:54, Epoch: 37, Batch: 510, Training Loss: 0.000495871901512146, LR: 0.0001
Time, 2019-01-07T02:17:55, Epoch: 37, Batch: 520, Training Loss: 0.0002970859408378601, LR: 0.0001
Time, 2019-01-07T02:17:56, Epoch: 37, Batch: 530, Training Loss: 0.00038367211818695067, LR: 0.0001
Time, 2019-01-07T02:17:56, Epoch: 37, Batch: 540, Training Loss: 0.00024124681949615477, LR: 0.0001
Time, 2019-01-07T02:17:57, Epoch: 37, Batch: 550, Training Loss: 0.0003540143370628357, LR: 0.0001
Time, 2019-01-07T02:17:58, Epoch: 37, Batch: 560, Training Loss: 0.0002691090106964111, LR: 0.0001
Time, 2019-01-07T02:17:58, Epoch: 37, Batch: 570, Training Loss: 0.00021807998418807984, LR: 0.0001
Time, 2019-01-07T02:17:59, Epoch: 37, Batch: 580, Training Loss: 0.0007010906934738159, LR: 0.0001
Time, 2019-01-07T02:18:00, Epoch: 37, Batch: 590, Training Loss: 0.00018143802881240845, LR: 0.0001
Time, 2019-01-07T02:18:01, Epoch: 37, Batch: 600, Training Loss: 0.0002909064292907715, LR: 0.0001
Time, 2019-01-07T02:18:02, Epoch: 37, Batch: 610, Training Loss: 0.0003613904118537903, LR: 0.0001
Time, 2019-01-07T02:18:02, Epoch: 37, Batch: 620, Training Loss: 0.00024597346782684326, LR: 0.0001
Time, 2019-01-07T02:18:03, Epoch: 37, Batch: 630, Training Loss: 0.0004329189658164978, LR: 0.0001
Time, 2019-01-07T02:18:04, Epoch: 37, Batch: 640, Training Loss: 0.0004995852708816528, LR: 0.0001
Time, 2019-01-07T02:18:04, Epoch: 37, Batch: 650, Training Loss: 0.0003001362085342407, LR: 0.0001
Time, 2019-01-07T02:18:05, Epoch: 37, Batch: 660, Training Loss: 0.00023739784955978394, LR: 0.0001
Time, 2019-01-07T02:18:06, Epoch: 37, Batch: 670, Training Loss: 0.0002612799406051636, LR: 0.0001
Time, 2019-01-07T02:18:07, Epoch: 37, Batch: 680, Training Loss: 0.0002782359719276428, LR: 0.0001
Time, 2019-01-07T02:18:07, Epoch: 37, Batch: 690, Training Loss: 0.00045700669288635255, LR: 0.0001
Time, 2019-01-07T02:18:08, Epoch: 37, Batch: 700, Training Loss: 0.0002549916505813599, LR: 0.0001
Time, 2019-01-07T02:18:09, Epoch: 37, Batch: 710, Training Loss: 0.0002183496952056885, LR: 0.0001
Time, 2019-01-07T02:18:09, Epoch: 37, Batch: 720, Training Loss: 0.0006281554698944092, LR: 0.0001
Time, 2019-01-07T02:18:10, Epoch: 37, Batch: 730, Training Loss: 0.0007574349641799927, LR: 0.0001
Time, 2019-01-07T02:18:11, Epoch: 37, Batch: 740, Training Loss: 0.00018104612827301024, LR: 0.0001
Time, 2019-01-07T02:18:12, Epoch: 37, Batch: 750, Training Loss: 0.0025945499539375303, LR: 0.0001
Time, 2019-01-07T02:18:12, Epoch: 37, Batch: 760, Training Loss: 0.00027220994234085084, LR: 0.0001
Time, 2019-01-07T02:18:13, Epoch: 37, Batch: 770, Training Loss: 0.00029620975255966185, LR: 0.0001
Time, 2019-01-07T02:18:14, Epoch: 37, Batch: 780, Training Loss: 0.00024021565914154053, LR: 0.0001
Time, 2019-01-07T02:18:14, Epoch: 37, Batch: 790, Training Loss: 0.00017701834440231323, LR: 0.0001
Time, 2019-01-07T02:18:15, Epoch: 37, Batch: 800, Training Loss: 0.0004063531756401062, LR: 0.0001
Time, 2019-01-07T02:18:16, Epoch: 37, Batch: 810, Training Loss: 0.00030299127101898196, LR: 0.0001
Time, 2019-01-07T02:18:16, Epoch: 37, Batch: 820, Training Loss: 0.0003200322389602661, LR: 0.0001
Time, 2019-01-07T02:18:17, Epoch: 37, Batch: 830, Training Loss: 0.0003547593951225281, LR: 0.0001
Time, 2019-01-07T02:18:18, Epoch: 37, Batch: 840, Training Loss: 0.0002484053373336792, LR: 0.0001
Time, 2019-01-07T02:18:18, Epoch: 37, Batch: 850, Training Loss: 0.0002485483884811401, LR: 0.0001
Time, 2019-01-07T02:18:19, Epoch: 37, Batch: 860, Training Loss: 0.0030085280537605285, LR: 0.0001
Time, 2019-01-07T02:18:20, Epoch: 37, Batch: 870, Training Loss: 0.0002738088369369507, LR: 0.0001
Time, 2019-01-07T02:18:20, Epoch: 37, Batch: 880, Training Loss: 0.00029605627059936523, LR: 0.0001
Time, 2019-01-07T02:18:21, Epoch: 37, Batch: 890, Training Loss: 0.00041286498308181765, LR: 0.0001
Time, 2019-01-07T02:18:22, Epoch: 37, Batch: 900, Training Loss: 0.0005405187606811523, LR: 0.0001
Time, 2019-01-07T02:18:23, Epoch: 37, Batch: 910, Training Loss: 0.0004431203007698059, LR: 0.0001
Time, 2019-01-07T02:18:23, Epoch: 37, Batch: 920, Training Loss: 0.00024365037679672242, LR: 0.0001
Time, 2019-01-07T02:18:24, Epoch: 37, Batch: 930, Training Loss: 0.00033495575189590454, LR: 0.0001
Epoch: 37, Validation Top 1 acc: 98.92515563964844
Epoch: 37, Validation Top 5 acc: 100.0
Epoch: 37, Validation Set Loss: 0.040757037699222565
Start training epoch 38
Time, 2019-01-07T02:18:28, Epoch: 38, Batch: 10, Training Loss: 0.0002757042646408081, LR: 0.0001
Time, 2019-01-07T02:18:29, Epoch: 38, Batch: 20, Training Loss: 0.00016430914402008057, LR: 0.0001
Time, 2019-01-07T02:18:29, Epoch: 38, Batch: 30, Training Loss: 0.00024968385696411133, LR: 0.0001
Time, 2019-01-07T02:18:30, Epoch: 38, Batch: 40, Training Loss: 0.00020273029804229736, LR: 0.0001
Time, 2019-01-07T02:18:31, Epoch: 38, Batch: 50, Training Loss: 0.00022339671850204467, LR: 0.0001
Time, 2019-01-07T02:18:32, Epoch: 38, Batch: 60, Training Loss: 0.00022258162498474122, LR: 0.0001
Time, 2019-01-07T02:18:33, Epoch: 38, Batch: 70, Training Loss: 0.0006654590368270874, LR: 0.0001
Time, 2019-01-07T02:18:33, Epoch: 38, Batch: 80, Training Loss: 0.00029384642839431764, LR: 0.0001
Time, 2019-01-07T02:18:34, Epoch: 38, Batch: 90, Training Loss: 0.0002388298511505127, LR: 0.0001
Time, 2019-01-07T02:18:35, Epoch: 38, Batch: 100, Training Loss: 0.0004964172840118408, LR: 0.0001
Time, 2019-01-07T02:18:35, Epoch: 38, Batch: 110, Training Loss: 0.0002238541841506958, LR: 0.0001
Time, 2019-01-07T02:18:36, Epoch: 38, Batch: 120, Training Loss: 0.00041618049144744875, LR: 0.0001
Time, 2019-01-07T02:18:37, Epoch: 38, Batch: 130, Training Loss: 0.0003841206431388855, LR: 0.0001
Time, 2019-01-07T02:18:37, Epoch: 38, Batch: 140, Training Loss: 0.00037891268730163576, LR: 0.0001
Time, 2019-01-07T02:18:38, Epoch: 38, Batch: 150, Training Loss: 0.0002124965190887451, LR: 0.0001
Time, 2019-01-07T02:18:39, Epoch: 38, Batch: 160, Training Loss: 0.0002549096941947937, LR: 0.0001
Time, 2019-01-07T02:18:40, Epoch: 38, Batch: 170, Training Loss: 0.0006140023469924927, LR: 0.0001
Time, 2019-01-07T02:18:41, Epoch: 38, Batch: 180, Training Loss: 0.0002721816301345825, LR: 0.0001
Time, 2019-01-07T02:18:41, Epoch: 38, Batch: 190, Training Loss: 0.00045085400342941283, LR: 0.0001
Time, 2019-01-07T02:18:42, Epoch: 38, Batch: 200, Training Loss: 9.498745203018188e-05, LR: 0.0001
Time, 2019-01-07T02:18:43, Epoch: 38, Batch: 210, Training Loss: 0.00023495405912399292, LR: 0.0001
Time, 2019-01-07T02:18:44, Epoch: 38, Batch: 220, Training Loss: 0.0001997038722038269, LR: 0.0001
Time, 2019-01-07T02:18:45, Epoch: 38, Batch: 230, Training Loss: 0.00029492825269699096, LR: 0.0001
Time, 2019-01-07T02:18:45, Epoch: 38, Batch: 240, Training Loss: 0.0004966571927070617, LR: 0.0001
Time, 2019-01-07T02:18:46, Epoch: 38, Batch: 250, Training Loss: 0.0002960950136184692, LR: 0.0001
Time, 2019-01-07T02:18:47, Epoch: 38, Batch: 260, Training Loss: 0.00020326226949691774, LR: 0.0001
Time, 2019-01-07T02:18:48, Epoch: 38, Batch: 270, Training Loss: 0.00021248012781143187, LR: 0.0001
Time, 2019-01-07T02:18:48, Epoch: 38, Batch: 280, Training Loss: 0.0008102566003799439, LR: 0.0001
Time, 2019-01-07T02:18:49, Epoch: 38, Batch: 290, Training Loss: 0.00013935714960098267, LR: 0.0001
Time, 2019-01-07T02:18:50, Epoch: 38, Batch: 300, Training Loss: 0.0003880620002746582, LR: 0.0001
Time, 2019-01-07T02:18:51, Epoch: 38, Batch: 310, Training Loss: 0.00022798627614974975, LR: 0.0001
Time, 2019-01-07T02:18:51, Epoch: 38, Batch: 320, Training Loss: 0.00020235627889633178, LR: 0.0001
Time, 2019-01-07T02:18:52, Epoch: 38, Batch: 330, Training Loss: 0.00027834773063659666, LR: 0.0001
Time, 2019-01-07T02:18:53, Epoch: 38, Batch: 340, Training Loss: 0.00026786327362060547, LR: 0.0001
Time, 2019-01-07T02:18:53, Epoch: 38, Batch: 350, Training Loss: 0.00022168755531311036, LR: 0.0001
Time, 2019-01-07T02:18:54, Epoch: 38, Batch: 360, Training Loss: 0.0008179724216461181, LR: 0.0001
Time, 2019-01-07T02:18:55, Epoch: 38, Batch: 370, Training Loss: 0.00016791373491287231, LR: 0.0001
Time, 2019-01-07T02:18:55, Epoch: 38, Batch: 380, Training Loss: 0.00027962327003479005, LR: 0.0001
Time, 2019-01-07T02:18:56, Epoch: 38, Batch: 390, Training Loss: 0.0001565009355545044, LR: 0.0001
Time, 2019-01-07T02:18:57, Epoch: 38, Batch: 400, Training Loss: 0.00022524744272232056, LR: 0.0001
Time, 2019-01-07T02:18:57, Epoch: 38, Batch: 410, Training Loss: 0.0002345159649848938, LR: 0.0001
Time, 2019-01-07T02:18:58, Epoch: 38, Batch: 420, Training Loss: 0.0003720864653587341, LR: 0.0001
Time, 2019-01-07T02:18:59, Epoch: 38, Batch: 430, Training Loss: 0.00035556107759475707, LR: 0.0001
Time, 2019-01-07T02:19:00, Epoch: 38, Batch: 440, Training Loss: 0.00021664798259735107, LR: 0.0001
Time, 2019-01-07T02:19:00, Epoch: 38, Batch: 450, Training Loss: 0.00014510899782180787, LR: 0.0001
Time, 2019-01-07T02:19:01, Epoch: 38, Batch: 460, Training Loss: 0.0005856499075889588, LR: 0.0001
Time, 2019-01-07T02:19:02, Epoch: 38, Batch: 470, Training Loss: 0.002298073470592499, LR: 0.0001
Time, 2019-01-07T02:19:02, Epoch: 38, Batch: 480, Training Loss: 0.0006200224161148071, LR: 0.0001
Time, 2019-01-07T02:19:03, Epoch: 38, Batch: 490, Training Loss: 0.0002378910779953003, LR: 0.0001
Time, 2019-01-07T02:19:04, Epoch: 38, Batch: 500, Training Loss: 0.00027296841144561765, LR: 0.0001
Time, 2019-01-07T02:19:04, Epoch: 38, Batch: 510, Training Loss: 0.0005953997373580932, LR: 0.0001
Time, 2019-01-07T02:19:05, Epoch: 38, Batch: 520, Training Loss: 0.0004066288471221924, LR: 0.0001
Time, 2019-01-07T02:19:06, Epoch: 38, Batch: 530, Training Loss: 0.0004753559827804565, LR: 0.0001
Time, 2019-01-07T02:19:07, Epoch: 38, Batch: 540, Training Loss: 0.00014719218015670776, LR: 0.0001
Time, 2019-01-07T02:19:07, Epoch: 38, Batch: 550, Training Loss: 0.0014114677906036377, LR: 0.0001
Time, 2019-01-07T02:19:08, Epoch: 38, Batch: 560, Training Loss: 0.0005426213145256042, LR: 0.0001
Time, 2019-01-07T02:19:09, Epoch: 38, Batch: 570, Training Loss: 0.00036843568086624144, LR: 0.0001
Time, 2019-01-07T02:19:09, Epoch: 38, Batch: 580, Training Loss: 8.745640516281128e-05, LR: 0.0001
Time, 2019-01-07T02:19:10, Epoch: 38, Batch: 590, Training Loss: 0.0004061684012413025, LR: 0.0001
Time, 2019-01-07T02:19:11, Epoch: 38, Batch: 600, Training Loss: 0.0003207966685295105, LR: 0.0001
Time, 2019-01-07T02:19:11, Epoch: 38, Batch: 610, Training Loss: 0.00020443052053451539, LR: 0.0001
Time, 2019-01-07T02:19:12, Epoch: 38, Batch: 620, Training Loss: 0.0003809630870819092, LR: 0.0001
Time, 2019-01-07T02:19:13, Epoch: 38, Batch: 630, Training Loss: 0.00031128525733947754, LR: 0.0001
Time, 2019-01-07T02:19:13, Epoch: 38, Batch: 640, Training Loss: 0.00020691454410552977, LR: 0.0001
Time, 2019-01-07T02:19:14, Epoch: 38, Batch: 650, Training Loss: 0.00029822438955307007, LR: 0.0001
Time, 2019-01-07T02:19:15, Epoch: 38, Batch: 660, Training Loss: 0.0002792581915855408, LR: 0.0001
Time, 2019-01-07T02:19:15, Epoch: 38, Batch: 670, Training Loss: 0.0003020927309989929, LR: 0.0001
Time, 2019-01-07T02:19:16, Epoch: 38, Batch: 680, Training Loss: 0.0001329183578491211, LR: 0.0001
Time, 2019-01-07T02:19:17, Epoch: 38, Batch: 690, Training Loss: 0.00030805617570877074, LR: 0.0001
Time, 2019-01-07T02:19:17, Epoch: 38, Batch: 700, Training Loss: 0.00037199556827545167, LR: 0.0001
Time, 2019-01-07T02:19:18, Epoch: 38, Batch: 710, Training Loss: 0.0003512218594551086, LR: 0.0001
Time, 2019-01-07T02:19:19, Epoch: 38, Batch: 720, Training Loss: 0.0002858459949493408, LR: 0.0001
Time, 2019-01-07T02:19:19, Epoch: 38, Batch: 730, Training Loss: 0.0001398041844367981, LR: 0.0001
Time, 2019-01-07T02:19:20, Epoch: 38, Batch: 740, Training Loss: 0.0003388255834579468, LR: 0.0001
Time, 2019-01-07T02:19:21, Epoch: 38, Batch: 750, Training Loss: 0.0002069622278213501, LR: 0.0001
Time, 2019-01-07T02:19:21, Epoch: 38, Batch: 760, Training Loss: 0.0003818824887275696, LR: 0.0001
Time, 2019-01-07T02:19:22, Epoch: 38, Batch: 770, Training Loss: 0.0004913866519927979, LR: 0.0001
Time, 2019-01-07T02:19:23, Epoch: 38, Batch: 780, Training Loss: 0.00027074962854385377, LR: 0.0001
Time, 2019-01-07T02:19:24, Epoch: 38, Batch: 790, Training Loss: 0.0004273191094398499, LR: 0.0001
Time, 2019-01-07T02:19:24, Epoch: 38, Batch: 800, Training Loss: 0.0002332225441932678, LR: 0.0001
Time, 2019-01-07T02:19:25, Epoch: 38, Batch: 810, Training Loss: 0.00031362324953079226, LR: 0.0001
Time, 2019-01-07T02:19:26, Epoch: 38, Batch: 820, Training Loss: 0.0009141817688941955, LR: 0.0001
Time, 2019-01-07T02:19:26, Epoch: 38, Batch: 830, Training Loss: 0.0001689732074737549, LR: 0.0001
Time, 2019-01-07T02:19:27, Epoch: 38, Batch: 840, Training Loss: 0.0002555951476097107, LR: 0.0001
Time, 2019-01-07T02:19:28, Epoch: 38, Batch: 850, Training Loss: 0.00047041773796081544, LR: 0.0001
Time, 2019-01-07T02:19:28, Epoch: 38, Batch: 860, Training Loss: 0.00018308162689208984, LR: 0.0001
Time, 2019-01-07T02:19:29, Epoch: 38, Batch: 870, Training Loss: 0.00017158687114715576, LR: 0.0001
Time, 2019-01-07T02:19:30, Epoch: 38, Batch: 880, Training Loss: 0.00023542121052742005, LR: 0.0001
Time, 2019-01-07T02:19:30, Epoch: 38, Batch: 890, Training Loss: 0.00021683424711227417, LR: 0.0001
Time, 2019-01-07T02:19:31, Epoch: 38, Batch: 900, Training Loss: 0.00012797266244888306, LR: 0.0001
Time, 2019-01-07T02:19:32, Epoch: 38, Batch: 910, Training Loss: 0.0001430615782737732, LR: 0.0001
Time, 2019-01-07T02:19:32, Epoch: 38, Batch: 920, Training Loss: 0.00015556812286376953, LR: 0.0001
Time, 2019-01-07T02:19:33, Epoch: 38, Batch: 930, Training Loss: 0.00010917037725448609, LR: 0.0001
Epoch: 38, Validation Top 1 acc: 98.97492218017578
Epoch: 38, Validation Top 5 acc: 100.0
Epoch: 38, Validation Set Loss: 0.04026002809405327
Start training epoch 39
Time, 2019-01-07T02:19:37, Epoch: 39, Batch: 10, Training Loss: 0.000249946117401123, LR: 0.0001
Time, 2019-01-07T02:19:38, Epoch: 39, Batch: 20, Training Loss: 0.00023658573627471924, LR: 0.0001
Time, 2019-01-07T02:19:38, Epoch: 39, Batch: 30, Training Loss: 0.00014785677194595337, LR: 0.0001
Time, 2019-01-07T02:19:39, Epoch: 39, Batch: 40, Training Loss: 0.00013656020164489746, LR: 0.0001
Time, 2019-01-07T02:19:40, Epoch: 39, Batch: 50, Training Loss: 0.00015719681978225707, LR: 0.0001
Time, 2019-01-07T02:19:41, Epoch: 39, Batch: 60, Training Loss: 0.00022359490394592286, LR: 0.0001
Time, 2019-01-07T02:19:41, Epoch: 39, Batch: 70, Training Loss: 0.00010496079921722412, LR: 0.0001
Time, 2019-01-07T02:19:42, Epoch: 39, Batch: 80, Training Loss: 0.00020872950553894044, LR: 0.0001
Time, 2019-01-07T02:19:43, Epoch: 39, Batch: 90, Training Loss: 0.00020664334297180176, LR: 0.0001
Time, 2019-01-07T02:19:44, Epoch: 39, Batch: 100, Training Loss: 0.0003061339259147644, LR: 0.0001
Time, 2019-01-07T02:19:45, Epoch: 39, Batch: 110, Training Loss: 0.0002838745713233948, LR: 0.0001
Time, 2019-01-07T02:19:45, Epoch: 39, Batch: 120, Training Loss: 0.0005185738205909729, LR: 0.0001
Time, 2019-01-07T02:19:46, Epoch: 39, Batch: 130, Training Loss: 0.0002629980444908142, LR: 0.0001
Time, 2019-01-07T02:19:47, Epoch: 39, Batch: 140, Training Loss: 0.00019728988409042358, LR: 0.0001
Time, 2019-01-07T02:19:47, Epoch: 39, Batch: 150, Training Loss: 0.00013522803783416748, LR: 0.0001
Time, 2019-01-07T02:19:48, Epoch: 39, Batch: 160, Training Loss: 0.00016706138849258422, LR: 0.0001
Time, 2019-01-07T02:19:49, Epoch: 39, Batch: 170, Training Loss: 0.00036717355251312254, LR: 0.0001
Time, 2019-01-07T02:19:49, Epoch: 39, Batch: 180, Training Loss: 0.00029732584953308107, LR: 0.0001
Time, 2019-01-07T02:19:50, Epoch: 39, Batch: 190, Training Loss: 0.00018738508224487306, LR: 0.0001
Time, 2019-01-07T02:19:51, Epoch: 39, Batch: 200, Training Loss: 0.0002172812819480896, LR: 0.0001
Time, 2019-01-07T02:19:51, Epoch: 39, Batch: 210, Training Loss: 0.0002920135855674744, LR: 0.0001
Time, 2019-01-07T02:19:52, Epoch: 39, Batch: 220, Training Loss: 0.0002055808901786804, LR: 0.0001
Time, 2019-01-07T02:19:53, Epoch: 39, Batch: 230, Training Loss: 0.0001573905348777771, LR: 0.0001
Time, 2019-01-07T02:19:54, Epoch: 39, Batch: 240, Training Loss: 0.0002660468220710754, LR: 0.0001
Time, 2019-01-07T02:19:54, Epoch: 39, Batch: 250, Training Loss: 0.00012857019901275636, LR: 0.0001
Time, 2019-01-07T02:19:55, Epoch: 39, Batch: 260, Training Loss: 0.00011485368013381958, LR: 0.0001
Time, 2019-01-07T02:19:56, Epoch: 39, Batch: 270, Training Loss: 0.0001444607973098755, LR: 0.0001
Time, 2019-01-07T02:19:56, Epoch: 39, Batch: 280, Training Loss: 0.00020809471607208252, LR: 0.0001
Time, 2019-01-07T02:19:57, Epoch: 39, Batch: 290, Training Loss: 9.737163782119751e-05, LR: 0.0001
Time, 2019-01-07T02:19:58, Epoch: 39, Batch: 300, Training Loss: 0.00020244717597961426, LR: 0.0001
Time, 2019-01-07T02:19:58, Epoch: 39, Batch: 310, Training Loss: 0.00019501596689224242, LR: 0.0001
Time, 2019-01-07T02:19:59, Epoch: 39, Batch: 320, Training Loss: 0.0003233984112739563, LR: 0.0001
Time, 2019-01-07T02:20:00, Epoch: 39, Batch: 330, Training Loss: 0.00017535537481307983, LR: 0.0001
Time, 2019-01-07T02:20:00, Epoch: 39, Batch: 340, Training Loss: 0.00048816651105880736, LR: 0.0001
Time, 2019-01-07T02:20:01, Epoch: 39, Batch: 350, Training Loss: 0.0005080163478851319, LR: 0.0001
Time, 2019-01-07T02:20:02, Epoch: 39, Batch: 360, Training Loss: 0.00015888810157775878, LR: 0.0001
Time, 2019-01-07T02:20:02, Epoch: 39, Batch: 370, Training Loss: 0.00014491230249404907, LR: 0.0001
Time, 2019-01-07T02:20:03, Epoch: 39, Batch: 380, Training Loss: 0.00011368989944458008, LR: 0.0001
Time, 2019-01-07T02:20:04, Epoch: 39, Batch: 390, Training Loss: 0.00016489475965499877, LR: 0.0001
Time, 2019-01-07T02:20:04, Epoch: 39, Batch: 400, Training Loss: 0.00018685460090637208, LR: 0.0001
Time, 2019-01-07T02:20:05, Epoch: 39, Batch: 410, Training Loss: 0.0001798585057258606, LR: 0.0001
Time, 2019-01-07T02:20:06, Epoch: 39, Batch: 420, Training Loss: 0.00014432817697525025, LR: 0.0001
Time, 2019-01-07T02:20:07, Epoch: 39, Batch: 430, Training Loss: 0.00014608800411224364, LR: 0.0001
Time, 2019-01-07T02:20:07, Epoch: 39, Batch: 440, Training Loss: 0.0003372415900230408, LR: 0.0001
Time, 2019-01-07T02:20:08, Epoch: 39, Batch: 450, Training Loss: 0.00020022243261337281, LR: 0.0001
Time, 2019-01-07T02:20:09, Epoch: 39, Batch: 460, Training Loss: 0.0002480745315551758, LR: 0.0001
Time, 2019-01-07T02:20:10, Epoch: 39, Batch: 470, Training Loss: 0.00024372190237045288, LR: 0.0001
Time, 2019-01-07T02:20:11, Epoch: 39, Batch: 480, Training Loss: 0.0002915844321250916, LR: 0.0001
Time, 2019-01-07T02:20:11, Epoch: 39, Batch: 490, Training Loss: 0.0002205699682235718, LR: 0.0001
Time, 2019-01-07T02:20:12, Epoch: 39, Batch: 500, Training Loss: 0.00025003105401992797, LR: 0.0001
Time, 2019-01-07T02:20:13, Epoch: 39, Batch: 510, Training Loss: 0.00040796399116516113, LR: 0.0001
Time, 2019-01-07T02:20:13, Epoch: 39, Batch: 520, Training Loss: 0.00018467307090759278, LR: 0.0001
Time, 2019-01-07T02:20:14, Epoch: 39, Batch: 530, Training Loss: 0.00033613443374633787, LR: 0.0001
Time, 2019-01-07T02:20:15, Epoch: 39, Batch: 540, Training Loss: 0.00021703243255615233, LR: 0.0001
Time, 2019-01-07T02:20:16, Epoch: 39, Batch: 550, Training Loss: 0.00039614737033843994, LR: 0.0001
Time, 2019-01-07T02:20:17, Epoch: 39, Batch: 560, Training Loss: 0.00018118023872375487, LR: 0.0001
Time, 2019-01-07T02:20:17, Epoch: 39, Batch: 570, Training Loss: 0.0002573028206825256, LR: 0.0001
Time, 2019-01-07T02:20:18, Epoch: 39, Batch: 580, Training Loss: 0.00031553953886032104, LR: 0.0001
Time, 2019-01-07T02:20:19, Epoch: 39, Batch: 590, Training Loss: 0.00017193853855133057, LR: 0.0001
Time, 2019-01-07T02:20:20, Epoch: 39, Batch: 600, Training Loss: 0.00014092475175857543, LR: 0.0001
Time, 2019-01-07T02:20:20, Epoch: 39, Batch: 610, Training Loss: 0.0004552140831947327, LR: 0.0001
Time, 2019-01-07T02:20:21, Epoch: 39, Batch: 620, Training Loss: 0.0001719847321510315, LR: 0.0001
Time, 2019-01-07T02:20:21, Epoch: 39, Batch: 630, Training Loss: 0.00021137595176696778, LR: 0.0001
Time, 2019-01-07T02:20:22, Epoch: 39, Batch: 640, Training Loss: 0.0002847805619239807, LR: 0.0001
Time, 2019-01-07T02:20:23, Epoch: 39, Batch: 650, Training Loss: 0.00016123652458190917, LR: 0.0001
Time, 2019-01-07T02:20:23, Epoch: 39, Batch: 660, Training Loss: 0.00030359327793121337, LR: 0.0001
Time, 2019-01-07T02:20:24, Epoch: 39, Batch: 670, Training Loss: 0.00018650144338607788, LR: 0.0001
Time, 2019-01-07T02:20:25, Epoch: 39, Batch: 680, Training Loss: 8.952617645263672e-05, LR: 0.0001
Time, 2019-01-07T02:20:25, Epoch: 39, Batch: 690, Training Loss: 0.0002473205327987671, LR: 0.0001
Time, 2019-01-07T02:20:26, Epoch: 39, Batch: 700, Training Loss: 0.00012057870626449584, LR: 0.0001
Time, 2019-01-07T02:20:27, Epoch: 39, Batch: 710, Training Loss: 0.00018198937177658082, LR: 0.0001
Time, 2019-01-07T02:20:27, Epoch: 39, Batch: 720, Training Loss: 0.001729324460029602, LR: 0.0001
Time, 2019-01-07T02:20:28, Epoch: 39, Batch: 730, Training Loss: 0.0001279890537261963, LR: 0.0001
Time, 2019-01-07T02:20:29, Epoch: 39, Batch: 740, Training Loss: 0.0002130970358848572, LR: 0.0001
Time, 2019-01-07T02:20:29, Epoch: 39, Batch: 750, Training Loss: 0.00016431063413619996, LR: 0.0001
Time, 2019-01-07T02:20:30, Epoch: 39, Batch: 760, Training Loss: 0.00017896592617034913, LR: 0.0001
Time, 2019-01-07T02:20:31, Epoch: 39, Batch: 770, Training Loss: 6.22972846031189e-05, LR: 0.0001
Time, 2019-01-07T02:20:31, Epoch: 39, Batch: 780, Training Loss: 0.0003288358449935913, LR: 0.0001
Time, 2019-01-07T02:20:32, Epoch: 39, Batch: 790, Training Loss: 0.00037833899259567263, LR: 0.0001
Time, 2019-01-07T02:20:33, Epoch: 39, Batch: 800, Training Loss: 0.00031253397464752196, LR: 0.0001
Time, 2019-01-07T02:20:33, Epoch: 39, Batch: 810, Training Loss: 0.00026597827672958374, LR: 0.0001
Time, 2019-01-07T02:20:34, Epoch: 39, Batch: 820, Training Loss: 0.00038623511791229247, LR: 0.0001
Time, 2019-01-07T02:20:35, Epoch: 39, Batch: 830, Training Loss: 0.00018246322870254516, LR: 0.0001
Time, 2019-01-07T02:20:35, Epoch: 39, Batch: 840, Training Loss: 0.00014788955450057984, LR: 0.0001
Time, 2019-01-07T02:20:36, Epoch: 39, Batch: 850, Training Loss: 0.00019397437572479248, LR: 0.0001
Time, 2019-01-07T02:20:37, Epoch: 39, Batch: 860, Training Loss: 7.406622171401978e-05, LR: 0.0001
Time, 2019-01-07T02:20:37, Epoch: 39, Batch: 870, Training Loss: 0.0003303244709968567, LR: 0.0001
Time, 2019-01-07T02:20:38, Epoch: 39, Batch: 880, Training Loss: 0.0024731710553169252, LR: 0.0001
Time, 2019-01-07T02:20:39, Epoch: 39, Batch: 890, Training Loss: 0.00011196434497833252, LR: 0.0001
Time, 2019-01-07T02:20:39, Epoch: 39, Batch: 900, Training Loss: 0.00029592961072921753, LR: 0.0001
Time, 2019-01-07T02:20:40, Epoch: 39, Batch: 910, Training Loss: 0.0007764205336570739, LR: 0.0001
Time, 2019-01-07T02:20:41, Epoch: 39, Batch: 920, Training Loss: 0.0004061654210090637, LR: 0.0001
Time, 2019-01-07T02:20:42, Epoch: 39, Batch: 930, Training Loss: 0.00030514150857925416, LR: 0.0001
Epoch: 39, Validation Top 1 acc: 99.01473236083984
Epoch: 39, Validation Top 5 acc: 100.0
Epoch: 39, Validation Set Loss: 0.04131252318620682
Start training epoch 40
Time, 2019-01-07T02:20:45, Epoch: 40, Batch: 10, Training Loss: 0.00011590272188186646, LR: 0.0001
Time, 2019-01-07T02:20:46, Epoch: 40, Batch: 20, Training Loss: 0.00028140097856521606, LR: 0.0001
Time, 2019-01-07T02:20:47, Epoch: 40, Batch: 30, Training Loss: 0.00015147477388381957, LR: 0.0001
Time, 2019-01-07T02:20:47, Epoch: 40, Batch: 40, Training Loss: 0.00023799240589141845, LR: 0.0001
Time, 2019-01-07T02:20:48, Epoch: 40, Batch: 50, Training Loss: 0.0001730024814605713, LR: 0.0001
Time, 2019-01-07T02:20:49, Epoch: 40, Batch: 60, Training Loss: 8.800476789474487e-05, LR: 0.0001
Time, 2019-01-07T02:20:49, Epoch: 40, Batch: 70, Training Loss: 0.00017893612384796144, LR: 0.0001
Time, 2019-01-07T02:20:50, Epoch: 40, Batch: 80, Training Loss: 0.00022195726633071898, LR: 0.0001
Time, 2019-01-07T02:20:51, Epoch: 40, Batch: 90, Training Loss: 0.00011110007762908936, LR: 0.0001
Time, 2019-01-07T02:20:51, Epoch: 40, Batch: 100, Training Loss: 0.00019360482692718506, LR: 0.0001
Time, 2019-01-07T02:20:52, Epoch: 40, Batch: 110, Training Loss: 0.00015787482261657714, LR: 0.0001
Time, 2019-01-07T02:20:53, Epoch: 40, Batch: 120, Training Loss: 0.0001996651291847229, LR: 0.0001
Time, 2019-01-07T02:20:53, Epoch: 40, Batch: 130, Training Loss: 0.00016922056674957276, LR: 0.0001
Time, 2019-01-07T02:20:54, Epoch: 40, Batch: 140, Training Loss: 6.91056251525879e-05, LR: 0.0001
Time, 2019-01-07T02:20:55, Epoch: 40, Batch: 150, Training Loss: 0.0002890363335609436, LR: 0.0001
Time, 2019-01-07T02:20:55, Epoch: 40, Batch: 160, Training Loss: 0.0002660572528839111, LR: 0.0001
Time, 2019-01-07T02:20:56, Epoch: 40, Batch: 170, Training Loss: 0.00017445981502532958, LR: 0.0001
Time, 2019-01-07T02:20:57, Epoch: 40, Batch: 180, Training Loss: 0.00016322433948516845, LR: 0.0001
Time, 2019-01-07T02:20:57, Epoch: 40, Batch: 190, Training Loss: 0.00021686106920242308, LR: 0.0001
Time, 2019-01-07T02:20:58, Epoch: 40, Batch: 200, Training Loss: 0.00032455772161483766, LR: 0.0001
Time, 2019-01-07T02:20:59, Epoch: 40, Batch: 210, Training Loss: 0.0001927405595779419, LR: 0.0001
Time, 2019-01-07T02:20:59, Epoch: 40, Batch: 220, Training Loss: 0.0001402720808982849, LR: 0.0001
Time, 2019-01-07T02:21:00, Epoch: 40, Batch: 230, Training Loss: 0.0002711191773414612, LR: 0.0001
Time, 2019-01-07T02:21:01, Epoch: 40, Batch: 240, Training Loss: 0.0001970827579498291, LR: 0.0001
Time, 2019-01-07T02:21:01, Epoch: 40, Batch: 250, Training Loss: 0.001693175733089447, LR: 0.0001
Time, 2019-01-07T02:21:02, Epoch: 40, Batch: 260, Training Loss: 0.00015551149845123292, LR: 0.0001
Time, 2019-01-07T02:21:03, Epoch: 40, Batch: 270, Training Loss: 0.00013161450624465942, LR: 0.0001
Time, 2019-01-07T02:21:04, Epoch: 40, Batch: 280, Training Loss: 0.00018446445465087892, LR: 0.0001
Time, 2019-01-07T02:21:05, Epoch: 40, Batch: 290, Training Loss: 0.00021688789129257203, LR: 0.0001
Time, 2019-01-07T02:21:06, Epoch: 40, Batch: 300, Training Loss: 0.00014634430408477783, LR: 0.0001
Time, 2019-01-07T02:21:06, Epoch: 40, Batch: 310, Training Loss: 0.00013432353734970092, LR: 0.0001
Time, 2019-01-07T02:21:07, Epoch: 40, Batch: 320, Training Loss: 0.00011944323778152466, LR: 0.0001
Time, 2019-01-07T02:21:08, Epoch: 40, Batch: 330, Training Loss: 0.00018315613269805908, LR: 0.0001
Time, 2019-01-07T02:21:09, Epoch: 40, Batch: 340, Training Loss: 0.00025201886892318727, LR: 0.0001
Time, 2019-01-07T02:21:10, Epoch: 40, Batch: 350, Training Loss: 0.0001366168260574341, LR: 0.0001
Time, 2019-01-07T02:21:11, Epoch: 40, Batch: 360, Training Loss: 0.00021526515483856202, LR: 0.0001
Time, 2019-01-07T02:21:11, Epoch: 40, Batch: 370, Training Loss: 0.0002528190612792969, LR: 0.0001
Time, 2019-01-07T02:21:12, Epoch: 40, Batch: 380, Training Loss: 0.00023811906576156616, LR: 0.0001
Time, 2019-01-07T02:21:13, Epoch: 40, Batch: 390, Training Loss: 0.0001511216163635254, LR: 0.0001
Time, 2019-01-07T02:21:14, Epoch: 40, Batch: 400, Training Loss: 0.00011572837829589843, LR: 0.0001
Time, 2019-01-07T02:21:15, Epoch: 40, Batch: 410, Training Loss: 0.0001251697540283203, LR: 0.0001
Time, 2019-01-07T02:21:15, Epoch: 40, Batch: 420, Training Loss: 0.00015878677368164062, LR: 0.0001
Time, 2019-01-07T02:21:16, Epoch: 40, Batch: 430, Training Loss: 0.00018056333065032958, LR: 0.0001
Time, 2019-01-07T02:21:17, Epoch: 40, Batch: 440, Training Loss: 5.019158124923706e-05, LR: 0.0001
Time, 2019-01-07T02:21:18, Epoch: 40, Batch: 450, Training Loss: 0.00023046284914016724, LR: 0.0001
Time, 2019-01-07T02:21:19, Epoch: 40, Batch: 460, Training Loss: 0.00020345449447631836, LR: 0.0001
Time, 2019-01-07T02:21:19, Epoch: 40, Batch: 470, Training Loss: 0.00017523616552352905, LR: 0.0001
Time, 2019-01-07T02:21:20, Epoch: 40, Batch: 480, Training Loss: 0.00048308968544006345, LR: 0.0001
Time, 2019-01-07T02:21:21, Epoch: 40, Batch: 490, Training Loss: 0.00045638680458068845, LR: 0.0001
Time, 2019-01-07T02:21:22, Epoch: 40, Batch: 500, Training Loss: 0.0001376405358314514, LR: 0.0001
Time, 2019-01-07T02:21:23, Epoch: 40, Batch: 510, Training Loss: 0.00014866739511489868, LR: 0.0001
Time, 2019-01-07T02:21:24, Epoch: 40, Batch: 520, Training Loss: 0.00028505921363830566, LR: 0.0001
Time, 2019-01-07T02:21:24, Epoch: 40, Batch: 530, Training Loss: 0.00018092095851898193, LR: 0.0001
Time, 2019-01-07T02:21:25, Epoch: 40, Batch: 540, Training Loss: 0.00026476383209228516, LR: 0.0001
Time, 2019-01-07T02:21:26, Epoch: 40, Batch: 550, Training Loss: 9.178519248962403e-05, LR: 0.0001
Time, 2019-01-07T02:21:27, Epoch: 40, Batch: 560, Training Loss: 0.00018263161182403566, LR: 0.0001
Time, 2019-01-07T02:21:27, Epoch: 40, Batch: 570, Training Loss: 0.00014266222715377807, LR: 0.0001
Time, 2019-01-07T02:21:28, Epoch: 40, Batch: 580, Training Loss: 0.0001924365758895874, LR: 0.0001
Time, 2019-01-07T02:21:29, Epoch: 40, Batch: 590, Training Loss: 0.00020385682582855225, LR: 0.0001
Time, 2019-01-07T02:21:30, Epoch: 40, Batch: 600, Training Loss: 0.00013247430324554444, LR: 0.0001
Time, 2019-01-07T02:21:31, Epoch: 40, Batch: 610, Training Loss: 0.00013156384229660035, LR: 0.0001
Time, 2019-01-07T02:21:31, Epoch: 40, Batch: 620, Training Loss: 0.00025136470794677733, LR: 0.0001
Time, 2019-01-07T02:21:32, Epoch: 40, Batch: 630, Training Loss: 0.00024306625127792358, LR: 0.0001
Time, 2019-01-07T02:21:33, Epoch: 40, Batch: 640, Training Loss: 0.00017601102590560913, LR: 0.0001
Time, 2019-01-07T02:21:34, Epoch: 40, Batch: 650, Training Loss: 9.120702743530274e-05, LR: 0.0001
Time, 2019-01-07T02:21:34, Epoch: 40, Batch: 660, Training Loss: 0.00014696121215820312, LR: 0.0001
Time, 2019-01-07T02:21:35, Epoch: 40, Batch: 670, Training Loss: 0.00014877915382385253, LR: 0.0001
Time, 2019-01-07T02:21:36, Epoch: 40, Batch: 680, Training Loss: 0.0003149151802062988, LR: 0.0001
Time, 2019-01-07T02:21:36, Epoch: 40, Batch: 690, Training Loss: 0.0001835748553276062, LR: 0.0001
Time, 2019-01-07T02:21:37, Epoch: 40, Batch: 700, Training Loss: 0.0002218380570411682, LR: 0.0001
Time, 2019-01-07T02:21:38, Epoch: 40, Batch: 710, Training Loss: 0.0001650005578994751, LR: 0.0001
Time, 2019-01-07T02:21:39, Epoch: 40, Batch: 720, Training Loss: 0.00041405707597732546, LR: 0.0001
Time, 2019-01-07T02:21:39, Epoch: 40, Batch: 730, Training Loss: 0.00012788325548171998, LR: 0.0001
Time, 2019-01-07T02:21:40, Epoch: 40, Batch: 740, Training Loss: 0.00027915686368942263, LR: 0.0001
Time, 2019-01-07T02:21:41, Epoch: 40, Batch: 750, Training Loss: 0.00021297633647918702, LR: 0.0001
Time, 2019-01-07T02:21:41, Epoch: 40, Batch: 760, Training Loss: 0.00020378679037094116, LR: 0.0001
Time, 2019-01-07T02:21:42, Epoch: 40, Batch: 770, Training Loss: 7.98150897026062e-05, LR: 0.0001
Time, 2019-01-07T02:21:43, Epoch: 40, Batch: 780, Training Loss: 0.0002225726842880249, LR: 0.0001
Time, 2019-01-07T02:21:44, Epoch: 40, Batch: 790, Training Loss: 0.00022431015968322755, LR: 0.0001
Time, 2019-01-07T02:21:44, Epoch: 40, Batch: 800, Training Loss: 0.0001593664288520813, LR: 0.0001
Time, 2019-01-07T02:21:45, Epoch: 40, Batch: 810, Training Loss: 0.0001591801643371582, LR: 0.0001
Time, 2019-01-07T02:21:46, Epoch: 40, Batch: 820, Training Loss: 0.00028744637966156007, LR: 0.0001
Time, 2019-01-07T02:21:47, Epoch: 40, Batch: 830, Training Loss: 0.00016539543867111206, LR: 0.0001
Time, 2019-01-07T02:21:47, Epoch: 40, Batch: 840, Training Loss: 0.0009923607110977172, LR: 0.0001
Time, 2019-01-07T02:21:48, Epoch: 40, Batch: 850, Training Loss: 0.00018258094787597655, LR: 0.0001
Time, 2019-01-07T02:21:49, Epoch: 40, Batch: 860, Training Loss: 0.00031745433807373047, LR: 0.0001
Time, 2019-01-07T02:21:49, Epoch: 40, Batch: 870, Training Loss: 0.00010836422443389892, LR: 0.0001
Time, 2019-01-07T02:21:50, Epoch: 40, Batch: 880, Training Loss: 0.0002136245369911194, LR: 0.0001
Time, 2019-01-07T02:21:51, Epoch: 40, Batch: 890, Training Loss: 0.0001542225480079651, LR: 0.0001
Time, 2019-01-07T02:21:51, Epoch: 40, Batch: 900, Training Loss: 0.00013153254985809326, LR: 0.0001
Time, 2019-01-07T02:21:52, Epoch: 40, Batch: 910, Training Loss: 0.00010741651058197022, LR: 0.0001
Time, 2019-01-07T02:21:53, Epoch: 40, Batch: 920, Training Loss: 0.00012536346912384033, LR: 0.0001
Time, 2019-01-07T02:21:53, Epoch: 40, Batch: 930, Training Loss: 0.00018492341041564941, LR: 0.0001
Epoch: 40, Validation Top 1 acc: 98.99482727050781
Epoch: 40, Validation Top 5 acc: 100.0
Epoch: 40, Validation Set Loss: 0.04126528277993202
Start training epoch 41
Time, 2019-01-07T02:21:57, Epoch: 41, Batch: 10, Training Loss: 0.00017215758562088012, LR: 0.0001
Time, 2019-01-07T02:21:58, Epoch: 41, Batch: 20, Training Loss: 0.00010720938444137573, LR: 0.0001
Time, 2019-01-07T02:21:58, Epoch: 41, Batch: 30, Training Loss: 0.00012691617012023927, LR: 0.0001
Time, 2019-01-07T02:21:59, Epoch: 41, Batch: 40, Training Loss: 6.737262010574341e-05, LR: 0.0001
Time, 2019-01-07T02:22:00, Epoch: 41, Batch: 50, Training Loss: 9.987056255340576e-05, LR: 0.0001
Time, 2019-01-07T02:22:00, Epoch: 41, Batch: 60, Training Loss: 0.0001448720693588257, LR: 0.0001
Time, 2019-01-07T02:22:01, Epoch: 41, Batch: 70, Training Loss: 0.00024354010820388795, LR: 0.0001
Time, 2019-01-07T02:22:02, Epoch: 41, Batch: 80, Training Loss: 0.00019615143537521362, LR: 0.0001
Time, 2019-01-07T02:22:03, Epoch: 41, Batch: 90, Training Loss: 0.00012710243463516236, LR: 0.0001
Time, 2019-01-07T02:22:03, Epoch: 41, Batch: 100, Training Loss: 0.0001199275255203247, LR: 0.0001
Time, 2019-01-07T02:22:04, Epoch: 41, Batch: 110, Training Loss: 0.00017036646604537964, LR: 0.0001
Time, 2019-01-07T02:22:05, Epoch: 41, Batch: 120, Training Loss: 0.00013458430767059325, LR: 0.0001
Time, 2019-01-07T02:22:05, Epoch: 41, Batch: 130, Training Loss: 0.00026065707206726076, LR: 0.0001
Time, 2019-01-07T02:22:06, Epoch: 41, Batch: 140, Training Loss: 0.00010780096054077148, LR: 0.0001
Time, 2019-01-07T02:22:07, Epoch: 41, Batch: 150, Training Loss: 6.203502416610718e-05, LR: 0.0001
Time, 2019-01-07T02:22:08, Epoch: 41, Batch: 160, Training Loss: 0.00011493712663650513, LR: 0.0001
Time, 2019-01-07T02:22:08, Epoch: 41, Batch: 170, Training Loss: 0.00014389604330062867, LR: 0.0001
Time, 2019-01-07T02:22:09, Epoch: 41, Batch: 180, Training Loss: 0.00014604777097702027, LR: 0.0001
Time, 2019-01-07T02:22:10, Epoch: 41, Batch: 190, Training Loss: 0.0002025887370109558, LR: 0.0001
Time, 2019-01-07T02:22:10, Epoch: 41, Batch: 200, Training Loss: 0.00012755095958709716, LR: 0.0001
Time, 2019-01-07T02:22:11, Epoch: 41, Batch: 210, Training Loss: 9.464621543884278e-05, LR: 0.0001
Time, 2019-01-07T02:22:12, Epoch: 41, Batch: 220, Training Loss: 9.03293490409851e-05, LR: 0.0001
Time, 2019-01-07T02:22:13, Epoch: 41, Batch: 230, Training Loss: 0.00011588335037231446, LR: 0.0001
Time, 2019-01-07T02:22:13, Epoch: 41, Batch: 240, Training Loss: 0.00022743642330169678, LR: 0.0001
Time, 2019-01-07T02:22:14, Epoch: 41, Batch: 250, Training Loss: 0.0001710742712020874, LR: 0.0001
Time, 2019-01-07T02:22:15, Epoch: 41, Batch: 260, Training Loss: 9.144246578216553e-05, LR: 0.0001
Time, 2019-01-07T02:22:16, Epoch: 41, Batch: 270, Training Loss: 0.00013856440782546997, LR: 0.0001
Time, 2019-01-07T02:22:16, Epoch: 41, Batch: 280, Training Loss: 8.114427328109741e-05, LR: 0.0001
Time, 2019-01-07T02:22:17, Epoch: 41, Batch: 290, Training Loss: 0.00012697726488113402, LR: 0.0001
Time, 2019-01-07T02:22:18, Epoch: 41, Batch: 300, Training Loss: 8.129924535751342e-05, LR: 0.0001
Time, 2019-01-07T02:22:19, Epoch: 41, Batch: 310, Training Loss: 0.000185452401638031, LR: 0.0001
Time, 2019-01-07T02:22:20, Epoch: 41, Batch: 320, Training Loss: 0.00016269981861114502, LR: 0.0001
Time, 2019-01-07T02:22:21, Epoch: 41, Batch: 330, Training Loss: 0.00044031590223312376, LR: 0.0001
Time, 2019-01-07T02:22:21, Epoch: 41, Batch: 340, Training Loss: 0.001961927115917206, LR: 0.0001
Time, 2019-01-07T02:22:22, Epoch: 41, Batch: 350, Training Loss: 0.00011739879846572876, LR: 0.0001
Time, 2019-01-07T02:22:23, Epoch: 41, Batch: 360, Training Loss: 9.807795286178588e-05, LR: 0.0001
Time, 2019-01-07T02:22:23, Epoch: 41, Batch: 370, Training Loss: 0.0004607230424880981, LR: 0.0001
Time, 2019-01-07T02:22:24, Epoch: 41, Batch: 380, Training Loss: 0.0001238808035850525, LR: 0.0001
Time, 2019-01-07T02:22:25, Epoch: 41, Batch: 390, Training Loss: 0.00014656484127044677, LR: 0.0001
Time, 2019-01-07T02:22:25, Epoch: 41, Batch: 400, Training Loss: 0.00021406710147857665, LR: 0.0001
Time, 2019-01-07T02:22:26, Epoch: 41, Batch: 410, Training Loss: 0.00013077110052108764, LR: 0.0001
Time, 2019-01-07T02:22:27, Epoch: 41, Batch: 420, Training Loss: 0.0004762202501296997, LR: 0.0001
Time, 2019-01-07T02:22:27, Epoch: 41, Batch: 430, Training Loss: 0.00010937601327896119, LR: 0.0001
Time, 2019-01-07T02:22:28, Epoch: 41, Batch: 440, Training Loss: 0.00023628026247024536, LR: 0.0001
Time, 2019-01-07T02:22:29, Epoch: 41, Batch: 450, Training Loss: 0.0001444980502128601, LR: 0.0001
Time, 2019-01-07T02:22:30, Epoch: 41, Batch: 460, Training Loss: 0.00015365183353424072, LR: 0.0001
Time, 2019-01-07T02:22:31, Epoch: 41, Batch: 470, Training Loss: 0.00012208819389343263, LR: 0.0001
Time, 2019-01-07T02:22:32, Epoch: 41, Batch: 480, Training Loss: 0.00031084269285202025, LR: 0.0001
Time, 2019-01-07T02:22:32, Epoch: 41, Batch: 490, Training Loss: 0.00020176321268081666, LR: 0.0001
Time, 2019-01-07T02:22:33, Epoch: 41, Batch: 500, Training Loss: 0.0001311779022216797, LR: 0.0001
Time, 2019-01-07T02:22:34, Epoch: 41, Batch: 510, Training Loss: 0.00011969208717346191, LR: 0.0001
Time, 2019-01-07T02:22:35, Epoch: 41, Batch: 520, Training Loss: 0.00015490204095840453, LR: 0.0001
Time, 2019-01-07T02:22:35, Epoch: 41, Batch: 530, Training Loss: 6.02036714553833e-05, LR: 0.0001
Time, 2019-01-07T02:22:36, Epoch: 41, Batch: 540, Training Loss: 0.00010469108819961548, LR: 0.0001
Time, 2019-01-07T02:22:37, Epoch: 41, Batch: 550, Training Loss: 0.0001708984375, LR: 0.0001
Time, 2019-01-07T02:22:37, Epoch: 41, Batch: 560, Training Loss: 0.00017028450965881348, LR: 0.0001
Time, 2019-01-07T02:22:38, Epoch: 41, Batch: 570, Training Loss: 0.00010520666837692261, LR: 0.0001
Time, 2019-01-07T02:22:39, Epoch: 41, Batch: 580, Training Loss: 0.00017324090003967285, LR: 0.0001
Time, 2019-01-07T02:22:39, Epoch: 41, Batch: 590, Training Loss: 9.265542030334473e-05, LR: 0.0001
Time, 2019-01-07T02:22:40, Epoch: 41, Batch: 600, Training Loss: 9.555667638778686e-05, LR: 0.0001
Time, 2019-01-07T02:22:41, Epoch: 41, Batch: 610, Training Loss: 0.0002278253436088562, LR: 0.0001
Time, 2019-01-07T02:22:42, Epoch: 41, Batch: 620, Training Loss: 0.0001685023307800293, LR: 0.0001
Time, 2019-01-07T02:22:42, Epoch: 41, Batch: 630, Training Loss: 0.00021550804376602173, LR: 0.0001
Time, 2019-01-07T02:22:43, Epoch: 41, Batch: 640, Training Loss: 0.00018690228462219237, LR: 0.0001
Time, 2019-01-07T02:22:44, Epoch: 41, Batch: 650, Training Loss: 0.0001235276460647583, LR: 0.0001
Time, 2019-01-07T02:22:45, Epoch: 41, Batch: 660, Training Loss: 0.0001232936978340149, LR: 0.0001
Time, 2019-01-07T02:22:45, Epoch: 41, Batch: 670, Training Loss: 0.00023453831672668458, LR: 0.0001
Time, 2019-01-07T02:22:46, Epoch: 41, Batch: 680, Training Loss: 0.00010164380073547363, LR: 0.0001
Time, 2019-01-07T02:22:47, Epoch: 41, Batch: 690, Training Loss: 0.00030909478664398193, LR: 0.0001
Time, 2019-01-07T02:22:47, Epoch: 41, Batch: 700, Training Loss: 0.00036704540252685547, LR: 0.0001
Time, 2019-01-07T02:22:48, Epoch: 41, Batch: 710, Training Loss: 9.916722774505615e-05, LR: 0.0001
Time, 2019-01-07T02:22:49, Epoch: 41, Batch: 720, Training Loss: 0.00021754205226898193, LR: 0.0001
Time, 2019-01-07T02:22:50, Epoch: 41, Batch: 730, Training Loss: 0.00018177330493927002, LR: 0.0001
Time, 2019-01-07T02:22:50, Epoch: 41, Batch: 740, Training Loss: 0.00010298639535903931, LR: 0.0001
Time, 2019-01-07T02:22:51, Epoch: 41, Batch: 750, Training Loss: 0.00023409128189086915, LR: 0.0001
Time, 2019-01-07T02:22:52, Epoch: 41, Batch: 760, Training Loss: 0.0001899242401123047, LR: 0.0001
Time, 2019-01-07T02:22:52, Epoch: 41, Batch: 770, Training Loss: 0.0001222848892211914, LR: 0.0001
Time, 2019-01-07T02:22:53, Epoch: 41, Batch: 780, Training Loss: 0.00017133653163909913, LR: 0.0001
Time, 2019-01-07T02:22:54, Epoch: 41, Batch: 790, Training Loss: 0.00018822252750396728, LR: 0.0001
Time, 2019-01-07T02:22:54, Epoch: 41, Batch: 800, Training Loss: 0.0002834990620613098, LR: 0.0001
Time, 2019-01-07T02:22:55, Epoch: 41, Batch: 810, Training Loss: 0.00015165507793426513, LR: 0.0001
Time, 2019-01-07T02:22:56, Epoch: 41, Batch: 820, Training Loss: 0.0001363217830657959, LR: 0.0001
Time, 2019-01-07T02:22:56, Epoch: 41, Batch: 830, Training Loss: 0.0001591816544532776, LR: 0.0001
Time, 2019-01-07T02:22:57, Epoch: 41, Batch: 840, Training Loss: 0.0001373589038848877, LR: 0.0001
Time, 2019-01-07T02:22:58, Epoch: 41, Batch: 850, Training Loss: 8.519142866134644e-05, LR: 0.0001
Time, 2019-01-07T02:22:58, Epoch: 41, Batch: 860, Training Loss: 0.0002567574381828308, LR: 0.0001
Time, 2019-01-07T02:22:59, Epoch: 41, Batch: 870, Training Loss: 0.00013493597507476806, LR: 0.0001
Time, 2019-01-07T02:23:00, Epoch: 41, Batch: 880, Training Loss: 8.339434862136841e-05, LR: 0.0001
Time, 2019-01-07T02:23:01, Epoch: 41, Batch: 890, Training Loss: 0.00017210990190505982, LR: 0.0001
Time, 2019-01-07T02:23:01, Epoch: 41, Batch: 900, Training Loss: 8.403658866882325e-05, LR: 0.0001
Time, 2019-01-07T02:23:02, Epoch: 41, Batch: 910, Training Loss: 0.00026157498359680176, LR: 0.0001
Time, 2019-01-07T02:23:03, Epoch: 41, Batch: 920, Training Loss: 6.370395421981812e-05, LR: 0.0001
Time, 2019-01-07T02:23:04, Epoch: 41, Batch: 930, Training Loss: 0.00020549744367599488, LR: 0.0001
Epoch: 41, Validation Top 1 acc: 99.00477600097656
Epoch: 41, Validation Top 5 acc: 100.0
Epoch: 41, Validation Set Loss: 0.04120367392897606
Start training epoch 42
Time, 2019-01-07T02:23:07, Epoch: 42, Batch: 10, Training Loss: 0.00011602044105529785, LR: 0.0001
Time, 2019-01-07T02:23:08, Epoch: 42, Batch: 20, Training Loss: 6.102919578552246e-05, LR: 0.0001
Time, 2019-01-07T02:23:08, Epoch: 42, Batch: 30, Training Loss: 0.0001249358057975769, LR: 0.0001
Time, 2019-01-07T02:23:09, Epoch: 42, Batch: 40, Training Loss: 8.218437433242798e-05, LR: 0.0001
Time, 2019-01-07T02:23:10, Epoch: 42, Batch: 50, Training Loss: 9.053349494934083e-05, LR: 0.0001
Time, 2019-01-07T02:23:10, Epoch: 42, Batch: 60, Training Loss: 6.916970014572143e-05, LR: 0.0001
Time, 2019-01-07T02:23:11, Epoch: 42, Batch: 70, Training Loss: 0.00012004673480987549, LR: 0.0001
Time, 2019-01-07T02:23:12, Epoch: 42, Batch: 80, Training Loss: 8.720308542251587e-05, LR: 0.0001
Time, 2019-01-07T02:23:12, Epoch: 42, Batch: 90, Training Loss: 0.00016833394765853883, LR: 0.0001
Time, 2019-01-07T02:23:13, Epoch: 42, Batch: 100, Training Loss: 9.335726499557496e-05, LR: 0.0001
Time, 2019-01-07T02:23:14, Epoch: 42, Batch: 110, Training Loss: 0.00018490552902221679, LR: 0.0001
Time, 2019-01-07T02:23:15, Epoch: 42, Batch: 120, Training Loss: 9.815990924835205e-05, LR: 0.0001
Time, 2019-01-07T02:23:15, Epoch: 42, Batch: 130, Training Loss: 0.00013208985328674316, LR: 0.0001
Time, 2019-01-07T02:23:16, Epoch: 42, Batch: 140, Training Loss: 0.0001944616436958313, LR: 0.0001
Time, 2019-01-07T02:23:17, Epoch: 42, Batch: 150, Training Loss: 8.911192417144775e-05, LR: 0.0001
Time, 2019-01-07T02:23:17, Epoch: 42, Batch: 160, Training Loss: 0.0001263931393623352, LR: 0.0001
Time, 2019-01-07T02:23:18, Epoch: 42, Batch: 170, Training Loss: 0.000403343141078949, LR: 0.0001
Time, 2019-01-07T02:23:19, Epoch: 42, Batch: 180, Training Loss: 9.700506925582885e-05, LR: 0.0001
Time, 2019-01-07T02:23:20, Epoch: 42, Batch: 190, Training Loss: 9.717047214508057e-05, LR: 0.0001
Time, 2019-01-07T02:23:20, Epoch: 42, Batch: 200, Training Loss: 0.0001364201307296753, LR: 0.0001
Time, 2019-01-07T02:23:21, Epoch: 42, Batch: 210, Training Loss: 0.00018135756254196168, LR: 0.0001
Time, 2019-01-07T02:23:22, Epoch: 42, Batch: 220, Training Loss: 0.00020634979009628295, LR: 0.0001
Time, 2019-01-07T02:23:22, Epoch: 42, Batch: 230, Training Loss: 4.933327436447143e-05, LR: 0.0001
Time, 2019-01-07T02:23:23, Epoch: 42, Batch: 240, Training Loss: 6.650388240814209e-05, LR: 0.0001
Time, 2019-01-07T02:23:24, Epoch: 42, Batch: 250, Training Loss: 6.471425294876098e-05, LR: 0.0001
Time, 2019-01-07T02:23:24, Epoch: 42, Batch: 260, Training Loss: 9.28834080696106e-05, LR: 0.0001
Time, 2019-01-07T02:23:25, Epoch: 42, Batch: 270, Training Loss: 8.912086486816407e-05, LR: 0.0001
Time, 2019-01-07T02:23:26, Epoch: 42, Batch: 280, Training Loss: 9.573698043823242e-05, LR: 0.0001
Time, 2019-01-07T02:23:27, Epoch: 42, Batch: 290, Training Loss: 0.00011765360832214356, LR: 0.0001
Time, 2019-01-07T02:23:28, Epoch: 42, Batch: 300, Training Loss: 8.509457111358643e-05, LR: 0.0001
Time, 2019-01-07T02:23:28, Epoch: 42, Batch: 310, Training Loss: 0.00011450052261352539, LR: 0.0001
Time, 2019-01-07T02:23:29, Epoch: 42, Batch: 320, Training Loss: 0.00015511661767959596, LR: 0.0001
Time, 2019-01-07T02:23:30, Epoch: 42, Batch: 330, Training Loss: 4.8910081386566165e-05, LR: 0.0001
Time, 2019-01-07T02:23:31, Epoch: 42, Batch: 340, Training Loss: 7.196366786956787e-05, LR: 0.0001
Time, 2019-01-07T02:23:31, Epoch: 42, Batch: 350, Training Loss: 0.0001319780945777893, LR: 0.0001
Time, 2019-01-07T02:23:32, Epoch: 42, Batch: 360, Training Loss: 7.56099820137024e-05, LR: 0.0001
Time, 2019-01-07T02:23:33, Epoch: 42, Batch: 370, Training Loss: 0.00016939043998718262, LR: 0.0001
Time, 2019-01-07T02:23:33, Epoch: 42, Batch: 380, Training Loss: 9.50828194618225e-05, LR: 0.0001
Time, 2019-01-07T02:23:34, Epoch: 42, Batch: 390, Training Loss: 5.309432744979858e-05, LR: 0.0001
Time, 2019-01-07T02:23:35, Epoch: 42, Batch: 400, Training Loss: 0.0003104373812675476, LR: 0.0001
Time, 2019-01-07T02:23:36, Epoch: 42, Batch: 410, Training Loss: 0.00013474225997924804, LR: 0.0001
Time, 2019-01-07T02:23:36, Epoch: 42, Batch: 420, Training Loss: 6.667226552963257e-05, LR: 0.0001
Time, 2019-01-07T02:23:37, Epoch: 42, Batch: 430, Training Loss: 0.00019028186798095704, LR: 0.0001
Time, 2019-01-07T02:23:38, Epoch: 42, Batch: 440, Training Loss: 0.0001383841037750244, LR: 0.0001
Time, 2019-01-07T02:23:38, Epoch: 42, Batch: 450, Training Loss: 5.8016180992126466e-05, LR: 0.0001
Time, 2019-01-07T02:23:39, Epoch: 42, Batch: 460, Training Loss: 0.00010362714529037476, LR: 0.0001
Time, 2019-01-07T02:23:40, Epoch: 42, Batch: 470, Training Loss: 0.00010832101106643677, LR: 0.0001
Time, 2019-01-07T02:23:41, Epoch: 42, Batch: 480, Training Loss: 8.56935977935791e-05, LR: 0.0001
Time, 2019-01-07T02:23:42, Epoch: 42, Batch: 490, Training Loss: 5.567818880081177e-05, LR: 0.0001
Time, 2019-01-07T02:23:42, Epoch: 42, Batch: 500, Training Loss: 6.704181432724e-05, LR: 0.0001
Time, 2019-01-07T02:23:43, Epoch: 42, Batch: 510, Training Loss: 0.00018537938594818116, LR: 0.0001
Time, 2019-01-07T02:23:44, Epoch: 42, Batch: 520, Training Loss: 0.00011279284954071044, LR: 0.0001
Time, 2019-01-07T02:23:45, Epoch: 42, Batch: 530, Training Loss: 0.0001543402671813965, LR: 0.0001
Time, 2019-01-07T02:23:45, Epoch: 42, Batch: 540, Training Loss: 7.38590955734253e-05, LR: 0.0001
Time, 2019-01-07T02:23:46, Epoch: 42, Batch: 550, Training Loss: 0.00012767016887664794, LR: 0.0001
Time, 2019-01-07T02:23:47, Epoch: 42, Batch: 560, Training Loss: 9.793639183044433e-05, LR: 0.0001
Time, 2019-01-07T02:23:47, Epoch: 42, Batch: 570, Training Loss: 0.0001432359218597412, LR: 0.0001
Time, 2019-01-07T02:23:48, Epoch: 42, Batch: 580, Training Loss: 0.000139695405960083, LR: 0.0001
Time, 2019-01-07T02:23:49, Epoch: 42, Batch: 590, Training Loss: 9.817630052566528e-05, LR: 0.0001
Time, 2019-01-07T02:23:50, Epoch: 42, Batch: 600, Training Loss: 0.00017548501491546631, LR: 0.0001
Time, 2019-01-07T02:23:50, Epoch: 42, Batch: 610, Training Loss: 0.00011887252330780029, LR: 0.0001
Time, 2019-01-07T02:23:51, Epoch: 42, Batch: 620, Training Loss: 7.233917713165284e-05, LR: 0.0001
Time, 2019-01-07T02:23:52, Epoch: 42, Batch: 630, Training Loss: 0.00015511810779571532, LR: 0.0001
Time, 2019-01-07T02:23:52, Epoch: 42, Batch: 640, Training Loss: 8.56250524520874e-05, LR: 0.0001
Time, 2019-01-07T02:23:53, Epoch: 42, Batch: 650, Training Loss: 7.153302431106567e-05, LR: 0.0001
Time, 2019-01-07T02:23:54, Epoch: 42, Batch: 660, Training Loss: 0.00015624910593032838, LR: 0.0001
Time, 2019-01-07T02:23:55, Epoch: 42, Batch: 670, Training Loss: 0.00012627989053726196, LR: 0.0001
Time, 2019-01-07T02:23:56, Epoch: 42, Batch: 680, Training Loss: 0.0003595069050788879, LR: 0.0001
Time, 2019-01-07T02:23:56, Epoch: 42, Batch: 690, Training Loss: 7.231384515762329e-05, LR: 0.0001
Time, 2019-01-07T02:23:57, Epoch: 42, Batch: 700, Training Loss: 0.0015677616000175476, LR: 0.0001
Time, 2019-01-07T02:23:58, Epoch: 42, Batch: 710, Training Loss: 0.00010288059711456299, LR: 0.0001
Time, 2019-01-07T02:23:59, Epoch: 42, Batch: 720, Training Loss: 0.00012661218643188475, LR: 0.0001
Time, 2019-01-07T02:23:59, Epoch: 42, Batch: 730, Training Loss: 0.00020734816789627075, LR: 0.0001
Time, 2019-01-07T02:24:00, Epoch: 42, Batch: 740, Training Loss: 8.210092782974243e-05, LR: 0.0001
Time, 2019-01-07T02:24:01, Epoch: 42, Batch: 750, Training Loss: 0.000305841863155365, LR: 0.0001
Time, 2019-01-07T02:24:02, Epoch: 42, Batch: 760, Training Loss: 5.9713423252105716e-05, LR: 0.0001
Time, 2019-01-07T02:24:02, Epoch: 42, Batch: 770, Training Loss: 0.00011443495750427247, LR: 0.0001
Time, 2019-01-07T02:24:03, Epoch: 42, Batch: 780, Training Loss: 0.00024616271257400514, LR: 0.0001
Time, 2019-01-07T02:24:04, Epoch: 42, Batch: 790, Training Loss: 0.00011286884546279907, LR: 0.0001
Time, 2019-01-07T02:24:04, Epoch: 42, Batch: 800, Training Loss: 0.00020060539245605468, LR: 0.0001
Time, 2019-01-07T02:24:05, Epoch: 42, Batch: 810, Training Loss: 0.0001896694302558899, LR: 0.0001
Time, 2019-01-07T02:24:06, Epoch: 42, Batch: 820, Training Loss: 0.00011108219623565674, LR: 0.0001
Time, 2019-01-07T02:24:06, Epoch: 42, Batch: 830, Training Loss: 9.093880653381348e-05, LR: 0.0001
Time, 2019-01-07T02:24:07, Epoch: 42, Batch: 840, Training Loss: 0.0001257270574569702, LR: 0.0001
Time, 2019-01-07T02:24:08, Epoch: 42, Batch: 850, Training Loss: 0.000555342435836792, LR: 0.0001
Time, 2019-01-07T02:24:09, Epoch: 42, Batch: 860, Training Loss: 0.00018972158432006836, LR: 0.0001
Time, 2019-01-07T02:24:09, Epoch: 42, Batch: 870, Training Loss: 5.441755056381226e-05, LR: 0.0001
Time, 2019-01-07T02:24:10, Epoch: 42, Batch: 880, Training Loss: 9.836554527282715e-05, LR: 0.0001
Time, 2019-01-07T02:24:11, Epoch: 42, Batch: 890, Training Loss: 7.527768611907959e-05, LR: 0.0001
Time, 2019-01-07T02:24:11, Epoch: 42, Batch: 900, Training Loss: 6.43223524093628e-05, LR: 0.0001
Time, 2019-01-07T02:24:12, Epoch: 42, Batch: 910, Training Loss: 0.0001358821988105774, LR: 0.0001
Time, 2019-01-07T02:24:13, Epoch: 42, Batch: 920, Training Loss: 0.00011554509401321412, LR: 0.0001
Time, 2019-01-07T02:24:13, Epoch: 42, Batch: 930, Training Loss: 0.0002491101622581482, LR: 0.0001
Epoch: 42, Validation Top 1 acc: 99.05453491210938
Epoch: 42, Validation Top 5 acc: 100.0
Epoch: 42, Validation Set Loss: 0.04267537221312523
Start training epoch 43
Time, 2019-01-07T02:24:17, Epoch: 43, Batch: 10, Training Loss: 0.00013702213764190673, LR: 0.0001
Time, 2019-01-07T02:24:18, Epoch: 43, Batch: 20, Training Loss: 7.266104221343994e-05, LR: 0.0001
Time, 2019-01-07T02:24:18, Epoch: 43, Batch: 30, Training Loss: 9.848326444625855e-05, LR: 0.0001
Time, 2019-01-07T02:24:19, Epoch: 43, Batch: 40, Training Loss: 8.590668439865113e-05, LR: 0.0001
Time, 2019-01-07T02:24:19, Epoch: 43, Batch: 50, Training Loss: 0.00014153569936752318, LR: 0.0001
Time, 2019-01-07T02:24:20, Epoch: 43, Batch: 60, Training Loss: 0.00010030567646026611, LR: 0.0001
Time, 2019-01-07T02:24:21, Epoch: 43, Batch: 70, Training Loss: 0.00010779201984405518, LR: 0.0001
Time, 2019-01-07T02:24:22, Epoch: 43, Batch: 80, Training Loss: 0.00015356093645095824, LR: 0.0001
Time, 2019-01-07T02:24:22, Epoch: 43, Batch: 90, Training Loss: 0.00017068535089492798, LR: 0.0001
Time, 2019-01-07T02:24:23, Epoch: 43, Batch: 100, Training Loss: 7.275640964508057e-05, LR: 0.0001
Time, 2019-01-07T02:24:24, Epoch: 43, Batch: 110, Training Loss: 8.935779333114624e-05, LR: 0.0001
Time, 2019-01-07T02:24:24, Epoch: 43, Batch: 120, Training Loss: 0.000127449631690979, LR: 0.0001
Time, 2019-01-07T02:24:25, Epoch: 43, Batch: 130, Training Loss: 0.00017541199922561647, LR: 0.0001
Time, 2019-01-07T02:24:26, Epoch: 43, Batch: 140, Training Loss: 0.00012459903955459596, LR: 0.0001
Time, 2019-01-07T02:24:26, Epoch: 43, Batch: 150, Training Loss: 0.00013162493705749513, LR: 0.0001
Time, 2019-01-07T02:24:27, Epoch: 43, Batch: 160, Training Loss: 0.00020548105239868165, LR: 0.0001
Time, 2019-01-07T02:24:28, Epoch: 43, Batch: 170, Training Loss: 5.386322736740112e-05, LR: 0.0001
Time, 2019-01-07T02:24:28, Epoch: 43, Batch: 180, Training Loss: 7.871538400650024e-05, LR: 0.0001
Time, 2019-01-07T02:24:29, Epoch: 43, Batch: 190, Training Loss: 0.00023121535778045655, LR: 0.0001
Time, 2019-01-07T02:24:30, Epoch: 43, Batch: 200, Training Loss: 0.00013053268194198608, LR: 0.0001
Time, 2019-01-07T02:24:31, Epoch: 43, Batch: 210, Training Loss: 0.00014975965023040773, LR: 0.0001
Time, 2019-01-07T02:24:31, Epoch: 43, Batch: 220, Training Loss: 6.36458396911621e-05, LR: 0.0001
Time, 2019-01-07T02:24:32, Epoch: 43, Batch: 230, Training Loss: 9.561479091644288e-05, LR: 0.0001
Time, 2019-01-07T02:24:33, Epoch: 43, Batch: 240, Training Loss: 0.00010992437601089477, LR: 0.0001
Time, 2019-01-07T02:24:33, Epoch: 43, Batch: 250, Training Loss: 7.557570934295655e-05, LR: 0.0001
Time, 2019-01-07T02:24:34, Epoch: 43, Batch: 260, Training Loss: 8.330941200256348e-05, LR: 0.0001
Time, 2019-01-07T02:24:35, Epoch: 43, Batch: 270, Training Loss: 7.509291172027587e-05, LR: 0.0001
Time, 2019-01-07T02:24:35, Epoch: 43, Batch: 280, Training Loss: 7.653534412384033e-05, LR: 0.0001
Time, 2019-01-07T02:24:36, Epoch: 43, Batch: 290, Training Loss: 9.55238938331604e-05, LR: 0.0001
Time, 2019-01-07T02:24:37, Epoch: 43, Batch: 300, Training Loss: 8.177310228347778e-05, LR: 0.0001
Time, 2019-01-07T02:24:37, Epoch: 43, Batch: 310, Training Loss: 0.00014627724885940552, LR: 0.0001
Time, 2019-01-07T02:24:38, Epoch: 43, Batch: 320, Training Loss: 8.886754512786866e-05, LR: 0.0001
Time, 2019-01-07T02:24:39, Epoch: 43, Batch: 330, Training Loss: 0.00015321969985961914, LR: 0.0001
Time, 2019-01-07T02:24:40, Epoch: 43, Batch: 340, Training Loss: 0.00037887245416641236, LR: 0.0001
Time, 2019-01-07T02:24:40, Epoch: 43, Batch: 350, Training Loss: 6.14255666732788e-05, LR: 0.0001
Time, 2019-01-07T02:24:41, Epoch: 43, Batch: 360, Training Loss: 0.0001229092478752136, LR: 0.0001
Time, 2019-01-07T02:24:42, Epoch: 43, Batch: 370, Training Loss: 0.00014876127243041993, LR: 0.0001
Time, 2019-01-07T02:24:42, Epoch: 43, Batch: 380, Training Loss: 0.0001429513096809387, LR: 0.0001
Time, 2019-01-07T02:24:43, Epoch: 43, Batch: 390, Training Loss: 0.0003008246421813965, LR: 0.0001
Time, 2019-01-07T02:24:44, Epoch: 43, Batch: 400, Training Loss: 0.00011872351169586181, LR: 0.0001
Time, 2019-01-07T02:24:45, Epoch: 43, Batch: 410, Training Loss: 7.175952196121216e-05, LR: 0.0001
Time, 2019-01-07T02:24:45, Epoch: 43, Batch: 420, Training Loss: 7.974356412887573e-05, LR: 0.0001
Time, 2019-01-07T02:24:46, Epoch: 43, Batch: 430, Training Loss: 6.166994571685792e-05, LR: 0.0001
Time, 2019-01-07T02:24:47, Epoch: 43, Batch: 440, Training Loss: 8.293241262435913e-05, LR: 0.0001
Time, 2019-01-07T02:24:48, Epoch: 43, Batch: 450, Training Loss: 0.00013328045606613158, LR: 0.0001
Time, 2019-01-07T02:24:48, Epoch: 43, Batch: 460, Training Loss: 8.738338947296142e-05, LR: 0.0001
Time, 2019-01-07T02:24:49, Epoch: 43, Batch: 470, Training Loss: 7.445067167282105e-05, LR: 0.0001
Time, 2019-01-07T02:24:50, Epoch: 43, Batch: 480, Training Loss: 0.00012258142232894896, LR: 0.0001
Time, 2019-01-07T02:24:51, Epoch: 43, Batch: 490, Training Loss: 9.169131517410279e-05, LR: 0.0001
Time, 2019-01-07T02:24:51, Epoch: 43, Batch: 500, Training Loss: 0.00012631714344024658, LR: 0.0001
Time, 2019-01-07T02:24:52, Epoch: 43, Batch: 510, Training Loss: 0.00013205856084823607, LR: 0.0001
Time, 2019-01-07T02:24:53, Epoch: 43, Batch: 520, Training Loss: 0.00013130605220794677, LR: 0.0001
Time, 2019-01-07T02:24:53, Epoch: 43, Batch: 530, Training Loss: 0.00011710375547409057, LR: 0.0001
Time, 2019-01-07T02:24:54, Epoch: 43, Batch: 540, Training Loss: 7.510781288146972e-05, LR: 0.0001
Time, 2019-01-07T02:24:55, Epoch: 43, Batch: 550, Training Loss: 9.313225746154785e-05, LR: 0.0001
Time, 2019-01-07T02:24:56, Epoch: 43, Batch: 560, Training Loss: 8.574128150939941e-05, LR: 0.0001
Time, 2019-01-07T02:24:56, Epoch: 43, Batch: 570, Training Loss: 6.746351718902587e-05, LR: 0.0001
Time, 2019-01-07T02:24:57, Epoch: 43, Batch: 580, Training Loss: 6.707608699798583e-05, LR: 0.0001
Time, 2019-01-07T02:24:58, Epoch: 43, Batch: 590, Training Loss: 8.62717628479004e-05, LR: 0.0001
Time, 2019-01-07T02:24:58, Epoch: 43, Batch: 600, Training Loss: 2.319514751434326e-05, LR: 0.0001
Time, 2019-01-07T02:24:59, Epoch: 43, Batch: 610, Training Loss: 8.898526430130005e-05, LR: 0.0001
Time, 2019-01-07T02:25:00, Epoch: 43, Batch: 620, Training Loss: 5.972236394882202e-05, LR: 0.0001
Time, 2019-01-07T02:25:01, Epoch: 43, Batch: 630, Training Loss: 6.58676028251648e-05, LR: 0.0001
Time, 2019-01-07T02:25:01, Epoch: 43, Batch: 640, Training Loss: 6.214678287506104e-05, LR: 0.0001
Time, 2019-01-07T02:25:02, Epoch: 43, Batch: 650, Training Loss: 0.00011672079563140869, LR: 0.0001
Time, 2019-01-07T02:25:03, Epoch: 43, Batch: 660, Training Loss: 6.183236837387085e-05, LR: 0.0001
Time, 2019-01-07T02:25:04, Epoch: 43, Batch: 670, Training Loss: 7.417500019073487e-05, LR: 0.0001
Time, 2019-01-07T02:25:04, Epoch: 43, Batch: 680, Training Loss: 4.636496305465698e-05, LR: 0.0001
Time, 2019-01-07T02:25:05, Epoch: 43, Batch: 690, Training Loss: 5.9244036674499514e-05, LR: 0.0001
Time, 2019-01-07T02:25:06, Epoch: 43, Batch: 700, Training Loss: 0.00015245378017425537, LR: 0.0001
Time, 2019-01-07T02:25:07, Epoch: 43, Batch: 710, Training Loss: 0.00011307448148727417, LR: 0.0001
Time, 2019-01-07T02:25:07, Epoch: 43, Batch: 720, Training Loss: 7.098615169525147e-05, LR: 0.0001
Time, 2019-01-07T02:25:08, Epoch: 43, Batch: 730, Training Loss: 0.0001017436385154724, LR: 0.0001
Time, 2019-01-07T02:25:09, Epoch: 43, Batch: 740, Training Loss: 0.00010295212268829346, LR: 0.0001
Time, 2019-01-07T02:25:09, Epoch: 43, Batch: 750, Training Loss: 8.97824764251709e-05, LR: 0.0001
Time, 2019-01-07T02:25:10, Epoch: 43, Batch: 760, Training Loss: 6.654411554336548e-05, LR: 0.0001
Time, 2019-01-07T02:25:11, Epoch: 43, Batch: 770, Training Loss: 0.00015493184328079223, LR: 0.0001
Time, 2019-01-07T02:25:12, Epoch: 43, Batch: 780, Training Loss: 8.752197027206421e-05, LR: 0.0001
Time, 2019-01-07T02:25:12, Epoch: 43, Batch: 790, Training Loss: 7.290542125701905e-05, LR: 0.0001
Time, 2019-01-07T02:25:13, Epoch: 43, Batch: 800, Training Loss: 6.266981363296509e-05, LR: 0.0001
Time, 2019-01-07T02:25:14, Epoch: 43, Batch: 810, Training Loss: 8.237957954406738e-05, LR: 0.0001
Time, 2019-01-07T02:25:14, Epoch: 43, Batch: 820, Training Loss: 6.105154752731324e-05, LR: 0.0001
Time, 2019-01-07T02:25:15, Epoch: 43, Batch: 830, Training Loss: 7.797777652740479e-05, LR: 0.0001
Time, 2019-01-07T02:25:16, Epoch: 43, Batch: 840, Training Loss: 0.0013879731297492981, LR: 0.0001
Time, 2019-01-07T02:25:16, Epoch: 43, Batch: 850, Training Loss: 8.521378040313721e-05, LR: 0.0001
Time, 2019-01-07T02:25:17, Epoch: 43, Batch: 860, Training Loss: 0.00012057870626449584, LR: 0.0001
Time, 2019-01-07T02:25:18, Epoch: 43, Batch: 870, Training Loss: 7.479190826416015e-05, LR: 0.0001
Time, 2019-01-07T02:25:18, Epoch: 43, Batch: 880, Training Loss: 6.552785634994507e-05, LR: 0.0001
Time, 2019-01-07T02:25:19, Epoch: 43, Batch: 890, Training Loss: 9.33721661567688e-05, LR: 0.0001
Time, 2019-01-07T02:25:20, Epoch: 43, Batch: 900, Training Loss: 8.920282125473023e-05, LR: 0.0001
Time, 2019-01-07T02:25:20, Epoch: 43, Batch: 910, Training Loss: 0.00012499094009399414, LR: 0.0001
Time, 2019-01-07T02:25:21, Epoch: 43, Batch: 920, Training Loss: 7.477402687072754e-05, LR: 0.0001
Time, 2019-01-07T02:25:22, Epoch: 43, Batch: 930, Training Loss: 0.00029474496841430664, LR: 0.0001
Epoch: 43, Validation Top 1 acc: 99.06449127197266
Epoch: 43, Validation Top 5 acc: 100.0
Epoch: 43, Validation Set Loss: 0.04147740826010704
Start training epoch 44
Time, 2019-01-07T02:25:25, Epoch: 44, Batch: 10, Training Loss: 0.00021812915802001953, LR: 0.0001
Time, 2019-01-07T02:25:26, Epoch: 44, Batch: 20, Training Loss: 0.00011676698923110962, LR: 0.0001
Time, 2019-01-07T02:25:27, Epoch: 44, Batch: 30, Training Loss: 8.997172117233276e-05, LR: 0.0001
Time, 2019-01-07T02:25:28, Epoch: 44, Batch: 40, Training Loss: 7.625371217727661e-05, LR: 0.0001
Time, 2019-01-07T02:25:28, Epoch: 44, Batch: 50, Training Loss: 0.00010062158107757569, LR: 0.0001
Time, 2019-01-07T02:25:29, Epoch: 44, Batch: 60, Training Loss: 0.00012144148349761963, LR: 0.0001
Time, 2019-01-07T02:25:30, Epoch: 44, Batch: 70, Training Loss: 0.00010709762573242187, LR: 0.0001
Time, 2019-01-07T02:25:30, Epoch: 44, Batch: 80, Training Loss: 9.789317846298218e-05, LR: 0.0001
Time, 2019-01-07T02:25:31, Epoch: 44, Batch: 90, Training Loss: 3.563612699508667e-05, LR: 0.0001
Time, 2019-01-07T02:25:32, Epoch: 44, Batch: 100, Training Loss: 7.745176553726196e-05, LR: 0.0001
Time, 2019-01-07T02:25:33, Epoch: 44, Batch: 110, Training Loss: 9.309202432632447e-05, LR: 0.0001
Time, 2019-01-07T02:25:33, Epoch: 44, Batch: 120, Training Loss: 9.966343641281128e-05, LR: 0.0001
Time, 2019-01-07T02:25:34, Epoch: 44, Batch: 130, Training Loss: 0.0001222267746925354, LR: 0.0001
Time, 2019-01-07T02:25:35, Epoch: 44, Batch: 140, Training Loss: 7.845461368560791e-05, LR: 0.0001
Time, 2019-01-07T02:25:36, Epoch: 44, Batch: 150, Training Loss: 0.00010505318641662598, LR: 0.0001
Time, 2019-01-07T02:25:36, Epoch: 44, Batch: 160, Training Loss: 9.073913097381591e-05, LR: 0.0001
Time, 2019-01-07T02:25:37, Epoch: 44, Batch: 170, Training Loss: 6.387233734130859e-05, LR: 0.0001
Time, 2019-01-07T02:25:38, Epoch: 44, Batch: 180, Training Loss: 4.709213972091675e-05, LR: 0.0001
Time, 2019-01-07T02:25:39, Epoch: 44, Batch: 190, Training Loss: 4.150271415710449e-05, LR: 0.0001
Time, 2019-01-07T02:25:39, Epoch: 44, Batch: 200, Training Loss: 0.00013507455587387086, LR: 0.0001
Time, 2019-01-07T02:25:40, Epoch: 44, Batch: 210, Training Loss: 8.036792278289794e-05, LR: 0.0001
Time, 2019-01-07T02:25:41, Epoch: 44, Batch: 220, Training Loss: 4.595965147018433e-05, LR: 0.0001
Time, 2019-01-07T02:25:41, Epoch: 44, Batch: 230, Training Loss: 7.513463497161865e-05, LR: 0.0001
Time, 2019-01-07T02:25:42, Epoch: 44, Batch: 240, Training Loss: 0.00012302994728088378, LR: 0.0001
Time, 2019-01-07T02:25:43, Epoch: 44, Batch: 250, Training Loss: 0.00013298094272613526, LR: 0.0001
Time, 2019-01-07T02:25:44, Epoch: 44, Batch: 260, Training Loss: 8.33556056022644e-05, LR: 0.0001
Time, 2019-01-07T02:25:44, Epoch: 44, Batch: 270, Training Loss: 6.459802389144898e-05, LR: 0.0001
Time, 2019-01-07T02:25:45, Epoch: 44, Batch: 280, Training Loss: 3.227740526199341e-05, LR: 0.0001
Time, 2019-01-07T02:25:46, Epoch: 44, Batch: 290, Training Loss: 7.175356149673462e-05, LR: 0.0001
Time, 2019-01-07T02:25:46, Epoch: 44, Batch: 300, Training Loss: 6.525218486785889e-05, LR: 0.0001
Time, 2019-01-07T02:25:47, Epoch: 44, Batch: 310, Training Loss: 6.832629442214966e-05, LR: 0.0001
Time, 2019-01-07T02:25:48, Epoch: 44, Batch: 320, Training Loss: 7.153600454330444e-05, LR: 0.0001
Time, 2019-01-07T02:25:49, Epoch: 44, Batch: 330, Training Loss: 8.28772783279419e-05, LR: 0.0001
Time, 2019-01-07T02:25:49, Epoch: 44, Batch: 340, Training Loss: 9.856373071670532e-05, LR: 0.0001
Time, 2019-01-07T02:25:50, Epoch: 44, Batch: 350, Training Loss: 3.970116376876831e-05, LR: 0.0001
Time, 2019-01-07T02:25:51, Epoch: 44, Batch: 360, Training Loss: 4.0277838706970215e-05, LR: 0.0001
Time, 2019-01-07T02:25:51, Epoch: 44, Batch: 370, Training Loss: 0.0007354140281677247, LR: 0.0001
Time, 2019-01-07T02:25:52, Epoch: 44, Batch: 380, Training Loss: 5.41001558303833e-05, LR: 0.0001
Time, 2019-01-07T02:25:53, Epoch: 44, Batch: 390, Training Loss: 4.786252975463867e-05, LR: 0.0001
Time, 2019-01-07T02:25:54, Epoch: 44, Batch: 400, Training Loss: 0.00011347532272338867, LR: 0.0001
Time, 2019-01-07T02:25:55, Epoch: 44, Batch: 410, Training Loss: 7.923096418380738e-05, LR: 0.0001
Time, 2019-01-07T02:25:55, Epoch: 44, Batch: 420, Training Loss: 0.00011971741914749146, LR: 0.0001
Time, 2019-01-07T02:25:56, Epoch: 44, Batch: 430, Training Loss: 0.0001821368932723999, LR: 0.0001
Time, 2019-01-07T02:25:57, Epoch: 44, Batch: 440, Training Loss: 8.482038974761963e-05, LR: 0.0001
Time, 2019-01-07T02:25:57, Epoch: 44, Batch: 450, Training Loss: 0.0001251429319381714, LR: 0.0001
Time, 2019-01-07T02:25:58, Epoch: 44, Batch: 460, Training Loss: 0.00013535022735595704, LR: 0.0001
Time, 2019-01-07T02:25:59, Epoch: 44, Batch: 470, Training Loss: 2.939105033874512e-05, LR: 0.0001
Time, 2019-01-07T02:25:59, Epoch: 44, Batch: 480, Training Loss: 0.0001815110445022583, LR: 0.0001
Time, 2019-01-07T02:26:00, Epoch: 44, Batch: 490, Training Loss: 0.00014107674360275269, LR: 0.0001
Time, 2019-01-07T02:26:01, Epoch: 44, Batch: 500, Training Loss: 5.761533975601196e-05, LR: 0.0001
Time, 2019-01-07T02:26:01, Epoch: 44, Batch: 510, Training Loss: 9.700804948806763e-05, LR: 0.0001
Time, 2019-01-07T02:26:02, Epoch: 44, Batch: 520, Training Loss: 7.19219446182251e-05, LR: 0.0001
Time, 2019-01-07T02:26:03, Epoch: 44, Batch: 530, Training Loss: 7.091164588928223e-05, LR: 0.0001
Time, 2019-01-07T02:26:04, Epoch: 44, Batch: 540, Training Loss: 5.970597267150879e-05, LR: 0.0001
Time, 2019-01-07T02:26:04, Epoch: 44, Batch: 550, Training Loss: 7.429718971252441e-05, LR: 0.0001
Time, 2019-01-07T02:26:05, Epoch: 44, Batch: 560, Training Loss: 0.00014194995164871216, LR: 0.0001
Time, 2019-01-07T02:26:05, Epoch: 44, Batch: 570, Training Loss: 0.00010286122560501098, LR: 0.0001
Time, 2019-01-07T02:26:06, Epoch: 44, Batch: 580, Training Loss: 9.360015392303466e-05, LR: 0.0001
Time, 2019-01-07T02:26:07, Epoch: 44, Batch: 590, Training Loss: 9.078830480575561e-05, LR: 0.0001
Time, 2019-01-07T02:26:07, Epoch: 44, Batch: 600, Training Loss: 0.0002652853727340698, LR: 0.0001
Time, 2019-01-07T02:26:08, Epoch: 44, Batch: 610, Training Loss: 3.360062837600708e-05, LR: 0.0001
Time, 2019-01-07T02:26:09, Epoch: 44, Batch: 620, Training Loss: 7.710456848144531e-05, LR: 0.0001
Time, 2019-01-07T02:26:09, Epoch: 44, Batch: 630, Training Loss: 7.458925247192383e-05, LR: 0.0001
Time, 2019-01-07T02:26:10, Epoch: 44, Batch: 640, Training Loss: 0.00014931857585906983, LR: 0.0001
Time, 2019-01-07T02:26:11, Epoch: 44, Batch: 650, Training Loss: 7.37413763999939e-05, LR: 0.0001
Time, 2019-01-07T02:26:11, Epoch: 44, Batch: 660, Training Loss: 6.507188081741332e-05, LR: 0.0001
Time, 2019-01-07T02:26:12, Epoch: 44, Batch: 670, Training Loss: 0.00010004788637161255, LR: 0.0001
Time, 2019-01-07T02:26:13, Epoch: 44, Batch: 680, Training Loss: 6.515979766845703e-05, LR: 0.0001
Time, 2019-01-07T02:26:13, Epoch: 44, Batch: 690, Training Loss: 8.033514022827148e-05, LR: 0.0001
Time, 2019-01-07T02:26:14, Epoch: 44, Batch: 700, Training Loss: 7.895231246948242e-05, LR: 0.0001
Time, 2019-01-07T02:26:15, Epoch: 44, Batch: 710, Training Loss: 8.090734481811524e-05, LR: 0.0001
Time, 2019-01-07T02:26:15, Epoch: 44, Batch: 720, Training Loss: 0.0001462787389755249, LR: 0.0001
Time, 2019-01-07T02:26:16, Epoch: 44, Batch: 730, Training Loss: 7.510632276535034e-05, LR: 0.0001
Time, 2019-01-07T02:26:17, Epoch: 44, Batch: 740, Training Loss: 0.00016516894102096557, LR: 0.0001
Time, 2019-01-07T02:26:17, Epoch: 44, Batch: 750, Training Loss: 5.5067241191864014e-05, LR: 0.0001
Time, 2019-01-07T02:26:18, Epoch: 44, Batch: 760, Training Loss: 6.173402070999145e-05, LR: 0.0001
Time, 2019-01-07T02:26:19, Epoch: 44, Batch: 770, Training Loss: 8.965134620666503e-05, LR: 0.0001
Time, 2019-01-07T02:26:20, Epoch: 44, Batch: 780, Training Loss: 5.17427921295166e-05, LR: 0.0001
Time, 2019-01-07T02:26:20, Epoch: 44, Batch: 790, Training Loss: 0.00026290565729141234, LR: 0.0001
Time, 2019-01-07T02:26:21, Epoch: 44, Batch: 800, Training Loss: 0.00010526180267333984, LR: 0.0001
Time, 2019-01-07T02:26:22, Epoch: 44, Batch: 810, Training Loss: 4.5229494571685794e-05, LR: 0.0001
Time, 2019-01-07T02:26:22, Epoch: 44, Batch: 820, Training Loss: 6.764978170394897e-05, LR: 0.0001
Time, 2019-01-07T02:26:23, Epoch: 44, Batch: 830, Training Loss: 4.873424768447876e-05, LR: 0.0001
Time, 2019-01-07T02:26:24, Epoch: 44, Batch: 840, Training Loss: 6.0041248798370364e-05, LR: 0.0001
Time, 2019-01-07T02:26:24, Epoch: 44, Batch: 850, Training Loss: 5.2507221698760984e-05, LR: 0.0001
Time, 2019-01-07T02:26:25, Epoch: 44, Batch: 860, Training Loss: 4.228502511978149e-05, LR: 0.0001
Time, 2019-01-07T02:26:26, Epoch: 44, Batch: 870, Training Loss: 0.00015473514795303345, LR: 0.0001
Time, 2019-01-07T02:26:27, Epoch: 44, Batch: 880, Training Loss: 8.139908313751221e-05, LR: 0.0001
Time, 2019-01-07T02:26:28, Epoch: 44, Batch: 890, Training Loss: 4.419237375259399e-05, LR: 0.0001
Time, 2019-01-07T02:26:28, Epoch: 44, Batch: 900, Training Loss: 7.161498069763184e-05, LR: 0.0001
Time, 2019-01-07T02:26:29, Epoch: 44, Batch: 910, Training Loss: 5.890578031539917e-05, LR: 0.0001
Time, 2019-01-07T02:26:30, Epoch: 44, Batch: 920, Training Loss: 0.00011310428380966187, LR: 0.0001
Time, 2019-01-07T02:26:30, Epoch: 44, Batch: 930, Training Loss: 5.7044625282287595e-05, LR: 0.0001
Epoch: 44, Validation Top 1 acc: 99.05453491210938
Epoch: 44, Validation Top 5 acc: 100.0
Epoch: 44, Validation Set Loss: 0.04282775893807411
Start training epoch 45
Time, 2019-01-07T02:26:34, Epoch: 45, Batch: 10, Training Loss: 4.057586193084717e-05, LR: 0.0001
Time, 2019-01-07T02:26:35, Epoch: 45, Batch: 20, Training Loss: 7.691085338592529e-05, LR: 0.0001
Time, 2019-01-07T02:26:36, Epoch: 45, Batch: 30, Training Loss: 6.670504808425903e-05, LR: 0.0001
Time, 2019-01-07T02:26:36, Epoch: 45, Batch: 40, Training Loss: 6.758719682693481e-05, LR: 0.0001
Time, 2019-01-07T02:26:37, Epoch: 45, Batch: 50, Training Loss: 7.470250129699707e-05, LR: 0.0001
Time, 2019-01-07T02:26:38, Epoch: 45, Batch: 60, Training Loss: 0.00017273873090744017, LR: 0.0001
Time, 2019-01-07T02:26:38, Epoch: 45, Batch: 70, Training Loss: 6.178319454193115e-05, LR: 0.0001
Time, 2019-01-07T02:26:39, Epoch: 45, Batch: 80, Training Loss: 5.366504192352295e-05, LR: 0.0001
Time, 2019-01-07T02:26:40, Epoch: 45, Batch: 90, Training Loss: 6.76080584526062e-05, LR: 0.0001
Time, 2019-01-07T02:26:41, Epoch: 45, Batch: 100, Training Loss: 5.6290626525878905e-05, LR: 0.0001
Time, 2019-01-07T02:26:41, Epoch: 45, Batch: 110, Training Loss: 8.266419172286987e-05, LR: 0.0001
Time, 2019-01-07T02:26:42, Epoch: 45, Batch: 120, Training Loss: 9.268075227737426e-05, LR: 0.0001
Time, 2019-01-07T02:26:43, Epoch: 45, Batch: 130, Training Loss: 3.3515691757202146e-05, LR: 0.0001
Time, 2019-01-07T02:26:43, Epoch: 45, Batch: 140, Training Loss: 0.0001336723566055298, LR: 0.0001
Time, 2019-01-07T02:26:44, Epoch: 45, Batch: 150, Training Loss: 5.112737417221069e-05, LR: 0.0001
Time, 2019-01-07T02:26:45, Epoch: 45, Batch: 160, Training Loss: 3.678053617477417e-05, LR: 0.0001
Time, 2019-01-07T02:26:46, Epoch: 45, Batch: 170, Training Loss: 8.885562419891357e-05, LR: 0.0001
Time, 2019-01-07T02:26:46, Epoch: 45, Batch: 180, Training Loss: 0.00013662725687026977, LR: 0.0001
Time, 2019-01-07T02:26:47, Epoch: 45, Batch: 190, Training Loss: 7.89552927017212e-05, LR: 0.0001
Time, 2019-01-07T02:26:48, Epoch: 45, Batch: 200, Training Loss: 6.617158651351928e-05, LR: 0.0001
Time, 2019-01-07T02:26:49, Epoch: 45, Batch: 210, Training Loss: 6.587356328964233e-05, LR: 0.0001
Time, 2019-01-07T02:26:49, Epoch: 45, Batch: 220, Training Loss: 5.495697259902954e-05, LR: 0.0001
Time, 2019-01-07T02:26:50, Epoch: 45, Batch: 230, Training Loss: 7.454454898834229e-05, LR: 0.0001
Time, 2019-01-07T02:26:51, Epoch: 45, Batch: 240, Training Loss: 5.8791041374206544e-05, LR: 0.0001
Time, 2019-01-07T02:26:51, Epoch: 45, Batch: 250, Training Loss: 4.981309175491333e-05, LR: 0.0001
Time, 2019-01-07T02:26:52, Epoch: 45, Batch: 260, Training Loss: 7.815062999725342e-05, LR: 0.0001
Time, 2019-01-07T02:26:53, Epoch: 45, Batch: 270, Training Loss: 0.00016179531812667846, LR: 0.0001
Time, 2019-01-07T02:26:53, Epoch: 45, Batch: 280, Training Loss: 4.928410053253174e-05, LR: 0.0001
Time, 2019-01-07T02:26:54, Epoch: 45, Batch: 290, Training Loss: 4.274994134902954e-05, LR: 0.0001
Time, 2019-01-07T02:26:55, Epoch: 45, Batch: 300, Training Loss: 5.598515272140503e-05, LR: 0.0001
Time, 2019-01-07T02:26:56, Epoch: 45, Batch: 310, Training Loss: 9.676814079284668e-05, LR: 0.0001
Time, 2019-01-07T02:26:56, Epoch: 45, Batch: 320, Training Loss: 8.780211210250855e-05, LR: 0.0001
Time, 2019-01-07T02:26:57, Epoch: 45, Batch: 330, Training Loss: 7.72327184677124e-05, LR: 0.0001
Time, 2019-01-07T02:26:58, Epoch: 45, Batch: 340, Training Loss: 0.00013973265886306762, LR: 0.0001
Time, 2019-01-07T02:26:59, Epoch: 45, Batch: 350, Training Loss: 2.180933952331543e-05, LR: 0.0001
Time, 2019-01-07T02:26:59, Epoch: 45, Batch: 360, Training Loss: 0.0006708413362503052, LR: 0.0001
Time, 2019-01-07T02:27:00, Epoch: 45, Batch: 370, Training Loss: 4.3283402919769284e-05, LR: 0.0001
Time, 2019-01-07T02:27:01, Epoch: 45, Batch: 380, Training Loss: 5.8510899543762206e-05, LR: 0.0001
Time, 2019-01-07T02:27:02, Epoch: 45, Batch: 390, Training Loss: 7.646381855010986e-05, LR: 0.0001
Time, 2019-01-07T02:27:03, Epoch: 45, Batch: 400, Training Loss: 0.0002001538872718811, LR: 0.0001
Time, 2019-01-07T02:27:03, Epoch: 45, Batch: 410, Training Loss: 0.00010077506303787231, LR: 0.0001
Time, 2019-01-07T02:27:04, Epoch: 45, Batch: 420, Training Loss: 5.169957876205444e-05, LR: 0.0001
Time, 2019-01-07T02:27:05, Epoch: 45, Batch: 430, Training Loss: 0.00013211816549301146, LR: 0.0001
Time, 2019-01-07T02:27:05, Epoch: 45, Batch: 440, Training Loss: 8.599460124969482e-05, LR: 0.0001
Time, 2019-01-07T02:27:06, Epoch: 45, Batch: 450, Training Loss: 0.00011469125747680664, LR: 0.0001
Time, 2019-01-07T02:27:07, Epoch: 45, Batch: 460, Training Loss: 9.452253580093384e-05, LR: 0.0001
Time, 2019-01-07T02:27:08, Epoch: 45, Batch: 470, Training Loss: 8.690059185028076e-05, LR: 0.0001
Time, 2019-01-07T02:27:08, Epoch: 45, Batch: 480, Training Loss: 3.9958953857421874e-05, LR: 0.0001
Time, 2019-01-07T02:27:09, Epoch: 45, Batch: 490, Training Loss: 4.96596097946167e-05, LR: 0.0001
Time, 2019-01-07T02:27:10, Epoch: 45, Batch: 500, Training Loss: 3.785490989685059e-05, LR: 0.0001
Time, 2019-01-07T02:27:11, Epoch: 45, Batch: 510, Training Loss: 5.016624927520752e-05, LR: 0.0001
Time, 2019-01-07T02:27:11, Epoch: 45, Batch: 520, Training Loss: 5.538761615753174e-05, LR: 0.0001
Time, 2019-01-07T02:27:12, Epoch: 45, Batch: 530, Training Loss: 5.0805509090423584e-05, LR: 0.0001
Time, 2019-01-07T02:27:13, Epoch: 45, Batch: 540, Training Loss: 0.00013557523488998412, LR: 0.0001
Time, 2019-01-07T02:27:14, Epoch: 45, Batch: 550, Training Loss: 6.135404109954833e-05, LR: 0.0001
Time, 2019-01-07T02:27:14, Epoch: 45, Batch: 560, Training Loss: 8.765608072280884e-05, LR: 0.0001
Time, 2019-01-07T02:27:15, Epoch: 45, Batch: 570, Training Loss: 0.0001894146203994751, LR: 0.0001
Time, 2019-01-07T02:27:16, Epoch: 45, Batch: 580, Training Loss: 3.694295883178711e-05, LR: 0.0001
Time, 2019-01-07T02:27:17, Epoch: 45, Batch: 590, Training Loss: 5.46574592590332e-05, LR: 0.0001
Time, 2019-01-07T02:27:18, Epoch: 45, Batch: 600, Training Loss: 8.094161748886108e-05, LR: 0.0001
Time, 2019-01-07T02:27:18, Epoch: 45, Batch: 610, Training Loss: 7.684528827667236e-05, LR: 0.0001
Time, 2019-01-07T02:27:19, Epoch: 45, Batch: 620, Training Loss: 8.256137371063233e-05, LR: 0.0001
Time, 2019-01-07T02:27:20, Epoch: 45, Batch: 630, Training Loss: 7.802695035934449e-05, LR: 0.0001
Time, 2019-01-07T02:27:21, Epoch: 45, Batch: 640, Training Loss: 7.816404104232788e-05, LR: 0.0001
Time, 2019-01-07T02:27:22, Epoch: 45, Batch: 650, Training Loss: 3.8936734199523926e-05, LR: 0.0001
Time, 2019-01-07T02:27:22, Epoch: 45, Batch: 660, Training Loss: 5.92350959777832e-05, LR: 0.0001
Time, 2019-01-07T02:27:23, Epoch: 45, Batch: 670, Training Loss: 8.641779422760009e-05, LR: 0.0001
Time, 2019-01-07T02:27:24, Epoch: 45, Batch: 680, Training Loss: 9.628981351852416e-05, LR: 0.0001
Time, 2019-01-07T02:27:24, Epoch: 45, Batch: 690, Training Loss: 4.7653913497924805e-05, LR: 0.0001
Time, 2019-01-07T02:27:25, Epoch: 45, Batch: 700, Training Loss: 4.9163401126861575e-05, LR: 0.0001
Time, 2019-01-07T02:27:26, Epoch: 45, Batch: 710, Training Loss: 6.24537467956543e-05, LR: 0.0001
Time, 2019-01-07T02:27:26, Epoch: 45, Batch: 720, Training Loss: 7.35357403755188e-05, LR: 0.0001
Time, 2019-01-07T02:27:27, Epoch: 45, Batch: 730, Training Loss: 7.541924715042115e-05, LR: 0.0001
Time, 2019-01-07T02:27:28, Epoch: 45, Batch: 740, Training Loss: 0.00010175108909606933, LR: 0.0001
Time, 2019-01-07T02:27:29, Epoch: 45, Batch: 750, Training Loss: 5.229115486145019e-05, LR: 0.0001
Time, 2019-01-07T02:27:29, Epoch: 45, Batch: 760, Training Loss: 8.90687108039856e-05, LR: 0.0001
Time, 2019-01-07T02:27:30, Epoch: 45, Batch: 770, Training Loss: 0.0001671642065048218, LR: 0.0001
Time, 2019-01-07T02:27:31, Epoch: 45, Batch: 780, Training Loss: 7.291138172149658e-05, LR: 0.0001
Time, 2019-01-07T02:27:32, Epoch: 45, Batch: 790, Training Loss: 4.4840574264526364e-05, LR: 0.0001
Time, 2019-01-07T02:27:33, Epoch: 45, Batch: 800, Training Loss: 9.634643793106079e-05, LR: 0.0001
Time, 2019-01-07T02:27:33, Epoch: 45, Batch: 810, Training Loss: 4.466772079467773e-05, LR: 0.0001
Time, 2019-01-07T02:27:34, Epoch: 45, Batch: 820, Training Loss: 9.970515966415406e-05, LR: 0.0001
Time, 2019-01-07T02:27:35, Epoch: 45, Batch: 830, Training Loss: 5.317777395248413e-05, LR: 0.0001
Time, 2019-01-07T02:27:36, Epoch: 45, Batch: 840, Training Loss: 6.231814622879029e-05, LR: 0.0001
Time, 2019-01-07T02:27:37, Epoch: 45, Batch: 850, Training Loss: 5.7616829872131346e-05, LR: 0.0001
Time, 2019-01-07T02:27:37, Epoch: 45, Batch: 860, Training Loss: 3.007054328918457e-05, LR: 0.0001
Time, 2019-01-07T02:27:38, Epoch: 45, Batch: 870, Training Loss: 5.88536262512207e-05, LR: 0.0001
Time, 2019-01-07T02:27:39, Epoch: 45, Batch: 880, Training Loss: 6.183087825775146e-05, LR: 0.0001
Time, 2019-01-07T02:27:40, Epoch: 45, Batch: 890, Training Loss: 4.8883259296417236e-05, LR: 0.0001
Time, 2019-01-07T02:27:40, Epoch: 45, Batch: 900, Training Loss: 4.6452879905700685e-05, LR: 0.0001
Time, 2019-01-07T02:27:41, Epoch: 45, Batch: 910, Training Loss: 5.044639110565186e-05, LR: 0.0001
Time, 2019-01-07T02:27:42, Epoch: 45, Batch: 920, Training Loss: 6.32062554359436e-05, LR: 0.0001
Time, 2019-01-07T02:27:42, Epoch: 45, Batch: 930, Training Loss: 0.0001557379961013794, LR: 0.0001
Epoch: 45, Validation Top 1 acc: 99.03463745117188
Epoch: 45, Validation Top 5 acc: 100.0
Epoch: 45, Validation Set Loss: 0.04339144006371498
Start training epoch 46
Time, 2019-01-07T02:27:46, Epoch: 46, Batch: 10, Training Loss: 3.2110512256622316e-05, LR: 0.0001
Time, 2019-01-07T02:27:47, Epoch: 46, Batch: 20, Training Loss: 0.0004120573401451111, LR: 0.0001
Time, 2019-01-07T02:27:48, Epoch: 46, Batch: 30, Training Loss: 5.006939172744751e-05, LR: 0.0001
Time, 2019-01-07T02:27:49, Epoch: 46, Batch: 40, Training Loss: 4.351288080215454e-05, LR: 0.0001
Time, 2019-01-07T02:27:50, Epoch: 46, Batch: 50, Training Loss: 0.00019831061363220214, LR: 0.0001
Time, 2019-01-07T02:27:50, Epoch: 46, Batch: 60, Training Loss: 6.259530782699585e-05, LR: 0.0001
Time, 2019-01-07T02:27:51, Epoch: 46, Batch: 70, Training Loss: 4.0067732334136964e-05, LR: 0.0001
Time, 2019-01-07T02:27:52, Epoch: 46, Batch: 80, Training Loss: 8.022487163543701e-05, LR: 0.0001
Time, 2019-01-07T02:27:53, Epoch: 46, Batch: 90, Training Loss: 3.865361213684082e-05, LR: 0.0001
Time, 2019-01-07T02:27:53, Epoch: 46, Batch: 100, Training Loss: 3.7638843059539796e-05, LR: 0.0001
Time, 2019-01-07T02:27:54, Epoch: 46, Batch: 110, Training Loss: 3.9367377758026124e-05, LR: 0.0001
Time, 2019-01-07T02:27:55, Epoch: 46, Batch: 120, Training Loss: 4.442334175109863e-05, LR: 0.0001
Time, 2019-01-07T02:27:55, Epoch: 46, Batch: 130, Training Loss: 4.771500825881958e-05, LR: 0.0001
Time, 2019-01-07T02:27:56, Epoch: 46, Batch: 140, Training Loss: 5.387961864471436e-05, LR: 0.0001
Time, 2019-01-07T02:27:57, Epoch: 46, Batch: 150, Training Loss: 3.6436319351196286e-05, LR: 0.0001
Time, 2019-01-07T02:27:57, Epoch: 46, Batch: 160, Training Loss: 3.684014081954956e-05, LR: 0.0001
Time, 2019-01-07T02:27:58, Epoch: 46, Batch: 170, Training Loss: 4.258006811141968e-05, LR: 0.0001
Time, 2019-01-07T02:27:59, Epoch: 46, Batch: 180, Training Loss: 7.159411907196045e-05, LR: 0.0001
Time, 2019-01-07T02:27:59, Epoch: 46, Batch: 190, Training Loss: 5.4648518562316896e-05, LR: 0.0001
Time, 2019-01-07T02:28:00, Epoch: 46, Batch: 200, Training Loss: 7.496029138565063e-05, LR: 0.0001
Time, 2019-01-07T02:28:01, Epoch: 46, Batch: 210, Training Loss: 4.6302378177642825e-05, LR: 0.0001
Time, 2019-01-07T02:28:01, Epoch: 46, Batch: 220, Training Loss: 4.548877477645874e-05, LR: 0.0001
Time, 2019-01-07T02:28:02, Epoch: 46, Batch: 230, Training Loss: 7.760077714920044e-05, LR: 0.0001
Time, 2019-01-07T02:28:03, Epoch: 46, Batch: 240, Training Loss: 9.47132706642151e-05, LR: 0.0001
Time, 2019-01-07T02:28:03, Epoch: 46, Batch: 250, Training Loss: 3.545284271240234e-05, LR: 0.0001
Time, 2019-01-07T02:28:04, Epoch: 46, Batch: 260, Training Loss: 0.00017834901809692382, LR: 0.0001
Time, 2019-01-07T02:28:05, Epoch: 46, Batch: 270, Training Loss: 0.00014076679944992067, LR: 0.0001
Time, 2019-01-07T02:28:05, Epoch: 46, Batch: 280, Training Loss: 3.5752356052398684e-05, LR: 0.0001
Time, 2019-01-07T02:28:06, Epoch: 46, Batch: 290, Training Loss: 4.104524850845337e-05, LR: 0.0001
Time, 2019-01-07T02:28:07, Epoch: 46, Batch: 300, Training Loss: 3.945678472518921e-05, LR: 0.0001
Time, 2019-01-07T02:28:07, Epoch: 46, Batch: 310, Training Loss: 5.2282214164733885e-05, LR: 0.0001
Time, 2019-01-07T02:28:08, Epoch: 46, Batch: 320, Training Loss: 6.257444620132447e-05, LR: 0.0001
Time, 2019-01-07T02:28:09, Epoch: 46, Batch: 330, Training Loss: 9.378194808959961e-05, LR: 0.0001
Time, 2019-01-07T02:28:09, Epoch: 46, Batch: 340, Training Loss: 3.938078880310059e-05, LR: 0.0001
Time, 2019-01-07T02:28:10, Epoch: 46, Batch: 350, Training Loss: 5.824565887451172e-05, LR: 0.0001
Time, 2019-01-07T02:28:11, Epoch: 46, Batch: 360, Training Loss: 4.030168056488037e-05, LR: 0.0001
Time, 2019-01-07T02:28:11, Epoch: 46, Batch: 370, Training Loss: 4.221796989440918e-05, LR: 0.0001
Time, 2019-01-07T02:28:12, Epoch: 46, Batch: 380, Training Loss: 0.00010918676853179931, LR: 0.0001
Time, 2019-01-07T02:28:13, Epoch: 46, Batch: 390, Training Loss: 5.903095006942749e-05, LR: 0.0001
Time, 2019-01-07T02:28:14, Epoch: 46, Batch: 400, Training Loss: 2.699047327041626e-05, LR: 0.0001
Time, 2019-01-07T02:28:14, Epoch: 46, Batch: 410, Training Loss: 4.2745471000671385e-05, LR: 0.0001
Time, 2019-01-07T02:28:15, Epoch: 46, Batch: 420, Training Loss: 4.345476627349853e-05, LR: 0.0001
Time, 2019-01-07T02:28:16, Epoch: 46, Batch: 430, Training Loss: 6.279498338699341e-05, LR: 0.0001
Time, 2019-01-07T02:28:16, Epoch: 46, Batch: 440, Training Loss: 4.3487548828125e-05, LR: 0.0001
Time, 2019-01-07T02:28:17, Epoch: 46, Batch: 450, Training Loss: 3.13490629196167e-05, LR: 0.0001
Time, 2019-01-07T02:28:18, Epoch: 46, Batch: 460, Training Loss: 9.725689888000489e-05, LR: 0.0001
Time, 2019-01-07T02:28:18, Epoch: 46, Batch: 470, Training Loss: 6.169378757476806e-05, LR: 0.0001
Time, 2019-01-07T02:28:19, Epoch: 46, Batch: 480, Training Loss: 5.8016180992126466e-05, LR: 0.0001
Time, 2019-01-07T02:28:20, Epoch: 46, Batch: 490, Training Loss: 2.6807188987731934e-05, LR: 0.0001
Time, 2019-01-07T02:28:20, Epoch: 46, Batch: 500, Training Loss: 4.145652055740356e-05, LR: 0.0001
Time, 2019-01-07T02:28:21, Epoch: 46, Batch: 510, Training Loss: 3.323256969451904e-05, LR: 0.0001
Time, 2019-01-07T02:28:22, Epoch: 46, Batch: 520, Training Loss: 7.960349321365357e-05, LR: 0.0001
Time, 2019-01-07T02:28:22, Epoch: 46, Batch: 530, Training Loss: 9.09551978111267e-05, LR: 0.0001
Time, 2019-01-07T02:28:23, Epoch: 46, Batch: 540, Training Loss: 9.431391954421997e-05, LR: 0.0001
Time, 2019-01-07T02:28:24, Epoch: 46, Batch: 550, Training Loss: 6.098151206970215e-05, LR: 0.0001
Time, 2019-01-07T02:28:24, Epoch: 46, Batch: 560, Training Loss: 5.134791135787964e-05, LR: 0.0001
Time, 2019-01-07T02:28:25, Epoch: 46, Batch: 570, Training Loss: 6.245821714401245e-05, LR: 0.0001
Time, 2019-01-07T02:28:26, Epoch: 46, Batch: 580, Training Loss: 5.051940679550171e-05, LR: 0.0001
Time, 2019-01-07T02:28:26, Epoch: 46, Batch: 590, Training Loss: 3.345012664794922e-05, LR: 0.0001
Time, 2019-01-07T02:28:27, Epoch: 46, Batch: 600, Training Loss: 2.5050342082977294e-05, LR: 0.0001
Time, 2019-01-07T02:28:28, Epoch: 46, Batch: 610, Training Loss: 5.075335502624512e-05, LR: 0.0001
Time, 2019-01-07T02:28:28, Epoch: 46, Batch: 620, Training Loss: 3.4312903881072995e-05, LR: 0.0001
Time, 2019-01-07T02:28:29, Epoch: 46, Batch: 630, Training Loss: 6.877332925796509e-05, LR: 0.0001
Time, 2019-01-07T02:28:29, Epoch: 46, Batch: 640, Training Loss: 5.240440368652344e-05, LR: 0.0001
Time, 2019-01-07T02:28:30, Epoch: 46, Batch: 650, Training Loss: 6.947815418243409e-05, LR: 0.0001
Time, 2019-01-07T02:28:31, Epoch: 46, Batch: 660, Training Loss: 3.341585397720337e-05, LR: 0.0001
Time, 2019-01-07T02:28:32, Epoch: 46, Batch: 670, Training Loss: 3.765225410461426e-05, LR: 0.0001
Time, 2019-01-07T02:28:32, Epoch: 46, Batch: 680, Training Loss: 5.626380443572998e-05, LR: 0.0001
Time, 2019-01-07T02:28:33, Epoch: 46, Batch: 690, Training Loss: 4.1413307189941404e-05, LR: 0.0001
Time, 2019-01-07T02:28:34, Epoch: 46, Batch: 700, Training Loss: 3.411769866943359e-05, LR: 0.0001
Time, 2019-01-07T02:28:34, Epoch: 46, Batch: 710, Training Loss: 5.962252616882324e-05, LR: 0.0001
Time, 2019-01-07T02:28:35, Epoch: 46, Batch: 720, Training Loss: 5.3894519805908206e-05, LR: 0.0001
Time, 2019-01-07T02:28:35, Epoch: 46, Batch: 730, Training Loss: 3.1371414661407473e-05, LR: 0.0001
Time, 2019-01-07T02:28:36, Epoch: 46, Batch: 740, Training Loss: 5.961954593658447e-05, LR: 0.0001
Time, 2019-01-07T02:28:37, Epoch: 46, Batch: 750, Training Loss: 5.613863468170166e-05, LR: 0.0001
Time, 2019-01-07T02:28:37, Epoch: 46, Batch: 760, Training Loss: 5.462467670440674e-05, LR: 0.0001
Time, 2019-01-07T02:28:38, Epoch: 46, Batch: 770, Training Loss: 8.093416690826415e-05, LR: 0.0001
Time, 2019-01-07T02:28:39, Epoch: 46, Batch: 780, Training Loss: 9.087920188903809e-05, LR: 0.0001
Time, 2019-01-07T02:28:39, Epoch: 46, Batch: 790, Training Loss: 2.215653657913208e-05, LR: 0.0001
Time, 2019-01-07T02:28:40, Epoch: 46, Batch: 800, Training Loss: 8.517801761627197e-05, LR: 0.0001
Time, 2019-01-07T02:28:41, Epoch: 46, Batch: 810, Training Loss: 3.092139959335327e-05, LR: 0.0001
Time, 2019-01-07T02:28:41, Epoch: 46, Batch: 820, Training Loss: 5.341172218322754e-05, LR: 0.0001
Time, 2019-01-07T02:28:42, Epoch: 46, Batch: 830, Training Loss: 4.820525646209717e-05, LR: 0.0001
Time, 2019-01-07T02:28:43, Epoch: 46, Batch: 840, Training Loss: 4.370659589767456e-05, LR: 0.0001
Time, 2019-01-07T02:28:43, Epoch: 46, Batch: 850, Training Loss: 7.130652666091919e-05, LR: 0.0001
Time, 2019-01-07T02:28:44, Epoch: 46, Batch: 860, Training Loss: 6.686151027679443e-05, LR: 0.0001
Time, 2019-01-07T02:28:45, Epoch: 46, Batch: 870, Training Loss: 6.750077009201049e-05, LR: 0.0001
Time, 2019-01-07T02:28:45, Epoch: 46, Batch: 880, Training Loss: 6.681233644485473e-05, LR: 0.0001
Time, 2019-01-07T02:28:46, Epoch: 46, Batch: 890, Training Loss: 5.337893962860107e-05, LR: 0.0001
Time, 2019-01-07T02:28:47, Epoch: 46, Batch: 900, Training Loss: 5.671083927154541e-05, LR: 0.0001
Time, 2019-01-07T02:28:47, Epoch: 46, Batch: 910, Training Loss: 6.49377703666687e-05, LR: 0.0001
Time, 2019-01-07T02:28:48, Epoch: 46, Batch: 920, Training Loss: 5.362480878829956e-05, LR: 0.0001
Time, 2019-01-07T02:28:49, Epoch: 46, Batch: 930, Training Loss: 3.2001733779907225e-05, LR: 0.0001
Epoch: 46, Validation Top 1 acc: 99.06449127197266
Epoch: 46, Validation Top 5 acc: 100.0
Epoch: 46, Validation Set Loss: 0.04381129518151283
Start training epoch 47
Time, 2019-01-07T02:28:52, Epoch: 47, Batch: 10, Training Loss: 5.5369734764099124e-05, LR: 0.0001
Time, 2019-01-07T02:28:52, Epoch: 47, Batch: 20, Training Loss: 3.658682107925415e-05, LR: 0.0001
Time, 2019-01-07T02:28:53, Epoch: 47, Batch: 30, Training Loss: 7.30067491531372e-05, LR: 0.0001
Time, 2019-01-07T02:28:54, Epoch: 47, Batch: 40, Training Loss: 3.4819543361663816e-05, LR: 0.0001
Time, 2019-01-07T02:28:54, Epoch: 47, Batch: 50, Training Loss: 6.81370496749878e-05, LR: 0.0001
Time, 2019-01-07T02:28:55, Epoch: 47, Batch: 60, Training Loss: 6.400644779205322e-05, LR: 0.0001
Time, 2019-01-07T02:28:56, Epoch: 47, Batch: 70, Training Loss: 5.587339401245117e-05, LR: 0.0001
Time, 2019-01-07T02:28:56, Epoch: 47, Batch: 80, Training Loss: 2.721250057220459e-05, LR: 0.0001
Time, 2019-01-07T02:28:57, Epoch: 47, Batch: 90, Training Loss: 5.6171417236328124e-05, LR: 0.0001
Time, 2019-01-07T02:28:58, Epoch: 47, Batch: 100, Training Loss: 8.629858493804931e-05, LR: 0.0001
Time, 2019-01-07T02:28:59, Epoch: 47, Batch: 110, Training Loss: 3.243088722229004e-05, LR: 0.0001
Time, 2019-01-07T02:28:59, Epoch: 47, Batch: 120, Training Loss: 2.557635307312012e-05, LR: 0.0001
Time, 2019-01-07T02:29:00, Epoch: 47, Batch: 130, Training Loss: 3.776401281356812e-05, LR: 0.0001
Time, 2019-01-07T02:29:01, Epoch: 47, Batch: 140, Training Loss: 3.1284987926483154e-05, LR: 0.0001
Time, 2019-01-07T02:29:02, Epoch: 47, Batch: 150, Training Loss: 4.7525763511657716e-05, LR: 0.0001
Time, 2019-01-07T02:29:02, Epoch: 47, Batch: 160, Training Loss: 4.3435394763946535e-05, LR: 0.0001
Time, 2019-01-07T02:29:03, Epoch: 47, Batch: 170, Training Loss: 2.9835104942321778e-05, LR: 0.0001
Time, 2019-01-07T02:29:04, Epoch: 47, Batch: 180, Training Loss: 1.786947250366211e-05, LR: 0.0001
Time, 2019-01-07T02:29:04, Epoch: 47, Batch: 190, Training Loss: 3.137737512588501e-05, LR: 0.0001
Time, 2019-01-07T02:29:05, Epoch: 47, Batch: 200, Training Loss: 3.62783670425415e-05, LR: 0.0001
Time, 2019-01-07T02:29:06, Epoch: 47, Batch: 210, Training Loss: 4.9185752868652346e-05, LR: 0.0001
Time, 2019-01-07T02:29:06, Epoch: 47, Batch: 220, Training Loss: 5.050152540206909e-05, LR: 0.0001
Time, 2019-01-07T02:29:07, Epoch: 47, Batch: 230, Training Loss: 2.182126045227051e-05, LR: 0.0001
Time, 2019-01-07T02:29:08, Epoch: 47, Batch: 240, Training Loss: 3.485381603240967e-05, LR: 0.0001
Time, 2019-01-07T02:29:09, Epoch: 47, Batch: 250, Training Loss: 4.862844944000244e-05, LR: 0.0001
Time, 2019-01-07T02:29:09, Epoch: 47, Batch: 260, Training Loss: 8.068680763244629e-05, LR: 0.0001
Time, 2019-01-07T02:29:10, Epoch: 47, Batch: 270, Training Loss: 3.536045551300049e-05, LR: 0.0001
Time, 2019-01-07T02:29:11, Epoch: 47, Batch: 280, Training Loss: 3.5677850246429446e-05, LR: 0.0001
Time, 2019-01-07T02:29:11, Epoch: 47, Batch: 290, Training Loss: 5.2940845489501956e-05, LR: 0.0001
Time, 2019-01-07T02:29:12, Epoch: 47, Batch: 300, Training Loss: 2.2689998149871827e-05, LR: 0.0001
Time, 2019-01-07T02:29:13, Epoch: 47, Batch: 310, Training Loss: 1.6866624355316163e-05, LR: 0.0001
Time, 2019-01-07T02:29:13, Epoch: 47, Batch: 320, Training Loss: 2.54020094871521e-05, LR: 0.0001
Time, 2019-01-07T02:29:15, Epoch: 47, Batch: 330, Training Loss: 2.5935471057891846e-05, LR: 0.0001
Time, 2019-01-07T02:29:16, Epoch: 47, Batch: 340, Training Loss: 2.724975347518921e-05, LR: 0.0001
Time, 2019-01-07T02:29:16, Epoch: 47, Batch: 350, Training Loss: 2.1383166313171387e-05, LR: 0.0001
Time, 2019-01-07T02:29:17, Epoch: 47, Batch: 360, Training Loss: 6.178170442581177e-05, LR: 0.0001
Time, 2019-01-07T02:29:18, Epoch: 47, Batch: 370, Training Loss: 3.990828990936279e-05, LR: 0.0001
Time, 2019-01-07T02:29:19, Epoch: 47, Batch: 380, Training Loss: 2.3373961448669432e-05, LR: 0.0001
Time, 2019-01-07T02:29:19, Epoch: 47, Batch: 390, Training Loss: 5.4614245891571044e-05, LR: 0.0001
Time, 2019-01-07T02:29:20, Epoch: 47, Batch: 400, Training Loss: 3.732889890670776e-05, LR: 0.0001
Time, 2019-01-07T02:29:21, Epoch: 47, Batch: 410, Training Loss: 2.9425323009490967e-05, LR: 0.0001
Time, 2019-01-07T02:29:22, Epoch: 47, Batch: 420, Training Loss: 5.44130802154541e-05, LR: 0.0001
Time, 2019-01-07T02:29:22, Epoch: 47, Batch: 430, Training Loss: 6.123334169387818e-05, LR: 0.0001
Time, 2019-01-07T02:29:23, Epoch: 47, Batch: 440, Training Loss: 4.1472911834716794e-05, LR: 0.0001
Time, 2019-01-07T02:29:24, Epoch: 47, Batch: 450, Training Loss: 3.50266695022583e-05, LR: 0.0001
Time, 2019-01-07T02:29:24, Epoch: 47, Batch: 460, Training Loss: 4.0321052074432374e-05, LR: 0.0001
Time, 2019-01-07T02:29:25, Epoch: 47, Batch: 470, Training Loss: 6.67288899421692e-05, LR: 0.0001
Time, 2019-01-07T02:29:26, Epoch: 47, Batch: 480, Training Loss: 5.390793085098266e-05, LR: 0.0001
Time, 2019-01-07T02:29:27, Epoch: 47, Batch: 490, Training Loss: 4.788488149642944e-05, LR: 0.0001
Time, 2019-01-07T02:29:27, Epoch: 47, Batch: 500, Training Loss: 3.1781196594238284e-05, LR: 0.0001
Time, 2019-01-07T02:29:28, Epoch: 47, Batch: 510, Training Loss: 3.867000341415405e-05, LR: 0.0001
Time, 2019-01-07T02:29:29, Epoch: 47, Batch: 520, Training Loss: 4.870295524597168e-05, LR: 0.0001
Time, 2019-01-07T02:29:30, Epoch: 47, Batch: 530, Training Loss: 4.7351419925689695e-05, LR: 0.0001
Time, 2019-01-07T02:29:30, Epoch: 47, Batch: 540, Training Loss: 4.9921870231628415e-05, LR: 0.0001
Time, 2019-01-07T02:29:31, Epoch: 47, Batch: 550, Training Loss: 4.0309131145477294e-05, LR: 0.0001
Time, 2019-01-07T02:29:32, Epoch: 47, Batch: 560, Training Loss: 0.00015952587127685547, LR: 0.0001
Time, 2019-01-07T02:29:32, Epoch: 47, Batch: 570, Training Loss: 2.6629865169525145e-05, LR: 0.0001
Time, 2019-01-07T02:29:33, Epoch: 47, Batch: 580, Training Loss: 3.1004846096038816e-05, LR: 0.0001
Time, 2019-01-07T02:29:34, Epoch: 47, Batch: 590, Training Loss: 5.024820566177368e-05, LR: 0.0001
Time, 2019-01-07T02:29:35, Epoch: 47, Batch: 600, Training Loss: 2.9636919498443605e-05, LR: 0.0001
Time, 2019-01-07T02:29:36, Epoch: 47, Batch: 610, Training Loss: 6.306618452072144e-05, LR: 0.0001
Time, 2019-01-07T02:29:36, Epoch: 47, Batch: 620, Training Loss: 1.3342499732971191e-05, LR: 0.0001
Time, 2019-01-07T02:29:37, Epoch: 47, Batch: 630, Training Loss: 3.6457180976867674e-05, LR: 0.0001
Time, 2019-01-07T02:29:38, Epoch: 47, Batch: 640, Training Loss: 5.112141370773315e-05, LR: 0.0001
Time, 2019-01-07T02:29:38, Epoch: 47, Batch: 650, Training Loss: 2.6813149452209474e-05, LR: 0.0001
Time, 2019-01-07T02:29:39, Epoch: 47, Batch: 660, Training Loss: 3.732442855834961e-05, LR: 0.0001
Time, 2019-01-07T02:29:40, Epoch: 47, Batch: 670, Training Loss: 3.402978181838989e-05, LR: 0.0001
Time, 2019-01-07T02:29:40, Epoch: 47, Batch: 680, Training Loss: 4.6630203723907473e-05, LR: 0.0001
Time, 2019-01-07T02:29:41, Epoch: 47, Batch: 690, Training Loss: 0.0005064830183982849, LR: 0.0001
Time, 2019-01-07T02:29:42, Epoch: 47, Batch: 700, Training Loss: 2.4047493934631347e-05, LR: 0.0001
Time, 2019-01-07T02:29:43, Epoch: 47, Batch: 710, Training Loss: 5.861818790435791e-05, LR: 0.0001
Time, 2019-01-07T02:29:44, Epoch: 47, Batch: 720, Training Loss: 3.0751526355743405e-05, LR: 0.0001
Time, 2019-01-07T02:29:44, Epoch: 47, Batch: 730, Training Loss: 4.762113094329834e-05, LR: 0.0001
Time, 2019-01-07T02:29:45, Epoch: 47, Batch: 740, Training Loss: 3.6431849002838136e-05, LR: 0.0001
Time, 2019-01-07T02:29:46, Epoch: 47, Batch: 750, Training Loss: 6.92710280418396e-05, LR: 0.0001
Time, 2019-01-07T02:29:46, Epoch: 47, Batch: 760, Training Loss: 3.131479024887085e-05, LR: 0.0001
Time, 2019-01-07T02:29:47, Epoch: 47, Batch: 770, Training Loss: 5.198270082473755e-05, LR: 0.0001
Time, 2019-01-07T02:29:48, Epoch: 47, Batch: 780, Training Loss: 3.142207860946655e-05, LR: 0.0001
Time, 2019-01-07T02:29:48, Epoch: 47, Batch: 790, Training Loss: 4.8677623271942136e-05, LR: 0.0001
Time, 2019-01-07T02:29:49, Epoch: 47, Batch: 800, Training Loss: 6.513893604278564e-05, LR: 0.0001
Time, 2019-01-07T02:29:50, Epoch: 47, Batch: 810, Training Loss: 2.5653839111328124e-05, LR: 0.0001
Time, 2019-01-07T02:29:50, Epoch: 47, Batch: 820, Training Loss: 5.468279123306274e-05, LR: 0.0001
Time, 2019-01-07T02:29:51, Epoch: 47, Batch: 830, Training Loss: 0.000157088041305542, LR: 0.0001
Time, 2019-01-07T02:29:52, Epoch: 47, Batch: 840, Training Loss: 3.173649311065674e-05, LR: 0.0001
Time, 2019-01-07T02:29:52, Epoch: 47, Batch: 850, Training Loss: 3.23072075843811e-05, LR: 0.0001
Time, 2019-01-07T02:29:53, Epoch: 47, Batch: 860, Training Loss: 4.646182060241699e-05, LR: 0.0001
Time, 2019-01-07T02:29:54, Epoch: 47, Batch: 870, Training Loss: 0.00012150555849075318, LR: 0.0001
Time, 2019-01-07T02:29:55, Epoch: 47, Batch: 880, Training Loss: 6.987303495407105e-05, LR: 0.0001
Time, 2019-01-07T02:29:56, Epoch: 47, Batch: 890, Training Loss: 8.303523063659667e-05, LR: 0.0001
Time, 2019-01-07T02:29:56, Epoch: 47, Batch: 900, Training Loss: 2.9055774211883546e-05, LR: 0.0001
Time, 2019-01-07T02:29:57, Epoch: 47, Batch: 910, Training Loss: 5.9238076210021974e-05, LR: 0.0001
Time, 2019-01-07T02:29:58, Epoch: 47, Batch: 920, Training Loss: 3.6722421646118165e-05, LR: 0.0001
Time, 2019-01-07T02:29:59, Epoch: 47, Batch: 930, Training Loss: 7.920861244201661e-05, LR: 0.0001
Epoch: 47, Validation Top 1 acc: 99.03463745117188
Epoch: 47, Validation Top 5 acc: 100.0
Epoch: 47, Validation Set Loss: 0.04516700282692909
Start training epoch 48
Time, 2019-01-07T02:30:03, Epoch: 48, Batch: 10, Training Loss: 2.6270747184753418e-05, LR: 0.0001
Time, 2019-01-07T02:30:03, Epoch: 48, Batch: 20, Training Loss: 2.222508192062378e-05, LR: 0.0001
Time, 2019-01-07T02:30:04, Epoch: 48, Batch: 30, Training Loss: 3.0228495597839354e-05, LR: 0.0001
Time, 2019-01-07T02:30:05, Epoch: 48, Batch: 40, Training Loss: 2.902001142501831e-05, LR: 0.0001
Time, 2019-01-07T02:30:05, Epoch: 48, Batch: 50, Training Loss: 3.058910369873047e-05, LR: 0.0001
Time, 2019-01-07T02:30:06, Epoch: 48, Batch: 60, Training Loss: 6.716698408126831e-05, LR: 0.0001
Time, 2019-01-07T02:30:07, Epoch: 48, Batch: 70, Training Loss: 2.6220083236694335e-05, LR: 0.0001
Time, 2019-01-07T02:30:07, Epoch: 48, Batch: 80, Training Loss: 1.2762844562530518e-05, LR: 0.0001
Time, 2019-01-07T02:30:08, Epoch: 48, Batch: 90, Training Loss: 3.480017185211182e-05, LR: 0.0001
Time, 2019-01-07T02:30:09, Epoch: 48, Batch: 100, Training Loss: 7.135272026062011e-05, LR: 0.0001
Time, 2019-01-07T02:30:09, Epoch: 48, Batch: 110, Training Loss: 7.004886865615844e-05, LR: 0.0001
Time, 2019-01-07T02:30:10, Epoch: 48, Batch: 120, Training Loss: 4.3085217475891115e-05, LR: 0.0001
Time, 2019-01-07T02:30:11, Epoch: 48, Batch: 130, Training Loss: 6.65709376335144e-05, LR: 0.0001
Time, 2019-01-07T02:30:12, Epoch: 48, Batch: 140, Training Loss: 2.435147762298584e-05, LR: 0.0001
Time, 2019-01-07T02:30:13, Epoch: 48, Batch: 150, Training Loss: 3.232657909393311e-05, LR: 0.0001
Time, 2019-01-07T02:30:13, Epoch: 48, Batch: 160, Training Loss: 4.5859813690185546e-05, LR: 0.0001
Time, 2019-01-07T02:30:14, Epoch: 48, Batch: 170, Training Loss: 2.1085143089294434e-05, LR: 0.0001
Time, 2019-01-07T02:30:15, Epoch: 48, Batch: 180, Training Loss: 5.0719082355499265e-05, LR: 0.0001
Time, 2019-01-07T02:30:15, Epoch: 48, Batch: 190, Training Loss: 2.6707351207733154e-05, LR: 0.0001
Time, 2019-01-07T02:30:16, Epoch: 48, Batch: 200, Training Loss: 5.218386650085449e-05, LR: 0.0001
Time, 2019-01-07T02:30:17, Epoch: 48, Batch: 210, Training Loss: 4.590600728988647e-05, LR: 0.0001
Time, 2019-01-07T02:30:17, Epoch: 48, Batch: 220, Training Loss: 2.298802137374878e-05, LR: 0.0001
Time, 2019-01-07T02:30:18, Epoch: 48, Batch: 230, Training Loss: 4.134327173233032e-05, LR: 0.0001
Time, 2019-01-07T02:30:19, Epoch: 48, Batch: 240, Training Loss: 3.7175416946411136e-05, LR: 0.0001
Time, 2019-01-07T02:30:20, Epoch: 48, Batch: 250, Training Loss: 4.639029502868652e-05, LR: 0.0001
Time, 2019-01-07T02:30:21, Epoch: 48, Batch: 260, Training Loss: 3.961622714996338e-05, LR: 0.0001
Time, 2019-01-07T02:30:21, Epoch: 48, Batch: 270, Training Loss: 4.329383373260498e-05, LR: 0.0001
Time, 2019-01-07T02:30:22, Epoch: 48, Batch: 280, Training Loss: 4.1641294956207275e-05, LR: 0.0001
Time, 2019-01-07T02:30:23, Epoch: 48, Batch: 290, Training Loss: 2.3347139358520507e-05, LR: 0.0001
Time, 2019-01-07T02:30:23, Epoch: 48, Batch: 300, Training Loss: 7.958412170410157e-05, LR: 0.0001
Time, 2019-01-07T02:30:24, Epoch: 48, Batch: 310, Training Loss: 2.9183924198150635e-05, LR: 0.0001
Time, 2019-01-07T02:30:25, Epoch: 48, Batch: 320, Training Loss: 3.847479820251465e-05, LR: 0.0001
Time, 2019-01-07T02:30:25, Epoch: 48, Batch: 330, Training Loss: 6.62297010421753e-05, LR: 0.0001
Time, 2019-01-07T02:30:26, Epoch: 48, Batch: 340, Training Loss: 2.7728080749511718e-05, LR: 0.0001
Time, 2019-01-07T02:30:27, Epoch: 48, Batch: 350, Training Loss: 7.047206163406372e-05, LR: 0.0001
Time, 2019-01-07T02:30:27, Epoch: 48, Batch: 360, Training Loss: 2.2229552268981933e-05, LR: 0.0001
Time, 2019-01-07T02:30:28, Epoch: 48, Batch: 370, Training Loss: 3.1490623950958255e-05, LR: 0.0001
Time, 2019-01-07T02:30:29, Epoch: 48, Batch: 380, Training Loss: 4.007667303085327e-05, LR: 0.0001
Time, 2019-01-07T02:30:30, Epoch: 48, Batch: 390, Training Loss: 6.04286789894104e-05, LR: 0.0001
Time, 2019-01-07T02:30:31, Epoch: 48, Batch: 400, Training Loss: 2.543926239013672e-05, LR: 0.0001
Time, 2019-01-07T02:30:31, Epoch: 48, Batch: 410, Training Loss: 2.8595328330993652e-05, LR: 0.0001
Time, 2019-01-07T02:30:32, Epoch: 48, Batch: 420, Training Loss: 3.426969051361084e-05, LR: 0.0001
Time, 2019-01-07T02:30:33, Epoch: 48, Batch: 430, Training Loss: 1.7178058624267577e-05, LR: 0.0001
Time, 2019-01-07T02:30:34, Epoch: 48, Batch: 440, Training Loss: 2.621561288833618e-05, LR: 0.0001
Time, 2019-01-07T02:30:35, Epoch: 48, Batch: 450, Training Loss: 2.813786268234253e-05, LR: 0.0001
Time, 2019-01-07T02:30:35, Epoch: 48, Batch: 460, Training Loss: 2.3759901523590088e-05, LR: 0.0001
Time, 2019-01-07T02:30:36, Epoch: 48, Batch: 470, Training Loss: 0.00013161152601242064, LR: 0.0001
Time, 2019-01-07T02:30:37, Epoch: 48, Batch: 480, Training Loss: 3.702044486999512e-05, LR: 0.0001
Time, 2019-01-07T02:30:37, Epoch: 48, Batch: 490, Training Loss: 6.514787673950195e-05, LR: 0.0001
Time, 2019-01-07T02:30:38, Epoch: 48, Batch: 500, Training Loss: 2.0597875118255615e-05, LR: 0.0001
Time, 2019-01-07T02:30:39, Epoch: 48, Batch: 510, Training Loss: 4.329979419708252e-05, LR: 0.0001
Time, 2019-01-07T02:30:39, Epoch: 48, Batch: 520, Training Loss: 2.2852420806884764e-05, LR: 0.0001
Time, 2019-01-07T02:30:40, Epoch: 48, Batch: 530, Training Loss: 1.3613700866699219e-05, LR: 0.0001
Time, 2019-01-07T02:30:41, Epoch: 48, Batch: 540, Training Loss: 3.6823749542236325e-05, LR: 0.0001
Time, 2019-01-07T02:30:42, Epoch: 48, Batch: 550, Training Loss: 3.690570592880249e-05, LR: 0.0001
Time, 2019-01-07T02:30:42, Epoch: 48, Batch: 560, Training Loss: 4.7321617603302e-05, LR: 0.0001
Time, 2019-01-07T02:30:43, Epoch: 48, Batch: 570, Training Loss: 9.863078594207764e-05, LR: 0.0001
Time, 2019-01-07T02:30:44, Epoch: 48, Batch: 580, Training Loss: 2.2886693477630617e-05, LR: 0.0001
Time, 2019-01-07T02:30:44, Epoch: 48, Batch: 590, Training Loss: 0.0003012537956237793, LR: 0.0001
Time, 2019-01-07T02:30:45, Epoch: 48, Batch: 600, Training Loss: 2.7978420257568358e-05, LR: 0.0001
Time, 2019-01-07T02:30:46, Epoch: 48, Batch: 610, Training Loss: 4.817843437194824e-05, LR: 0.0001
Time, 2019-01-07T02:30:46, Epoch: 48, Batch: 620, Training Loss: 3.1682848930358884e-05, LR: 0.0001
Time, 2019-01-07T02:30:47, Epoch: 48, Batch: 630, Training Loss: 3.754794597625733e-05, LR: 0.0001
Time, 2019-01-07T02:30:48, Epoch: 48, Batch: 640, Training Loss: 1.6264617443084717e-05, LR: 0.0001
Time, 2019-01-07T02:30:48, Epoch: 48, Batch: 650, Training Loss: 2.6832520961761475e-05, LR: 0.0001
Time, 2019-01-07T02:30:49, Epoch: 48, Batch: 660, Training Loss: 3.8930773735046385e-05, LR: 0.0001
Time, 2019-01-07T02:30:49, Epoch: 48, Batch: 670, Training Loss: 3.0428171157836914e-05, LR: 0.0001
Time, 2019-01-07T02:30:50, Epoch: 48, Batch: 680, Training Loss: 6.727278232574463e-05, LR: 0.0001
Time, 2019-01-07T02:30:51, Epoch: 48, Batch: 690, Training Loss: 1.9602477550506592e-05, LR: 0.0001
Time, 2019-01-07T02:30:52, Epoch: 48, Batch: 700, Training Loss: 2.5428831577301025e-05, LR: 0.0001
Time, 2019-01-07T02:30:52, Epoch: 48, Batch: 710, Training Loss: 4.77910041809082e-05, LR: 0.0001
Time, 2019-01-07T02:30:53, Epoch: 48, Batch: 720, Training Loss: 4.897862672805786e-05, LR: 0.0001
Time, 2019-01-07T02:30:54, Epoch: 48, Batch: 730, Training Loss: 3.2924115657806396e-05, LR: 0.0001
Time, 2019-01-07T02:30:54, Epoch: 48, Batch: 740, Training Loss: 6.802529096603394e-05, LR: 0.0001
Time, 2019-01-07T02:30:55, Epoch: 48, Batch: 750, Training Loss: 3.139972686767578e-05, LR: 0.0001
Time, 2019-01-07T02:30:56, Epoch: 48, Batch: 760, Training Loss: 2.23010778427124e-05, LR: 0.0001
Time, 2019-01-07T02:30:56, Epoch: 48, Batch: 770, Training Loss: 3.132522106170654e-05, LR: 0.0001
Time, 2019-01-07T02:30:57, Epoch: 48, Batch: 780, Training Loss: 2.7763843536376954e-05, LR: 0.0001
Time, 2019-01-07T02:30:58, Epoch: 48, Batch: 790, Training Loss: 3.388971090316773e-05, LR: 0.0001
Time, 2019-01-07T02:30:58, Epoch: 48, Batch: 800, Training Loss: 6.104111671447753e-05, LR: 0.0001
Time, 2019-01-07T02:30:59, Epoch: 48, Batch: 810, Training Loss: 1.8770992755889893e-05, LR: 0.0001
Time, 2019-01-07T02:31:00, Epoch: 48, Batch: 820, Training Loss: 4.0459632873535154e-05, LR: 0.0001
Time, 2019-01-07T02:31:01, Epoch: 48, Batch: 830, Training Loss: 4.813522100448609e-05, LR: 0.0001
Time, 2019-01-07T02:31:01, Epoch: 48, Batch: 840, Training Loss: 4.2368471622467044e-05, LR: 0.0001
Time, 2019-01-07T02:31:02, Epoch: 48, Batch: 850, Training Loss: 1.965761184692383e-05, LR: 0.0001
Time, 2019-01-07T02:31:03, Epoch: 48, Batch: 860, Training Loss: 2.476125955581665e-05, LR: 0.0001
Time, 2019-01-07T02:31:03, Epoch: 48, Batch: 870, Training Loss: 1.4100968837738036e-05, LR: 0.0001
Time, 2019-01-07T02:31:04, Epoch: 48, Batch: 880, Training Loss: 5.950480699539185e-05, LR: 0.0001
Time, 2019-01-07T02:31:05, Epoch: 48, Batch: 890, Training Loss: 3.898739814758301e-05, LR: 0.0001
Time, 2019-01-07T02:31:06, Epoch: 48, Batch: 900, Training Loss: 4.4135749340057374e-05, LR: 0.0001
Time, 2019-01-07T02:31:06, Epoch: 48, Batch: 910, Training Loss: 2.0882487297058107e-05, LR: 0.0001
Time, 2019-01-07T02:31:07, Epoch: 48, Batch: 920, Training Loss: 2.7726590633392335e-05, LR: 0.0001
Time, 2019-01-07T02:31:08, Epoch: 48, Batch: 930, Training Loss: 2.25946307182312e-05, LR: 0.0001
Epoch: 48, Validation Top 1 acc: 99.06449127197266
Epoch: 48, Validation Top 5 acc: 100.0
Epoch: 48, Validation Set Loss: 0.04441729187965393
Start training epoch 49
Time, 2019-01-07T02:31:11, Epoch: 49, Batch: 10, Training Loss: 1.5787780284881592e-05, LR: 0.0001
Time, 2019-01-07T02:31:12, Epoch: 49, Batch: 20, Training Loss: 2.7211010456085204e-05, LR: 0.0001
Time, 2019-01-07T02:31:13, Epoch: 49, Batch: 30, Training Loss: 7.355958223342896e-05, LR: 0.0001
Time, 2019-01-07T02:31:13, Epoch: 49, Batch: 40, Training Loss: 4.631727933883667e-05, LR: 0.0001
Time, 2019-01-07T02:31:14, Epoch: 49, Batch: 50, Training Loss: 3.2308697700500486e-05, LR: 0.0001
Time, 2019-01-07T02:31:15, Epoch: 49, Batch: 60, Training Loss: 1.0979175567626952e-05, LR: 0.0001
Time, 2019-01-07T02:31:15, Epoch: 49, Batch: 70, Training Loss: 2.8431415557861328e-05, LR: 0.0001
Time, 2019-01-07T02:31:16, Epoch: 49, Batch: 80, Training Loss: 1.693516969680786e-05, LR: 0.0001
Time, 2019-01-07T02:31:17, Epoch: 49, Batch: 90, Training Loss: 2.072453498840332e-05, LR: 0.0001
Time, 2019-01-07T02:31:17, Epoch: 49, Batch: 100, Training Loss: 3.041774034500122e-05, LR: 0.0001
Time, 2019-01-07T02:31:18, Epoch: 49, Batch: 110, Training Loss: 5.025714635848999e-05, LR: 0.0001
Time, 2019-01-07T02:31:19, Epoch: 49, Batch: 120, Training Loss: 4.142969846725464e-05, LR: 0.0001
Time, 2019-01-07T02:31:20, Epoch: 49, Batch: 130, Training Loss: 1.6351044178009033e-05, LR: 0.0001
Time, 2019-01-07T02:31:20, Epoch: 49, Batch: 140, Training Loss: 2.6425719261169435e-05, LR: 0.0001
Time, 2019-01-07T02:31:21, Epoch: 49, Batch: 150, Training Loss: 1.671314239501953e-05, LR: 0.0001
Time, 2019-01-07T02:31:22, Epoch: 49, Batch: 160, Training Loss: 2.960115671157837e-05, LR: 0.0001
Time, 2019-01-07T02:31:23, Epoch: 49, Batch: 170, Training Loss: 1.578181982040405e-05, LR: 0.0001
Time, 2019-01-07T02:31:23, Epoch: 49, Batch: 180, Training Loss: 5.2562355995178224e-05, LR: 0.0001
Time, 2019-01-07T02:31:24, Epoch: 49, Batch: 190, Training Loss: 1.5754997730255126e-05, LR: 0.0001
Time, 2019-01-07T02:31:25, Epoch: 49, Batch: 200, Training Loss: 1.0478496551513673e-05, LR: 0.0001
Time, 2019-01-07T02:31:26, Epoch: 49, Batch: 210, Training Loss: 1.8903613090515135e-05, LR: 0.0001
Time, 2019-01-07T02:31:26, Epoch: 49, Batch: 220, Training Loss: 2.710968255996704e-05, LR: 0.0001
Time, 2019-01-07T02:31:27, Epoch: 49, Batch: 230, Training Loss: 2.352893352508545e-05, LR: 0.0001
Time, 2019-01-07T02:31:28, Epoch: 49, Batch: 240, Training Loss: 3.5284459590911867e-05, LR: 0.0001
Time, 2019-01-07T02:31:29, Epoch: 49, Batch: 250, Training Loss: 2.973973751068115e-05, LR: 0.0001
Time, 2019-01-07T02:31:29, Epoch: 49, Batch: 260, Training Loss: 2.4370849132537842e-05, LR: 0.0001
Time, 2019-01-07T02:31:30, Epoch: 49, Batch: 270, Training Loss: 3.559887409210205e-05, LR: 0.0001
Time, 2019-01-07T02:31:31, Epoch: 49, Batch: 280, Training Loss: 2.514868974685669e-05, LR: 0.0001
Time, 2019-01-07T02:31:31, Epoch: 49, Batch: 290, Training Loss: 4.7062337398529055e-05, LR: 0.0001
Time, 2019-01-07T02:31:32, Epoch: 49, Batch: 300, Training Loss: 1.678168773651123e-05, LR: 0.0001
Time, 2019-01-07T02:31:33, Epoch: 49, Batch: 310, Training Loss: 3.217756748199463e-05, LR: 0.0001
Time, 2019-01-07T02:31:33, Epoch: 49, Batch: 320, Training Loss: 2.0763278007507326e-05, LR: 0.0001
Time, 2019-01-07T02:31:34, Epoch: 49, Batch: 330, Training Loss: 2.49326229095459e-05, LR: 0.0001
Time, 2019-01-07T02:31:35, Epoch: 49, Batch: 340, Training Loss: 1.5182793140411377e-05, LR: 0.0001
Time, 2019-01-07T02:31:35, Epoch: 49, Batch: 350, Training Loss: 3.5423040390014646e-05, LR: 0.0001
Time, 2019-01-07T02:31:36, Epoch: 49, Batch: 360, Training Loss: 6.281286478042603e-05, LR: 0.0001
Time, 2019-01-07T02:31:37, Epoch: 49, Batch: 370, Training Loss: 3.305673599243164e-05, LR: 0.0001
Time, 2019-01-07T02:31:37, Epoch: 49, Batch: 380, Training Loss: 2.3563206195831298e-05, LR: 0.0001
Time, 2019-01-07T02:31:38, Epoch: 49, Batch: 390, Training Loss: 2.4424493312835692e-05, LR: 0.0001
Time, 2019-01-07T02:31:38, Epoch: 49, Batch: 400, Training Loss: 1.5285611152648925e-05, LR: 0.0001
Time, 2019-01-07T02:31:39, Epoch: 49, Batch: 410, Training Loss: 3.6835670471191406e-05, LR: 0.0001
Time, 2019-01-07T02:31:40, Epoch: 49, Batch: 420, Training Loss: 3.1867623329162596e-05, LR: 0.0001
Time, 2019-01-07T02:31:41, Epoch: 49, Batch: 430, Training Loss: 2.444833517074585e-05, LR: 0.0001
Time, 2019-01-07T02:31:41, Epoch: 49, Batch: 440, Training Loss: 4.6549737453460695e-05, LR: 0.0001
Time, 2019-01-07T02:31:42, Epoch: 49, Batch: 450, Training Loss: 2.0575523376464843e-05, LR: 0.0001
Time, 2019-01-07T02:31:43, Epoch: 49, Batch: 460, Training Loss: 2.0450353622436525e-05, LR: 0.0001
Time, 2019-01-07T02:31:43, Epoch: 49, Batch: 470, Training Loss: 2.5567412376403808e-05, LR: 0.0001
Time, 2019-01-07T02:31:44, Epoch: 49, Batch: 480, Training Loss: 2.2105872631072998e-05, LR: 0.0001
Time, 2019-01-07T02:31:45, Epoch: 49, Batch: 490, Training Loss: 2.2242963314056397e-05, LR: 0.0001
Time, 2019-01-07T02:31:46, Epoch: 49, Batch: 500, Training Loss: 2.7061998844146727e-05, LR: 0.0001
Time, 2019-01-07T02:31:46, Epoch: 49, Batch: 510, Training Loss: 2.3373961448669432e-05, LR: 0.0001
Time, 2019-01-07T02:31:47, Epoch: 49, Batch: 520, Training Loss: 3.3371150493621826e-05, LR: 0.0001
Time, 2019-01-07T02:31:48, Epoch: 49, Batch: 530, Training Loss: 3.3046305179595945e-05, LR: 0.0001
Time, 2019-01-07T02:31:49, Epoch: 49, Batch: 540, Training Loss: 4.058778285980225e-05, LR: 0.0001
Time, 2019-01-07T02:31:49, Epoch: 49, Batch: 550, Training Loss: 4.149377346038818e-05, LR: 0.0001
Time, 2019-01-07T02:31:50, Epoch: 49, Batch: 560, Training Loss: 5.0249695777893064e-05, LR: 0.0001
Time, 2019-01-07T02:31:51, Epoch: 49, Batch: 570, Training Loss: 2.1144747734069824e-05, LR: 0.0001
Time, 2019-01-07T02:31:52, Epoch: 49, Batch: 580, Training Loss: 2.288222312927246e-05, LR: 0.0001
Time, 2019-01-07T02:31:52, Epoch: 49, Batch: 590, Training Loss: 3.767311573028564e-05, LR: 0.0001
Time, 2019-01-07T02:31:53, Epoch: 49, Batch: 600, Training Loss: 2.8152763843536376e-05, LR: 0.0001
Time, 2019-01-07T02:31:54, Epoch: 49, Batch: 610, Training Loss: 0.00017831474542617798, LR: 0.0001
Time, 2019-01-07T02:31:55, Epoch: 49, Batch: 620, Training Loss: 5.25161623954773e-05, LR: 0.0001
Time, 2019-01-07T02:31:56, Epoch: 49, Batch: 630, Training Loss: 1.8021464347839357e-05, LR: 0.0001
Time, 2019-01-07T02:31:56, Epoch: 49, Batch: 640, Training Loss: 4.648268222808838e-05, LR: 0.0001
Time, 2019-01-07T02:31:57, Epoch: 49, Batch: 650, Training Loss: 3.114193677902222e-05, LR: 0.0001
Time, 2019-01-07T02:31:57, Epoch: 49, Batch: 660, Training Loss: 3.2582879066467284e-05, LR: 0.0001
Time, 2019-01-07T02:31:58, Epoch: 49, Batch: 670, Training Loss: 3.053992986679077e-05, LR: 0.0001
Time, 2019-01-07T02:31:59, Epoch: 49, Batch: 680, Training Loss: 4.0753185749053956e-05, LR: 0.0001
Time, 2019-01-07T02:31:59, Epoch: 49, Batch: 690, Training Loss: 3.215670585632324e-05, LR: 0.0001
Time, 2019-01-07T02:32:00, Epoch: 49, Batch: 700, Training Loss: 3.677904605865478e-05, LR: 0.0001
Time, 2019-01-07T02:32:01, Epoch: 49, Batch: 710, Training Loss: 5.622208118438721e-05, LR: 0.0001
Time, 2019-01-07T02:32:01, Epoch: 49, Batch: 720, Training Loss: 1.8624961376190187e-05, LR: 0.0001
Time, 2019-01-07T02:32:02, Epoch: 49, Batch: 730, Training Loss: 2.915412187576294e-05, LR: 0.0001
Time, 2019-01-07T02:32:02, Epoch: 49, Batch: 740, Training Loss: 2.9055774211883546e-05, LR: 0.0001
Time, 2019-01-07T02:32:03, Epoch: 49, Batch: 750, Training Loss: 2.4023652076721192e-05, LR: 0.0001
Time, 2019-01-07T02:32:04, Epoch: 49, Batch: 760, Training Loss: 2.4421513080596925e-05, LR: 0.0001
Time, 2019-01-07T02:32:04, Epoch: 49, Batch: 770, Training Loss: 2.1238625049591064e-05, LR: 0.0001
Time, 2019-01-07T02:32:05, Epoch: 49, Batch: 780, Training Loss: 5.827993154525757e-05, LR: 0.0001
Time, 2019-01-07T02:32:06, Epoch: 49, Batch: 790, Training Loss: 3.0490756034851075e-05, LR: 0.0001
Time, 2019-01-07T02:32:06, Epoch: 49, Batch: 800, Training Loss: 2.785623073577881e-05, LR: 0.0001
Time, 2019-01-07T02:32:07, Epoch: 49, Batch: 810, Training Loss: 2.495497465133667e-05, LR: 0.0001
Time, 2019-01-07T02:32:08, Epoch: 49, Batch: 820, Training Loss: 3.251135349273682e-05, LR: 0.0001
Time, 2019-01-07T02:32:08, Epoch: 49, Batch: 830, Training Loss: 2.771615982055664e-05, LR: 0.0001
Time, 2019-01-07T02:32:09, Epoch: 49, Batch: 840, Training Loss: 4.329681396484375e-05, LR: 0.0001
Time, 2019-01-07T02:32:09, Epoch: 49, Batch: 850, Training Loss: 6.797760725021362e-05, LR: 0.0001
Time, 2019-01-07T02:32:10, Epoch: 49, Batch: 860, Training Loss: 1.6620755195617677e-05, LR: 0.0001
Time, 2019-01-07T02:32:11, Epoch: 49, Batch: 870, Training Loss: 4.6381354331970213e-05, LR: 0.0001
Time, 2019-01-07T02:32:11, Epoch: 49, Batch: 880, Training Loss: 4.239976406097412e-05, LR: 0.0001
Time, 2019-01-07T02:32:12, Epoch: 49, Batch: 890, Training Loss: 3.028959035873413e-05, LR: 0.0001
Time, 2019-01-07T02:32:12, Epoch: 49, Batch: 900, Training Loss: 4.4135749340057374e-05, LR: 0.0001
Time, 2019-01-07T02:32:13, Epoch: 49, Batch: 910, Training Loss: 2.5081634521484376e-05, LR: 0.0001
Time, 2019-01-07T02:32:14, Epoch: 49, Batch: 920, Training Loss: 4.6242773532867434e-05, LR: 0.0001
Time, 2019-01-07T02:32:14, Epoch: 49, Batch: 930, Training Loss: 2.6760995388031007e-05, LR: 0.0001
Epoch: 49, Validation Top 1 acc: 98.99482727050781
Epoch: 49, Validation Top 5 acc: 100.0
Epoch: 49, Validation Set Loss: 0.04483097046613693
Start training epoch 50
Time, 2019-01-07T02:32:18, Epoch: 50, Batch: 10, Training Loss: 1.4269351959228516e-05, LR: 0.0001
Time, 2019-01-07T02:32:18, Epoch: 50, Batch: 20, Training Loss: 1.7127394676208497e-05, LR: 0.0001
Time, 2019-01-07T02:32:19, Epoch: 50, Batch: 30, Training Loss: 2.222806215286255e-05, LR: 0.0001
Time, 2019-01-07T02:32:20, Epoch: 50, Batch: 40, Training Loss: 3.650933504104614e-05, LR: 0.0001
Time, 2019-01-07T02:32:20, Epoch: 50, Batch: 50, Training Loss: 1.503676176071167e-05, LR: 0.0001
Time, 2019-01-07T02:32:21, Epoch: 50, Batch: 60, Training Loss: 6.503015756607056e-05, LR: 0.0001
Time, 2019-01-07T02:32:22, Epoch: 50, Batch: 70, Training Loss: 1.5968084335327147e-05, LR: 0.0001
Time, 2019-01-07T02:32:22, Epoch: 50, Batch: 80, Training Loss: 3.620833158493042e-05, LR: 0.0001
Time, 2019-01-07T02:32:23, Epoch: 50, Batch: 90, Training Loss: 1.95041298866272e-05, LR: 0.0001
Time, 2019-01-07T02:32:24, Epoch: 50, Batch: 100, Training Loss: 2.969801425933838e-05, LR: 0.0001
Time, 2019-01-07T02:32:24, Epoch: 50, Batch: 110, Training Loss: 2.5881826877593995e-05, LR: 0.0001
Time, 2019-01-07T02:32:25, Epoch: 50, Batch: 120, Training Loss: 5.052387714385986e-05, LR: 0.0001
Time, 2019-01-07T02:32:26, Epoch: 50, Batch: 130, Training Loss: 3.366321325302124e-05, LR: 0.0001
Time, 2019-01-07T02:32:26, Epoch: 50, Batch: 140, Training Loss: 1.885741949081421e-05, LR: 0.0001
Time, 2019-01-07T02:32:27, Epoch: 50, Batch: 150, Training Loss: 2.13623046875e-05, LR: 0.0001
Time, 2019-01-07T02:32:28, Epoch: 50, Batch: 160, Training Loss: 1.5322864055633544e-05, LR: 0.0001
Time, 2019-01-07T02:32:28, Epoch: 50, Batch: 170, Training Loss: 3.028959035873413e-05, LR: 0.0001
Time, 2019-01-07T02:32:29, Epoch: 50, Batch: 180, Training Loss: 2.8406083583831786e-05, LR: 0.0001
Time, 2019-01-07T02:32:30, Epoch: 50, Batch: 190, Training Loss: 2.4552643299102784e-05, LR: 0.0001
Time, 2019-01-07T02:32:30, Epoch: 50, Batch: 200, Training Loss: 1.4857947826385498e-05, LR: 0.0001
Time, 2019-01-07T02:32:31, Epoch: 50, Batch: 210, Training Loss: 2.580881118774414e-05, LR: 0.0001
Time, 2019-01-07T02:32:32, Epoch: 50, Batch: 220, Training Loss: 2.643316984176636e-05, LR: 0.0001
Time, 2019-01-07T02:32:32, Epoch: 50, Batch: 230, Training Loss: 1.2613832950592041e-05, LR: 0.0001
Time, 2019-01-07T02:32:33, Epoch: 50, Batch: 240, Training Loss: 2.1417438983917236e-05, LR: 0.0001
Time, 2019-01-07T02:32:34, Epoch: 50, Batch: 250, Training Loss: 2.2976100444793703e-05, LR: 0.0001
Time, 2019-01-07T02:32:34, Epoch: 50, Batch: 260, Training Loss: 2.4774670600891112e-05, LR: 0.0001
Time, 2019-01-07T02:32:35, Epoch: 50, Batch: 270, Training Loss: 1.6106665134429933e-05, LR: 0.0001
Time, 2019-01-07T02:32:36, Epoch: 50, Batch: 280, Training Loss: 2.6766955852508544e-05, LR: 0.0001
Time, 2019-01-07T02:32:37, Epoch: 50, Batch: 290, Training Loss: 2.5385618209838866e-05, LR: 0.0001
Time, 2019-01-07T02:32:37, Epoch: 50, Batch: 300, Training Loss: 3.840923309326172e-05, LR: 0.0001
Time, 2019-01-07T02:32:38, Epoch: 50, Batch: 310, Training Loss: 1.8961727619171143e-05, LR: 0.0001
Time, 2019-01-07T02:32:39, Epoch: 50, Batch: 320, Training Loss: 2.0025670528411867e-05, LR: 0.0001
Time, 2019-01-07T02:32:39, Epoch: 50, Batch: 330, Training Loss: 1.732856035232544e-05, LR: 0.0001
Time, 2019-01-07T02:32:40, Epoch: 50, Batch: 340, Training Loss: 2.6874244213104248e-05, LR: 0.0001
Time, 2019-01-07T02:32:41, Epoch: 50, Batch: 350, Training Loss: 3.241449594497681e-05, LR: 0.0001
Time, 2019-01-07T02:32:42, Epoch: 50, Batch: 360, Training Loss: 3.332048654556274e-05, LR: 0.0001
Time, 2019-01-07T02:32:42, Epoch: 50, Batch: 370, Training Loss: 2.4707615375518797e-05, LR: 0.0001
Time, 2019-01-07T02:32:43, Epoch: 50, Batch: 380, Training Loss: 3.5709142684936525e-05, LR: 0.0001
Time, 2019-01-07T02:32:44, Epoch: 50, Batch: 390, Training Loss: 1.765340566635132e-05, LR: 0.0001
Time, 2019-01-07T02:32:45, Epoch: 50, Batch: 400, Training Loss: 1.7070770263671874e-05, LR: 0.0001
Time, 2019-01-07T02:32:45, Epoch: 50, Batch: 410, Training Loss: 3.144443035125732e-05, LR: 0.0001
Time, 2019-01-07T02:32:46, Epoch: 50, Batch: 420, Training Loss: 1.6339123249053955e-05, LR: 0.0001
Time, 2019-01-07T02:32:47, Epoch: 50, Batch: 430, Training Loss: 1.8203258514404296e-05, LR: 0.0001
Time, 2019-01-07T02:32:48, Epoch: 50, Batch: 440, Training Loss: 2.5989115238189696e-05, LR: 0.0001
Time, 2019-01-07T02:32:49, Epoch: 50, Batch: 450, Training Loss: 1.586228609085083e-05, LR: 0.0001
Time, 2019-01-07T02:32:49, Epoch: 50, Batch: 460, Training Loss: 2.8945505619049072e-05, LR: 0.0001
Time, 2019-01-07T02:32:50, Epoch: 50, Batch: 470, Training Loss: 1.958608627319336e-05, LR: 0.0001
Time, 2019-01-07T02:32:51, Epoch: 50, Batch: 480, Training Loss: 2.8455257415771483e-05, LR: 0.0001
Time, 2019-01-07T02:32:51, Epoch: 50, Batch: 490, Training Loss: 2.9778480529785158e-05, LR: 0.0001
Time, 2019-01-07T02:32:52, Epoch: 50, Batch: 500, Training Loss: 2.2205710411071778e-05, LR: 0.0001
Time, 2019-01-07T02:32:53, Epoch: 50, Batch: 510, Training Loss: 2.5540590286254883e-05, LR: 0.0001
Time, 2019-01-07T02:32:53, Epoch: 50, Batch: 520, Training Loss: 1.821070909500122e-05, LR: 0.0001
Time, 2019-01-07T02:32:54, Epoch: 50, Batch: 530, Training Loss: 1.8812716007232666e-05, LR: 0.0001
Time, 2019-01-07T02:32:55, Epoch: 50, Batch: 540, Training Loss: 1.739710569381714e-05, LR: 0.0001
Time, 2019-01-07T02:32:56, Epoch: 50, Batch: 550, Training Loss: 0.00023198574781417846, LR: 0.0001
Time, 2019-01-07T02:32:56, Epoch: 50, Batch: 560, Training Loss: 3.0563771724700926e-05, LR: 0.0001
Time, 2019-01-07T02:32:57, Epoch: 50, Batch: 570, Training Loss: 3.729164600372314e-05, LR: 0.0001
Time, 2019-01-07T02:32:58, Epoch: 50, Batch: 580, Training Loss: 3.129839897155762e-05, LR: 0.0001
Time, 2019-01-07T02:32:59, Epoch: 50, Batch: 590, Training Loss: 1.5647709369659422e-05, LR: 0.0001
Time, 2019-01-07T02:32:59, Epoch: 50, Batch: 600, Training Loss: 1.4950335025787353e-05, LR: 0.0001
Time, 2019-01-07T02:33:00, Epoch: 50, Batch: 610, Training Loss: 1.939237117767334e-05, LR: 0.0001
Time, 2019-01-07T02:33:01, Epoch: 50, Batch: 620, Training Loss: 2.0416080951690672e-05, LR: 0.0001
Time, 2019-01-07T02:33:02, Epoch: 50, Batch: 630, Training Loss: 2.0481646060943604e-05, LR: 0.0001
Time, 2019-01-07T02:33:02, Epoch: 50, Batch: 640, Training Loss: 4.547387361526489e-05, LR: 0.0001
Time, 2019-01-07T02:33:03, Epoch: 50, Batch: 650, Training Loss: 2.5722384452819825e-05, LR: 0.0001
Time, 2019-01-07T02:33:04, Epoch: 50, Batch: 660, Training Loss: 2.000778913497925e-05, LR: 0.0001
Time, 2019-01-07T02:33:04, Epoch: 50, Batch: 670, Training Loss: 1.963376998901367e-05, LR: 0.0001
Time, 2019-01-07T02:33:05, Epoch: 50, Batch: 680, Training Loss: 3.337264060974121e-05, LR: 0.0001
Time, 2019-01-07T02:33:06, Epoch: 50, Batch: 690, Training Loss: 1.4956295490264893e-05, LR: 0.0001
Time, 2019-01-07T02:33:06, Epoch: 50, Batch: 700, Training Loss: 2.270042896270752e-05, LR: 0.0001
Time, 2019-01-07T02:33:07, Epoch: 50, Batch: 710, Training Loss: 1.9143521785736085e-05, LR: 0.0001
Time, 2019-01-07T02:33:08, Epoch: 50, Batch: 720, Training Loss: 1.2746453285217285e-05, LR: 0.0001
Time, 2019-01-07T02:33:08, Epoch: 50, Batch: 730, Training Loss: 3.043711185455322e-05, LR: 0.0001
Time, 2019-01-07T02:33:09, Epoch: 50, Batch: 740, Training Loss: 3.0292570590972902e-05, LR: 0.0001
Time, 2019-01-07T02:33:10, Epoch: 50, Batch: 750, Training Loss: 2.2432208061218263e-05, LR: 0.0001
Time, 2019-01-07T02:33:11, Epoch: 50, Batch: 760, Training Loss: 1.6710162162780763e-05, LR: 0.0001
Time, 2019-01-07T02:33:12, Epoch: 50, Batch: 770, Training Loss: 1.607239246368408e-05, LR: 0.0001
Time, 2019-01-07T02:33:13, Epoch: 50, Batch: 780, Training Loss: 2.2086501121520997e-05, LR: 0.0001
Time, 2019-01-07T02:33:13, Epoch: 50, Batch: 790, Training Loss: 3.353804349899292e-05, LR: 0.0001
Time, 2019-01-07T02:33:14, Epoch: 50, Batch: 800, Training Loss: 1.8118321895599367e-05, LR: 0.0001
Time, 2019-01-07T02:33:15, Epoch: 50, Batch: 810, Training Loss: 3.526508808135986e-05, LR: 0.0001
Time, 2019-01-07T02:33:16, Epoch: 50, Batch: 820, Training Loss: 1.7599761486053465e-05, LR: 0.0001
Time, 2019-01-07T02:33:17, Epoch: 50, Batch: 830, Training Loss: 1.8705427646636962e-05, LR: 0.0001
Time, 2019-01-07T02:33:18, Epoch: 50, Batch: 840, Training Loss: 3.488212823867798e-05, LR: 0.0001
Time, 2019-01-07T02:33:19, Epoch: 50, Batch: 850, Training Loss: 3.626495599746704e-05, LR: 0.0001
Time, 2019-01-07T02:33:20, Epoch: 50, Batch: 860, Training Loss: 2.1830201148986816e-05, LR: 0.0001
Time, 2019-01-07T02:33:21, Epoch: 50, Batch: 870, Training Loss: 2.8927624225616454e-05, LR: 0.0001
Time, 2019-01-07T02:33:21, Epoch: 50, Batch: 880, Training Loss: 4.179477691650391e-05, LR: 0.0001
Time, 2019-01-07T02:33:22, Epoch: 50, Batch: 890, Training Loss: 2.3265182971954345e-05, LR: 0.0001
Time, 2019-01-07T02:33:23, Epoch: 50, Batch: 900, Training Loss: 1.5041232109069824e-05, LR: 0.0001
Time, 2019-01-07T02:33:24, Epoch: 50, Batch: 910, Training Loss: 1.722574234008789e-05, LR: 0.0001
Time, 2019-01-07T02:33:25, Epoch: 50, Batch: 920, Training Loss: 1.7912685871124266e-05, LR: 0.0001
Time, 2019-01-07T02:33:25, Epoch: 50, Batch: 930, Training Loss: 3.6522746086120605e-05, LR: 0.0001
Epoch: 50, Validation Top 1 acc: 99.0744400024414
Epoch: 50, Validation Top 5 acc: 100.0
Epoch: 50, Validation Set Loss: 0.04550308734178543
